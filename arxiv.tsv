Week	title	category	Paper-Link	Description	Tweet-Link	Other-Links	arxiv_id	authors	abstract	comments	subjects	links	submission_history	references_and_citations
2025-06-29	Mercury: Ultra-Fast Language Models Based on Diffusion	Computation and Language	https://arxiv.org/abs/2506.17298	Ultra-Fast Diffusion-based Language Models  This paper introduces Mercury, a family of large-scale diffusion-based language models (dLLMs) optimized for ultra-fast inference. Unlike standard autoregressive LLMs, Mercury models generate multiple tokens in parallel via a coarse-to-fine refinement process. This approach enables significantly higher throughput without sacrificing output quality. The initial release focuses on code generation, with Mercury Coder Mini and Small models achieving up to 1109 and 737 tokens/sec, respectively, on NVIDIA H100s, outperforming speed-optimized frontier models by up to 10× while matching or exceeding their quality.  <br>● Mercury uses a Transformer-based architecture adapted for diffusion-based generation, enabling it to retain compatibility with existing LLM infrastructure. <br>● On benchmarks such as HumanEval, MBPP, and MultiPL-E, the Mercury Coder models perform competitively with top proprietary models like Claude 3.5 Haiku and Gemini 2.0 Flash Lite, while being drastically faster. <br>● Mercury achieves state-of-the-art results on fill-in-the-middle (FIM) code completion tasks, outperforming all evaluated models, including Codestral 2501 and GPT-4o Mini. <br>● Human evaluations on Copilot Arena show Mercury Coder Mini is tied for second in Elo score and is the fastest model with just 25ms latency.	https://x.com/omarsar0/status/1937600372430045494		2506.17298	['Inception Labs', 'Samar Khanna', 'Siddhant Kharbanda', 'Shufan Li', 'Harshit Varma', 'Eric Wang', 'Sawyer Birnbaum', 'Ziyang Luo', 'Yanis Miraoui', 'Akash Palrecha', 'Stefano Ermon', 'Aditya Grover', 'Volodymyr Kuleshov']	ct:We present Mercury, a new generation of commercial-scale large language models (LLMs) based on diffusion. These models are parameterized via the Transformer architecture and trained to predict multiple tokens in parallel. In this report, we detail Mercury Coder, our first set of diffusion LLMs designed for coding applications. Currently, Mercury Coder comes in two sizes: Mini and Small. These models set a new state-of-the-art on the speed-quality frontier. Based on independent evaluations conducted by Artificial Analysis, Mercury Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109 tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform speed-optimized frontier models by up to 10x on average while maintaining comparable quality. We discuss additional results on a variety of code benchmarks spanning multiple languages and use-cases as well as real-world validation by developers on Copilot Arena, where the model currently ranks second on quality and is the fastest model overall. We also release a public API atthis https URLand free playground atthis https URL	es; equal core, cross-function, senior authors listed alphabetically	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2506.17298', 'html': 'https://arxiv.org/html/2506.17298v1', 'tex': '/src/2506.17298', 'doi': 'https://doi.org/10.48550/arXiv.2506.17298'}	Submission history From: Aditya Grover [ view email ] [v1] Tue, 17 Jun 2025 17:06:18 UTC (60 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.17298'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.17298'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.17298'}]
2025-06-29	MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents	Computation and Language	https://arxiv.org/abs/2506.15841	MEM1  This work introduces MEM1, an RL framework for training language agents that operate efficiently over long-horizon, multi-turn tasks by learning to consolidate memory and reasoning into a compact internal state. Unlike traditional agents that append all past interactions, leading to ballooning memory usage and degraded performance, MEM1 maintains a constant memory size by discarding obsolete context after each reasoning step. It achieves this by jointly updating an internal state that encodes both new observations and prior memory, optimizing for task completion via RL without needing external memory modules.  Key contributions and findings:  <br>● Memory-consolidating internal state: Instead of accumulating thoughts, actions, and observations, MEM1 updates a single shared internal state (<IS>) each turn, discarding the old context. This results in nearly constant memory use regardless of task length. <br>● Reinforcement learning for consolidation: MEM1 is trained end-to-end using PPO-style RL with a novel masked trajectory technique to handle the dynamic context updates. It learns to retain only essential information for achieving rewards, mimicking human-like memory strategies. <br>● Scalable task construction: The authors introduce a method to turn standard single-objective QA datasets (e.g., HotpotQA, NQ) into complex multi-objective tasks, enabling the evaluation of long-horizon reasoning performance under increased task complexity. <br>● Superior efficiency and generalization: MEM1-7B outperforms baselines like Qwen2.5-14B-Instruct in 16-objective multi-hop QA tasks while using 3.7× less memory and 1.78× faster inference. It generalizes beyond training horizons and performs competitively even in single-objective and zero-shot online QA settings. <br>● Emergent agent behaviors: Analysis of internal states shows MEM1 develops structured memory management, selective attention, focus-shifting, verification, and query reformulation strategies, key to handling complex reasoning tasks.	https://x.com/omarsar0/status/1937252072954691813		2506.15841	['Zijian Zhou', 'Ao Qu', 'Zhaoxuan Wu', 'Sunghwan Kim', 'Alok Prakash', 'Daniela Rus', 'Jinhua Zhao', 'Bryan Kian Hsiang Low', 'Paul Pu Liang']	ct:Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2506.15841', 'html': 'https://arxiv.org/html/2506.15841v2', 'tex': '/src/2506.15841', 'doi': 'https://doi.org/10.48550/arXiv.2506.15841'}	Submission history From: Ao Qu [ view email ] [v1] Wed, 18 Jun 2025 19:44:46 UTC (5,049 KB) [v2] Thu, 17 Jul 2025 08:53:48 UTC (5,050 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.15841'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.15841'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.15841'}]
2025-06-29	Towards AI Search Paradigm	Computation and Language	https://arxiv.org/abs/2506.17188	Towards AI Search Paradigm  Proposes a modular multi-agent system that reimagines how AI handles complex search tasks, aiming to emulate human-like reasoning and information synthesis. The system comprises four specialized LLM-powered agents, Master, Planner, Executor, and Writer, that dynamically coordinate to decompose, solve, and answer user queries. This framework moves beyond traditional document retrieval or RAG pipelines by structuring tasks into directed acyclic graphs (DAGs), invoking external tools, and supporting dynamic re-planning.  Key contributions include:  <br>● Multi-agent, modular architecture: The system’s agents each serve distinct roles. Master analyzes queries and orchestrates the workflow; Planner builds a DAG of sub-tasks using a dynamic capability boundary informed by the query; Executor runs these sub-tasks using appropriate tools (e.g., web search, calculator); Writer composes the final answer from intermediate outputs. <br>● Dynamic Capability Boundary & MCP abstraction: To handle tool selection efficiently, the system introduces Model-Context Protocol (MCP) servers and dynamically selects a small, semantically relevant subset of tools. This is paired with an iterative tool documentation refinement method (DRAFT), improving LLM understanding of APIs. <br>● DAG-based task planning and re-action: The Planner produces DAGs of sub-tasks using structured reasoning and tool bindings, enabling multi-step execution. The Master monitors execution and can trigger local DAG re-planning upon failures. <br>● Executor innovations with LLM-preference alignment: The Executor aligns search results with LLM preferences (not just relevance) using RankGPT and TourRank strategies. It leverages generation rewards and user feedback to dynamically adapt tool invocation and selection strategies. <br>● Robust generation with adversarial tuning and alignment: The Writer component is trained to resist noisy retrievals via adversarial tuning (ATM), and to meet RAG requirements via PA-RAG, ensuring informativeness, robustness, and citation quality. The model also supports joint multi-agent	https://x.com/omarsar0/status/1937161765604692400		2506.17188	['Yuchen Li', 'Hengyi Cai', 'Rui Kong', 'Xinran Chen', 'Jiamin Chen', 'Jun Yang', 'Haojie Zhang', 'Jiayi Li', 'Jiayi Wu', 'Yiqun Chen', 'Changle Qu', 'Keyi Kong', 'Wenwen Ye', 'Lixin Su', 'Xinyu Ma', 'Long Xia', 'Daiting Shi', 'Jiashu Zhao', 'Haoyi Xiong', 'Shuaiqiang Wang', 'Dawei Yin']	ct:In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2506.17188', 'html': 'https://arxiv.org/html/2506.17188v1', 'tex': '/src/2506.17188', 'doi': 'https://doi.org/10.48550/arXiv.2506.17188'}	Submission history From: Yuchen Li [ view email ] [v1] Fri, 20 Jun 2025 17:42:13 UTC (3,728 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.17188'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.17188'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.17188'}]
2025-06-29	Reinforcement Learning Teachers of Test Time Scaling	Machine Learning	https://www.arxiv.org/abs/2506.08388	Reinforcement-Learned Teachers of Test Time Scaling  Introduces Reinforcement-Learned Teachers (RLTs), small, efficient LMs trained with RL not to solve problems from scratch, but to generate high-quality explanations that help downstream student models learn better. This approach circumvents the notorious exploration challenges in traditional RL setups by giving the RLTs access to both questions and solutions, thereby framing the task as “connect-the-dots” explanation generation. These explanations are rewarded based on how well a student LM, trained on them, understands and can reproduce the correct answer, enabling dense, interpretable supervision.  Key contributions and findings:  <br>● New teacher-training paradigm: RLTs are trained to explain, not solve. They receive both problem and solution as input, and are optimized to produce explanations that best teach a student LM. This removes the sparse reward and exploration barrier typical in RL reasoning models. <br>● Dense RL rewards for teaching: RLTs use two core reward terms: one measuring if the student can reproduce the correct solution given the explanation, and another ensuring the explanation appears logically sound from the student’s perspective. These combined objectives lead to richer, more instructive traces. <br>● Outperforms much larger pipelines: Despite being only 7B in size, RLTs produce raw explanations that outperform distillation pipelines using 32B+ LMs (e.g. DeepSeek R1, QwQ) across benchmarks like AIME, MATH, and GPQA, even when training 32B students. <br>● Generalizes out-of-distribution: RLTs can be transferred zero-shot to new domains (like the countdown arithmetic game), producing distillation datasets that yield better students than direct RL trained with access to task rewards. <br>● Efficient and scalable: Training RLTs is computationally lightweight (125 steps, 1 epoch) and requires no postprocessing or verifiers, making the framework more reproducible and accessible compared to prior RL pipelines.	https://x.com/SakanaAILabs/status/1936965841188425776		2506.08388	['Edoardo Cetin', 'Tianyu Zhao', 'Yujin Tang']	"ct:Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply ""connect-the-dots"" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework."	vailable at:this https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2506.08388', 'html': 'https://arxiv.org/html/2506.08388v2', 'tex': '/src/2506.08388', 'doi': 'https://doi.org/10.48550/arXiv.2506.08388'}	Submission history From: Edoardo Cetin [ view email ] [v1] Tue, 10 Jun 2025 02:53:24 UTC (1,820 KB) [v2] Sun, 22 Jun 2025 10:04:49 UTC (1,820 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.08388'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.08388'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.08388'}]
2025-06-29	An Agentic System for Rare Disease Diagnosis with Traceable Reasoning	Computation and Language	https://arxiv.org/abs/2506.20430	DeepRare  Introduces DeepRare, a modular agentic system powered by LLMs to aid rare disease diagnosis from multimodal clinical inputs (text, HPO terms, VCFs). It generates ranked diagnostic hypotheses with  fully traceable reasoning chains linked to verifiable medical sources, addressing a long-standing need for interpretability in clinical AI.  <br>● DeepRare is built on a 3-tier MCP-inspired architecture: a central LLM-powered host with memory, multiple specialized agent servers for tasks like phenotype extraction and variant prioritization, and access to over 40 tools and web-scale medical sources. <br>● It demonstrates strong performance on 6,401 cases across 8 diverse datasets spanning 2,919 rare diseases, achieving 100% accuracy on 1,013 diseases and Recall@1 of 57.18%, outperforming the next best method (Claude-3.7-Sonnet-thinking) by +23.79% on HPO-only evaluations. <br>● For multimodal inputs (HPO + gene), it achieves 70.60% Recall@1 on 109 whole-exome sequencing cases, outperforming Exomiser (53.20%). Expert review of 180 diagnostic reasoning chains showed 95.4% agreement, validating its medical soundness. <br>● Ablation studies show that DeepRare’s agentic modules, especially self-reflection, similar case retrieval, and web knowledge, substantially improve LLM-only baselines by 28–70% across datasets, independent of which central LLM is used.	https://x.com/omarsar0/status/1938256196626153624		2506.20430	['Weike Zhao', 'Chaoyi Wu', 'Yanjie Fan', 'Xiaoman Zhang', 'Pengcheng Qiu', 'Yuze Sun', 'Xiao Zhou', 'Yanfeng Wang', 'Ya Zhang', 'Yongguo Yu', 'Kun Sun', 'Weidi Xie']	ct:Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web applicationthis http URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2506.20430', 'html': 'https://arxiv.org/html/2506.20430v1', 'tex': '/src/2506.20430', 'doi': 'https://doi.org/10.48550/arXiv.2506.20430'}	Submission history From: Weike Zhao [ view email ] [v1] Wed, 25 Jun 2025 13:42:26 UTC (5,805 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.20430'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.20430'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.20430'}]
2025-06-29	A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures	Cryptography and Security	https://arxiv.org/abs/2506.19676	AI Agent Communication Protocols  This paper presents the first comprehensive survey on security in LLM-driven agent communication, categorizing it into three stages: user-agent interaction, agent-agent communication, and  agent-environment communication. It details protocols, security threats (e.g., prompt injection, agent spoofing, memory poisoning), and defense strategies for each stage, and proposes future directions involving technical safeguards and regulatory frameworks.	https://x.com/omarsar0/status/1938998557354115509		2506.19676	['Dezhang Kong', 'Shi Lin', 'Zhenhua Xu', 'Zhebo Wang', 'Minghao Li', 'Yufeng Li', 'Yilun Zhang', 'Hujin Peng', 'Zeyang Sha', 'Yuyuan Li', 'Changting Lin', 'Xun Wang', 'Xuan Liu', 'Ningyu Zhang', 'Chaochao Chen', 'Muhammad Khurram Khan', 'Meng Han']	ct:In recent years, Large-Language-Model-driven AI agents have exhibited unprecedented intelligence and adaptability, and are rapidly changing human production and life. Nowadays, agents are undergoing a new round of evolution. They no longer act as an isolated island like LLMs. Instead, they start to communicate with diverse external entities, such as other agents and tools, to perform more complex tasks collectively. Under this trend, agent communication is regarded as a foundational pillar of the future AI ecosystem, and many organizations have intensively begun to design related communication protocols (e.g., Anthropic's MCP and Google's A2A) within the recent few months. However, this new field exposes significant security hazards, which can cause severe damage to real-world scenarios. To help researchers quickly figure out this promising topic and benefit the future agent communication development, this paper presents a comprehensive survey of agent communication security. More precisely, we first present a clear definition of agent communication and categorize the entire lifecycle of agent communication into three stages: user-agent interaction, agent-agent communication, and agent-environment communication. Next, for each communication phase, we dissect related protocols and analyze the security risks according to the communication characteristics. Then, we summarize and outlook on the possible defense countermeasures for each risk. In addition, we conduct experiments using MCP and A2A to help readers better understand the novel vulnerabilities brought by agent communication. Finally, we discuss open issues and future directions in this promising research field.	es, 13 figures, submitted to IEEE COMST	['Cryptography and Security (cs.CR)']	{'pdf': '/pdf/2506.19676', 'html': 'https://arxiv.org/html/2506.19676v3', 'tex': '/src/2506.19676', 'doi': 'https://doi.org/10.48550/arXiv.2506.19676'}	Submission history From: Dezhang Kong [ view email ] [v1] Tue, 24 Jun 2025 14:44:28 UTC (670 KB) [v2] Tue, 1 Jul 2025 09:47:45 UTC (2,588 KB) [v3] Wed, 2 Jul 2025 08:50:11 UTC (2,588 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.19676'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.19676'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.19676'}]
2025-06-29	Steering Your Diffusion Policy with Latent Space Reinforcement Learning	Robotics	https://arxiv.org/abs/2506.15799	Diffusion Steering via RL  This paper introduces Diffusion Steering via Reinforcement Learning (DSRL), a method for adapting pretrained diffusion policies by learning in their latent-noise space instead of finetuning model weights. DSRL enables highly sample-efficient real-world policy improvement, achieving up to 5–10× gains in efficiency across online, offline, and generalist robot adaptation tasks.	https://x.com/svlevine/status/1938101714361766023		2506.15799	['Andrew Wagenmaker', 'Mitsuhiko Nakamoto', 'Yunchu Zhang', 'Seohong Park', 'Waleed Yagoub', 'Anusha Nagabandi', 'Abhishek Gupta', 'Sergey Levine']	ct:Robotic control policies learned from human demonstrations have achieved impressive results in many real-world applications. However, in scenarios where initial performance is not satisfactory, as is often the case in novel open-world settings, such behavioral cloning (BC)-learned policies typically require collecting additional human demonstrations to further improve their behavior -- an expensive and time-consuming process. In contrast, reinforcement learning (RL) holds the promise of enabling autonomous online policy improvement, but often falls short of achieving this due to the large number of samples it typically requires. In this work we take steps towards enabling fast autonomous adaptation of BC-trained policies via efficient real-world RL. Focusing in particular on diffusion policies -- a state-of-the-art BC methodology -- we propose diffusion steering via reinforcement learning (DSRL): adapting the BC policy by running RL over its latent-noise space. We show that DSRL is highly sample efficient, requires only black-box access to the BC policy, and enables effective real-world autonomous policy improvement. Furthermore, DSRL avoids many of the challenges associated with finetuning diffusion policies, obviating the need to modify the weights of the base policy at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks, and for adapting pretrained generalist policies, illustrating its sample efficiency and effective performance at real-world policy improvement.		['Robotics (cs.RO)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2506.15799', 'html': 'https://arxiv.org/html/2506.15799v2', 'tex': '/src/2506.15799', 'doi': 'https://doi.org/10.48550/arXiv.2506.15799'}	Submission history From: Andrew Wagenmaker [ view email ] [v1] Wed, 18 Jun 2025 18:35:57 UTC (38,843 KB) [v2] Wed, 25 Jun 2025 19:09:52 UTC (38,843 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.15799'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.15799'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.15799'}]
2025-06-29	Whole-Body Conditioned Egocentric Video Prediction	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2506.21552	Whole-Body Conditioned Egocentric Video Prediction  This paper introduces PEVA, a conditional diffusion transformer that predicts egocentric video conditioned on 3D human body motion. Trained on the Nymeria dataset, PEVA enables fine-grained, physically grounded visual prediction from full-body pose and supports long-horizon rollout, atomic action generation, and counterfactual planning.	https://x.com/YutongBAI1002/status/1938442251866411281		2506.21552	['Yutong Bai', 'Danny Tran', 'Amir Bar', 'Yann LeCun', 'Trevor Darrell', 'Jitendra Malik']	ct:We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human.	t Page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Multimedia (cs.MM)', 'Robotics (cs.RO)']	{'pdf': '/pdf/2506.21552', 'html': 'https://arxiv.org/html/2506.21552v1', 'tex': '/src/2506.21552', 'doi': 'https://doi.org/10.48550/arXiv.2506.21552'}	Submission history From: Yutong Bai [ view email ] [v1] Thu, 26 Jun 2025 17:59:59 UTC (44,274 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.21552'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.21552'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.21552'}]
2025-06-22	RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning	Artificial Intelligence	https://arxiv.org/abs/2506.11555	RAG+  Introduces RAG+, a modular framework that improves traditional RAG systems by explicitly incorporating application-level reasoning into the retrieval and generation pipeline. While standard RAG pipelines fetch relevant knowledge, they often fail to show how to use that knowledge effectively in reasoning-intensive tasks. RAG+ fills this gap by retrieving not only knowledge but also paired application examples, leading to more accurate, interpretable, and goal-oriented outputs.  Key highlights:  <br>● Dual corpus retrieval: RAG+ constructs two aligned corpora: one of factual knowledge and another of task-specific applications (e.g., step-by-step reasoning traces or worked examples). During inference, both are jointly retrieved, providing the LLM with explicit procedural guidance rather than relying solely on semantic similarity. <br>● Plug-and-play design: The system is retrieval-agnostic and model-agnostic—no fine-tuning or architectural changes are required. This makes it easy to augment any RAG system with application-awareness. <br>● Significant gains across domains: Evaluated on MathQA, MedQA, and legal sentencing prediction, RAG+ outperforms vanilla RAG variants by 2.5–7.5% on average, with peak gains of up to 10% for large models like Qwen2.5-72B in legal reasoning.. <br>● Stronger with scale and reranking: Larger models benefit more from RAG+ augmentation, especially when combined with reranking via stronger LLMs. For example, reranking with Qwen2.5-72B boosted smaller models' performance by up to 7%. <br>● Application-only helps, but full combo is best: Including only application examples (without knowledge) still improves performance, but the full combination (RAG+) consistently yields the best results, demonstrating the synergistic effect of pairing knowledge with its usage.	https://x.com/omarsar0/status/1934667096828399641		2506.11555	['Yu Wang', 'Shiwan Zhao', 'Zhihu Wang', 'Ming Fan', 'Yubo Zhang', 'Xicheng Zhang', 'Zhengfan Wang', 'Heyuan Huang', 'Ting Liu']	ct:The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2506.11555', 'html': 'https://arxiv.org/html/2506.11555v3', 'tex': '/src/2506.11555', 'doi': 'https://doi.org/10.48550/arXiv.2506.11555'}	Submission history From: Yu Wang [ view email ] [v1] Fri, 13 Jun 2025 08:06:49 UTC (7,675 KB) [v2] Tue, 24 Jun 2025 05:50:06 UTC (7,675 KB) [v3] Fri, 4 Jul 2025 14:43:14 UTC (7,675 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.11555'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.11555'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.11555'}]
2025-06-22	Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce	Computers and Society	https://arxiv.org/abs/2506.06576	Future of Work with AI Agents  Proposes a large-scale framework for understanding where AI agents should automate or augment human labor. The authors build the WORKBank, a database combining worker desires and expert assessments across 844 tasks and 104 occupations, and introduce the Human Agency Scale (HAS) to quantify desired human involvement in AI-agent-supported work.  Key findings:  <br>● Workers support automation for low-value tasks: 46.1% of tasks received positive worker attitudes toward automation, mainly to free up time for higher-value work. Attitudes vary by sector; workers in creative or interpersonal fields (e.g., media, design) resist automation despite technical feasibility. <br>● Desire-capability gaps reveal 4 AI deployment zones: By cross-referencing worker desire and AI expert capability, tasks were sorted into: <br>● Human Agency Scale shows strong preference for collaboration: 45.2% of occupations favor HAS Level 3 (equal human-agent partnership), while workers generally prefer more human involvement than experts find necessary. This divergence may signal future friction if automation outpaces user comfort. <br>● Interpersonal skills are becoming more valuable: While high-wage skills today emphasize information analysis, the tasks requiring the highest human agency increasingly emphasize interpersonal communication, coordination, and emotional intelligence. This suggests a long-term shift in valued workplace competencies.	https://x.com/omarsar0/status/1936134951520682123		2506.06576	['Yijia Shao', 'Humishka Zope', 'Yucheng Jiang', 'Jiaxin Pei', 'David Nguyen', 'Erik Brynjolfsson', 'Diyi Yang']	"ct:The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. Our framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, we construct the WORKBank database, building on the U.S. Department of Labor's O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation ""Green Light"" Zone, Automation ""Red Light"" Zone, R&D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, our study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics."	nt	['Computers and Society (cs.CY)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2506.06576', 'html': 'https://arxiv.org/html/2506.06576v2', 'tex': '/src/2506.06576', 'doi': 'https://doi.org/10.48550/arXiv.2506.06576'}	Submission history From: Yijia Shao [ view email ] [v1] Fri, 6 Jun 2025 23:05:52 UTC (8,302 KB) [v2] Wed, 11 Jun 2025 21:25:21 UTC (8,302 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.06576'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.06576'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.06576'}]
2025-06-22	From Bytes to Ideas: Language Modeling with Autoregressive U-Nets	Computation and Language	https://arxiv.org/abs/2506.14761	From Bytes to Ideas  Proposes AU-Net, a hierarchical byte-level language model that internalizes tokenization by learning to embed text from raw bytes through a multiscale, autoregressive U-Net architecture. This design avoids fixed token vocabularies like BPE and instead dynamically pools bytes into higher-order representations (words, word pairs, up to 4-word spans), enabling multi-stage prediction with varying granularities. Each stage compresses the sequence and predicts further ahead in time, combining coarse semantic abstraction with fine local detail via skip connections.  Key insights:  <br>● Hierarchical architecture: AU-Net processes input in multiple stages, bytes → words → multi-word units, using adaptive pooling and multi-linear upsampling. Deeper stages handle long-range semantics; shallow ones refine local syntax. <br>● Strong performance under budget: On a compute-equal basis (up to 5e21 FLOPs), AU-Net matches or exceeds strong BPE-based Transformers across many tasks. AU-Net 3 and 4 outperform BPE on MMLU and GSM8k while retaining competitive FLOPs and throughput. <br>● Multilingual and low-resource gains: Despite being trained on mostly English, AU-Net shows consistent improvements in BLEU and MMLU on low-resource and Latin-script languages, thanks to its byte-level generalization and vocabulary-agnostic design. <br>● Scaling behavior: AU-Net follows predictable scaling laws. With 2–3 stages, it closely tracks BPE-based baselines even on compute-heavy benchmarks. Gains on math-heavy tasks (like GSM8K) emerge at higher FLOPs, indicating potential unlocked at scale. <br>● Character-level robustness: On the CUTE benchmark, AU-Net outperforms BPE models on character-level tasks (e.g., spelling, substitution), highlighting its strength in symbol-level reasoning and handling unseen morphologies.	https://x.com/omarsar0/status/1935420763722629478		2506.14761	['Mathurin Videau', 'Badr Youbi Idrissi', 'Alessandro Leite', 'Marc Schoenauer', 'Olivier Teytaud', 'David Lopez-Paz']	ct:Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2506.14761', 'html': 'https://arxiv.org/html/2506.14761v1', 'tex': '/src/2506.14761', 'doi': 'https://doi.org/10.48550/arXiv.2506.14761'}	Submission history From: Badr Youbi Idrissi [ view email ] [v1] Tue, 17 Jun 2025 17:55:11 UTC (422 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.14761'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.14761'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.14761'}]
2025-06-22	ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering	Artificial Intelligence	https://arxiv.org/abs/2506.09050	ALE-Agent & ALE-Bench  Proposes a new benchmark for evaluating AI systems in score-based, long-horizon algorithmic contests. Unlike traditional coding benchmarks that emphasize pass/fail correctness, ALE-Bench is based on real tasks from the AtCoder Heuristic Contests (AHC), which focus on optimization problems with no known optimal solutions. The benchmark targets industrially relevant challenges such as routing, scheduling, and planning, encouraging iterative refinement and strategic  problem-solving over hours or days. Key points:  <br>● Realistic, optimization-focused tasks: ALE-Bench collects 40 real AHC problems involving NP-hard optimization tasks across domains like logistics, production planning, and games. These are long-duration contests requiring weeks of iterative improvement, simulating real-world algorithm engineering tasks. <br>● Interactive framework and agent support: The benchmark includes a full software stack with a Python API, code sandbox, scoring engine, and visualizers. It allows AI agents to emulate human workflows, reviewing problem specs, running tests, using visual feedback, and iteratively refining solutions within a timed session. <br>● Rigorous evaluation protocols: Performance is assessed using AtCoder-style Elo-based scoring, with fine-grained per-problem metrics and aggregate metrics like average performance and rating. Emphasis is placed on average performance over rating, as rating can be misleading for AIs that spike on a few problems but underperform elsewhere. <br>● Benchmarking LLMs and agents: Experiments with 22 models, including GPT-4o, Claude 3.7, Gemini 2.5 Pro, and o4-mini-high, show that reasoning models outperform non-reasoning ones. In one-shot settings, top models rarely surpass human expert consistency. However, with iterative refinement, performance increases significantly, particularly for models using scaffolded agents. <br>● ALE-Agent: a specialized scaffolded agent: Designed for ALE-Bench, ALE-Agent incorporates domain-knowledge prompts (e.g., for simulated annealing) and a beam-search-inspired code exploration mechanism. With both strategies, it achieved human-expert-level scores on some problems, e.g., 5th place in a real AHC contest.	https://x.com/SakanaAILabs/status/1934767254715117812		2506.09050	['Yuki Imajuku', 'Kohki Horie', 'Yoichi Iwata', 'Kensho Aoki', 'Naohiro Takahashi', 'Takuya Akiba']	ct:How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, a new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, a notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements.	es	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2506.09050', 'html': None, 'tex': '/src/2506.09050', 'doi': 'https://doi.org/10.48550/arXiv.2506.09050'}	Submission history From: Yuki Imajuku [ view email ] [v1] Tue, 10 Jun 2025 17:59:56 UTC (2,653 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.09050'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.09050'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.09050'}]
2025-06-22	Eliciting Reasoning in Language Models with Cognitive Tools	Computation and Language	https://arxiv.org/abs/2506.12115	"Eliciting Reasoning with Cognitive Tools  Proposes a modular, tool-based approach to eliciting reasoning in LLMs, inspired by cognitive science. Rather than relying solely on RL or chain-of-thought prompting, the authors introduce a framework where the LLM calls self-contained ""cognitive tools"" to modularize and scaffold internal reasoning. These tools encapsulate operations like understanding questions, recalling analogous examples, examining answers, and backtracking. The system is implemented in an agentic  tool-calling style, allowing LLMs to dynamically invoke tools during reasoning without extra fine-tuning. Highlights:  <br>● Cognitive tools as internal modules: Each tool (e.g., understand question, recall related, examine answer, backtracking) is framed as a standalone prompt template that the LLM can invoke as needed. Unlike conventional tool use (e.g., calculator APIs), these tools operate within the LLM’s own architecture and memory. <br>● Consistent performance gains: On math-heavy reasoning benchmarks like AIME 2024, MATH500, and AMC, the cognitive tools pipeline significantly boosts pass@1 accuracy across models, including Qwen2.5, Llama3, and GPT-4.1. For instance, Llama3.3-70B improves from 13.1% to 29.8% on AIME2024, and GPT-4.1 rises from 26.7% to 43.3%, nearly matching the o1-preview RL-trained reasoning model at 44.6%. <br>● Superior to cognitive prompting: Compared to prior work on cognitive prompting, the modular tool approach shows stronger generalization and reduced reasoning interference. Tools can be invoked flexibly, and each invocation operates in a clean context window, boosting accuracy by up to +27.2% over baseline on Smolbenchmark. <br>● Interpretable and transferable: The modular nature of the tools enhances transparency, and their plug-and-play design allows transfer across models and benchmarks with minimal changes. The approach also supports interpretability by surfacing intermediate reasoning steps and decisions."	https://x.com/omarsar0/status/1935070412313973196		2506.12115	['Brown Ebouky', 'Andrea Bartezzaghi', 'Mattia Rigotti']	"ct:The recent advent of reasoning models like OpenAI's o1 was met with excited speculation by the AI community about the mechanisms underlying these capabilities in closed models, followed by a rush of replication efforts, particularly from the open source community. These speculations were largely settled by the demonstration from DeepSeek-R1 that chains-of-thought and reinforcement learning (RL) can effectively replicate reasoning on top of base LLMs. However, it remains valuable to explore alternative methods for theoretically eliciting reasoning that could help elucidate the underlying mechanisms, as well as providing additional methods that may offer complementary benefits.Here, we build on the long-standing literature in cognitive psychology and cognitive architectures, which postulates that reasoning arises from the orchestrated, sequential execution of a set of modular, predetermined cognitive operations. Crucially, we implement this key idea within a modern agentic tool-calling framework. In particular, we endow an LLM with a small set of ""cognitive tools"" encapsulating specific reasoning operations, each executed by the LLM itself. Surprisingly, this simple strategy results in considerable gains in performance on standard mathematical reasoning benchmarks compared to base LLMs, for both closed and open-weight models. For instance, providing our ""cognitive tools"" to GPT-4.1 increases its pass@1 performance on AIME2024 from 26.7% to 43.3%, bringing it very close to the performance of o1-preview.In addition to its practical implications, this demonstration contributes to the debate regarding the role of post-training methods in eliciting reasoning in LLMs versus the role of inherent capabilities acquired during pre-training, and whether post-training merely uncovers these latent abilities."	es, 2 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2506.12115', 'html': None, 'tex': '/src/2506.12115', 'doi': 'https://doi.org/10.48550/arXiv.2506.12115'}	Submission history From: Brown Ebouky [ view email ] [v1] Fri, 13 Jun 2025 13:56:52 UTC (222 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.12115'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.12115'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.12115'}]
2025-06-22	Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers	Computation and Language	https://arxiv.org/abs/2506.15674	Leaky Thoughts  This work explores how reasoning traces in large reasoning models (LRMs) leak private user data, despite being assumed internal and safe. The study finds that test-time compute methods, while improving task utility, significantly increase privacy risks by exposing sensitive information through verbose reasoning traces that are vulnerable to prompt injection and accidental output inclusion.	https://x.com/omarsar0/status/1935711966351470678		2506.15674	['Tommaso Green', 'Martin Gubri', 'Haritz Puerto', 'Sangdoo Yun', 'Seong Joon Oh']	ct:We study privacy leakage in the reasoning traces of large reasoning models used as personal agents. Unlike final outputs, reasoning traces are often assumed to be internal and safe. We challenge this assumption by showing that reasoning traces frequently contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. Through probing and agentic evaluations, we demonstrate that test-time compute approaches, particularly increased reasoning steps, amplify such leakage. While increasing the budget of those test-time compute approaches makes models more cautious in their final answers, it also leads them to reason more verbosely and leak more in their own thinking. This reveals a core tension: reasoning improves utility but enlarges the privacy attack surface. We argue that safety efforts must extend to the model's internal thinking, not just its outputs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Cryptography and Security (cs.CR)']	{'pdf': '/pdf/2506.15674', 'html': 'https://arxiv.org/html/2506.15674v1', 'tex': '/src/2506.15674', 'doi': 'https://doi.org/10.48550/arXiv.2506.15674'}	Submission history From: Tommaso Green [ view email ] [v1] Wed, 18 Jun 2025 17:57:01 UTC (9,586 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.15674'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.15674'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.15674'}]
2025-06-22	Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics	Computation and Language	https://arxiv.org/abs/2506.12365	Advances in LLMs  This paper surveys recent advancements in LLMs focusing on reasoning, adaptability, efficiency, and ethics. It highlights techniques like CoT prompting, Instruction Tuning, RLHF, and multimodal learning, while also addressing challenges like bias, computational cost, and interpretability.	https://x.com/omarsar0/status/1934996216909324336		2506.12365	['Asifullah Khan', 'Muhammad Zaeem Khan', 'Saleha Jamshed', 'Sadia Ahmad', 'Aleesha Zainab', 'Kaynat Khatib', 'Faria Bibi', 'Abdul Rehman']	ct:This survey paper outlines the key developments in the field of Large Language Models (LLMs), including enhancements to their reasoning skills, adaptability to various tasks, increased computational efficiency, and the ability to make ethical decisions. The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback. The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. A significant focus is placed on efficiency, detailing scaling strategies, optimization techniques, and the influential Mixture-of-Experts (MoE) architecture, which strategically routes inputs to specialized subnetworks to boost predictive accuracy, while optimizing resource allocation. This survey also offers a broader perspective on recent advancements in LLMs, going beyond isolated aspects such as model architecture or ethical concerns. Additionally, it explores the role of LLMs in Agentic AI and their use as Autonomous Decision-Making Systems, and categorizes emerging methods that enhance LLM reasoning, efficiency, and ethical alignment. The survey also identifies underexplored areas such as interpretability, cross-modal integration, and sustainability. While significant advancements have been made in LLMs, challenges such as high computational costs, biases, and ethical risks remain. Overcoming these requires a focus on bias mitigation, transparent decision-making, and explicit ethical guidelines. Future research will generally focus on enhancing the model's ability to handle multiple inputs, thereby making it more intelligent, safe, and reliable.		['Computation and Language (cs.CL)', 'Databases (cs.DB)']	{'pdf': '/pdf/2506.12365', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2506.12365'}	Submission history From: Asifullah Khan [ view email ] [v1] Sat, 14 Jun 2025 05:55:19 UTC (1,075 KB) [v2] Thu, 31 Jul 2025 07:09:27 UTC (1,159 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.12365'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.12365'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.12365'}]
2025-06-15	Text-to-LoRA: Instant Transformer Adaption	Machine Learning	https://arxiv.org/abs/2506.06105	Text-to-LoRA  Introduces a hypernetwork-based approach for instantly generating LoRA adapters from natural language task descriptions, removing the need for conventional task-specific fine-tuning. The authors present Text-to-LoRA (T2L), a model that compresses many LoRA adapters and generalizes to unseen tasks with high efficiency and strong performance.  <br>● T2L is trained to generate low-rank adaptation matrices (LoRAs) for LLMs using only task descriptions, leveraging a hypernetwork to output LoRA weights in a single forward pass. It supports two training modes: reconstruction of pre-trained LoRAs and supervised fine-tuning (SFT) across multiple tasks. <br>● In benchmark experiments, SFT-trained T2L performs competitively in zero-shot adaptation, outperforming multi-task LoRA baselines and even task-specific LoRAs on some tasks (e.g., PIQA and Winogrande), showcasing generalization and compression benefits. <br>● The authors test three architectural variants (L, M, S) of increasing parameter efficiency. Ablations show that T2L scales well with the number of training tasks, and its performance is robust across different task description embeddings (e.g., from GTE or Mistral models). <br>● Qualitative and visual analyses confirm that T2L produces task-specific and semantically meaningful adapters even for unseen tasks, with steerability controlled by how the task is described.	https://x.com/omarsar0/status/1933166911359221943		2506.06105	['Rujikorn Charakorn', 'Edoardo Cetin', 'Yujin Tang', 'Robert Tjarko Lange']	ct:While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyperparameter choices. To overcome these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting large language models (LLMs) on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements.Our code is available atthis https URL	ed at ICML 2025	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2506.06105', 'html': None, 'tex': '/src/2506.06105', 'doi': 'https://doi.org/10.48550/arXiv.2506.06105'}	Submission history From: Rujikorn Charakorn [ view email ] [v1] Fri, 6 Jun 2025 14:11:27 UTC (442 KB) [v2] Mon, 9 Jun 2025 14:19:59 UTC (460 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.06105'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.06105'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.06105'}]
2025-06-15	Reinforcement Pre-Training	Computation and Language	https://arxiv.org/abs/2506.08007	Reinforcement Pre-Training  This paper introduces Reinforcement Pre-Training (RPT), a new paradigm that bridges LLM pretraining and RL by reinterpreting next-token prediction as a reasoning task rewarded via verifiable correctness. Instead of relying on hand-curated annotations or costly human feedback, RPT applies RL on vast unannotated text corpora, assigning intrinsic rewards based on whether a predicted token matches the ground truth. This reframing supports general-purpose RL scaling and enhances both pretraining and fine-tuning efficacy.  <br>● Core method: At each token position in a text sequence, the model first generates a reasoning trace (chain-of-thought) and then predicts the next token. If the prediction is a valid prefix of the ground-truth continuation, a reward is assigned. Multiple rollouts are used per context, and the model is trained via on-policy RL. <br>● Better than standard pretraining: RPT significantly outperforms standard next-token prediction and chain-of-thought reasoning baselines (without RL), achieving higher accuracy on tokens of varying difficulty and even rivaling larger models in performance. RPT-14B, for instance, matches or exceeds R1-Qwen-32B’s accuracy on the OmniMATH benchmark. <br>● Strong scaling laws: RPT exhibits clean power-law scaling with respect to training compute across difficulty levels, with prediction accuracy consistently improving as compute increases, and fitting closely to theoretical curves. <br>● Improves downstream RL and generalization: Fine-tuning RPT models with reinforcement learning on tasks with verifiable answers (e.g., Skywork-OR1) shows faster and stronger gains compared to models trained with standard objectives. Zero-shot evaluation on SuperGPQA and MMLU-Pro benchmarks reveals that RPT-14B in reasoning mode surpasses R1-Distill-Qwen-32B by a significant margin. <br>● Promotes structured thinking: Analysis of reasoning traces reveals that RPT-14B employs more hypothesis generation, deduction, and reflective patterns compared to traditional problem-solving models, supporting the claim that RPT fosters deeper reasoning habits during training.	https://x.com/omarsar0/status/1932522665182703664		2506.08007	['Qingxiu Dong', 'Li Dong', 'Yao Tang', 'Tianzhu Ye', 'Yutao Sun', 'Zhifang Sui', 'Furu Wei']	ct:In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2506.08007', 'html': 'https://arxiv.org/html/2506.08007v1', 'tex': '/src/2506.08007', 'doi': 'https://doi.org/10.48550/arXiv.2506.08007'}	Submission history From: Li Dong [ view email ] [v1] Mon, 9 Jun 2025 17:59:53 UTC (1,234 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.08007'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.08007'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.08007'}]
2025-06-15	TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning	Computation and Language	https://arxiv.org/abs/2506.10380	TableRAG  TableRAG tackles a core limitation of existing RAG approaches: their inability to reason effectively over heterogeneous documents that combine both unstructured text and structured tables. Typical RAG pipelines flatten tables and intermix them with surrounding text, losing essential structural information and hampering multi-hop reasoning. TableRAG overcomes this by introducing a hybrid system that integrates SQL-based symbolic execution with text retrieval in a unified, iterative reasoning framework.  <br>● TableRAG operates in four iterative stages: (1) context-sensitive query decomposition, (2) text retrieval, (3) SQL programming and execution, and (4) intermediate answer generation. This design allows it to preserve tabular structure and leverage both symbolic and neural reasoning paths. <br>● A new benchmark, HeteQA, was developed to evaluate heterogeneous reasoning across 304 multi-hop QA examples covering nine domains and five types of tabular operations (e.g., aggregation, filtering, grouping). <br>● Experiments on HeteQA and existing benchmarks (HybridQA, WikiTableQuestions) show that TableRAG consistently outperforms prior methods like NaiveRAG, ReAct, and TableGPT2, achieving >10% gains in accuracy over the strongest baseline. <br>● Ablations reveal that all major components of TableRAG contribute significantly. Notably, SQL execution is critical for nested reasoning tasks, while textual retrieval is crucial for entity and numeric references. <br>● TableRAG achieves greater reasoning efficiency, solving over 90% of HeteQA tasks in five or fewer steps and exhibiting the lowest failure rate among evaluated methods. Its robustness holds across multiple LLM backbones (Claude, DeepSeek, Qwen).	https://x.com/omarsar0/status/1933520740147736634		2506.10380	['Xiaohan Yu', 'Pu Jian', 'Chong Chen']	ct:Retrieval-Augmented Generation (RAG) has demonstrated considerable effectiveness in open-domain question answering. However, when applied to heterogeneous documents, comprising both textual and tabular components, existing RAG approaches exhibit critical limitations. The prevailing practice of flattening tables and chunking strategies disrupts the intrinsic tabular structure, leads to information loss, and undermines the reasoning capabilities of LLMs in multi-hop, global queries. To address these challenges, we propose TableRAG, an hybrid framework that unifies textual understanding and complex manipulations over tabular data. TableRAG iteratively operates in four steps: context-sensitive query decomposition, text retrieval, SQL programming and execution, and compositional intermediate answer generation. We also develop HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous reasoning capabilities. Experimental results demonstrate that TableRAG consistently outperforms existing baselines on both public datasets and our HeteQA, establishing a new state-of-the-art for heterogeneous document question answering. We release TableRAG atthis https URL.	review. Codes are available atthis https URL	['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2506.10380', 'html': 'https://arxiv.org/html/2506.10380v1', 'tex': '/src/2506.10380', 'doi': 'https://doi.org/10.48550/arXiv.2506.10380'}	Submission history From: Xiaohan Yu [ view email ] [v1] Thu, 12 Jun 2025 06:16:49 UTC (498 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.10380'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.10380'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.10380'}]
2025-06-15	Self-Adapting Language Models	Machine Learning	https://arxiv.org/abs/2506.10943	Self-Adapting Language Models  It proposes a novel framework that enables LLMs to adapt themselves through reinforcement learning by generating their own fine-tuning data and update directives, referred to as “self-edits.” This approach allows models to autonomously optimize their learning process, without relying on separate adaptation modules or human supervision.  Key highlights:  <br>● Self-Edits as Adaptation Mechanism: Instead of updating weights directly with raw data, SEAL uses the model to generate self-edits, natural language instructions that might include restated facts, optimization parameters, or tool invocations. These are then used for supervised fine-tuning. The generation of self-edits is optimized via RL using downstream task performance as the reward. <br>● Two Domains Evaluated: <br>● Learning Framework: SEAL employs an outer RL loop (optimizing the self-edit generation policy) and an inner loop (applying the edits via supervised fine-tuning). The RL objective is approximated via filtered behavior cloning (ReSTEM), reinforcing only edits that lead to performance gains. <br>● Limitations and Forward View: While SEAL achieves compelling gains, it remains susceptible to catastrophic forgetting during sequential updates and incurs high computational cost due to repeated fine-tuning. Future directions include combining SEAL with continual learning strategies and extending it toward autonomous agents that update weights as part of long-horizon interactions.	https://x.com/jyo_pari/status/1933350025284702697		2506.10943	['Adam Zweiger', 'Jyothish Pari', 'Han Guo', 'Ekin Akyürek', 'Yoon Kim', 'Pulkit Agrawal']	ct:Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available atthis https URL.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2506.10943', 'html': 'https://arxiv.org/html/2506.10943v1', 'tex': '/src/2506.10943', 'doi': 'https://doi.org/10.48550/arXiv.2506.10943'}	Submission history From: Jyothish Pari [ view email ] [v1] Thu, 12 Jun 2025 17:48:13 UTC (1,999 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.10943'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.10943'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.10943'}]
2025-06-15	ComfyUI-R1: Exploring Reasoning Models for Workflow Generation	Computation and Language	https://arxiv.org/abs/2506.09790	ComfyUI-R1  Introduces ComfyUI-R1, a 7B-parameter large reasoning model fine-tuned for automatic workflow generation in the ComfyUI ecosystem. Built on Qwen2.5-Coder and trained through a two-stage pipeline (supervised chain-of-thought reasoning followed by reinforcement learning), ComfyUI-R1 significantly outperforms prior state-of-the-art approaches that rely on commercial models like GPT-4o and Claude 3.5.  Key highlights:  <br>● Curated Workflow & Node KBs: The team collected and cleaned 27K community workflows down to 3.9K high-quality entries, each with JSON and code representations. They also assembled a node documentation KB using 7,238 nodes, some enhanced via Claude 3.5. <br>● CoT + RL Training: The model first undergoes supervised fine-tuning on simulated long CoT reasoning sequences involving node selection, planning, and code generation. Then, reinforcement learning with a fine-grained rule-metric hybrid reward encourages format validity, graph correctness, and node accuracy. <br>● Superior Performance: ComfyUI-R1 achieves 97% format validity and the highest node-level and graph-level F1 scores across benchmarks, beating all GPT-4o and Claude baselines. On the ComfyBench benchmark, it improves the execution pass rate to 67%, a full 11% above the previous best (ComfyAgent with GPT-4o). <br>● Qualitative Improvements: Case studies show ComfyUI-R1 creates more faithful, complex, and executable workflows than prior multi-agent approaches. Notably, it better aligns image generation outputs with user instructions in terms of style, structure, and coherence.	https://x.com/omarsar0/status/1933175492716224876		2506.09790	['Zhenran Xu', 'Yiyu Wang', 'Xue Yang', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang']	ct:AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation.	n progress. Try it out in ComfyUI-Copilotthis https URL	['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2506.09790', 'html': 'https://arxiv.org/html/2506.09790v1', 'tex': '/src/2506.09790', 'doi': 'https://doi.org/10.48550/arXiv.2506.09790'}	Submission history From: Zhenran Xu [ view email ] [v1] Wed, 11 Jun 2025 14:35:15 UTC (3,763 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.09790'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.09790'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.09790'}]
2025-06-15	Magistral	Computation and Language	https://arxiv.org/abs/2506.10910	Magistral  Mistral introduces Magistral, its first reasoning-focused LLM line, alongside a custom RL training stack that enables pure reinforcement learning from scratch. In contrast to prior approaches that rely on distillation from teacher models, Magistral trains directly using online RL with text-only data and custom reward shaping. The work yields two open models: Magistral Medium (based on Mistral Medium 3) and the open-sourced Magistral Small (24B), which is bootstrapped via SFT on Medium’s outputs, followed by RL.  Key insights:  <br>● Pure RL can rival distillation: Magistral Medium was trained without any reasoning traces and achieved a 50% boost in AIME-24 (pass@1) over the base model. Across reasoning benchmarks like LiveCodeBench and GPQA, it performs on par or better than DeepSeek-R1 despite lacking a distillation phase. <br>● Multilingual and multimodal generalization emerge: Reinforcement learning on textual math/code data surprisingly enhances multimodal reasoning and preserves instruction-following and tool-calling capabilities. Multilingual reasoning is achieved via simple reward shaping, enforcing the same-language output and chain-of-thought. <br>● Efficient async infrastructure: An asynchronous RL setup with NCCL weight broadcasting enables generators to roll out sequences continuously, receiving weight updates without blocking. This helps maintain on-policyness while maximizing GPU utilization. <br>● Reward shaping and CoT format enforcement: Rewards are conditioned on correct formatting (<thinktags, boxed answers, markdown blocks), correctness (via SymPy and test cases), length penalties, and language consistency (using fastText). Failure to meet formatting results in an immediate zero reward. <br>● Training heuristics and ablations: Analysis shows that reward and performance scale logarithmically with output length. Longer completions correlate with higher reward. RL moves weights in a low-dimensional space, as shown by PCA of checkpoints. Ablations on batch size, advantage normalization, and entropy (via ε ᵢg ) highlight stability tradeoffs. <br>● Open-source release: Magistral Small (24B) is released under Apache 2.0 and performs competitively under three setups, SFT-only, RL-only, and SFT+RL, with the final combo yielding the best results across math and code tasks.	https://x.com/nrehiew_/status/1932872389798605060		2506.10910	['Mistral-AI', 'Abhinav Rastogi', 'Albert Q. Jiang', 'Andy Lo', 'Gabrielle Berrada', 'Guillaume Lample', 'Jason Rute', 'Joep Barmentlo', 'Karmesh Yadav', 'Kartik Khandelwal', 'Khyathi Raghavi Chandu', 'Léonard Blier', 'Lucile Saulnier', 'Matthieu Dinot', 'Maxime Darrin', 'Neha Gupta', 'Roman Soletskyi', 'Sagar Vaze', 'Teven Le Scao', 'Yihan Wang', 'Adam Yang', 'Alexander H. Liu', 'Alexandre Sablayrolles', 'Amélie Héliou', 'Amélie Martin', 'Andy Ehrenberg', 'Anmol Agarwal', 'Antoine Roux', 'Arthur Darcet', 'Arthur Mensch', 'Baptiste Bout', 'Baptiste Rozière', 'Baudouin De Monicault', 'Chris Bamford', 'Christian Wallenwein', 'Christophe Renaudin', 'Clémence Lanfranchi', 'Darius Dabert', 'Devon Mizelle', 'Diego de las Casas', 'Elliot Chane-Sane', 'Emilien Fugier', 'Emma Bou Hanna', 'Gauthier Delerce', 'Gauthier Guinet', 'Georgii Novikov', 'Guillaume Martin', 'Himanshu Jaju', 'Jan Ludziejewski', 'Jean-Hadrien Chabran', 'Jean-Malo Delignon', 'Joachim Studnia', 'Jonas Amar', 'Josselin Somerville Roberts', 'Julien Denize', 'Karan Saxena', 'Kush Jain', 'Lingxiao Zhao', 'Louis Martin', 'Luyu Gao', 'Lélio Renard Lavaud', 'Marie Pellat', 'Mathilde Guillaumin', 'Mathis Felardos', 'Maximilian Augustin', 'Mickaël Seznec', 'Nikhil Raghuraman', 'Olivier Duchenne', 'Patricia Wang', 'Patrick von Platen', 'Patryk Saffer', 'Paul Jacob', 'Paul Wambergue', 'Paula Kurylowicz', 'Pavankumar Reddy Muddireddy', 'Philomène Chagniot', 'Pierre Stock', 'Pravesh Agrawal', 'Romain Sauvestre', 'Rémi Delacourt', 'Sanchit Gandhi', 'Sandeep Subramanian', 'Shashwat Dalal', 'Siddharth Gandhi', 'Soham Ghosh', 'Srijan Mishra', 'Sumukh Aithal', 'Szymon Antoniak', 'Thibault Schueller', 'Thibaut Lavril', 'Thomas Robert', 'Thomas Wang', 'Timothée Lacroix', 'Valeriia Nemychnikova', 'Victor Paltz', 'Virgile Richard', 'Wen-Ding Li', 'William Marshall', 'Xuanyu Zhang', 'Yunhao Tang']	ct:We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2506.10910', 'html': 'https://arxiv.org/html/2506.10910v1', 'tex': '/src/2506.10910', 'doi': 'https://doi.org/10.48550/arXiv.2506.10910'}	Submission history From: Teven Le Scao [ view email ] [v1] Thu, 12 Jun 2025 17:22:37 UTC (4,749 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.10910'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.10910'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.10910'}]
2025-06-15	LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training	Machine Learning	https://arxiv.org/abs/2505.24034	LLamaRL  LlamaRL is a fully-distributed, asynchronous reinforcement learning framework designed for efficient large-scale LLM training (8B to 405B+ models). It achieves up to 10.7× speedup over  DeepSpeed-Chat by combining co-located model offloading, asynchronous off-policy training (AIPO), and fast GPU-native weight sync (DDMA), while maintaining model quality across tasks like math reasoning.	https://x.com/robinphysics/status/1931181903689719875		2505.24034	['Bo Wu', 'Sid Wang', 'Yunhao Tang', 'Jia Ding', 'Eryk Helenowski', 'Liang Tan', 'Tengyu Xu', 'Tushar Gowda', 'Zhengxing Chen', 'Chen Zhu', 'Xiaocheng Tang', 'Yundi Qian', 'Beibei Zhu', 'Rui Hou']	ct:Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically during the Llama 3 post-training, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2505.24034', 'html': 'https://arxiv.org/html/2505.24034v2', 'tex': '/src/2505.24034', 'doi': 'https://doi.org/10.48550/arXiv.2505.24034'}	Submission history From: Bo Wu [ view email ] [v1] Thu, 29 May 2025 22:14:15 UTC (4,284 KB) [v2] Mon, 2 Jun 2025 01:49:51 UTC (4,284 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.24034'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.24034'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.24034'}]
2025-06-08	From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning	Computation and Language	https://arxiv.org/abs/2505.17117	From Tokens to Thoughts  This paper introduces an information-theoretic framework to examine whether LLMs organize semantic knowledge like humans, balancing compression and meaning. Drawing from Rate-Distortion Theory and the Information Bottleneck principle, the authors evaluate token embeddings from 30+ LLMs against classic human categorization benchmarks from cognitive psychology.  <br>● LLMs do form broad conceptual categories that align well with human groupings. Adjusted Mutual Information scores show LLM clusters consistently outperform random baselines, with even small encoder models like BERT matching or beating larger decoder-only models on this alignment task. <br>● However, LLMs struggle with fine-grained semantics. When tested on their ability to mirror human notions of item typicality (e.g., robin as a more typical bird than penguin), correlations between LLM embedding similarity and human ratings were weak and inconsistent. Most models failed to capture graded prototype structures evident in human cognition. <br>● Using their unified loss function L (balancing information complexity and semantic distortion), the authors find that LLMs produce statistically efficient clusters with lower entropy and distortion, while human conceptual clusters are less compact but preserve richer nuance. This suggests LLMs over-optimize for compression at the expense of meaning, unlike humans, who tolerate inefficiency to retain adaptive, flexible structure. <br>● The paper concludes that while LLMs can mimic surface-level categorization, they diverge fundamentally in how they represent meaning, highlighting a core gap between artificial and human semantic systems and offering a quantitative tool for improving human-aligned conceptual representations.			2505.17117	['Chen Shani', 'Dan Jurafsky', 'Yann LeCun', 'Ravid Shwartz-Ziv']	ct:Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Theory (cs.IT)']	{'pdf': '/pdf/2505.17117', 'html': 'https://arxiv.org/html/2505.17117v3', 'tex': '/src/2505.17117', 'doi': 'https://doi.org/10.48550/arXiv.2505.17117'}	Submission history From: Chen Shani [ view email ] [v1] Wed, 21 May 2025 16:29:00 UTC (2,582 KB) [v2] Mon, 26 May 2025 21:13:36 UTC (2,582 KB) [v3] Mon, 30 Jun 2025 21:22:39 UTC (2,582 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.17117'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.17117'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.17117'}]
2025-06-08	Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains	Computation and Language	https://arxiv.org/abs/2506.02126	Knowledge or Reasoning  Introduces a fine-grained evaluation framework to dissect LLM thinking into two components: knowledge correctness and reasoning informativeness, measured via Knowledge Index (KI) and Information Gain (InfoGain), respectively. The authors apply this framework to evaluate how reasoning transfers across domains, particularly medical and mathematical, using Qwen2.5-7B and its DeepSeek-R1-distilled variant trained via SFT and RL.  Key findings include:  <br>● SFT improves knowledge but can harm reasoning: Supervised fine-tuning improves factual accuracy (e.g., 6.2% KI gain in medical tasks), but often leads to verbose or redundant reasoning that reduces InfoGain by 38.9% on average, compared to the base model. <br>● RL boosts both reasoning and knowledge in medical settings: Reinforcement learning enhances reasoning clarity and prunes incorrect knowledge, leading to a 12.4-point average gain in KI. It improves inference by guiding models toward more factually sound reasoning paths. <br>● Domain matters: While math tasks benefit more from reasoning (higher InfoGain), medical tasks rely heavily on domain knowledge (higher KI). In fact, KI shows a stronger correlation (0.998) with task accuracy than InfoGain (0.698) in medical benchmarks. <br>● Base models outperform R1-distilled versions in medicine: Qwen-Base consistently outperforms DeepSeek-R1-distilled models across accuracy, InfoGain, and KI. The R1-distilled model struggles with medical adaptation, likely due to pretraining bias toward math/code domains	https://x.com/omarsar0/status/1930640490786951365		2506.02126	['Juncheng Wu', 'Sheng Liu', 'Haoqin Tu', 'Hang Yu', 'Xiaoke Huang', 'James Zou', 'Cihang Xie', 'Yuyin Zhou']	ct:Recent advances in reasoning-enhanced Large Language Models such as OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex tasks. However, the quality and transparency of their internal reasoning processes remain underexplored. This work moves beyond the final-answer accuracy and investigates step-by-step reasoning in the medical and mathematical domains by explicitly decomposing the thinking trajectories into two parts: knowledge and reasoning. Specifically, we introduce a fine-grained evaluation framework that judges: (1) the correctness of knowledge used (measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured by Information Gain (InfoGain)). Using this framework, we study R1-distilled and base Qwen models trained with supervised fine-tuning (SFT) and/or reinforcement learning (RL) in the medical and math domains. Three intriguing findings emerge: (1) The general reasoning abilities in R1-distilled models do not transfer effectively to the medical domain through either SFT or RL. (2) SFT raises final-answer accuracy in both domains, but often at the cost of reasoning quality: InfoGain drops by 38.9% on average compared with untrained models; In the medical domain, however, SFT remains crucial because domain knowledge is indispensable. (3) RL enhances medical reasoning by pruning inaccurate or irrelevant knowledge from reasoning paths, thereby improving both reasoning accuracy and knowledge correctness.	es, preprint	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2506.02126', 'html': 'https://arxiv.org/html/2506.02126v1', 'tex': '/src/2506.02126', 'doi': 'https://doi.org/10.48550/arXiv.2506.02126'}	Submission history From: Haoqin Tu [ view email ] [v1] Mon, 2 Jun 2025 18:01:00 UTC (2,302 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.02126'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.02126'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.02126'}]
2025-06-08	OpenThoughts: Data Recipes for Reasoning Models	Machine Learning	https://arxiv.org/abs/2506.04178	Open Thoughts  This paper presents OpenThoughts3, a systematic recipe for curating supervised fine-tuning (SFT) data that advances the performance of open-source reasoning models. The authors develop OpenThinker3-7B, a 7B parameter model trained on their new 1.2M example dataset (OpenThoughts3-1.2M) derived from over 1,000 controlled experiments. Despite using no reinforcement learning, OpenThinker3-7B outperforms all other open-data 7B and 8B models on standard math, code, and science reasoning benchmarks, even beating models trained with  larger-scale or mixed SFT+RL pipelines. Key insights and contributions:  <br>● Best-in-class 7B open model: OpenThinker3-7B achieves state-of-the-art results on AIME25 (53.3%), LiveCodeBench (51.7%), and GPQA Diamond (53.7%), outperforming DeepSeek-R1-Distill-Qwen-7B by 15–20 percentage points across tasks. <br>● Scaling laws with clean design: The authors ablate every step in the data pipeline, question sourcing, filtering, teacher choice, deduplication, and answer sampling, showing how each incrementally lifts performance. For instance, using multiple answers per question (16×) improved results more than simply increasing question diversity. <br>● QwQ-32B as a better teacher than stronger models: Surprisingly, QwQ-32B yielded better student models than DeepSeek-R1 or Phi-4 despite lower benchmark scores, suggesting teacher choice affects trace quality more than raw performance. <br>● Filtering matters more than verification: Question filtering based on response length and LLM-estimated difficulty was more predictive of downstream gains than traditional heuristics (e.g., fastText) or even filtering based on correctness verification, which had negligible effects. <br>● Data quality over diversity: Mixing only the top 1–2 question sources per domain consistently outperformed using many sources, indicating that question quality is more important than dataset heterogeneity. <br>● Open-source impact: The full datasets and models are released at [openthoughts.ai](http://openthoughts.ai/), providing a reproducible benchmark for open reasoning research.	https://x.com/lschmidt3/status/1930717405812269273		2506.04178	['Etash Guha', 'Ryan Marten', 'Sedrick Keh', 'Negin Raoof', 'Georgios Smyrnis', 'Hritik Bansal', 'Marianna Nezhurina', 'Jean Mercat', 'Trung Vu', 'Zayne Sprague', 'Ashima Suvarna', 'Benjamin Feuer', 'Liangyu Chen', 'Zaid Khan', 'Eric Frankel', 'Sachin Grover', 'Caroline Choi', 'Niklas Muennighoff', 'Shiye Su', 'Wanjia Zhao', 'John Yang', 'Shreyas Pimpalgaonkar', 'Kartik Sharma', 'Charlie Cheng-Jie Ji', 'Yichuan Deng', 'Sarah Pratt', 'Vivek Ramanujan', 'Jon Saad-Falcon', 'Jeffrey Li', 'Achal Dave', 'Alon Albalak', 'Kushal Arora', 'Blake Wulfe', 'Chinmay Hegde', 'Greg Durrett', 'Sewoong Oh', 'Mohit Bansal', 'Saadia Gabriel', 'Aditya Grover', 'Kai-Wei Chang', 'Vaishaal Shankar', 'Aaron Gokaslan', 'Mike A. Merrill', 'Tatsunori Hashimoto', 'Yejin Choi', 'Jenia Jitsev', 'Reinhard Heckel', 'Maheswaran Sathiamoorthy', 'Alexandros G. Dimakis', 'Ludwig Schmidt']	ct:Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThoughts3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond - improvements of 15.3, 17.2, and 20.5 percentage points compared to the DeepSeek-R1-Distill-Qwen-7B. All of our datasets and models are available onthis https URL.	ttps URL. arXiv admin note: text overlap witharXiv:2505.23754by other authors	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2506.04178', 'html': 'https://arxiv.org/html/2506.04178v2', 'tex': '/src/2506.04178', 'doi': 'https://doi.org/10.48550/arXiv.2506.04178'}	Submission history From: Etash Guha [ view email ] [v1] Wed, 4 Jun 2025 17:25:39 UTC (1,779 KB) [v2] Thu, 5 Jun 2025 02:21:52 UTC (1,779 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.04178'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.04178'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.04178'}]
2025-06-08	Coding Agents with Multimodal Browsing are Generalist Problem Solvers	Computation and Language	https://arxiv.org/abs/2506.03011	Coding Agents with Multimodal Browsing  Introduces OpenHands-Versa, a unified agent designed to perform strongly across diverse domains, coding, web browsing, and multimodal information access, by equipping a single agent with three general capabilities: code execution, multimodal web browsing, and file/search access. In contrast to specialist or multi-agent systems optimized for narrow domains, OpenHands-Versa aims to solve a wide variety of real-world tasks with minimal architectural complexity.  Key highlights:  <br>● Unified Toolset, Superior Coverage: OpenHands-Versa integrates visual web browsing, search API access, and multimodal file processing into the OpenHands coding framework. Despite its simplicity, it surpasses specialized agents across three benchmarks: SWE-Bench Multimodal (+9.1%), GAIA (+1.3%), and The Agent Company (+9.1%) in success rates. <br>● Benchmark Generalization: The agent matches or outperforms multi-agent systems like OWL-roleplaying and Magentic-One, which struggle to generalize across domains. For example, OWL-roleplaying, though strong on GAIA, performs poorly on The Agent Company due to limited tool generality. <br>● Domain-Aware Tool Use: Analysis reveals that OpenHands-Versa effectively adapts its tool usage per benchmark (e.g., search APIs in GAIA, browser in The Agent Company, and visual validation in SWE-Bench M), unlike its predecessor, OpenHands, which misuses or lacks crucial tools like search. <br>● Minimal Agent, Strong Results: By relying on a single-agent design and Claude-3.7 or Claude Sonnet-4 as backbone LLMs, OpenHands-Versa achieves SOTA results without per-task tool customization. For example, it attains 64.24% on GAIA val split, outperforming multi-agent baselines by up to +18%.	https://x.com/omarsar0/status/1930277871999955166		2506.03011	['Aditya Bharat Soni', 'Boxuan Li', 'Xingyao Wang', 'Valerie Chen', 'Graham Neubig']	ct:Modern human labor is characterized by specialization; we train for years and develop particular tools that allow us to perform well across a variety of tasks. In addition, AI agents have been specialized for domains such as software engineering, web navigation, and workflow automation. However, this results in agents that are good for one thing but fail to generalize beyond their intended scope. One reason for this is that agent developers provide a highly specialized set of tools or make architectural decisions optimized for a specific use case or benchmark. In this work, we ask the question: what is the minimal set of general tools that can be used to achieve high performance across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist agent built with a modest number of general tools: code editing and execution, web search, as well as multimodal web browsing and file access. Importantly, OpenHands-Versa demonstrates superior or competitive performance over leading specialized agents across three diverse and challenging benchmarks: SWE-Bench Multimodal, GAIA, and The Agent Company, outperforming the best-performing previously published results with absolute improvements in success rate of 9.1, 1.3, and 9.1 points respectively. Further, we show how existing state-of-the-art multi-agent systems fail to generalize beyond their target domains. These results demonstrate the feasibility of developing a generalist agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline for future research.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2506.03011', 'html': 'https://arxiv.org/html/2506.03011v1', 'tex': '/src/2506.03011', 'doi': 'https://doi.org/10.48550/arXiv.2506.03011'}	Submission history From: Aditya Bharat Soni [ view email ] [v1] Tue, 3 Jun 2025 15:50:55 UTC (535 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.03011'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.03011'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.03011'}]
2025-06-08	Self-Challenging Language Model Agents	Artificial Intelligence	https://arxiv.org/abs/2506.01716	Self-Challenging Language Model Agents  Proposes a novel self-improvement method for multi-turn tool-use LLM agents, called the  Self-Challenging Agent (SCA). It trains LLMs entirely from tasks they generate themselves, avoiding the need for human-annotated tasks or evaluations. The framework introduces a new task format called Code-as-Task (CaT), ensuring generated tasks are feasible, verifiable, and challenging. SCA is shown to double performance in a self-improvement setting and significantly boost performance in distillation.  Key contributions and findings:  <br>● Self-generated tasks via dual-agent roles: The agent alternates between a challenger role, where it explores the environment and creates tasks, and an executor role, where it learns to solve these tasks via reinforcement learning. The process is designed to emulate how human annotators interact with tools to design meaningful tasks. <br>● Code-as-Task (CaT) formulation: Each synthetic task includes an instruction, a Python-based verification function, a working solution, and several failure cases. This structure ensures task quality by filtering out trivial, impossible, or non-verifiable tasks using automatic code execution checks. <br>● Strong results in both distillation and self-improvement: SCA improves the Llama-3.1-8B-Instruct model’s success rate from 12.0% to 23.5% when learning from its own tasks. In the distillation setting (using a 70B teacher), SCA lifts performance to 32.2% Pass@1, outperforming the prior PAE baseline across all tool-use environments. <br>● Human annotation and ablation confirm task quality: Tasks generated with CaT significantly reduce false positives and negatives compared to PAE. A detailed analysis shows CaT’s filtering removes flawed tasks while retaining diversity when used with stronger models like Llama-3.1-70B. <br>● Scaling and training dynamics: More diverse tasks (not just more trajectories per task) yield better generalization, emphasizing the importance of broad synthetic coverage. Online RL methods like PPO and GRPO can further boost performance, but at higher tuning and compute cost.	https://x.com/omarsar0/status/1930748591242424439		2506.01716	['Yifei Zhou', 'Sergey Levine', 'Jason Weston', 'Xian Li', 'Sainbayar Sukhbaatar']	ct:Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2506.01716', 'html': 'https://arxiv.org/html/2506.01716v1', 'tex': '/src/2506.01716', 'doi': 'https://doi.org/10.48550/arXiv.2506.01716'}	Submission history From: Jason Weston [ view email ] [v1] Mon, 2 Jun 2025 14:23:33 UTC (704 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.01716'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.01716'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.01716'}]
2025-06-08	AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time	Computation and Language	https://arxiv.org/abs/2505.24863	AlphaOne  Introduces a universal framework, α1, for modulating the reasoning progress of large reasoning models (LRMs) during inference. Rather than relying on rigid or automatic schedules, α1 explicitly controls when and how models engage in “slow thinking” using a tunable parameter α. The method dynamically inserts “wait” tokens to encourage deeper reasoning and then deterministically ends slow thinking with a “</think>” token to prompt efficient answer generation. This yields better accuracy and efficiency than previous test-time scaling approaches.  Key insights:  <br>● Slow-then-fast reasoning outperforms other strategies: Contrary to human intuition (fast-then-slow), models benefit from beginning with slow reasoning before transitioning to faster inference. This “frontloaded effort” schedule leads to more accurate problem solving. <br>● Dense modulation via α1 boosts accuracy and efficiency: By continuously adjusting reasoning pace via α-scheduled “wait” token insertions, α1 outperforms existing test-time strategies like s1 (monotonic increase) and CoD (monotonic decrease), achieving up to +6.15% accuracy gain while using up to 14% fewer tokens on some benchmarks. <br>● Linear annealing is the most effective scheduling strategy: Among several tested functions for controlling “wait” insertion (constant, linear increase, exponential/linear anneal), linear anneal—gradually reducing “wait” token frequency, proved best across multiple models and datasets. <br>● Post-α moment modulation is critical: Simply inserting “wait” tokens leads to inertia in slow thinking. α1 ensures efficient termination by replacing future “wait” tokens with “</think>”, effectively forcing a shift to fast reasoning and boosting performance by up to +20% in some tasks.	https://x.com/omarsar0/status/1929551555948400840		2505.24863	['Junyu Zhang', 'Runpei Dong', 'Han Wang', 'Xuying Ning', 'Haoran Geng', 'Peihao Li', 'Xialin He', 'Yutong Bai', 'Jitendra Malik', 'Saurabh Gupta', 'Huan Zhang']	ct:This paper presents AlphaOne ($\alpha$1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. $\alpha$1 first introduces $\alpha$ moment, which represents the scaled thinking phase with a universal parameter $\alpha$. Within this scaled pre-$\alpha$ moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the $\alpha$ moment, $\alpha$1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate $\alpha$1's superior reasoning capability and efficiency. Project page:this https URL		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.24863', 'html': 'https://arxiv.org/html/2505.24863v1', 'tex': '/src/2505.24863', 'doi': 'https://doi.org/10.48550/arXiv.2505.24863'}	Submission history From: Junyu Zhang [ view email ] [v1] Fri, 30 May 2025 17:58:36 UTC (1,183 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.24863'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.24863'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.24863'}]
2025-06-08	The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text	Computation and Language	https://arxiv.org/abs/2506.05209	Common Pile v0.1  The Common Pile v0.1 is an 8TB dataset of openly licensed text designed for LLM pretraining, addressing legal and ethical concerns of unlicensed data use. Two 7B parameter models trained on it, Comma v0.1-1T and 2T, achieve performance comparable to LLaMA 1 and 2, and the dataset, code, and model checkpoints are all publicly released.	https://x.com/AiEleuther/status/1931021637991755906		2506.05209	['Nikhil Kandpal', 'Brian Lester', 'Colin Raffel', 'Sebastian Majstorovic', 'Stella Biderman', 'Baber Abbasi', 'Luca Soldaini', 'Enrico Shippole', 'A. Feder Cooper', 'Aviya Skowron', 'John Kirchenbauer', 'Shayne Longpre', 'Lintang Sutawika', 'Alon Albalak', 'Zhenlin Xu', 'Guilherme Penedo', 'Loubna Ben Allal', 'Elie Bakouch', 'John David Pressman', 'Honglu Fan', 'Dashiell Stander', 'Guangyu Song', 'Aaron Gokaslan', 'Tom Goldstein', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Tyler Murray']	ct:Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2506.05209', 'html': 'https://arxiv.org/html/2506.05209v1', 'tex': '/src/2506.05209', 'doi': 'https://doi.org/10.48550/arXiv.2506.05209'}	Submission history From: Colin Raffel [ view email ] [v1] Thu, 5 Jun 2025 16:21:30 UTC (3,580 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.05209'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.05209'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.05209'}]
2025-06-08	RewardBench 2: Advancing Reward Model Evaluation	Computation and Language	https://arxiv.org/abs/2506.01937	RewardBench 2  RewardBench 2 is a new multi-skill benchmark for evaluating reward models with more challenging human prompts and stronger correlation to downstream performance. It highlights gaps in the current  reward model's effectiveness and aims to support more rigorous evaluation, showing existing models score ~20 points lower than their predecessor.	https://x.com/saumyamalik44/status/1929654864604549348		2506.01937	['Saumya Malik', 'Valentina Pyatkin', 'Sander Land', 'Jacob Morrison', 'Noah A. Smith', 'Hannaneh Hajishirzi', 'Nathan Lambert']	ct:Reward models are used throughout the post-training of language models to capture nuanced signals from preference data and provide a training target for optimization across instruction following, reasoning, safety, and more domains. The community has begun establishing best practices for evaluating reward models, from the development of benchmarks that test capabilities in specific skill areas to others that test agreement with human preferences. At the same time, progress in evaluation has not been mirrored by the effectiveness of reward models in downstream tasks -- simpler direct alignment algorithms are reported to work better in many cases. This paper introduces RewardBench 2, a new multi-skill reward modeling benchmark designed to bring new, challenging data for accuracy-based reward model evaluation -- models score about 20 points on average lower on RewardBench 2 compared to the first RewardBench -- while being highly correlated with downstream performance. Compared to most other benchmarks, RewardBench 2 sources new human prompts instead of existing prompts from downstream evaluations, facilitating more rigorous evaluation practices. In this paper, we describe our benchmark construction process and report how existing models perform on it, while quantifying how performance on the benchmark correlates with downstream use of the models in both inference-time scaling algorithms, like best-of-N sampling, and RLHF training algorithms like proximal policy optimization.	models, and leaderboard available atthis https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2506.01937', 'html': 'https://arxiv.org/html/2506.01937v1', 'tex': '/src/2506.01937', 'doi': 'https://doi.org/10.48550/arXiv.2506.01937'}	Submission history From: Saumya Malik [ view email ] [v1] Mon, 2 Jun 2025 17:54:04 UTC (483 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2506.01937'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2506.01937'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2506.01937'}]
2025-06-08	How much do language models memorize?	Computation and Language	https://www.arxiv.org/abs/2505.24832	Memorization in LLMs  This study introduces a method to quantify how much a model memorizes versus generalizes, estimating GPT models have a capacity of ~3.6 bits per parameter. By training hundreds of models, the authors show that memorization saturates with data before generalization (“grokking”) kicks in, and derive new scaling laws linking capacity, data size, and membership inference.			2505.24832	['John X. Morris', 'Chawin Sitawarin', 'Chuan Guo', 'Narine Kokhlikyan', 'G. Edward Suh', 'Alexander M. Rush', 'Kamalika Chaudhuri', 'Saeed Mahloujifar']	"ct:We propose a new method for estimating how much a model knows about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle memorization from generalization. We formally separate memorization into two components: unintended memorization, the information a model contains about a specific dataset, and generalization, the information a model contains about the true data-generation process. When we completely eliminate generalization, we can compute the total memorization, which provides an estimate of model capacity: our measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter. We train language models on datasets of increasing size and observe that models memorize until their capacity fills, at which point ""grokking"" begins, and unintended memorization decreases as models begin to generalize. We train hundreds of transformer language models ranging from $500K$ to $1.5B$ parameters and produce a series of scaling laws relating model capacity and data size to membership inference."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.24832', 'html': 'https://arxiv.org/html/2505.24832v3', 'tex': '/src/2505.24832', 'doi': 'https://doi.org/10.48550/arXiv.2505.24832'}	Submission history From: John Morris [ view email ] [v1] Fri, 30 May 2025 17:34:03 UTC (6,686 KB) [v2] Mon, 2 Jun 2025 14:13:41 UTC (3,282 KB) [v3] Wed, 18 Jun 2025 15:27:03 UTC (6,624 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.24832'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.24832'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.24832'}]
2025-06-01	Sufficient Context: A New Lens on Retrieval Augmented Generation Systems	Computation and Language	https://arxiv.org/abs/2411.06037	New Lens on RAG Systems  Introduces a new conceptual and empirical framework for analyzing RAG systems through the lens of sufficient context, whether the retrieved content alone enables answering a query. This notion helps decouple retrieval failures from generation errors in LLMs, providing clarity on model behavior under different contextual adequacy.  Key findings:  <br>● New definition and classifier for sufficient context: The authors formalize “sufficient context” as context that plausibly allows answering a query, without requiring ground truth. They develop a high-accuracy LLM-based *autorater* (Gemini 1.5 Pro, 93% accuracy) to label instances as having sufficient or insufficient context, enabling large-scale evaluation without needing ground-truth answers. <br>● Sufficient context ≠ guaranteed correctness: Even when sufficient context is present, state-of-the-art LLMs like GPT-4o, Claude 3.5, and Gemini 1.5 still hallucinate answers more often than they abstain. Conversely, models can sometimes answer correctly despite insufficient context, likely leveraging parametric memory. <br>● Benchmarks contain substantial insufficient context: Analysis of datasets like HotPotQA, Musique, and FreshQA shows that a significant fraction of queries (e.g., >50% in Musique and HotPotQA) lack sufficient context, even with curated or oracle retrieval setups. <br>● Selective generation improves factuality: The authors propose a “selective RAG” method that combines model self-confidence with the sufficient context autorater to decide whether to answer or abstain. This yields consistent 2–10% gains in correctness (of answered queries) across Gemini, GPT, and Gemma models. <br>● Fine-tuning alone is insufficient: Attempts to fine-tune smaller models like Mistral 3 7B for better abstention (e.g., training them to say “I don’t know” on insufficient examples) modestly increased abstention but often reduced accuracy or failed to meaningfully curb hallucinations.	https://x.com/omarsar0/status/1927737131478188295		2411.06037	['Hailey Joren', 'Jianyi Zhang', 'Chun-Sung Ferng', 'Da-Cheng Juan', 'Ankur Taly', 'Cyrus Rashtchian']	ct:Augmenting LLMs with context leads to improved performance across many applications. Despite much research on Retrieval Augmented Generation (RAG) systems, an open question is whether errors arise because LLMs fail to utilize the context from retrieval or the context itself is insufficient to answer the query. To shed light on this, we develop a new notion of sufficient context, along with a method to classify instances that have enough information to answer the query. We then use sufficient context to analyze several models and datasets. By stratifying errors based on context sufficiency, we find that larger models with higher baseline performance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when the context is sufficient, but often output incorrect answers instead of abstaining when the context is not. On the other hand, smaller models with lower baseline performance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient context. We further categorize cases when the context is useful, and improves accuracy, even though it does not fully answer the query and the model errs without the context. Building on our findings, we explore ways to reduce hallucinations in RAG systems, including a new selective generation method that leverages sufficient context information for guided abstention. Our method improves the fraction of correct answers among times where the model responds by 2--10\% for Gemini, GPT, and Gemma. Key findings and the prompts used in our autorater analysis are available on our github.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.06037', 'html': 'https://arxiv.org/html/2411.06037v3', 'tex': '/src/2411.06037', 'doi': 'https://doi.org/10.48550/arXiv.2411.06037'}	Submission history From: Hailey Joren [ view email ] [v1] Sat, 9 Nov 2024 02:13:14 UTC (11,771 KB) [v2] Sat, 7 Dec 2024 04:06:41 UTC (11,743 KB) [v3] Wed, 23 Apr 2025 03:07:14 UTC (11,743 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.06037'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.06037'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.06037'}]
2025-06-01	Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents	Artificial Intelligence	https://arxiv.org/abs/2505.22954	Open-Ended Evolution of Self-Improving Agents  This work presents the Darwin Gödel Machine (DGM), a system that advances the vision of self-improving AI by combining self-referential code modification with open-ended evolutionary  search. Unlike the original Gödel machine, which requires provable benefits for code changes (a practically intractable constraint), the DGM adopts an empirical approach: it modifies its own codebase and evaluates improvements on coding benchmarks.  Key contributions and findings:  <br>● Self-referential self-improvement loop: The DGM starts with a single coding agent that edits its own Python-based codebase to improve its ability to read, write, and execute code using frozen foundation models (FMs). Each modification is evaluated on benchmarks like SWE-bench and Polyglot, with only successful agents retained for further iterations. <br>● Open-ended exploration via evolutionary archive: Inspired by Darwinian evolution, the system maintains an archive of all prior agents and samples parents based on performance and novelty. This enables exploration beyond local optima and supports continual innovation, including revisiting previously suboptimal variants that become valuable stepping stones later. <br>● Empirical performance gains: Across 80 iterations, DGM boosts coding success on SWE-bench from 20.0% to 50.0% and on Polyglot from 14.2% to 30.7%, outperforming strong baselines that lack either self-improvement or open-endedness. Its best agents match or exceed leading human-designed, open-source coding agents. <br>● Emergent tool and workflow improvements: Through self-improvement, DGM enhances its capabilities by evolving more granular editing tools, retry and evaluation mechanisms, history-aware patch generation, and code summarization for long contexts. <br>● Generalization across models and tasks: Agents discovered by DGM generalize well when transferred across foundation models (e.g., Claude 3.5 to 3.7, o3-mini) and programming languages, demonstrating robust improvements not overfit to a particular setup. <br>● Safety-conscious design: All experiments were sandboxed, monitored, and scoped to confined domains. The paper also discusses how future self-improvement systems could evolve safer, more interpretable behaviors if these traits are part of the evaluation criteria.	https://x.com/hardmaru/status/1928284568756629756		2505.22954	['Jenny Zhang', 'Shengran Hu', 'Cong Lu', 'Robert Lange', 'Jeff Clune']	ct:Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The Gödel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin Gödel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.	tthis https URL	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2505.22954', 'html': None, 'tex': '/src/2505.22954', 'doi': 'https://doi.org/10.48550/arXiv.2505.22954'}	Submission history From: Jenny Zhuoting Zhang [ view email ] [v1] Thu, 29 May 2025 00:26:15 UTC (2,231 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.22954'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.22954'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.22954'}]
2025-06-01	MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models	Computation and Language	https://arxiv.org/abs/2505.22101	An Operating System for Memory-Augmented Generation in LLMs Introduces a unified operating system for managing memory LLMs, addressing a key limitation in  current architectures: their lack of structured, persistent, and governable memory. While today's LLMs rely primarily on parametric memory (model weights) and limited short-term context, MemOS proposes a comprehensive memory lifecycle and management infrastructure designed to support continual learning, behavioral consistency, and knowledge evolution.  Key contributions and components include:  <br>● Three-tier memory taxonomy: MemOS distinguishes between parametric memory (long-term weights), activation memory (short-term runtime states), and plaintext memory (editable, external content). These types are unified through a shared abstraction called the Memory Cube (MemCube), enabling seamless transformation (e.g., plaintext to parametric) and lifecycle governance. <br>● MemCube abstraction: Each MemCube encapsulates memory metadata (creation time, type, access policies, etc.) and a semantic payload (text, tensors, LoRA patches). This enables dynamic scheduling, traceable updates, and interoperability between modules and agents. <br>● Modular OS-style architecture: MemOS consists of three layers—Interface (user/API interaction), Operation (memory scheduling, lifecycle management), and Infrastructure (storage, access governance), that work together to manage memory parsing, injection, transformation, and archival. <br>● Closed-loop execution flow: Every interaction (e.g., prompt response) can trigger memory operations governed by scheduling rules and lifecycle policies. Retrieved memory can be injected into generation, stored in archives, or transformed into other types for long-term use. <br>● Vision for a memory-centric future: The paper proposes “memory training” as the next frontier beyond pretraining and finetuning, enabling models that learn continuously. Future work includes cross-model memory sharing, self-evolving memory blocks, and a decentralized memory marketplace.	https://x.com/omarsar0/status/1928116365640225222		2505.22101	['Zhiyu Li', 'Shichao Song', 'Hanyu Wang', 'Simin Niu', 'Ding Chen', 'Jiawei Yang', 'Chenyang Xi', 'Huayi Lai', 'Jihao Zhao', 'Yezhaohui Wang', 'Junpeng Ren', 'Zehao Lin', 'Jiahao Huo', 'Tianyi Chen', 'Kai Chen', 'Kehang Li', 'Zhiqiang Yin', 'Qingchen Yu', 'Bo Tang', 'Hongkang Yang', 'Zhi-Qin John Xu', 'Feiyu Xiong']	ct:Large Language Models (LLMs) have emerged as foundational infrastructure in the pursuit of Artificial General Intelligence (AGI). Despite their remarkable capabilities in language perception and generation, current LLMs fundamentally lack a unified and structured architecture for handling memory. They primarily rely on parametric memory (knowledge encoded in model weights) and ephemeral activation memory (context-limited runtime states). While emerging methods like Retrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack lifecycle management and multi-modal integration, limiting their capacity for long-term knowledge evolution. To address this, we introduce MemOS, a memory operating system designed for LLMs that, for the first time, elevates memory to a first-class operational resource. It builds unified mechanisms for representation, organization, and governance across three core memory types: parametric, activation, and plaintext. At its core is the MemCube, a standardized memory abstraction that enables tracking, fusion, and migration of heterogeneous memory, while offering structured, traceable access across tasks and contexts. MemOS establishes a memory-centric execution framework with strong controllability, adaptability, and evolvability. It fills a critical gap in current LLM infrastructure and lays the groundwork for continual adaptation, personalized intelligence, and cross-platform coordination in next-generation intelligent systems.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.22101', 'html': 'https://arxiv.org/html/2505.22101v1', 'tex': '/src/2505.22101', 'doi': 'https://doi.org/10.48550/arXiv.2505.22101'}	Submission history From: Zhiyu Li [ view email ] [v1] Wed, 28 May 2025 08:27:12 UTC (1,627 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.22101'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.22101'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.22101'}]
2025-06-01	A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs	Computation and Language	https://arxiv.org/abs/2505.23006	Building Production-Grade Conversational Agents with Workflow Graphs This paper presents a pragmatic, production-ready framework for building LLM-powered conversational agents using workflow graphs, with a specific focus on e-commerce scenarios. Instead of relying solely on end-to-end generation, the authors design agents using a directed acyclic graph (DAG), enabling flexible yet controllable interactions that adhere to strict business rules and format constraints.  Key contributions and findings include:  <br>● Multi-State DAG Framework: Each node in the graph corresponds to a conversational state with its own system prompt, tool access, and execution rules. This structure enables robust constraint handling (e.g., avoiding hallucinated responses or non-compliant suggestions) by localizing logic and formatting within specific graph nodes. <br>● Fine-Tuning via Response Masking: Because conversation turns come from different states in the DAG, the authors introduce a fine-tuning strategy that applies selective loss masking to train LLMs only on responses relevant to a specific node’s context. This prevents prompt conflicts and improves adherence to node-specific constraints. <br>● Real-World Deployment and Results: In a deployment across KakaoTalk and web platforms, the graph-based approach significantly outperformed baseline agents and even GPT-4o across key metrics like task accuracy (+52%) and format adherence (+50%). In human preference tests, their internal model was favored over GPT-4o in 63% of real-world user cases, especially in product recommendation and safety-critical tasks.	https://x.com/omarsar0/status/1928492639906607297		2505.23006	['Chiwan Park', 'Wonjun Jang', 'Daeryong Kim', 'Aelim Ahn', 'Kichang Yang', 'Woosung Hwang', 'Jihyeon Roh', 'Hyerin Park', 'Hyosun Wang', 'Min Seok Kim', 'Jihoon Kang']	ct:The advancement of Large Language Models (LLMs) has led to significant improvements in various service domains, including search, recommendation, and chatbot applications. However, applying state-of-the-art (SOTA) research to industrial settings presents challenges, as it requires maintaining flexible conversational abilities while also strictly complying with service-specific constraints. This can be seen as two conflicting requirements due to the probabilistic nature of LLMs. In this paper, we propose our approach to addressing this challenge and detail the strategies we employed to overcome their inherent limitations in real-world applications. We conduct a practical case study of a conversational agent designed for the e-commerce domain, detailing our implementation workflow and optimizations. Our findings provide insights into bridging the gap between academic research and real-world application, introducing a framework for developing scalable, controllable, and reliable AI-driven agents.	ed to ACL 2025 Industry Track. 12 pages, 5 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2505.23006', 'html': 'https://arxiv.org/html/2505.23006v1', 'tex': '/src/2505.23006', 'doi': 'https://doi.org/10.48550/arXiv.2505.23006'}	Submission history From: Daeryong Kim [ view email ] [v1] Thu, 29 May 2025 02:30:27 UTC (3,222 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.23006'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.23006'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.23006'}]
2025-06-01	Learning to Reason without External Rewards	Machine Learning	https://arxiv.org/abs/2505.19590	Learn to Reason without External Rewards  Proposes a method for training LLMs via reinforcement learning without any external rewards or labeled data. Instead, it uses the model’s own self-certainty, a confidence measure based on KL divergence from uniform, as the sole intrinsic reward. This self-improvement strategy, part of the broader Reinforcement Learning from Internal Feedback (RLIF) paradigm, bypasses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR), which requires domain-specific verifiers and gold-standard outputs.  Key highlights:  <br>● INTUITOR matches GRPO without external supervision: When applied to mathematical reasoning tasks like GSM8K and MATH500, INTUITOR achieves performance on par with GRPO (a strong RLVR method), even without using gold solutions. On out-of-domain tasks such as LiveCodeBench and CRUXEval, INTUITOR generalizes better, achieving higher gains than GRPO (+65% vs. 0% and +76% vs. +44%, respectively). <br>● Rapid early learning and enhanced instruction-following: INTUITOR significantly boosts early training performance, particularly on models like Qwen2.5-1.5B, and improves adherence to chat-style instructions, reducing repetitive or nonsensical output. <br>● Emergent structured reasoning: Trained models display spontaneous reasoning even when not explicitly required, often generating explanations or planning steps before producing code or answers. This behavior correlates with better transfer performance to domains like code generation. <br>● Self-certainty as a robust, hack-resistant signal: Unlike fixed reward models prone to exploitation, online self-certainty adapts with the model and avoids reward hacking. INTUITOR-trained models show the strongest correlation between self-certainty and correct answers, confirmed by statistical tests.	https://x.com/xuandongzhao/status/1927270931874910259		2505.19590	['Xuandong Zhao', 'Zhewei Kang', 'Aosong Feng', 'Sergey Levine', 'Dawn Song']	ct:Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available atthis https URL		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.19590', 'html': None, 'tex': '/src/2505.19590', 'doi': 'https://doi.org/10.48550/arXiv.2505.19590'}	Submission history From: Xuandong Zhao [ view email ] [v1] Mon, 26 May 2025 07:01:06 UTC (1,085 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.19590'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.19590'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.19590'}]
2025-06-01	Learning to Reason via Mixture-of-Thought for Logical Reasoning	Computation and Language	https://arxiv.org/abs/2505.15817	Learn to Reason via Mixture-of-Thought  While most prior approaches train with a single modality and only ensemble during inference, this work introduces Mixture-of-Thought (MoT) to jointly train and infer across modalities, resulting in notable gains in logical reasoning performance. Key findings:  <br>● Three-modality synergy: MoT uses natural language for interpretability, code for structured procedural reasoning, and truth tables to explicitly enumerate logical cases. Error analysis shows that truth tables significantly reduce common LLM failure modes like missing branches or invalid converses. <br>● Self-evolving training: MoT introduces an iterative, on-policy training loop where the model generates, filters, and learns from its own multi-modal reasoning traces. This joint training outperforms both single-modality and partial-modality setups. <br>● Inference via voting: At test time, MoT generates predictions from each modality and selects the majority answer, leading to robust predictions. Results show up to +11.7pp average accuracy gains on FOLIO and ProofWriter, with 9B models matching GPT-4 + Logic-LM performance. <br>● Stronger on harder tasks: MoT delivers the largest improvements on problems with higher reasoning depth (5–8 steps). It also shows superior test-time scaling, with more diverse and accurate outputs under fixed inference budgets. MoT demonstrates that LLMs can achieve significantly more robust logical reasoning by reasoning like humans (using multiple modes of thought), not just by sampling more from a single modality.	https://x.com/omarsar0/status/1925574200405721210		2505.15817	['Tong Zheng', 'Lichang Chen', 'Simeng Han', 'R. Thomas McCoy', 'Heng Huang']	ct:Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, we propose Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) self-evolving MoT training, which jointly learns from filtered, self-generated rationales across modalities; and (2) MoT inference, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches, achieving up to +11.7pp average accuracy gain. Further analyses show that our MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference.	es	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.15817', 'html': None, 'tex': '/src/2505.15817', 'doi': 'https://doi.org/10.48550/arXiv.2505.15817'}	Submission history From: Tong Zheng [ view email ] [v1] Wed, 21 May 2025 17:59:54 UTC (3,036 KB) [v2] Mon, 9 Jun 2025 21:22:15 UTC (604 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.15817'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.15817'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.15817'}]
2025-06-01	QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning	Computation and Language	https://www.arxiv.org/abs/2505.17667	QwenLong-L1  A new reinforcement learning framework that scales large reasoning models (LRMs) from short to long contexts using progressive context scaling and hybrid rewards. It achieves top performance on seven long-context benchmarks, surpassing models like OpenAI-o3-mini and Qwen3-235B-A22B, and matching Claude-3.7-Sonnet-Thinking, demonstrating strong reasoning with up to 120K token inputs.			2505.17667	['Fanqi Wan', 'Weizhou Shen', 'Shengyi Liao', 'Yingcheng Shi', 'Chenliang Li', 'Ziyi Yang', 'Ji Zhang', 'Fei Huang', 'Jingren Zhou', 'Ming Yan']	ct:Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments.	cal Report	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.17667', 'html': None, 'tex': '/src/2505.17667', 'doi': 'https://doi.org/10.48550/arXiv.2505.17667'}	Submission history From: Fanqi Wan [ view email ] [v1] Fri, 23 May 2025 09:31:55 UTC (2,354 KB) [v2] Tue, 27 May 2025 09:39:47 UTC (2,354 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.17667'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.17667'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.17667'}]
2025-06-01	ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay	Computer Vision and Pattern Recognition	https://www.arxiv.org/abs/2505.16282	End-to-End Policy Optimization for GUI Agents  ARPO introduces an end-to-end reinforcement learning method for training GUI agents using Group Relative Policy Optimization (GRPO) with experience replay. It significantly improves in-domain performance on the OSWorld benchmark, outperforming baselines by up to 6.7%, while offering modest gains on out-of-domain tasks and enabling self-corrective behaviors through structured reward feedback.	https://x.com/TsingYoga/status/1926646893175615943		2505.16282	['Fanbin Lu', 'Zhisheng Zhong', 'Shu Liu', 'Chi-Wing Fu', 'Jiaya Jia']	ct:Training large language models (LLMs) as interactive agents for controlling graphical user interfaces (GUIs) presents a unique challenge to optimize long-horizon action sequences with multimodal feedback from complex environments. While recent works have advanced multi-turn reinforcement learning (RL) for reasoning and tool-using capabilities in LLMs, their application to GUI-based agents remains relatively underexplored due to the difficulty of sparse rewards, delayed feedback, and high rollout costs. In this paper, we investigate end-to-end policy optimization for vision-language-based GUI agents with the aim of improving performance on complex, long-horizon computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an end-to-end RL approach that augments Group Relative Policy Optimization (GRPO) with a replay buffer to reuse the successful experience across training iterations. To further stabilize the training process, we propose a task selection strategy that filters tasks based on baseline agent performance, allowing the agent to focus on learning from informative interactions. Additionally, we compare ARPO with offline preference optimization approaches, highlighting the advantages of policy-based methods in GUI environments. Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive results, establishing a new performance baseline for LLM-based GUI agents trained via reinforcement learning. Our findings underscore the effectiveness of reinforcement learning for training multi-turn, vision-language GUI agents capable of managing complex real-world UI interactions. Codes and models:this https URL.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2505.16282', 'html': 'https://arxiv.org/html/2505.16282v1', 'tex': '/src/2505.16282', 'doi': 'https://doi.org/10.48550/arXiv.2505.16282'}	Submission history From: Fanbin Lu [ view email ] [v1] Thu, 22 May 2025 06:24:32 UTC (1,256 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.16282'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.16282'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.16282'}]
2025-06-01	Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution	Artificial Intelligence	https://www.arxiv.org/abs/2505.20286	Generalist Agent Enabling Scalable Agentic Reasoning  Proposes Alita, a generalist agent framework that enables scalable agentic reasoning through minimal predefinition and maximal self-evolution. Unlike traditional agents reliant on handcrafted tools, Alita autonomously constructs reusable MCPs (Model Context Protocols) using web search and code  synthesis, outperforming more complex systems like OpenAI DeepResearch and OctoTools on GAIA, MathVista, and PathVQA benchmarks.			2505.20286	['Jiahao Qiu', 'Xuan Qi', 'Tongcheng Zhang', 'Xinzhe Juan', 'Jiacheng Guo', 'Yifu Lu', 'Yimin Wang', 'Zixin Yao', 'Qihan Ren', 'Xun Jiang', 'Xing Zhou', 'Dongrui Liu', 'Ling Yang', 'Yue Wu', 'Kaixuan Huang', 'Shilong Liu', 'Hongru Wang', 'Mengdi Wang']	"ct:Recent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. In this work, we introduce Alita--a generalist agent designed with the principle of ""Simplicity is the ultimate sophistication,"" enabling scalable agentic reasoning through minimal predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. For Maximal self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. More details will be updated at $\href{this https URL}{this https URL}$."	s, 3 figures	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2505.20286', 'html': None, 'tex': '/src/2505.20286', 'doi': 'https://doi.org/10.48550/arXiv.2505.20286'}	Submission history From: Xinzhe Juan [ view email ] [v1] Mon, 26 May 2025 17:58:53 UTC (714 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.20286'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.20286'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.20286'}]
2025-05-25	Visual Planning: Let's Think Only with Images	Machine Learning	https://arxiv.org/abs/2505.11409	Visual Planning  Proposes a novel reasoning paradigm that replaces language-based planning with image-based reasoning. The authors argue that language is not always the optimal medium for tasks involving spatial or physical reasoning. They introduce Visual Planning, where reasoning is executed as a sequence of visual states (images) without any text mediation, allowing models to “think” directly in images. This is realized through a reinforcement learning framework called VPRL (Visual Planning via Reinforcement Learning), which trains a vision-only model (LVM-3B) to plan using images.  Key contributions and findings:  <br>● Visual-only reasoning paradigm: The authors formally define planning as autoregressive visual state generation, trained using image-only data. Unlike multimodal LLMs that map vision to language and reason textually, this approach performs inference entirely in the visual modality, sidestepping the modality gap. <br>● VPRL framework: A two-stage training process is introduced. Stage 1 uses supervised learning on randomly sampled trajectories to ensure format consistency and promote exploration. Stage 2 applies GRPO (Group Relative Policy Optimization) to refine planning behavior via progress-based rewards, avoiding invalid or regressive moves. <br>● Superior performance: On three visual navigation tasks (FronzeLake, Maze, and MiniBehavior), VPRL outperforms language-based models (e.g., Gemini 2.5 Pro, Qwen 2.5-VL) by over 40% in Exact Match scores. It also generalizes better to out-of-distribution tasks (larger grid sizes), with visual planners degrading more gracefully than textual ones. <br>● Visual planning yields robustness and interpretability: Unlike textual outputs, visual plans enable step-by-step inspection and show stronger adherence to physical constraints. Qualitative examples illustrate how VPRL can avoid invalid moves and recover from non-optimal paths, while language models often hallucinate or misinterpret spatial layouts. <br>● Exploration and invalid action reduction: The random policy initialization in Stage 1 enables better exploration than supervised baselines (VPFT), as evidenced by higher entropy and fewer invalid actions. This leads to a more effective RL stage and ultimately stronger planning capabilities.	https://x.com/_yixu/status/1924497238908375072		2505.11409	['Yi Xu', 'Chengzu Li', 'Han Zhou', 'Xingchen Wan', 'Caiqi Zhang', 'Anna Korhonen', 'Ivan Vulić']	ct:Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.	es, 6 figures, 1 table (26 pages, 12 figures, 8 tables including references and appendices)	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2505.11409', 'html': 'https://arxiv.org/html/2505.11409v1', 'tex': '/src/2505.11409', 'doi': 'https://doi.org/10.48550/arXiv.2505.11409'}	Submission history From: Yi Xu [ view email ] [v1] Fri, 16 May 2025 16:17:22 UTC (23,184 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.11409'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.11409'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.11409'}]
2025-05-25	EfficientLLM: Efficiency in Large Language Models	Computation and Language	https://arxiv.org/abs/2505.13840	EfficientLLM  Introduces the first large-scale, empirical benchmark for evaluating efficiency trade-offs in LLMs across architecture, fine-tuning, and inference. Conducted on a high-performance cluster (48×GH200, 8×H200 GPUs), the study evaluates over 100 model–technique pairs spanning 0.5B–72B parameters, using six metrics: memory utilization, compute utilization, latency, throughput, energy consumption, and compression rate.  Key insights include:  <br>● No one-size-fits-all solution: Every efficiency technique improves some metrics while degrading others. For instance, MoE boosts accuracy and reduces FLOPs but increases VRAM usage by ~40%, while int4 quantization reduces memory and energy by up to 3.9× at a small 3–5% performance cost. <br>● Resource-specific optima: Efficiency depends on context. MQA achieves the best memory-latency trade-off for constrained devices; MLA has the lowest perplexity for high-quality generation; RSLoRA is more efficient than LoRA only for models above 14B parameters. <br>● Cross-modal transferability: Efficiency techniques like MQA and PEFT generalize well to vision and vision-language models, improving FID scores and maintaining strong trade-offs. <br>● Training and tuning: LoRA and DoRA perform best for small models (1–3B), while RSLoRA excels at large scale (≥14B). Parameter freezing achieves the lowest latency but at a slight cost to accuracy. <br>● Inference: int4 post-training quantization yields the highest compression and throughput gains with minor quality degradation, while bfloat16 consistently outperforms float16 in latency and energy on modern GPUs.	https://x.com/omarsar0/status/1925191664475222186		2505.13840	['Zhengqing Yuan', 'Weixiang Sun', 'Yixin Liu', 'Huichi Zhou', 'Rong Zhou', 'Yiyang Li', 'Zheyuan Zhang', 'Wei Song', 'Yue Huang', 'Haolong Jia', 'Keerthiram Murugesan', 'Yu Wang', 'Lifang He', 'Jianfeng Gao', 'Lichao Sun', 'Yanfang Ye']	ct:Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2505.13840', 'html': 'https://arxiv.org/html/2505.13840v1', 'tex': '/src/2505.13840', 'doi': 'https://doi.org/10.48550/arXiv.2505.13840'}	Submission history From: Weixiang Sun [ view email ] [v1] Tue, 20 May 2025 02:27:08 UTC (9,724 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.13840'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.13840'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.13840'}]
2025-05-25	J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning	Computation and Language	https://arxiv.org/abs/2505.10320	J1  Introduces a novel training approach for LLMs to act as evaluators (LLM-as-a-Judge) by explicitly incentivizing thoughtful reasoning during judgment. Instead of relying solely on prompting or preference fine-tuning, J1 employs online reinforcement learning with verifiable rewards to teach models to think through evaluations systematically.  Key insights:  <br>● Verifiable framing for judgment: J1 converts both verifiable (e.g., math) and non-verifiable (e.g., user queries) prompts into tasks with verifiable rewards by generating synthetic preference pairs. This reframing enables the use of reinforcement learning and consistent training signals across diverse tasks. <br>● Chain-of-thought-driven RL optimization: J1 trains models to reason through evaluations via explicit thought traces, including outlining evaluation criteria, reference answer generation, and self-comparison before producing judgments. Two model types are trained: Pairwise-J1 (outputs verdicts) and Pointwise-J1 (outputs quality scores). Pairwise-J1 models are further improved by consistency rewards to reduce positional bias. <br>● Superior performance at scale: J1-Llama-8B and J1-Llama-70B outperform existing 8B and 70B LLM judges across five benchmarks (PPE, RewardBench, RM-Bench, JudgeBench, FollowBenchEval), beating models trained with much more data like DeepSeek-GRM and distillations of DeepSeek-R1. J1-70B even surpasses o1-mini and closes the gap with the much larger R1 model, particularly on non-verifiable tasks. <br>● Pointwise-J1 mitigates positional bias: While pairwise judges can flip verdicts based on response order, Pointwise-J1 (trained only from pairwise supervision) offers position-consistent scoring with fewer ties and better consistency. Both judge types benefit from test-time scaling via self-consistency, further improving reliability.	https://x.com/jaseweston/status/1923186392420450545		2505.10320	['Chenxi Whitehouse', 'Tianlu Wang', 'Ping Yu', 'Xian Li', 'Jason Weston', 'Ilia Kulikov', 'Swarnadeep Saha']	ct:The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.	es, 8 tables, 11 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2505.10320', 'html': 'https://arxiv.org/html/2505.10320v1', 'tex': '/src/2505.10320', 'doi': 'https://doi.org/10.48550/arXiv.2505.10320'}	Submission history From: Chenxi Whitehouse [ view email ] [v1] Thu, 15 May 2025 14:05:15 UTC (4,205 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.10320'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.10320'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.10320'}]
2025-05-25	When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs	Computation and Language	https://arxiv.org/abs/2505.11423	The Pitfalls of Reasoning for Instruction- Following in LLMs  Explores an unexpected flaw in reasoning-augmented large language models (RLLMs): while chain-of-thought (CoT) prompting often boosts performance on complex reasoning tasks, it can  degrade instruction-following accuracy. The authors evaluate 15 models (e.g., GPT, Claude, LLaMA, DeepSeek) on two instruction-following benchmarks and find that CoT prompting consistently reduces performance across nearly all models and datasets.  Key findings:  <br>● Reasoning hurts instruction adherence: On IFEval, 13 of 14 models saw accuracy drops with CoT; all 15 models regressed on ComplexBench. For example, Meta-LLaMA3-8B’s IFEval accuracy dropped from 75.2% to 59.0% with CoT. Even reasoning-tuned models like Claude3.7-Sonnet-Think performed slightly worse than their base counterparts. <br>● Why reasoning fails: Manual case studies show CoT can help with structural formatting (e.g., JSON or Markdown) and precise lexical constraints (like exact punctuation). But it often hurts by (a) neglecting simple constraints during high-level content planning and (b) inserting helpful but constraint-violating content (e.g., translations in language-restricted outputs). <br>● Attention-based diagnosis: The authors introduce a constraint attention metric and find that CoT reduces the model's focus on instruction-relevant tokens, especially in the answer generation phase. This diminished constraint awareness correlates with performance drops. <br>● Mitigation strategies: Four techniques are proposed to selectively apply reasoning:	https://x.com/omarsar0/status/1924458157444579700		2505.11423	['Xiaomin Li', 'Zhou Yu', 'Zhiwei Zhang', 'Xupeng Chen', 'Ziji Zhang', 'Yingying Zhuang', 'Narayanan Sadagopan', 'Anurag Beniwal']	ct:Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.11423', 'html': None, 'tex': '/src/2505.11423', 'doi': 'https://doi.org/10.48550/arXiv.2505.11423'}	Submission history From: Xiaomin Li [ view email ] [v1] Fri, 16 May 2025 16:36:00 UTC (2,517 KB) [v2] Tue, 20 May 2025 05:31:43 UTC (2,509 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.11423'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.11423'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.11423'}]
2025-05-25	Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models	Artificial Intelligence	https://arxiv.org/abs/2505.10543	Towards a Deeper Understanding of Reasoning in LLMs  This paper investigates whether LLMs can adapt and reason in dynamic environments, moving beyond static benchmarks. Using the SmartPlay benchmark—a suite of four interactive games that require diverse cognitive skills—the authors evaluate three prompting strategies: self-reflection, heuristic mutation (via an Oracle), and planning. They test these methods across models of varying size (Llama3-8B to Llama3.3-70B) and draw several conclusions on how model scale and prompting interact with task complexity.  Key findings:  <br>● Model size dominates performance, especially on reactive and structured reasoning tasks. Larger models (e.g., Llama3.3-70B) significantly outperform smaller ones on tasks like Tower of Hanoi and Bandit, where fast exploitation or spatial planning is critical. <br>● Advanced prompting helps smaller models more, particularly on complex tasks. For example, Llama3-8B with Reflection+Oracle surpasses Llama3.3-70B’s baseline on Rock-Paper-Scissors. However, these strategies introduce high variance and can lead to worse-than-baseline performance depending on the run. <br>● Long prompts hurt smaller models on simple tasks. In Bandit, adding reflective reasoning decreases performance by distracting the model or prolonging exploration. This aligns with prior findings on prompt length and signal-to-noise ratio. <br>● Prompting strategy gains depend on task type. Instruction following improves across all models, while long-text understanding benefits mid-sized models. In contrast, strategies show weak or negative impact on planning, reasoning, and spatial challenges for large models. <br>● Dense reward shaping improves performance more reliably than prompting. In follow-up experiments, modifying sparse reward signals (especially in Hanoi and Messenger) led to more consistent gains than tweaking prompt strategies.	https://x.com/omarsar0/status/1924182825693061403		2505.10543	['Annie Wong', 'Thomas Bäck', 'Aske Plaat', 'Niki van Stein', 'Anna V. Kononova']	ct:While large language models demonstrate impressive performance on static benchmarks, the true potential of large language models as self-learning and reasoning agents in dynamic environments remains unclear. This study systematically evaluates the efficacy of self-reflection, heuristic mutation, and planning as prompting techniques to test the adaptive capabilities of agents. We conduct experiments with various open-source language models in dynamic environments and find that larger models generally outperform smaller ones, but that strategic prompting can close this performance gap. Second, a too-long prompt can negatively impact smaller models on basic reactive tasks, while larger models show more robust behaviour. Third, advanced prompting techniques primarily benefit smaller models on complex games, but offer less improvement for already high-performing large language models. Yet, we find that advanced reasoning methods yield highly variable outcomes: while capable of significantly improving performance when reasoning and decision-making align, they also introduce instability and can lead to big performance drops. Compared to human performance, our findings reveal little evidence of true emergent reasoning. Instead, large language model performance exhibits persistent limitations in crucial areas such as planning, reasoning, and spatial coordination, suggesting that current-generation large language models still suffer fundamental shortcomings that may not be fully overcome through self-reflective prompting alone. Reasoning is a multi-faceted task, and while reasoning methods like Chain of thought improves multi-step reasoning on math word problems, our findings using dynamic benchmarks highlight important shortcomings in general reasoning capabilities, indicating a need to move beyond static benchmarks to capture the complexity of reasoning.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.10543', 'html': 'https://arxiv.org/html/2505.10543v1', 'tex': '/src/2505.10543', 'doi': 'https://doi.org/10.48550/arXiv.2505.10543'}	Submission history From: Annie Wong [ view email ] [v1] Thu, 15 May 2025 17:53:47 UTC (183 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.10543'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.10543'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.10543'}]
2025-05-25	AdaptThink: Reasoning Models Can Learn When to Think	Computation and Language	https://arxiv.org/abs/2505.13417	"AdaptThink  This paper introduces AdaptThink, an RL framework designed to help reasoning models decide when to use detailed chain-of-thought reasoning (“Thinking”) versus directly producing an answer (“NoThinking”), based on task difficulty. This approach challenges the prevailing assumption that deep reasoning should be applied uniformly across all problems, showing that skipping the “thinking” step often yields better efficiency and even higher accuracy on simpler tasks.  Key insights:  <br>● NoThinking outperforms Thinking on simple problems: The authors demonstrate that models like DeepSeek-R1 perform better (in both accuracy and efficiency) when using NoThinking mode, an empty <think></think>token prompt, for easy problems. For example, on Level 1 MATH500 problems, NoThinking achieved slightly better accuracy with significantly fewer tokens used. <br>● AdaptThink learns to switch modes: The proposed RL algorithm introduces a constrained optimization that promotes NoThinking as long as accuracy doesn’t degrade. It uses a novel importance sampling strategy to enable cold-start learning of both modes from the beginning, avoiding the collapse into all-Thinking behavior. <br>● Massive gains in efficiency and performance: On GSM8K, MATH500, and AIME 2024, AdaptThink reduced response length by up to 53% and improved accuracy by up to 2.4% over DeepSeek-R1-Distill-Qwen-1.5B. It also outperformed prior methods (e.g., DPOShortest, TLMRE, ModelMerging) in the trade-off between accuracy and response length. <br>● Robustness and generalization: AdaptThink generalizes to out-of-distribution tasks such as MMLU, maintaining or improving accuracy while reducing token usage. It also avoids ""implicit thinking"" in NoThinking responses, showing controlled behavior during inference."			2505.13417	['Jiajie Zhang', 'Nianyi Lin', 'Lei Hou', 'Ling Feng', 'Juanzi Li']	ct:Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2505.13417', 'html': 'https://arxiv.org/html/2505.13417v1', 'tex': '/src/2505.13417', 'doi': 'https://doi.org/10.48550/arXiv.2505.13417'}	Submission history From: Jiajie Zhang [ view email ] [v1] Mon, 19 May 2025 17:50:52 UTC (444 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.13417'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.13417'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.13417'}]
2025-05-25	MedBrowseComp: Benchmarking Medical Deep Research and Computer Use	Computation and Language	https://arxiv.org/abs/2505.14963	MedBrowseComp  MedBrowseComp is a new benchmark designed to evaluate LLM agents’ ability to perform complex, multi-hop medical fact-finding by browsing real-world, domain-specific web resources. Testing over 1,000 clinically grounded questions, the benchmark reveals major capability gaps in current models, with top systems achieving only 50% accuracy and GUI-based agents performing even worse.	https://x.com/shan23chen/status/1925549357308236029		2505.14963	['Shan Chen', 'Pedro Moreira', 'Yuxin Xiao', 'Sam Schmidgall', 'Jeremy Warner', 'Hugo Aerts', 'Thomas Hartvigsen', 'Jack Gallifant', 'Danielle S. Bitterman']	ct:Large language models (LLMs) are increasingly envisioned as decision-support tools in clinical practice, yet safe clinical reasoning demands integrating heterogeneous knowledge bases -- trials, primary studies, regulatory documents, and cost data -- under strict accuracy constraints. Existing evaluations often rely on synthetic prompts, reduce the task to single-hop factoid queries, or conflate reasoning with open-ended generation, leaving their real-world utility unclear. To close this gap, we present MedBrowseComp, the first benchmark that systematically tests an agent's ability to reliably retrieve and synthesize multi-hop medical facts from live, domain-specific knowledge bases. MedBrowseComp contains more than 1,000 human-curated questions that mirror clinical scenarios where practitioners must reconcile fragmented or conflicting information to reach an up-to-date conclusion. Applying MedBrowseComp to frontier agentic systems reveals performance shortfalls as low as ten percent, exposing a critical gap between current LLM capabilities and the rigor demanded in clinical settings. MedBrowseComp therefore offers a clear testbed for reliable medical information seeking and sets concrete goals for future model and toolchain upgrades. You can visit our project page at:this https URL	n visit our project page at:this https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.14963', 'html': 'https://arxiv.org/html/2505.14963v1', 'tex': '/src/2505.14963', 'doi': 'https://doi.org/10.48550/arXiv.2505.14963'}	Submission history From: Shan Chen [ view email ] [v1] Tue, 20 May 2025 22:42:33 UTC (1,235 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.14963'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.14963'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.14963'}]
2025-05-25	ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems	Artificial Intelligence	https://arxiv.org/abs/2505.11831	ARC-AGI-2  ARC-AGI-2 is a new benchmark designed to push the boundaries of AI reasoning beyond the original ARC-AGI. It introduces harder, more unique tasks emphasizing compositional generalization and human-like fluid intelligence, with baseline AI models performing below 5% accuracy despite strong ARC-AGI-1 results.	https://x.com/arcprize/status/1924869061542085041		2505.11831	['Francois Chollet', 'Mike Knoop', 'Gregory Kamradt', 'Bryan Landers', 'Henry Pinkard']	ct:The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI), introduced in 2019, established a challenging benchmark for evaluating the general fluid intelligence of artificial systems via a set of unique, novel tasks only requiring minimal prior knowledge. While ARC-AGI has spurred significant research activity over the past five years, recent AI progress calls for benchmarks capable of finer-grained evaluation at higher levels of cognitive complexity. We introduce ARC-AGI-2, an upgraded version of the benchmark. ARC-AGI-2 preserves the input-output pair task format of its predecessor, ensuring continuity for researchers. It incorporates a newly curated and expanded set of tasks specifically designed to provide a more granular signal to assess abstract reasoning and problem-solving abilities at higher levels of fluid intelligence. To contextualize the difficulty and characteristics of ARC-AGI-2, we present extensive results from human testing, providing a robust baseline that highlights the benchmark's accessibility to human intelligence, yet difficulty for current AI systems. ARC-AGI-2 aims to serve as a next-generation tool for rigorously measuring progress towards more general and human-like AI capabilities.		['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2505.11831', 'html': 'https://arxiv.org/html/2505.11831v1', 'tex': '/src/2505.11831', 'doi': 'https://doi.org/10.48550/arXiv.2505.11831'}	Submission history From: Francois Chollet [ view email ] [v1] Sat, 17 May 2025 04:34:48 UTC (714 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.11831'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.11831'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.11831'}]
2025-05-25	GRIT: Teaching MLLMs to Think with Images	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2505.15879	Teaching MLLMs to Think with Images  GRIT is a new method that enables MLLMs to perform grounded visual reasoning by interleaving natural language with bounding box references. Using a reinforcement learning approach (GRPO-GR), GRIT achieves strong reasoning and grounding performance with as few as 20 image-question-answer triplets, outperforming baselines in both accuracy and visual coherence.	https://x.com/YFan_UCSC/status/1925719736043569188		2505.15879	['Yue Fan', 'Xuehai He', 'Diji Yang', 'Kaizhi Zheng', 'Ching-Chen Kuo', 'Yuting Zheng', 'Sravana Jyothi Narayanaraju', 'Xinze Guan', 'Xin Eric Wang']	ct:Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.15879', 'html': 'https://arxiv.org/html/2505.15879v1', 'tex': '/src/2505.15879', 'doi': 'https://doi.org/10.48550/arXiv.2505.15879'}	Submission history From: Yue Fan [ view email ] [v1] Wed, 21 May 2025 17:54:49 UTC (3,245 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.15879'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.15879'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.15879'}]
2025-05-18	LLMs Get Lost In Multi-Turn Conversation	Computation and Language	https://arxiv.org/abs/2505.06120	"LLMs Get Lost in Multi-Turn Conversation  Investigates how top LLMs degrade in performance during underspecified, multi-turn interactions, common in real-world usage but rarely evaluated. The authors introduce a novel ""sharded simulation"" framework that breaks down fully-specified instructions into gradual conversation shards, simulating how users naturally provide information over time.  Key findings:   <br>● Massive performance drop: Across 15 top LLMs (e.g.,  GPT-4.1, Gemini 2.5 Pro, Claude 3.7), average performance dropped 39%  in multi-turn vs. single-turn settings. Even a two-turn interaction  was enough to cause a significant decline.   <br>● High unreliability, not just low aptitude:  Decomposition shows only a small drop in   best-case capability (aptitude) but a 112% increase in unreliability,  meaning models are wildly inconsistent depending on how the  conversation unfolds.   <br>● Root causes of failure: Through log analysis and  experiments, the paper identifies four major issues:   <br>● Sharded evaluation tasks: The authors built 600+  multi-turn simulations across 6 tasks (coding, math, SQL, API calls,  summarization, and table captioning), showing consistent degradation  across domains.   <br>● Agent-style interventions only partially help:  Techniques like recap and snowballing (repeating all prior turns)  improved outcomes by ~15–20% but did not restore single-turn levels,  suggesting that model internals, not prompting strategies, are the  bottleneck.   <br>● Temperature and test-time compute don't  solve the issue: Even at temperature 0.0 or with reasoning  models (like o3 and DeepSeek-R1), models remained highly unreliable in  multi-turn settings."	https://x.com/omarsar0/status/1922755721428598988		2505.06120	['Philippe Laban', 'Hiroaki Hayashi', 'Yingbo Zhou', 'Jennifer Neville']	ct:Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.		['Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2505.06120', 'html': 'https://arxiv.org/html/2505.06120v1', 'tex': '/src/2505.06120', 'doi': 'https://doi.org/10.48550/arXiv.2505.06120'}	Submission history From: Philippe Laban [ view email ] [v1] Fri, 9 May 2025 15:21:44 UTC (1,496 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.06120'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.06120'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.06120'}]
2025-05-18	Reinforcement Learning for Reasoning in Large Language Models with One Training Example	Machine Learning	https://arxiv.org/abs/2504.20571	"RL for Reasoning in LLMs with One Training Example  This paper shows that Reinforcement Learning with Verifiable Rewards (RLVR) can significantly improve mathematical reasoning in LLMs even when trained with just a single example. On the Qwen2.5-Math-1.5B model, one-shot RLVR improves accuracy on the MATH500 benchmark from 36.0% to 73.6%, nearly matching performance achieved with over 1,200 examples. Two-shot RLVR (with two examples) even slightly surpasses that, matching results from full 7.5k example training.   <br>● Extreme data efficiency: A single training example (π₁₃)  boosts MATH500 accuracy to 73.6% and average performance across six  math benchmarks to 35.7%, rivaling full-dataset RLVR. Two-shot RLVR  goes further (74.8% and 36.6%).   <br>● Broad applicability: 1-shot RLVR works not only on  Qwen2.5-Math-1.5B, but also on Qwen2.5-Math-7B, Llama3.2-3B-Instruct,  and DeepSeek-R1-Distill-Qwen-1.5B. It remains effective across GRPO  and PPO RL algorithms.   <br>● Post-saturation generalization: Despite training accuracy  saturating early (within 100 steps), test accuracy continues improving  well beyond, reaching gains of +10% after 2,000 steps. The model  eventually overfits the single example (mixing gibberish into  outputs), yet test performance remains stable.   <br>● Cross-domain and reflection behavior: A single  example from one domain (e.g., geometry) improves performance across  others (e.g., number theory). Additionally, models trained with 1-shot  RLVR exhibit increased self-reflection (e.g., “rethink”,  “recalculate”) and longer output sequences.   <br>● Loss function insights: Ablation studies confirm that  policy gradient loss is the primary driver of improvements, not weight  decay, distinguishing 1-shot RLVR from ""grokking"". Entropy loss  further enhances performance and generalization; even without reward  signals, entropy-only training can still yield a 27% performance  boost."	https://x.com/ypwang61/status/1917596101953348000		2504.20571	['Yiping Wang', 'Qing Yang', 'Zhiyuan Zeng', 'Liliang Ren', 'Liyuan Liu', 'Baolin Peng', 'Hao Cheng', 'Xuehai He', 'Kuan Wang', 'Jianfeng Gao', 'Weizhu Chen', 'Shuohang Wang', 'Simon Shaolei Du', 'Yelong Shen']	"ct:We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the mathematical reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Furthermore, RLVR with only two examples even slightly exceeds these results (MATH500: 74.8%, average: 36.6%). Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the ""grokking"" phenomenon. We also show the critical role of promoting exploration (e.g., by incorporating entropy loss with an appropriate coefficient) in 1-shot RLVR training. We also further discuss related observations about format correction, label robustness and prompt modification. These findings can inspire future work on RLVR efficiency and encourage a re-examination of recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source atthis https URL."	es, link:this https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.20571', 'html': 'https://arxiv.org/html/2504.20571v2', 'tex': '/src/2504.20571', 'doi': 'https://doi.org/10.48550/arXiv.2504.20571'}	Submission history From: Yiping Wang [ view email ] [v1] Tue, 29 Apr 2025 09:24:30 UTC (1,109 KB) [v2] Sun, 25 May 2025 06:25:40 UTC (1,580 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.20571'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.20571'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.20571'}]
2025-05-18	AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale	Computation and Language	https://arxiv.org/abs/2505.08311	AM-Thinking-v1  Introduces a dense, open-source 32B language model that achieves state-of-the-art performance in reasoning tasks, rivaling significantly larger Mixture-of-Experts (MoE) models. Built upon  Qwen2.5-32B, the model is trained entirely with public data and showcases how a meticulously crafted post-training pipeline can unlock competitive performance at mid-scale sizes.  Key points:   <br>● Benchmark performance: AM-Thinking-v1 scores 85.3 on AIME  2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, outperforming  DeepSeek-R1 (671B MoE) and matching or exceeding Qwen3-32B and  Seed1.5-Thinking. On Arena-Hard (general chat), it hits 92.5, near the  level of OpenAI o1 and o3-mini but behind Qwen3-235B-A22B and Gemini  2.5 Pro.   <br>● Training pipeline: The model uses a two-stage post-training  approach combining Supervised Fine-Tuning (SFT) and Reinforcement  Learning (RL). SFT emphasizes a “think-then-answer” format and uses  2.84M samples, while RL incorporates difficulty-aware sampling and a   two-stage curriculum optimized via Group Relative Policy Optimization  (GRPO).   <br>● Data and filtering: All training data is publicly  sourced and heavily filtered. Math data goes through LLM-assisted  cleaning and cross-model ground-truth validation. Responses are  filtered using perplexity, n-gram repetition, and structural checks to  ensure coherence and correctness.   <br>● Inference and deployment: The authors implement a custom  rollout framework atop, decoupling rollout from inference via a  streaming load balancer. This reduces long-tail latency and increases  throughput across distributed GPU nodes, enabling scalable RL training  at 32k sequence length.	https://x.com/omarsar0/status/1922668488826741061		2505.08311	['Yunjie Ji', 'Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yiping Peng', 'Han Zhao', 'Xiangang Li']	ct:We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on \href{this https URL}{Hugging Face}.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.08311', 'html': 'https://arxiv.org/html/2505.08311v2', 'tex': '/src/2505.08311', 'doi': 'https://doi.org/10.48550/arXiv.2505.08311'}	Submission history From: Yunjie Ji [ view email ] [v1] Tue, 13 May 2025 07:41:15 UTC (2,444 KB) [v2] Sun, 25 May 2025 07:57:14 UTC (2,444 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.08311'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.08311'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.08311'}]
2025-05-18	Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning	Computation and Language	https://arxiv.org/abs/2505.00024	Nemotron-Research-Tool-N1  Introduces Tool-N1, a family of tool-using LLMs trained using a rule-based reinforcement learning (R1-style RL) approach, without reliance on supervised reasoning trajectories. The key idea is to enable models to learn to invoke external tools correctly through binary feedback based on functional correctness and format adherence, rather than step-by-step imitation.   <br>● Rule-based RL over SFT: Tool-N1 models are trained  using a lightweight binary reward that only evaluates whether the  model's tool calls are structurally correct and functionally valid.  This allows the model to develop its reasoning process, sidestepping  the limitations of mimicking distilled trajectories via supervised  fine-tuning (SFT).   <br>● Strong benchmark results: Tool-N1-7B and Tool-N1-14B  outperform GPT-4o and domain-specialized models on several benchmarks,  including BFCL, API-Bank, and   ACEBench. For example, Tool-N1-14B beats GPT-4o on BFCL overall (85.97  vs 83.97) and achieves +5% over GPT-4o on API-Bank.   <br>● Pure RL outperforms SFT-then-RL: A systematic  comparison on 5,518 distilled trajectories shows that pure RL yields  better results than the SFT-then-RL pipeline, challenging the dominant  paradigm. For instance, 100% RL achieves 83.24% average vs. 83.17% for  SFT+RL.   <br>● Binary reward  fine-grained reward: Ablation  studies reveal that strict binary rewards (requiring correct reasoning  format and exact tool call) lead to better generalization than partial  credit schemes, especially on realistic “Live” data (80.38% vs  76.61%).   <br>● Scaling and generalization: Performance scales well with  model size, with the most gains observed in larger models. The method  generalizes across backbones, with Qwen2.5-Instruct outperforming  LLaMA3 variants at the same scale.	https://x.com/ShaokunZhang1/status/1922105694167433501		2505.00024	['Shaokun Zhang', 'Yi Dong', 'Jieyu Zhang', 'Jan Kautz', 'Bryan Catanzaro', 'Andrew Tao', 'Qingyun Wu', 'Zhiding Yu', 'Guilin Liu']	ct:Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text space. To enhance LLMs' tool-calling abilities, previous approaches primarily rely on supervised fine-tuning (SFT) with trajectories distilled from stronger models, often resulting in imitative reasoning that limits generalization. In this work, we explore rule-based reinforcement learning to enhance tool-calling in LLMs, resulting in Nemotron-Research-Tool-N1, a series of tool-calling reasoning models. Rather than enforcing supervision over intermediate distilled reasoning traces, Tool-N1 is trained with a binary RL reward that assesses only the format validity and functional correctness of tool invocations. This lightweight supervision allows the model to develop reasoning strategies independently, without relying on annotated trajectories. Experiments on several major benchmarks show that Tool-N1-7B/14B clearly outperform GPT-4o. We conduct a systematic study on the design of rule-based reinforcement learning strategies for training tool-calling models. Using 5,518 distilled reasoning trajectories, we compare SFT, RL, and the SFT-then-RL pipeline, finding that the widely adopted SFT-then-RL paradigm does not necessarily outperform pure RL.	es, 6 tables, 12 figures. - update new results - add more details	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2505.00024', 'html': None, 'tex': '/src/2505.00024', 'doi': 'https://doi.org/10.48550/arXiv.2505.00024'}	Submission history From: Shaokun Zhang [ view email ] [v1] Fri, 25 Apr 2025 02:55:21 UTC (1,079 KB) [v2] Mon, 12 May 2025 03:01:39 UTC (1,428 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.00024'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.00024'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.00024'}]
2025-05-18	SEM: Reinforcement Learning for Search-Efficient Large Language Models	Computation and Language	https://arxiv.org/abs/2505.07903	RL for Search-Efficient LLMs  Proposes a new RL-based framework (SEM) that explicitly teaches LLMs when to invoke search and when to rely on internal knowledge, aiming to reduce redundant tool use while maintaining answer accuracy.  Key points:   <br>● Motivation & Setup: LLMs often overuse external search  even for trivial queries. SEM addresses this by using a balanced  training dataset (Musique for unknowns, MMLU for   knowns) and a structured format (<think, <answer, <search,  <result) to train the model to distinguish between situations where  search is necessary or not.   <br>● Reward Optimization: The authors employ Group Relative  Policy Optimization (GRPO) to compare outputs within query groups. The  reward function penalizes unnecessary search and rewards correct  answers, either without search or with efficient search-and-reasoning  when needed.   <br>● Experimental Results: On HotpotQA and MuSiQue, SEM  significantly outperforms Naive RAG and ReSearch, achieving higher EM  and LLM-Judged (LJ) accuracy with smarter search ratios. On MMLU and  GSM8K (where search is often unnecessary), SEM maintains high accuracy  while invoking search far less than baseline methods (e.g., 1.77% SR  vs 47.98% for Naive RAG on MMLU.   <br>● Case Study & Efficiency: SEM avoids absurd search  behavior like querying “What is 1+1?” multiple times. It also uses  fewer but more targeted searches for unknowns, enhancing both  interpretability and computational efficiency. Training dynamics  further show that SEM enables faster and more stable learning than  prior methods.	https://x.com/omarsar0/status/1922665313117552664		2505.07903	['Zeyang Sha', 'Shiwen Cui', 'Weiqiang Wang']	ct:Recent advancements in Large Language Models(LLMs) have demonstrated their capabilities not only in reasoning but also in invoking external tools, particularly search engines. However, teaching models to discern when to invoke search and when to rely on their internal knowledge remains a significant challenge. Existing reinforcement learning approaches often lead to redundant search behaviors, resulting in inefficiencies and over-cost. In this paper, we propose SEM, a novel post-training reinforcement learning framework that explicitly trains LLMs to optimize search usage. By constructing a balanced dataset combining MuSiQue and MMLU, we create scenarios where the model must learn to distinguish between questions it can answer directly and those requiring external retrieval. We design a structured reasoning template and employ Group Relative Policy Optimization(GRPO) to post-train the model's search behaviors. Our reward function encourages accurate answering without unnecessary search while promoting effective retrieval when needed. Experimental results demonstrate that our method significantly reduces redundant search operations while maintaining or improving answer accuracy across multiple challenging benchmarks. This framework advances the model's reasoning efficiency and extends its capability to judiciously leverage external knowledge.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2505.07903', 'html': 'https://arxiv.org/html/2505.07903v1', 'tex': '/src/2505.07903', 'doi': 'https://doi.org/10.48550/arXiv.2505.07903'}	Submission history From: Zeyang Sha [ view email ] [v1] Mon, 12 May 2025 09:45:40 UTC (103 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.07903'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.07903'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.07903'}]
2025-05-18	Cost-Effective, Low Latency Vector Search with Azure Cosmos DB	Databases	https://arxiv.org/abs/2505.05885	Cost-Efficient, Low-Latency Vector Search  Integrates DiskANN (a vector indexing library) inside of Azure Cosmos DB NoSQL (an operational dataset) that uses a single vector index per partition stored in existing index trees. Benefit: It supports < 20ms query latency over an index spanning 10 million vectors, has stable recall over updates, and offers nearly 15× and 41× lower query cost compared to Zilliz and Pinecone serverless enterprise products. It can further scale to billions of vectors with automatic partitioning.	https://x.com/omarsar0/status/1921938925142384736		2505.05885	['Nitish Upreti', 'Harsha Vardhan Simhadri', 'Hari Sudan Sundar', 'Krishnan Sundaram', 'Samer Boshra', 'Balachandar Perumalswamy', 'Shivam Atri', 'Martin Chisholm', 'Revti Raman Singh', 'Greg Yang', 'Tamara Hass', 'Nitesh Dudhey', 'Subramanyam Pattipaka', 'Mark Hildebrand', 'Magdalen Manohar', 'Jack Moffitt', 'Haiyang Xu', 'Naren Datha', 'Suryansh Gupta', 'Ravishankar Krishnaswamy', 'Prashant Gupta', 'Abhishek Sahu', 'Hemeswari Varada', 'Sudhanshu Barthwal', 'Ritika Mor', 'James Codella', 'Shaun Cooper', 'Kevin Pilch', 'Simon Moreno', 'Aayush Kataria', 'Santosh Kulkarni', 'Neil Deshpande', 'Amar Sagare', 'Dinesh Billa', 'Zishan Fu', 'Vipul Vishal']	ct:Vector indexing enables semantic search over diverse corpora and has become an important interface to databases for both users and AI agents. Efficient vector search requires deep optimizations in database systems. This has motivated a new class of specialized vector databases that optimize for vector search quality and cost. Instead, we argue that a scalable, high-performance, and cost-efficient vector search system can be built inside a cloud-native operational database like Azure Cosmos DB while leveraging the benefits of a distributed database such as high availability, durability, and scale. We do this by deeply integrating DiskANN, a state-of-the-art vector indexing library, inside Azure Cosmos DB NoSQL. This system uses a single vector index per partition stored in existing index trees, and kept in sync with underlying data. It supports < 20ms query latency over an index spanning 10 million vectors, has stable recall over updates, and offers approximately 43x and 12x lower query cost compared to Pinecone and Zilliz serverless enterprise products. It also scales out to billions of vectors via automatic partitioning. This convergent design presents a point in favor of integrating vector indices into operational databases in the context of recent debates on specialized vector databases, and offers a template for vector indexing in other databases.		['Databases (cs.DB)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2505.05885', 'html': 'https://arxiv.org/html/2505.05885v2', 'tex': '/src/2505.05885', 'doi': 'https://doi.org/10.48550/arXiv.2505.05885'}	Submission history From: Harsha Vardhan Simhadri [ view email ] [v1] Fri, 9 May 2025 08:53:59 UTC (1,278 KB) [v2] Thu, 31 Jul 2025 20:41:49 UTC (1,281 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.05885'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.05885'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.05885'}]
2025-05-18	AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges	Artificial Intelligence	https://arxiv.org/abs/2505.10468	AI Agents vs. Agentic AI  This review paper distinguishes AI Agents from Agentic AI, presenting a structured taxonomy and comparing their architectures, capabilities, and challenges. AI Agents are defined as modular, task-specific systems powered by LLMs and tools, while Agentic AI represents a shift toward  multi-agent collaboration, dynamic task decomposition, and orchestrated autonomy, with applications and challenges mapped out for both paradigms, along with proposed solutions like RAG, orchestration layers, and causal modeling.	https://x.com/omarsar0/status/1923817691455873420		2505.10468	['Ranjan Sapkota', 'Konstantinos I. Roumeliotis', 'Manoj Karkee']	ct:This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by Large Language Models (LLMs) and Large Image Models (LIMs) for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications	es, 14 figures, 11 tables	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2505.10468', 'html': 'https://arxiv.org/html/2505.10468v4', 'tex': '/src/2505.10468', 'doi': 'https://doi.org/10.48550/arXiv.2505.10468'}	Submission history From: Ranjan Sapkota [ view email ] [v1] Thu, 15 May 2025 16:21:33 UTC (13,055 KB) [v2] Fri, 16 May 2025 23:31:18 UTC (13,058 KB) [v3] Tue, 20 May 2025 04:49:56 UTC (13,059 KB) [v4] Wed, 28 May 2025 01:28:08 UTC (13,076 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.10468'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.10468'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.10468'}]
2025-05-18	CellVerse: Do Large Language Models Really Understand Cell Biology?	Quantitative Biology > Quantitative Methods	https://arxiv.org/abs/2505.07865	CellVerse  Introduces a benchmark to evaluate LLMs on single-cell biology tasks by converting multi-omics data into natural language. While generalist LLMs like DeepSeek and GPT-4 families show some reasoning ability, none significantly outperform random guessing on key tasks like drug response prediction, exposing major gaps in biological understanding by current LLMs.	https://x.com/omarsar0/status/1922662317986099522		2505.07865	['Fan Zhang', 'Tianyu Liu', 'Zhihong Zhu', 'Hao Wu', 'Haixin Wang', 'Donghao Zhou', 'Yefeng Zheng', 'Kun Wang', 'Xian Wu', 'Pheng-Ann Heng']	ct:Recent studies have demonstrated the feasibility of modeling single-cell data as natural languages and the potential of leveraging powerful large language models (LLMs) for understanding cell biology. However, a comprehensive evaluation of LLMs' performance on language-driven single-cell analysis tasks still remains unexplored. Motivated by this challenge, we introduce CellVerse, a unified language-centric question-answering benchmark that integrates four types of single-cell multi-omics data and encompasses three hierarchical levels of single-cell analysis tasks: cell type annotation (cell-level), drug response prediction (drug-level), and perturbation analysis (gene-level). Going beyond this, we systematically evaluate the performance across 14 open-source and closed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the experimental results reveal: (1) Existing specialist models (C2S-Pythia) fail to make reasonable decisions across all sub-tasks within CellVerse, while generalist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit preliminary understanding capabilities within the realm of cell biology. (2) The performance of current LLMs falls short of expectations and has substantial room for improvement. Notably, in the widely studied drug response prediction task, none of the evaluated LLMs demonstrate significant performance improvement over random guessing. CellVerse offers the first large-scale empirical demonstration that significant challenges still remain in applying LLMs to cell biology. By introducing CellVerse, we lay the foundation for advancing cell biology through natural languages and hope this paradigm could facilitate next-generation single-cell analysis.		['Quantitative Methods (q-bio.QM)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Cell Behavior (q-bio.CB)']	{'pdf': '/pdf/2505.07865', 'html': 'https://arxiv.org/html/2505.07865v1', 'tex': '/src/2505.07865', 'doi': 'https://doi.org/10.48550/arXiv.2505.07865'}	Submission history From: Fan Zhang [ view email ] [v1] Fri, 9 May 2025 06:47:23 UTC (20,614 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.07865'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.07865'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.07865'}]
2025-05-11	The Leaderboard Illusion	Artificial Intelligence	https://arxiv.org/abs/2504.20879	The Leaderboard Illusion  The Leaderboard Illusion investigates systemic distortions in how the Chatbot Arena leaderboard evaluates LLMs, arguing that current practices undermine fair model comparison and scientific  progress. Through extensive data analysis covering 2M Arena battles, the authors identify four key issues distorting rankings:   <br>● Selective score reporting through private  testing: Some providers (notably Meta, Google, and OpenAI) are  allowed to test dozens of model variants privately and only publish  the   best-performing one. This violates the unbiased sampling assumption of  the Bradley-Terry (BT) model, which powers Arena rankings. Simulations  show that testing just 10 variants can artificially inflate a model’s  Arena score by ~100 points.   <br>● Extreme data asymmetries: Proprietary models are  oversampled compared to open-weight and open-source models. OpenAI and  Google alone received over 39% of all Arena data, while 83 open-weight  models collectively received only 29.7%. These data advantages  translate into significant performance gains: a model trained on 70%  Arena data outperforms its baseline by 112% on the ArenaHard  benchmark.   <br>● Unfair and opaque deprecations: 205 models were  silently removed from the leaderboard despite only 47 being officially  marked as deprecated. Open-source models are disproportionately  affected, breaking the comparison graph and violating BT model  assumptions, leading to unreliable rankings.   <br>● Overfitting to Arena-specific dynamics: Due to  partial prompt repetition and distributional drift over time, access  to Arena data allows providers to tune models specifically for Arena  performance. This leads to high win rates on Arena benchmarks, but not  on out-of-distribution tasks like MMLU, where gains diminish or  reverse.			2504.20879	"['Shivalika Singh', 'Yiyang Nan', 'Alex Wang', ""Daniel D'Souza"", 'Sayash Kapoor', 'Ahmet Üstün', 'Sanmi Koyejo', 'Yuntian Deng', 'Shayne Longpre', 'Noah A. Smith', 'Beyza Ermis', 'Marzieh Fadaee', 'Sara Hooker']"	ct:Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field	es, 18 figures, 9 tables	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Methodology (stat.ME)']	{'pdf': '/pdf/2504.20879', 'html': 'https://arxiv.org/html/2504.20879v2', 'tex': '/src/2504.20879', 'doi': 'https://doi.org/10.48550/arXiv.2504.20879'}	Submission history From: Marzieh Fadaee [ view email ] [v1] Tue, 29 Apr 2025 15:48:49 UTC (854 KB) [v2] Mon, 12 May 2025 16:33:58 UTC (1,108 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.20879'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.20879'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.20879'}]
2025-05-11	Llama-Nemotron: Efficient Reasoning Models	Computation and Language	https://arxiv.org/abs/2505.00949v1	"Llama-Nemotron  NVIDIA introduces the Llama-Nemotron model series, LN-Nano (8B), LN-Super (49B), and LN-Ultra (253B), a family of open, efficient, and high-performing reasoning models. These models rival or outperform DeepSeek-R1 on various benchmarks while offering significantly better inference throughput and memory efficiency. LN-Ultra is noted as the most ""intelligent"" open model by Artificial Analysis. A key innovation is a dynamic reasoning toggle (""detailed thinking on/off"") that allows users to control reasoning behavior at inference time.  Highlights:   <br>● Multi-stage training: Models were built via neural  architecture search (Puzzle), knowledge distillation, continued  pretraining, supervised fine-tuning (SFT), and large-scale RL.  LN-Ultra is enhanced with FP8 inference and FFN Fusion for speed and  scalability.   <br>● Reasoning Toggle: The models can switch between reasoning  and non-reasoning modes via a simple prompt instruction, making them  adaptable for various use cases.   <br>● Synthetic dataset: Over 33M examples across math, code,  science, and instruction-following were curated, with reasoning-mode  samples tagged explicitly. LN-Ultra's training used curriculum RL and  GRPO to surpass its teachers on benchmarks like GPQA-D.   <br>● Evaluation dominance: LN-Ultra outperforms DeepSeek-R1 and  Llama-3.1-405B in reasoning tasks like AIME25, MATH500, and  GPQA-Diamond while also achieving strong chat alignment scores  (Arena-Hard: 87.0). LN-Super scores 88.3, beating Claude 3.5 and  GPT-4o.  NVIDIA provides the weights, training code (NeMo, Megatron-LM, NeMo-Aligner), and the full  post-training dataset under a permissive license, aiming to push open research in reasoning models."		"{""Models"": ""https://huggingface.co/nvidia""}"	2505.00949v1	['Akhiad Bercovich', 'Itay Levy', 'Izik Golan', 'Mohammad Dabbah', 'Ran El-Yaniv', 'Omri Puny', 'Ido Galil', 'Zach Moshe', 'Tomer Ronen', 'Najeeb Nabwani', 'Ido Shahaf', 'Oren Tropp', 'Ehud Karpas', 'Ran Zilberstein', 'Jiaqi Zeng', 'Soumye Singhal', 'Alexander Bukharin', 'Yian Zhang', 'Tugrul Konuk', 'Gerald Shen', 'Ameya Sunil Mahabaleshwarkar', 'Bilal Kartal', 'Yoshi Suhara', 'Olivier Delalleau', 'Zijia Chen', 'Zhilin Wang', 'David Mosallanezhad', 'Adi Renduchintala', 'Haifeng Qian', 'Dima Rekesh', 'Fei Jia', 'Somshubra Majumdar', 'Vahid Noroozi', 'Wasi Uddin Ahmad', 'Sean Narenthiran', 'Aleksander Ficek', 'Mehrzad Samadi', 'Jocelyn Huang', 'Siddhartha Jain', 'Igor Gitman', 'Ivan Moshkov', 'Wei Du', 'Shubham Toshniwal', 'George Armstrong', 'Branislav Kisacanin', 'Matvei Novikov', 'Daria Gitman', 'Evelina Bakhturina', 'Jane Polak Scowcroft', 'John Kamalu', 'Dan Su', 'Kezhi Kong', 'Markus Kliegl', 'Rabeeh Karimi', 'Ying Lin', 'Sanjeev Satheesh', 'Jupinder Parmar', 'Pritam Gundecha', 'Brandon Norick', 'Joseph Jennings', 'Shrimai Prabhumoye', 'Syeda Nahida Akter', 'Mostofa Patwary', 'Abhinav Khattar', 'Deepak Narayanan', 'Roger Waleffe', 'Jimmy Zhang', 'Bor-Yiing Su', 'Guyue Huang', 'Terry Kong', 'Parth Chadha', 'Sahil Jain', 'Christine Harvey', 'Elad Segal', 'Jining Huang', 'Sergey Kashirsky', 'Robert McQueen', 'Izzy Putterman', 'George Lam', 'Arun Venkatesan', 'Sherry Wu', 'Vinh Nguyen', 'Manoj Kilaru', 'Andrew Wang', 'Anna Warno', 'Abhilash Somasamudramath', 'Sandip Bhaskar', 'Maka Dong', 'Nave Assaf', 'Shahar Mor', 'Omer Ullman Argov', 'Scot Junkin', 'Oleksandr Romanenko', 'Pedro Larroy', 'Monika Katariya', 'Marco Rovinelli', 'Viji Balas', 'Nicholas Edelman', 'Anahita Bhiwandiwalla', 'Muthu Subramaniam', 'Smita Ithape', 'Karthik Ramamoorthy', 'Yuting Wu', 'Suguna Varshini Velury', 'Omri Almog', 'Joyjit Daw', 'Denys Fridman', 'Erick Galinkin', 'Michael Evans', 'Katherine Luna', 'Leon Derczynski', 'Nikki Pope', 'Eileen Long', 'Seth Schneider', 'Guillermo Siman', 'Tomasz Grzegorzek', 'Pablo Ribalta', 'Monika Katariya', 'Joey Conway', 'Trisha Saar', 'Ann Guan', 'Krzysztof Pawelec', 'Shyamala Prayaga', 'Oleksii Kuchaiev', 'Boris Ginsburg', 'Oluwatobi Olabiyi', 'Kari Briski', 'Jonathan Cohen', 'Bryan Catanzaro', 'Jonah Alben', 'Yonatan Geifman', 'Eric Chung', 'et al. (32 additional authors not shown)']	ct:We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2505.00949v1', 'html': 'https://arxiv.org/html/2505.00949v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2505.00949'}	Submission history From: Jiaqi Zeng [ view email ] [v1] Fri, 2 May 2025 01:35:35 UTC (2,263 KB) [v2] Mon, 5 May 2025 21:03:44 UTC (2,263 KB) [v3] Wed, 14 May 2025 16:47:23 UTC (2,263 KB) [v4] Mon, 30 Jun 2025 20:37:51 UTC (712 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.00949'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.00949'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.00949'}]
2025-05-11	Absolute Zero: Reinforced Self-play Reasoning with Zero Data	Machine Learning	https://arxiv.org/abs/2505.03335	Absolute Zero  Introduces an LLM training framework that eliminates the need for human-curated data. Key highlights:   <br>● It learns to propose and solve its reasoning tasks entirely through  self-play, guided by verifiable feedback from an execution  environment. This zero-data RLVR (RL with Verifiable Rewards) setting  achieves SOTA coding and math reasoning performance.   <br>● AZR learns by generating its code-based reasoning tasks using three  core reasoning modes (deduction, abduction, and induction), validating  solutions via Python execution, not human labels.   <br>● A single LLM plays both roles, proposing new tasks based on  learnability and solving them with feedback-based reinforcement.  Rewards favor moderately difficult tasks to maximize the learning  signal.   <br>● Despite using zero in-domain examples, AZR outperforms all previous  zero-setting models on average by +1.8 points and even beats models  trained on tens to hundreds of thousands of curated samples.  AZR-Coder-7B achieves the highest average score across all tested  models.   <br>● AZR trained in a coding-only environment improves mathematical  reasoning performance by up to +15.2 points, far more than expert code  models trained with RLVR, showing strong generalization.   <br>● Larger AZR models (3B → 7B → 14B) consistently show greater  improvements, confirming scalability and suggesting promise for even  larger models.   <br>● AZR develops natural ReAct-like intermediate planning in code (e.g.,  interleaved comments and logic), trial-and-error strategies in  abduction, and systematic state tracking, behaviors typically observed  in much larger models.   <br>● Llama-3.1-8B variants of AZR sometimes produce concerning reasoning  chains (dubbed “uh-oh moments”), highlighting the importance of  safety-aware training in autonomous systems.	https://x.com/AndrewZ45732491/status/1919920459748909288		2505.03335	['Andrew Zhao', 'Yiran Wu', 'Yang Yue', 'Tong Wu', 'Quentin Xu', 'Yang Yue', 'Matthieu Lin', 'Shenzhi Wang', 'Qingyun Wu', 'Zilong Zheng', 'Gao Huang']	ct:Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.03335', 'html': 'https://arxiv.org/html/2505.03335v2', 'tex': '/src/2505.03335', 'doi': 'https://doi.org/10.48550/arXiv.2505.03335'}	Submission history From: Andrew Zhao [ view email ] [v1] Tue, 6 May 2025 09:08:00 UTC (3,686 KB) [v2] Wed, 7 May 2025 13:01:17 UTC (4,145 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.03335'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.03335'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.03335'}]
2025-05-11	Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA	Computation and Language	https://arxiv.org/abs/2504.21252	Discuss-RAG  This paper introduces Discuss-RAG, a plug-and-play agent-based framework that enhances retrieval-augmented generation (RAG) for medical question answering by mimicking human-like  clinical reasoning. Standard RAG systems rely on embedding-based retrieval and lack mechanisms to verify relevance or logical coherence, often leading to hallucinations or outdated answers.  Discuss-RAG addresses these gaps via a modular agent setup that simulates multi-turn medical discussions and performs post-retrieval verification.  Key ideas:   <br>● Multi-agent collaboration: A summarizer agent orchestrates a  team of medical domain experts who iteratively refine a contextual  summary through simulated brainstorming, providing deeper and more  structured information to guide retrieval.   <br>● Decision-making agent: After retrieval, a verifier and a  decision-making agent assess snippet quality and trigger fallback  strategies when relevance is low, improving answer accuracy and  contextual grounding.   <br>● Plug-and-play design: Discuss-RAG is training-free and  modular, allowing easy integration into existing RAG pipelines.   <br>● Strong performance gains: Across four benchmarks,  Discuss-RAG outperforms MedRAG with substantial accuracy improvements,  notably +16.67% on BioASQ and +12.20% on PubMedQA.			2504.21252	['Xuanzhao Dong', 'Wenhui Zhu', 'Hao Wang', 'Xiwen Chen', 'Peijie Qiu', 'Rui Yin', 'Yi Su', 'Yalin Wang']	ct:Medical question answering (QA) is a reasoning-intensive task that remains challenging for large language models (LLMs) due to hallucinations and outdated domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising post-training solution by leveraging external knowledge. However, existing medical RAG systems suffer from two key limitations: (1) a lack of modeling for human-like reasoning behaviors during information retrieval, and (2) reliance on suboptimal medical corpora, which often results in the retrieval of irrelevant or noisy snippets. To overcome these challenges, we propose Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG system through collaborative agent-based reasoning. Our method introduces a summarizer agent that orchestrates a team of medical experts to emulate multi-turn brainstorming, thereby improving the relevance of retrieved content. Additionally, a decision-making agent evaluates the retrieved snippets before their final integration. Experimental results on four benchmark medical QA datasets show that Discuss-RAG consistently outperforms MedRAG, especially significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA. The code is available at:this https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.21252', 'html': 'https://arxiv.org/html/2504.21252v1', 'tex': '/src/2504.21252', 'doi': 'https://doi.org/10.48550/arXiv.2504.21252'}	Submission history From: Xuanzhao Dong [ view email ] [v1] Wed, 30 Apr 2025 01:37:44 UTC (608 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.21252'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.21252'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.21252'}]
2025-05-11	All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning	Machine Learning	https://arxiv.org/abs/2503.01067	The Value of RL in Fine-Tuning  This work shows that, in theory, every popular preference-fine-tuning objective collapses to maximum-likelihood estimation (MLE), yet experiments show a consistent RL advantage on real tasks. They reconcile this gap with a generation-verification complexity hypothesis.   <br>● Theory: RLHF ≈ MLE			2503.01067	['Gokul Swamy', 'Sanjiban Choudhury', 'Wen Sun', 'Zhiwei Steven Wu', 'J. Andrew Bagnell']	ct:From a first-principles perspective, it may seem odd that the strongest results in foundation model fine-tuning (FT) are achieved via a relatively complex, two-stage training procedure. Specifically, one first trains a reward model (RM) on some dataset (e.g. human preferences) before using it to provide online feedback as part of a downstream reinforcement learning (RL) procedure, rather than directly optimizing the policy parameters on the dataset via offline maximum likelihood estimation. In fact, from an information-theoretic perspective, we can only lose information via passing through a reward model and cannot create any new information via on-policy sampling. To explain this discrepancy, we scrutinize several hypotheses on the value of RL in FT through both theoretical and empirical lenses. Of the hypotheses considered, we find the most support for the explanation that on problems with a generation-verification gap, the combination of the ease of learning the relatively simple RM (verifier) from the preference data, coupled with the ability of the downstream RL procedure to then filter its search space to the subset of policies (generators) that are optimal for relatively simple verifiers is what leads to the superior performance of online FT.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2503.01067', 'html': None, 'tex': '/src/2503.01067', 'doi': 'https://doi.org/10.48550/arXiv.2503.01067'}	Submission history From: Gokul Swamy [ view email ] [v1] Mon, 3 Mar 2025 00:15:19 UTC (1,518 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.01067'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.01067'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.01067'}]
2025-05-11	WebThinker: Empowering Large Reasoning Models with Deep Research Capability	Computation and Language	https://arxiv.org/abs/2504.21776	WebThinker  This paper introduces a reasoning agent framework that equips large reasoning models (LRMs) with autonomous web exploration and report writing abilities to overcome limitations of static internal knowledge.  WebThinker integrates a Deep Web Explorer module and an Autonomous Think-Search-and-Draft strategy that lets models search the web, reason through tasks, and generate comprehensive outputs simultaneously. It also incorporates an RL-based training loop using online DPO to improve tool usage. The system supports two modes: complex problem solving and scientific report generation. Key points:   <br>● Superior performance in complex reasoning: On  GPQA, GAIA, WebWalkerQA, and HLE, WebThinker-32B-RL achieved new  state-of-the-art results among 32B models, outperforming both  retrieval-augmented and proprietary systems like GPT-4o and  DeepSeek-R1-671B. For example, it reached 70.7% on GPQA and 15.8% on  HLE, with gains of up to +21.5% over baselines.   <br>● Best-in-class scientific report writing: On the  Glaive dataset, WebThinker outperformed Gemini2.0 Deep Research and  Grok3 DeeperSearch, scoring 8.1 in average quality metrics such as  completeness and coherence.   <br>● RL refinement matters: The RL-trained version  outperformed its base counterpart across all benchmarks, showing that  iterative preference-based learning significantly enhances  reasoning-tool coordination.   <br>● Ablation validates design: Removing components like Deep  Web Explorer or automatic report drafting significantly degraded  performance, confirming their necessity.			2504.21776	['Xiaoxi Li', 'Jiajie Jin', 'Guanting Dong', 'Hongjin Qian', 'Yutao Zhu', 'Yongkang Wu', 'Ji-Rong Wen', 'Zhicheng Dou']	ct:Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2504.21776', 'html': None, 'tex': '/src/2504.21776', 'doi': 'https://doi.org/10.48550/arXiv.2504.21776'}	Submission history From: Xiaoxi Li [ view email ] [v1] Wed, 30 Apr 2025 16:25:25 UTC (574 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.21776'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.21776'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.21776'}]
2025-05-11	RM-R1: Reward Modeling as Reasoning	Computation and Language	https://arxiv.org/abs/2505.02387	Reward Modeling as Reasoning  This work proposes a new class of reward models, called ReasRMs, that reformulate reward modeling as a reasoning task. The authors introduce RM-R1, a family of generative reward models that produce interpretable reasoning traces and rubrics during preference judgments. Instead of relying on scalar scores or shallow generation, RM-R1 models leverage structured reasoning and reinforcement learning to improve both interpretability and performance across benchmarks.   <br>● RM-R1 adopts a two-stage training process:  (1) distillation of reasoning traces from stronger models, and (2)  reinforcement learning with verifiable rewards. The Chain-of-Rubrics  (CoR) prompting framework guides the model to either solve reasoning  problems or generate evaluation rubrics depending on the task type  (reasoning or chat).   <br>● On RewardBench, RM-Bench, and RMB, RM-R1 models achieve  state-of-the-art or near-SOTA performance, outperforming models like  GPT-4o and Llama3.1-405B by up to 13.8% despite using fewer parameters  and less data.   <br>● Ablation studies show that cold-start RL alone is  insufficient; task-type classification and high-quality distillation  are key. RM-R1's distilled warm-start training leads to more stable  learning and longer, more accurate reasoning traces.   <br>● RM-R1 also shows strong generalization across  domains and better rubric quality than baseline methods, especially in  sensitive contexts like safety and medical judgment. The authors  open-sourced six RM-R1 models, training data, and code to support  reproducibility.			2505.02387	['Xiusi Chen', 'Gaotang Li', 'Ziqi Wang', 'Bowen Jin', 'Cheng Qian', 'Yu Wang', 'Hongru Wang', 'Yu Zhang', 'Denghui Zhang', 'Tong Zhang', 'Hanghang Tong', 'Heng Ji']	ct:Reward modeling is essential for aligning large language models with human preferences through reinforcement learning from human feedback. To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. Inspired by recent advances of long chain-of-thought on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RMs interpretability and performance. To this end, we introduce a new class of generative reward models - Reasoning Reward Models (ReasRMs) - which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism - self-generating sample-level chat rubrics or math/code solutions, and evaluating candidate responses against them. The training of RM-R1 consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. Empirically, our models achieve state-of-the-art performance across three reward model benchmarks on average, outperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones (e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough empirical analyses to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six REASRM models along with code and data atthis https URL.	es, 8 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2505.02387', 'html': 'https://arxiv.org/html/2505.02387v3', 'tex': '/src/2505.02387', 'doi': 'https://doi.org/10.48550/arXiv.2505.02387'}	Submission history From: Xiusi Chen [ view email ] [v1] Mon, 5 May 2025 06:11:12 UTC (187 KB) [v2] Thu, 15 May 2025 04:14:49 UTC (192 KB) [v3] Sun, 18 May 2025 03:26:32 UTC (194 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.02387'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.02387'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.02387'}]
2025-05-11	Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning	Computation and Language	https://arxiv.org/abs/2504.17192	Paper2Code  Introduces PaperCoder, a multi-agent LLM framework that transforms ML papers into full code repositories without relying on pre-existing implementations.   <br>● PaperCoder decomposes the code generation process into three stages:  Planning (roadmap, architecture, file dependencies, config files),  Analyzing (file-specific logic extraction), and Coding  (dependency-aware file generation). Each step is handled by  specialized LLM agents.   <br>● It is evaluated using both the proposed Paper2Code benchmark (90  papers from ICML, NeurIPS, and ICLR 2024) and PaperBench Code-Dev.  Results show PaperCoder outperforms ChatDev, MetaGPT, and naive  baselines across reference-based, reference-free, and human  evaluations.   <br>● In human assessments by original paper authors, 77% chose PaperCoder  as best implementation; 85% said it helped them reproduce their work.  On average, only 0.48% of code lines required changes for  executability.   <br>● A detailed ablation study shows consistent performance gains from  each stage, especially logic design and file dependency ordering.  PaperCoder, using the o3-mini-high backbone, notably outperforms other  LLM variants.			2504.17192	['Minju Seo', 'Jinheon Baek', 'Seongyun Lee', 'Sung Ju Hwang']	ct:Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, particularly from the authors of those papers, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins. Code is available at:this https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.17192', 'html': 'https://arxiv.org/html/2504.17192v3', 'tex': '/src/2504.17192', 'doi': 'https://doi.org/10.48550/arXiv.2504.17192'}	Submission history From: Minju Seo [ view email ] [v1] Thu, 24 Apr 2025 01:57:01 UTC (1,206 KB) [v2] Sat, 26 Apr 2025 14:03:23 UTC (1,209 KB) [v3] Sun, 18 May 2025 13:38:09 UTC (1,923 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.17192'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.17192'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.17192'}]
2025-05-11	ZeroSearch: Incentivize the Search Capability of LLMs without Searching	Computation and Language	https://arxiv.org/abs/2505.04588	ZeroSearch  ZeroSearch is an RL framework that trains LLMs to develop search capabilities without using real search engines. It uses simulated LLM-generated documents with a curriculum-based degradation strategy and outperforms real-search methods like Search-R1 in both performance and cost, achieving better QA accuracy across multiple benchmarks.	https://x.com/omarsar0/status/1920469148968362407		2505.04588	['Hao Sun', 'Zile Qiao', 'Jiayan Guo', 'Xuanbo Fan', 'Yingyan Hou', 'Yong Jiang', 'Pengjun Xie', 'Yan Zhang', 'Fei Huang', 'Jingren Zhou']	ct:Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a novel RL framework that incentivizes the capabilities of LLMs to use a real search engine with simulated searches during training. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both useful and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2505.04588', 'html': 'https://arxiv.org/html/2505.04588v2', 'tex': '/src/2505.04588', 'doi': 'https://doi.org/10.48550/arXiv.2505.04588'}	Submission history From: Hao Sun [ view email ] [v1] Wed, 7 May 2025 17:30:22 UTC (1,075 KB) [v2] Fri, 16 May 2025 13:53:00 UTC (1,072 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.04588'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.04588'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.04588'}]
2025-05-11	Practical Efficiency of Muon for Pretraining	Machine Learning	https://arxiv.org/abs/2505.02222	Practical Efficiency of Muon for Pretraining  Discusses how Muon, a simple second-order optimizer, outperforms AdamW in large-batch pretraining by expanding the compute-time Pareto frontier and maintaining better data efficiency. Combined with muP scaling and a novel telescoping algorithm for hyperparameter transfer, it enables faster training with minimal tuning overhead up to 4B parameter models.			2505.02222	['Essential AI', 'Ishaan Shah', 'Anthony M. Polloreno', 'Karl Stratos', 'Philip Monk', 'Adarsh Chaluvaraju', 'Andrew Hojel', 'Andrew Ma', 'Anil Thomas', 'Ashish Tanwer', 'Darsh J Shah', 'Khoi Nguyen', 'Kurt Smith', 'Michael Callahan', 'Michael Pust', 'Mohit Parmar', 'Peter Rushton', 'Platon Mazarakis', 'Ritvik Kapila', 'Saurabh Srivastava', 'Somanshu Singla', 'Tim Romanski', 'Yash Vanjani', 'Ashish Vaswani']	ct:We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.		['Machine Learning (cs.LG)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2505.02222', 'html': 'https://arxiv.org/html/2505.02222v4', 'tex': '/src/2505.02222', 'doi': 'https://doi.org/10.48550/arXiv.2505.02222'}	Submission history From: Karl Stratos [ view email ] [v1] Sun, 4 May 2025 19:14:43 UTC (8,763 KB) [v2] Tue, 6 May 2025 21:14:24 UTC (9,504 KB) [v3] Sun, 11 May 2025 01:55:11 UTC (9,504 KB) [v4] Tue, 20 May 2025 01:04:35 UTC (9,504 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2505.02222'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2505.02222'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2505.02222'}]
2025-05-04	Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math	Computation and Language	https://arxiv.org/abs/2504.21233	Phi-4-Mini-Reasoning  Microsoft released Phi-4-Mini-Reasoning to explore small reasoning language models for math. Highlights:   <br>● Phi-4-Mini-Reasoning: The paper introduces Phi-4-Mini-Reasoning,  a 3.8B parameter small language model (SLM) that achieves  state-of-the-art mathematical reasoning performance, rivaling or  outperforming models nearly twice its size.   <br>● Unlocking Reasoning: They use a systematic, multi-stage  training pipeline to unlock strongbr> reasoning capabilities in compact  models, addressing the challenges posed by their limited capacity.  Uses large-scale distillation, preference learning, and RL with  verifiable rewards.   <br>● Four-Stage Training Pipeline: The model is trained using  (1) mid-training with large-scale long CoT data, (2) supervised  fine-tuning on high-quality CoT data, (3) rollout-based Direct  Preference Optimization (DPO), and (4) RL using verifiable reward  signals.   <br>● Math Performance: On MATH-500, Phi-4-Mini-Reasoning reaches  94.6%, surpassing DeepSeek-R1-Distill-Qwen-7B (91.4%) and  DeepSeek-R1-Distill-Llama-8B (86.9%), despite being smaller.   <br>● Verifiable Reward Reinforcement Learning: The final  RL stage, tailored for small models, includes prompt filtering,  oversampling for balanced training signals, and temperature annealing.  This improves training stability and aligns exploration with  evaluation conditions.   <br>● Massive Synthetic Data Generation: The model is  mid-trained on 10M CoT rollouts generated by DeepSeek-R1, filtered for  correctness using math verifiers and GPT-4o-mini, and categorized by  domain and difficulty to ensure broad generalization.   <br>● Ablation Study: Each phase of the pipeline shows clear  gains. Notably, fine-tuning and RL each deliver ~5–7 point  improvements after mid-training and DPO, showing the value of the full  pipeline over isolated techniques.	https://x.com/omarsar0/status/1917954418173247909		2504.21233	['Haoran Xu', 'Baolin Peng', 'Hany Awadalla', 'Dongdong Chen', 'Yen-Chun Chen', 'Mei Gao', 'Young Jin Kim', 'Yunsheng Li', 'Liliang Ren', 'Yelong Shen', 'Shuohang Wang', 'Weijian Xu', 'Jianfeng Gao', 'Weizhu Chen']	ct:Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.21233', 'html': 'https://arxiv.org/html/2504.21233v1', 'tex': '/src/2504.21233', 'doi': 'https://doi.org/10.48550/arXiv.2504.21233'}	Submission history From: Haoran Xu [ view email ] [v1] Wed, 30 Apr 2025 00:04:35 UTC (305 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.21233'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.21233'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.21233'}]
2025-05-04	Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory	Computation and Language	https://arxiv.org/abs/2504.19413	Building Production-Ready AI Agents with Scalable Long-Term Memory  This paper proposes a memory-centric architecture for LLM agents to maintain coherence across long conversations and sessions, solving the fixed-context window limitation. Main highlights:   <br>● The solution introduces two systems: Mem0, a  dense, language-based memory system, and Mem0g, an enhanced version  with graph-based memory to model complex relationships. Both aim to  extract, consolidate, and retrieve salient facts over time  efficiently.   <br>● Mem0: Uses a two-stage architecture (extraction & update) to  maintain salient conversational memories. It detects redundant or  conflicting information and manages updates using   tool-calls, resulting in a lightweight, highly responsive memory store  (7K tokens per conversation).   <br>● Mem0g: By structuring memory as a knowledge graph of entities  and relationships, Mem0g improves performance in tasks needing  temporal and relational reasoning (e.g., event ordering, preference  tracking) while maintaining reasonable latency and memory cost (14K  tokens/convo).   <br>● Benchmarking on LOCOMO: Both systems were evaluated  against six memory system baselines (e.g., A-Mem, OpenAI, Zep,  LangMem, RAG). Mem0g achieves the best overall LLM-as-a-Judge (J)  score of 68.44%, outperforming all RAG and memory baselines by 7–28%  in J and reducing p95 latency by 91% over full-context methods.   <br>● Latency and efficiency: Mem0 achieves the lowest search  and total latencies (p95 = 1.44s), and Mem0g still outperforms other  graph-based or RAG systems by large margins in speed and efficiency.  Great for real-time deployments.   <br>● Use-case strengths:  Mem0 and Mem0g offer a scalable memory architecture for long-term LLM agents to improve factual recall, reasoning depth, and efficiency, making them id	https://x.com/omarsar0/status/1917247776221700134		2504.19413	['Prateek Chhikara', 'Dev Khant', 'Saket Aryan', 'Taranjeet Singh', 'Deshraj Yadav']	ct:Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2504.19413', 'html': 'https://arxiv.org/html/2504.19413v1', 'tex': '/src/2504.19413', 'doi': 'https://doi.org/10.48550/arXiv.2504.19413'}	Submission history From: Prateek Chhikara [ view email ] [v1] Mon, 28 Apr 2025 01:46:35 UTC (946 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.19413'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.19413'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.19413'}]
2025-05-04	UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities	Computation and Language	https://arxiv.org/abs/2504.20734	UniversalRAG  UniversalRAG is a framework that overcomes the limitations of existing RAG systems confined to single modalities or corpora. It supports retrieval across modalities (text, image, video) and at multiple granularities (e.g., paragraph vs. document, clip vs. video). Contributions from the paper:   <br>● Modality-aware routing: To counter modality bias in unified  embedding spaces (where queries often retrieve same-modality results  regardless of relevance), UniversalRAG introduces a router that  dynamically selects the appropriate modality (e.g., image vs. text)  for each query.   <br>● Granularity-aware retrieval: Each modality is broken into  granularity levels (e.g., paragraphs vs. documents for text, clips vs.  full-length videos). This allows queries to retrieve content that  matches their complexity -- factual queries use short segments while  complex reasoning accesses long-form data.   <br>● Flexible routing: It supports both training-free (zero-shot  GPT-4o prompting) and trained (T5-Large) routers. Trained routers  perform better on in-domain data, while GPT-4o generalizes better to  out-of-domain tasks. An ensemble router combines both for robust  performance.   <br>● Performance: UniversalRAG outperforms modality-specific and  unified RAG baselines across 8 benchmarks spanning text (e.g., MMLU,  SQuAD), image (WebQA), and video (LVBench, VideoRAG). With T5-Large,  it achieves the highest average score across modalities.   <br>● Case study: In WebQA, UniversalRAG correctly routes a visual  query to the image corpus (retrieving an actual photo of the event),  while TextRAG and VideoRAG fail. Similarly, on HotpotQA and LVBench,  it chooses the right granularity, retrieving documents or short clips.  Overall, this is a great paper showing the importance of considering  modality and granularity in a RAG system.	https://x.com/omarsar0/status/1917637837295608180		2504.20734	['Woongyeong Yeo', 'Kangsan Kim', 'Soyeong Jeong', 'Jinheon Baek', 'Sung Ju Hwang']	ct:Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single aggregated corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over various modality-specific and unified baselines.	t page :this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2504.20734', 'html': None, 'tex': '/src/2504.20734', 'doi': 'https://doi.org/10.48550/arXiv.2504.20734'}	Submission history From: Woongyeong Yeo [ view email ] [v1] Tue, 29 Apr 2025 13:18:58 UTC (1,244 KB) [v2] Mon, 19 May 2025 11:09:12 UTC (3,023 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.20734'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.20734'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.20734'}]
2025-05-04	DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition	Computation and Language	https://arxiv.org/abs/2504.21801	DeepSeek-Prover-V2  DeepSeek-Prover-V2 is an LLM (671B) that significantly advances formal theorem proving in Lean 4. The model is built through a novel cold-start training pipeline that combines informal chain-of-thought reasoning with formal subgoal decomposition, enhanced through reinforcement learning. It surpasses prior state-of-the-art on multiple theorem-proving benchmarks. Key highlights:   <br>● Cold-start data via recursive decomposition: The  authors prompt DeepSeek-V3 to generate natural-language proof  sketches, decompose them into subgoals, and formalize these steps in  Lean with sorry placeholders. A 7B prover model then recursively fills  in the subgoal proofs, enabling efficient construction of complete  formal proofs and training data.   <br>● Curriculum learning + RL: A subgoal-based curriculum  trains the model on increasingly complex problems. Reinforcement  learning with a consistency reward is used to enforce alignment  between proof structure and CoT decomposition, improving performance  on complex tasks.   <br>● Dual proof generation modes: The model is trained in  two modes, non-CoT (efficient, minimal proofs) and CoT  (high-precision, interpretable). The CoT mode yields significantly  better performance, particularly on hard problems.   <br>● Benchmark results:	https://x.com/zhs05232838/status/1917600755936018715		2504.21801	['Z.Z. Ren', 'Zhihong Shao', 'Junxiao Song', 'Huajian Xin', 'Haocheng Wang', 'Wanjia Zhao', 'Liyue Zhang', 'Zhe Fu', 'Qihao Zhu', 'Dejian Yang', 'Z.F. Wu', 'Zhibin Gou', 'Shirong Ma', 'Hongxuan Tang', 'Yuxuan Liu', 'Wenjun Gao', 'Daya Guo', 'Chong Ruan']	ct:We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2504.21801', 'html': 'https://arxiv.org/html/2504.21801v2', 'tex': '/src/2504.21801', 'doi': 'https://doi.org/10.48550/arXiv.2504.21801'}	Submission history From: Zhihong Shao [ view email ] [v1] Wed, 30 Apr 2025 16:57:48 UTC (1,239 KB) [v2] Fri, 18 Jul 2025 08:20:23 UTC (1,245 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.21801'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.21801'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.21801'}]
2025-05-04	Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems	Artificial Intelligence	https://arxiv.org/abs/2504.01990	Advances and Challenges in Foundation Agents  A new survey frames intelligent agents with a modular, brain-inspired architecture that integrates ideas from cognitive science, neuroscience, and computational research. Key topics covered:   <br>● Human Brain and LLM Agents: Helps to better  understand what differentiates LLM agents from human/brain cognition,  and what inspirations we can get from the way humans learn and  operate.   <br>● Definitions: Provides a nice, detailed, and formal definition of  what makes up an AI agent.   <br>● Reasoning: It has a detailed section on the core components of  intelligent agents. There is a deep dive into reasoning, which is one  of the key development areas of AI agents and what unlocks things like  planning, multi-turn tooling, backtracking, and much more.   <br>● Memory: Agent memory is a challenging area of building agentic  systems, but there is already a lot of good literature out there from  which to get inspiration.   <br>● Action Systems: You can already build very complex agentic  systems today, but the next frontier is agents that take actions and  make decisions in the real world. We need better tooling, better  training algorithms, and robust operation in different action spaces.   <br>● Self-Evolving Agents: For now, building effective agentic  systems requires human effort and careful optimization tricks.  However, one of the bigger opportunities in the field is to build AI  that can itself build powerful and self-improving AI systems.	https://x.com/omarsar0/status/1916542394746421333		2504.01990	['Bang Liu', 'Xinfeng Li', 'Jiayi Zhang', 'Jinlin Wang', 'Tanjin He', 'Sirui Hong', 'Hongzhang Liu', 'Shaokun Zhang', 'Kaitao Song', 'Kunlun Zhu', 'Yuheng Cheng', 'Suyuchen Wang', 'Xiaoqiang Wang', 'Yuyu Luo', 'Haibo Jin', 'Peiyan Zhang', 'Ollie Liu', 'Jiaqi Chen', 'Huan Zhang', 'Zhaoyang Yu', 'Haochen Shi', 'Boyan Li', 'Dekun Wu', 'Fengwei Teng', 'Xiaojun Jia', 'Jiawei Xu', 'Jinyu Xiang', 'Yizhang Lin', 'Tianming Liu', 'Tongliang Liu', 'Yu Su', 'Huan Sun', 'Glen Berseth', 'Jianyun Nie', 'Ian Foster', 'Logan Ward', 'Qingyun Wu', 'Yu Gu', 'Mingchen Zhuge', 'Xinbing Liang', 'Xiangru Tang', 'Haohan Wang', 'Jiaxuan You', 'Chi Wang', 'Jian Pei', 'Qiang Yang', 'Xiaoliang Qi', 'Chenglin Wu']	ct:The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This book provides a comprehensive overview, framing intelligent agents within modular, brain-inspired architectures that integrate principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we systematically investigate the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities and elucidating core components such as memory, world modeling, reward processing, goal, and emotion. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms. Third, we examine multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures. Finally, we address the critical imperative of building safe and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment. By synthesizing modular AI architectures with insights from different disciplines, this survey identifies key research challenges and opportunities, encouraging innovations that harmonize technological advancement with meaningful societal benefit.		['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2504.01990', 'html': None, 'tex': '/src/2504.01990', 'doi': 'https://doi.org/10.48550/arXiv.2504.01990'}	Submission history From: Bang Liu [ view email ] [v1] Mon, 31 Mar 2025 18:00:29 UTC (29,647 KB) [v2] Sat, 2 Aug 2025 12:44:02 UTC (35,518 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.01990'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.01990'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.01990'}]
2025-05-04	MAGI: Multi-Agent Guided Interview for Psychiatric Assessment	Computation and Language	https://arxiv.org/abs/2504.18260	MAGI  MAGI is a multi-agent system designed to automate structured psychiatric interviews by operationalizing the MINI (Mini International Neuropsychiatric Interview) protocol. It involves 4 specialized agents: navigation, question generation, judgment, and diagnosis. Other highlights:   <br>● Multi-Agent Clinical Workflow: MAGI is built with a  navigation agent (interview flow control), a question agent (dynamic,  empathetic probing), a judgment agent (response validation), and a  diagnosis agent using Psychometric CoT to trace diagnoses explicitly  to MINI/DSM-5 criteria.   <br>● Explainable Reasoning (PsyCoT): Instead of treating  diagnoses as opaque outputs, PsyCoT decomposes psychiatric reasoning  into symptom anchoring, syndromal validation, and evidence binding.  This helps with auditability for each diagnostic conclusion. CoT put  to great use.   <br>● Results: Evaluated on 1,002 real-world interviews, MAGI  outperforms baselines (Direct prompting, Role-play,  Knowledge-enhanced, and MINI-simulated LLMs) across relevance,  accuracy, completeness, and guidance.   <br>● Strong Clinical Agreement: Diagnostic evaluations show  PsyCoT consistently improves F1 scores, accuracy, and Cohen’s κ across  disorders like depression, generalized anxiety, social anxiety, and  suicide risk, reaching clinical-grade reliability (κ  0.8) in  high-risk tasks.	https://x.com/omarsar0/status/1916862752410554423		2504.18260	['Guanqun Bi', 'Zhuang Chen', 'Zhoufu Liu', 'Hongkai Wang', 'Xiyao Xiao', 'Yuqiang Xie', 'Wen Zhang', 'Yongkang Huang', 'Yuxuan Chen', 'Libiao Peng', 'Yi Feng', 'Minlie Huang']	ct:Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI's branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.	gress	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.18260', 'html': None, 'tex': '/src/2504.18260', 'doi': 'https://doi.org/10.48550/arXiv.2504.18260'}	Submission history From: Guanqun Bi [ view email ] [v1] Fri, 25 Apr 2025 11:08:27 UTC (1,741 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.18260'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.18260'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.18260'}]
2025-05-04	Taming the Titans: A Survey of Efficient LLM Inference Serving	Computation and Language	https://arxiv.org/abs/2504.19720	A Survey of Efficient LLM Inference Serving  This survey reviews recent advancements in optimizing LLM inference, addressing memory and computational bottlenecks. It covers instance-level techniques (like model placement and request scheduling), cluster-level strategies (like GPU deployment and load balancing), and emerging scenario-specific solutions, concluding with future research directions.			2504.19720	['Ranran Zhen', 'Juntao Li', 'Yixin Ji', 'Zhenlin Yang', 'Tong Liu', 'Qingrong Xia', 'Xinyu Duan', 'Zhefeng Wang', 'Baoxing Huai', 'Min Zhang']	ct:Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.	n progress;11 pages of main paper with 7 main figures, overall 20 pages	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Distributed, Parallel, and Cluster Computing (cs.DC)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2504.19720', 'html': 'https://arxiv.org/html/2504.19720v1', 'tex': '/src/2504.19720', 'doi': 'https://doi.org/10.48550/arXiv.2504.19720'}	Submission history From: Ranran Zhen [ view email ] [v1] Mon, 28 Apr 2025 12:14:02 UTC (1,200 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.19720'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.19720'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.19720'}]
2025-05-04	LLMs for Engineering: Teaching Models to Design High Powered Rockets	Software Engineering	https://arxiv.org/abs/2504.19394	LLM for Engineering  This work finds that when RL is used, a 7B parameter model outperforms both SoTA foundation models and human experts at high-powered rocketry design.			2504.19394	['Toby Simonds']	ct:Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development.		['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2504.19394', 'html': 'https://arxiv.org/html/2504.19394v2', 'tex': '/src/2504.19394', 'doi': 'https://doi.org/10.48550/arXiv.2504.19394'}	Submission history From: Toby Simonds [ view email ] [v1] Sun, 27 Apr 2025 23:59:39 UTC (638 KB) [v2] Tue, 29 Apr 2025 22:15:42 UTC (638 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.19394'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.19394'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.19394'}]
2025-04-27	Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?	Artificial Intelligence	https://arxiv.org/abs/2504.13837	Does RL Incentivize Reasoning in LLMs Beyond the Base Model?  This paper revisits a key assumption in recent LLM development: that Reinforcement Learning with Verifiable Rewards (RLVR) helps models acquire genuinely new reasoning capabilities. By analyzing models across tasks (math, code, vision) using pass@k metrics (with large k), the authors find that RLVR improves sample efficiency but does not expand reasoning capacity beyond the base model.   <br>● Key insight: RLVR-trained models do better at low *k* (e.g.,  pass@1), but as *k* increases (up to 256 or more), base models  eventually match or outperform them. This suggests RLVR doesn’t  generate fundamentally new reasoning paths but just increases the  likelihood of sampling already-existing correct ones.   <br>● Reasoning already in the base: RLVR models'  successful CoTs are shown to be present within the base model's  sampling distribution. Perplexity analyses confirm that RL outputs are  often high-probability continuations for the base model.   <br>● Efficiency vs. exploration: RLVR narrows the model’s  exploration space, improving efficiency but shrinking its coverage of  diverse reasoning paths, thereby reducing overall problem-solving  reach at scale.   <br>● Distillation helps more: Unlike RLVR, distillation from  a stronger teacher model (e.g., DeepSeek-R1) introduces genuinely new  reasoning patterns, expanding the model’s capabilities.   <br>● Algorithmic limits: Across PPO, GRPO, Reinforce++, etc., RL  algorithms offer similar sample-efficiency improvements, but none  closes the gap to the base model’s pass@256—highlighting the limits of  current RL strategies.	https://x.com/DaveShapi/status/1915408405201629684		2504.13837	['Yang Yue', 'Zhiqi Chen', 'Rui Lu', 'Andrew Zhao', 'Zhaokai Wang', 'Yang Yue', 'Shiji Song', 'Gao Huang']	ct:Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.	es, 27 figures	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2504.13837', 'html': 'https://arxiv.org/html/2504.13837v2', 'tex': '/src/2504.13837', 'doi': 'https://doi.org/10.48550/arXiv.2504.13837'}	Submission history From: Yang Yue [ view email ] [v1] Fri, 18 Apr 2025 17:59:56 UTC (1,566 KB) [v2] Fri, 16 May 2025 15:39:33 UTC (1,813 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.13837'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.13837'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.13837'}]
2025-04-27	BitNet b1.58 2B4T Technical Report	Computation and Language	https://arxiv.org/abs/2504.12285	BitNet b1.58 2B4T  This work introduces BitNet b1.58 2B4T, the first open-source, natively trained 1-bit LLM at the 2B parameter scale, achieving strong performance while being extremely efficient. The model uses a  custom ternary quantization scheme (1.58 bits per weight), enabling dramatic reductions in memory (0.4 GB), energy (0.028J/token), and latency (29ms), while still competing with state-of-the-art  full-precision models across diverse benchmarks.   <br>● New Pareto frontier in efficiency-performance:  Trained from scratch on 4T tokens, BitNet b1.58 2B4T outperforms or  matches open full-precision models (e.g., Qwen2.5 1.5B, MiniCPM 2B) on  tasks like ARC-Challenge, PIQA, WinoGrande, and GSM8K. It achieves  54.19% average. across 16 benchmarks, comparable to Qwen2.5-1.5B’s  55.23%, but with ~6.5× lower memory and 10× lower energy usage.   <br>● Outperforms quantized baselines: Against INT4  post-training quantized Qwen2.5 models (GPTQ/AWQ), BitNet is both  smaller and more accurate, showing the advantage of native 1-bit  training over PTQ approaches.   <br>● Architectural & training innovations: It replaces  standard linear layers with BitLinear layers using absmean ternary  quantization and 8-bit activations, combines RoPE embeddings, squared  ReLU activation, and bias-free layers. Training includes cosine LR and  weight decay schedules, plus supervised fine-tuning and Direct  Preference Optimization (DPO) instead of full RLHF.   <br>● Best-in-class among 1-bit LLMs: When compared to  other 1-bit models like OLMo-Bitnet (1B) and post-quantized  Falcon3/Llama3 (7B–8B), BitNet b1.58 2B4T is +10 pts stronger on  average, establishing a new benchmark for ultra-efficient LLMs.  The authors also release optimized CUDA kernels for GPU and a C++ inference library for CPU, enabling practical deployment of 1-bit LLMs on diverse hardware. BitNet b1.58 2B4T demonstrates that extreme quantization does not mean compromised capability, and it opens the door to the broader adoption of LLMs in resource-constrained environments.			2504.12285	['Shuming Ma', 'Hongyu Wang', 'Shaohan Huang', 'Xingxing Zhang', 'Ying Hu', 'Ting Song', 'Yan Xia', 'Furu Wei']	ct:We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.	n progress	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2504.12285', 'html': 'https://arxiv.org/html/2504.12285v2', 'tex': '/src/2504.12285', 'doi': 'https://doi.org/10.48550/arXiv.2504.12285'}	Submission history From: Shuming Ma [ view email ] [v1] Wed, 16 Apr 2025 17:51:43 UTC (67 KB) [v2] Fri, 25 Apr 2025 03:07:55 UTC (67 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.12285'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.12285'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.12285'}]
2025-04-27	UI-TARS: Pioneering Automated GUI Interaction with Native Agents	Artificial Intelligence	https://arxiv.org/abs/2501.12326	UI-TARS  UI-TARS introduces a powerful, end-to-end native GUI agent that operates purely from visual screenshots, performing human-like keyboard and mouse interactions across platforms. Unlike existing modular agent frameworks that rely on prompt engineering and external scripts, UI-TARS integrates perception, action, reasoning, and memory directly into its architecture, achieving strong generalization and adaptability in dynamic real-world settings.  Key contributions:   <br>● Enhanced GUI Perception: UI-TARS is trained on a  large-scale, richly annotated dataset of screenshots with metadata,  enabling dense captioning, state transition understanding, and precise  element description. It excels in perception benchmarks like  VisualWebBench, scoring 82.8, outperforming GPT-4o’s.   <br>● Unified Action Modeling and Grounding: UI-TARS  standardizes actions across platforms into a shared action space and  learns from large-scale multi-step action traces. It surpasses  baselines in grounding tasks with 38.1 on ScreenSpot Pro, the new  SOTA.   <br>● System-2 Reasoning via “Thoughts”: Inspired by  ReAct-style frameworks, UI-TARS generates internal reasoning steps  (thoughts) before actions. These thoughts reflect patterns like task  decomposition, reflection, and long-term consistency, significantly  improving performance in complex scenarios. For example, in OSWorld,  UI-TARS-72B-DPO scores 24.6 with a 50-step budget, outperforming  Claude’s.   <br>● Iterative Self-Improvement with Reflective  Learning: UI-TARS continuously refines itself through online trace  collection and reflection tuning using error correction and post-error  adaptation data. This allows it to recover from mistakes and adapt  with minimal human oversight.  Overall, UI-TARS marks a significant step forward in GUI automation, setting new benchmarks across more than 10 datasets and outperforming top commercial agents like GPT-4o and Claude. Its  open-source release aims to drive further innovation in native agent development.		"{""Blog"": ""https://seed-tars.com/1.5/""}"	2501.12326	['Yujia Qin', 'Yining Ye', 'Junjie Fang', 'Haoming Wang', 'Shihao Liang', 'Shizuo Tian', 'Junda Zhang', 'Jiahao Li', 'Yunxin Li', 'Shijue Huang', 'Wanjun Zhong', 'Kuanye Li', 'Jiale Yang', 'Yu Miao', 'Woyu Lin', 'Longxiang Liu', 'Xu Jiang', 'Qianli Ma', 'Jingyu Li', 'Xiaojun Xiao', 'Kai Cai', 'Chuang Li', 'Yaowei Zheng', 'Chaolin Jin', 'Chen Li', 'Xiao Zhou', 'Minchao Wang', 'Haoli Chen', 'Zhaojian Li', 'Haihua Yang', 'Haifeng Liu', 'Feng Lin', 'Tao Peng', 'Xin Liu', 'Guang Shi']	ct:This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2501.12326', 'html': 'https://arxiv.org/html/2501.12326v1', 'tex': '/src/2501.12326', 'doi': 'https://doi.org/10.48550/arXiv.2501.12326'}	Submission history From: Yujia Qin [ view email ] [v1] Tue, 21 Jan 2025 17:48:10 UTC (44,822 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.12326'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.12326'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.12326'}]
2025-04-27	Describe Anything: Detailed Localized Image and Video Captioning	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2504.16072	Describe Anything  Introduces DAM, a model that generates fine-grained, region-specific captions in both images and videos. The authors address key limitations in prior vision-language models—namely, the inability to preserve local detail and the lack of suitable datasets and benchmarks for detailed localized captioning (DLC).  Key contributions:   <br>● DAM (Describe Anything Model) uses two main  innovations to capture both fine regional detail and global scene  context: a focal prompt that provides high-resolution encoding of  user-specified regions, and a localized vision backbone that uses  gated cross-attention to integrate context from the entire image. This  enables DAM to generate multi-granular, accurate descriptions,  especially for small or occluded regions.   <br>● DLC-SDP (Semi-supervised Data Pipeline) tackles data  scarcity by expanding segmentation datasets with VLM-generated  detailed captions, followed by self-training on web images. This  produces high-quality, diverse training data, enabling DAM to  outperform API-only baselines like GPT-4o across several benchmarks.   <br>● DLC-Bench is a reference-free benchmark that scores models on  their ability to accurately include or exclude region-specific details  using LLM judges. It provides a more reliable evaluation than  traditional caption-matching metrics, which often penalize models for  valid but unmatched details.   <br>● Performance: DAM sets a new state-of-the-art on 7 benchmarks  across keyword, phrase, and detailed multi-sentence captioning tasks  in both images and videos. It outperforms GPT-4o, Claude 3.7, and  other top VLMs in both zero-shot and in-domain evaluations, achieving  up to 33.4% improvement over prior models on detailed image captioning  and 19.8% on video captioning.			2504.16072	['Long Lian', 'Yifan Ding', 'Yunhao Ge', 'Sifei Liu', 'Hanzi Mao', 'Boyi Li', 'Marco Pavone', 'Ming-Yu Liu', 'Trevor Darrell', 'Adam Yala', 'Yin Cui']	ct:Generating detailed and accurate descriptions for specific regions in images and videos remains a fundamental challenge for vision-language models. We introduce the Describe Anything Model (DAM), a model designed for detailed localized captioning (DLC). DAM preserves both local details and global context through two key innovations: a focal prompt, which ensures high-resolution encoding of targeted regions, and a localized vision backbone, which integrates precise localization with its broader context. To tackle the scarcity of high-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and expands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark designed to evaluate DLC without relying on reference captions. DAM sets new state-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and detailed multi-sentence localized image and video captioning.	t page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2504.16072', 'html': None, 'tex': '/src/2504.16072', 'doi': 'https://doi.org/10.48550/arXiv.2504.16072'}	Submission history From: Long Lian [ view email ] [v1] Tue, 22 Apr 2025 17:51:41 UTC (24,062 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.16072'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.16072'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.16072'}]
2025-04-27	UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents	Computation and Language	https://arxiv.org/abs/2504.09407	UXAgent  Introduces a novel framework, UXAgent, for simulating large-scale usability testing using LLM-driven agents. The system empowers UX researchers to test and iterate web design and study protocols before engaging real users. This is achieved through the orchestration of simulated agents with diverse personas interacting in real web environments, providing both behavioral and reasoning data. Key highlights:   <br>● LLM-Powered Simulation with Personas: UXAgent begins  with a Persona Generator that can produce thousands of demographically  diverse simulated users based on custom distributions. Each persona is  fed into an LLM Agent that embodies user intent and interacts with the  website via a Universal Browser Connector—a module capable of  interpreting and manipulating real HTML structures.   <br>● Dual-Loop Reasoning Architecture: At the heart of  UXAgent is a dual-process agent architecture inspired by cognitive  psychology: a Fast Loop for low-latency actions and a Slow Loop for  deep reasoning. This design mimics System 1 and System 2 thinking and  allows agents to act responsively while maintaining coherent  high-level plans and reflections.   <br>● Rich Memory Stream: All observations, actions, plans,  reflections, and spontaneous thoughts (“wonders”) are stored in a  Memory Stream. These memories are dynamically prioritized for  retrieval using a weighted scoring system based on importance,  recency, and relevance, tailored separately for fast and slow modules.   <br>● Replay and Interview Interfaces: UX researchers can  review simulated sessions via a Simulation Replay Interface and  conduct natural language conversations with agents using an   Agent Interview Interface. This supports qualitative analysis, such as  asking agents about their decisions or presenting mockups for  feedback.   <br>● Empirical Evaluation: A case study involving 60 LLM agent  simulations on a shopping platform (WebArena) showed that researchers  were able to detect usability study flaws and gather early insights. A  follow-up user study with five UX professionals found the system  helpful for iterating study design, despite some concerns over realism  and data noise. Particularly appreciated was the ability to converse  with agents and gather qualitative insights that would be infeasible  in traditional pilots.   <br>● Future Implications: The authors position LLM agents not as  replacements for real participants, but as early-stage collaborators  in the design process, reducing the cost and risk of flawed studies.  They also discuss extensions to multimodal settings, desktop or mobile  interfaces, and broader agentic tasks such as digital twins or  simulated A/B testing.			2504.09407	['Yuxuan Lu', 'Bingsheng Yao', 'Hansu Gu', 'Jing Huang', 'Jessie Wang', 'Yang Li', 'Jiri Gesi', 'Qi He', 'Toby Jia-Jun Li', 'Dakuo Wang']	ct:Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate a web design, but\textbf{ how to evaluate and iterate the usability testing study design } itself? Recent advances in Large Language Model-simulated Agent (\textbf{LLM Agent}) research inspired us to design \textbf{UXAgent} to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users to interactively test the target website. The system also provides an Agent Interview Interface and a Video Replay Interface so that the UX researchers can easily review and analyze the generated qualitative and quantitative log data. Through a heuristic evaluation, five UX researcher participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.		['Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2504.09407', 'html': 'https://arxiv.org/html/2504.09407v2', 'tex': '/src/2504.09407', 'doi': 'https://doi.org/10.48550/arXiv.2504.09407'}	Submission history From: Yuxuan Lu [ view email ] [v1] Sun, 13 Apr 2025 02:34:22 UTC (5,562 KB) [v2] Mon, 21 Apr 2025 05:22:55 UTC (6,247 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.09407'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.09407'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.09407'}]
2025-04-27	TTRL: Test-Time Reinforcement Learning	Computation and Language	https://www.arxiv.org/abs/2504.16084	Test-Time Reinforcement Learning  Test-Time Reinforcement Learning (TTRL) is a method that allows LLMs to improve themselves during inference without ground-truth labels. Instead of relying on labeled datasets, TTRL uses majority voting over multiple model generations to estimate pseudo-rewards, enabling reinforcement learning (RL) on unlabeled test data. The method integrates Test-Time Scaling (TTS) and Test-Time Training (TTT) strategies, letting models adapt dynamically to new and challenging inputs.  Key highlights:   <br>● Majority Voting as Reward: TTRL generates multiple  candidate outputs for a query and uses majority voting to derive a  pseudo-label. Rewards are assigned based on agreement with the  consensus answer.   <br>● Significant Performance Gains: Applying TTRL to  Qwen2.5-Math-7B leads to a +159% improvement on AIME 2024 and +84%  average gains across AIME, AMC, and MATH-500 benchmarks, without using  any labeled training data.   <br>● Self-Evolution Beyond Supervision: Remarkably, TTRL  surpasses the performance ceiling of its own majority-vote supervision  (Maj@N) and approaches the performance of models trained with full  label leakage, indicating efficient and stable unsupervised RL.   <br>● Generalization and Robustness: TTRL generalizes well  across tasks, maintains effectiveness even under label estimation  noise, and is compatible with different RL algorithms like PPO and  GRPO.   <br>● Limitations: TTRL may fail when the base model lacks sufficient  prior knowledge about the domain or when hyperparameters (like batch  size and temperature) are poorly tuned.			2504.16084	['Yuxin Zuo', 'Kaiyan Zhang', 'Li Sheng', 'Shang Qu', 'Ganqu Cui', 'Xuekai Zhu', 'Haozhan Li', 'Yuchen Zhang', 'Xinwei Long', 'Ermo Hua', 'Biqing Qi', 'Youbang Sun', 'Zhiyuan Ma', 'Lifan Yuan', 'Ning Ding', 'Bowen Zhou']	ct:This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 211% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the maj@n metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model maj@n, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks and highlight TTRL's potential for broader tasks and domains. GitHub:this https URL		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2504.16084', 'html': None, 'tex': '/src/2504.16084', 'doi': 'https://doi.org/10.48550/arXiv.2504.16084'}	Submission history From: Kaiyan Zhang [ view email ] [v1] Tue, 22 Apr 2025 17:59:56 UTC (260 KB) [v2] Thu, 22 May 2025 16:26:55 UTC (265 KB) [v3] Mon, 30 Jun 2025 15:59:26 UTC (277 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.16084'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.16084'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.16084'}]
2025-04-27	Evaluating the Goal-Directedness of Large Language Models	Artificial Intelligence	https://arxiv.org/abs/2504.11844	Evaluate the Goal-Directedness of LLMs  Introduces a new framework to assess whether LLMs use their capabilities effectively toward achieving given goals. The study finds that even top models like GPT-4o and Claude 3.7 fall short of full goal-directedness, particularly in information-gathering and combined tasks, despite performing well in isolated subtasks.	https://x.com/tom4everitt/status/1912806499862139275	"{""GitHub"": ""https://github.com/Crista23/goal_directedness_llms""}"	2504.11844	['Tom Everitt', 'Cristina Garbacea', 'Alexis Bellot', 'Jonathan Richens', 'Henry Papadatos', 'Siméon Campos', 'Rohin Shah']	ct:To what extent do LLMs use their capabilities towards their given goal? We take this as a measure of their goal-directedness. We evaluate goal-directedness on tasks that require information gathering, cognitive effort, and plan execution, where we use subtasks to infer each model's relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI, and Anthropic show that goal-directedness is relatively consistent across tasks, differs from task performance, and is only moderately sensitive to motivational prompts. Notably, most models are not fully goal-directed. We hope our goal-directedness evaluations will enable better monitoring of LLM progress, and enable more deliberate design choices of agentic properties in LLMs.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2504.11844', 'html': 'https://arxiv.org/html/2504.11844v1', 'tex': '/src/2504.11844', 'doi': 'https://doi.org/10.48550/arXiv.2504.11844'}	Submission history From: Tom Everitt [ view email ] [v1] Wed, 16 Apr 2025 08:07:08 UTC (1,709 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.11844'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.11844'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.11844'}]
2025-04-27	Tina: Tiny Reasoning Models via LoRA	Computation and Language	https://arxiv.org/abs/2504.15777	Tiny Reasoning Models  Tina is a family of 1.5B parameter reasoning models trained using LoRA-based reinforcement learning (RL) to achieve high reasoning accuracy at very low cost. It outperforms or matches full fine-tuned models on reasoning tasks like AIME and MATH with only ~$9 post-training cost, demonstrating that efficient reasoning can be instilled via minimal updates to a tiny model.			2504.15777	['Shangshang Wang', 'Julian Asilis', 'Ömer Faruk Akgül', 'Enes Burak Bilgin', 'Ollie Liu', 'Willie Neiswanger']	ct:How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\% reasoning performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \& checkpoints.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2504.15777', 'html': 'https://arxiv.org/html/2504.15777v1', 'tex': '/src/2504.15777', 'doi': 'https://doi.org/10.48550/arXiv.2504.15777'}	Submission history From: Shangshang Wang [ view email ] [v1] Tue, 22 Apr 2025 10:38:00 UTC (6,676 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.15777'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.15777'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.15777'}]
2025-04-20	GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2504.10458	GUI-R1  Researchers from the National University of Singapore and the Chinese Academy of Sciences introduce GUI-R1, a reinforcement learning (RL) framework aimed at improving graphical user interface (GUI) agents through unified action-space modeling. Key insights include:   <br>● Reinforcement Fine-Tuning (RFT) over Supervised  Fine-Tuning (SFT)			2504.10458	['Run Luo', 'Lu Wang', 'Wanwei He', 'Xiaobo Xia']	ct:Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks.		['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2504.10458', 'html': 'https://arxiv.org/html/2504.10458v3', 'tex': '/src/2504.10458', 'doi': 'https://doi.org/10.48550/arXiv.2504.10458'}	Submission history From: Run Luo [ view email ] [v1] Mon, 14 Apr 2025 17:45:54 UTC (580 KB) [v2] Tue, 15 Apr 2025 14:42:43 UTC (580 KB) [v3] Fri, 18 Apr 2025 16:57:48 UTC (580 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.10458'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.10458'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.10458'}]
2025-04-20	d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning	Computation and Language	https://arxiv.org/abs/2504.12216	Scaling Reasoning in Diffusion LLMs via RL  Proposes d1, a two‑stage recipe that equips masked diffusion LLMs with strong step‑by‑step reasoning.   <br>● Two‑stage pipeline (SFT → diffu‑GRPO)			2504.12216	['Siyan Zhao', 'Devaansh Gupta', 'Qinqing Zheng', 'Aditya Grover']	ct:Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. Our code is released atthis https URL.	es, project page atthis https URL	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2504.12216', 'html': None, 'tex': '/src/2504.12216', 'doi': 'https://doi.org/10.48550/arXiv.2504.12216'}	Submission history From: Siyan Zhao [ view email ] [v1] Wed, 16 Apr 2025 16:08:45 UTC (649 KB) [v2] Tue, 3 Jun 2025 17:02:25 UTC (451 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.12216'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.12216'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.12216'}]
2025-04-20	Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability	Computation and Language	https://arxiv.org/abs/2504.09639	Enhancing Non-Reasoning Models with Reasoning Models  Researchers explore how to distill reasoning-intensive outputs (answers and explanations) from top-tier LLMs into more lightweight models that don’t explicitly reason step by step. By fine-tuning smaller models on the high-quality final answers (and optionally summarized thinking traces) from advanced reasoning models, they demonstrate consistent performance boosts across multiple benchmarks.   <br>● Test-time scaling vs. knowledge distillation			2504.09639	['Haotian Wang', 'Han Zhao', 'Shuaiting Chen', 'Xiaoyu Tian', 'Sitong Zhao', 'Yunjie Ji', 'Yiping Peng', 'Xiangang Li']	"ct:Recent advancements in large language models (LLMs), such as DeepSeek-R1 and OpenAI-o1, have demonstrated the significant effectiveness of test-time scaling, achieving substantial performance gains across various benchmarks. These advanced models utilize deliberate ""thinking"" steps to systematically enhance answer quality. In this paper, we propose leveraging these high-quality outputs generated by reasoning-intensive models to improve less computationally demanding, non-reasoning models. We explore and compare methodologies for utilizing the answers produced by reasoning models to train and improve non-reasoning models. Through straightforward Supervised Fine-Tuning (SFT) experiments on established benchmarks, we demonstrate consistent improvements across various benchmarks, underscoring the potential of this approach for advancing the ability of models to answer questions directly."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.09639', 'html': 'https://arxiv.org/html/2504.09639v1', 'tex': '/src/2504.09639', 'doi': 'https://doi.org/10.48550/arXiv.2504.09639'}	Submission history From: Yunjie Ji [ view email ] [v1] Sun, 13 Apr 2025 16:26:56 UTC (2,777 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.09639'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.09639'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.09639'}]
2025-04-20	AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents	Human-Computer Interaction	https://arxiv.org/abs/2504.09723	AgentA/B  AgentA/B is a fully automated A/B testing framework that replaces live human traffic with large-scale LLM-based agents. These agents simulate realistic, intention-driven user behaviors on actual web environments, enabling faster, cheaper, and risk-free UX evaluations			2504.09723	['Dakuo Wang', 'Ting-Yao Hsu', 'Yuxuan Lu', 'Hansu Gu', 'Limeng Cui', 'Yaochen Xie', 'William Headean', 'Bingsheng Yao', 'Akash Veeragouni', 'Jiapeng Liu', 'Sreyashi Nag', 'Jessie Wang']	ct:A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agentsthis http URL, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.		['Human-Computer Interaction (cs.HC)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.09723', 'html': 'https://arxiv.org/html/2504.09723v2', 'tex': '/src/2504.09723', 'doi': 'https://doi.org/10.48550/arXiv.2504.09723'}	Submission history From: Yuxuan Lu [ view email ] [v1] Sun, 13 Apr 2025 21:10:56 UTC (2,379 KB) [v2] Mon, 21 Apr 2025 23:57:49 UTC (2,379 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.09723'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.09723'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.09723'}]
2025-04-20	Reasoning Models Can Be Effective Without Thinking	Artificial Intelligence	https://www.arxiv.org/abs/2504.09858	"Reasoning Models Can Be Effective Without Thinking  This paper challenges the necessity of long chain-of-thought (CoT) reasoning in LLMs by introducing a simple prompting method called NoThinking, which bypasses explicit ""thinking"" steps. Surprisingly, NoThinking performs comparably to or better than traditional reasoning under comparable or even lower compute budgets, especially when paired with parallel decoding and best-of-N selection.  Key Insights:   <br>● NoThinking prepends a dummy “Thinking” block and jumps straight to  final answers.   <br>● Despite skipping structured reasoning, it outperforms Thinking in  pass@k (1–64) on many benchmarks, especially under token constraints.   <br>● With parallel scaling, NoThinking achieves higher pass@1 accuracy  than Thinking while using 4× fewer tokens and up to 9× lower latency.   <br>● Tasks evaluated: competitive math (AIME24/25, AMC23, OlympiadBench),  coding (LiveCodeBench), and formal theorem proving (MiniF2F,  ProofNet).   <br>● NoThinking is shown to provide superior accuracy–latency tradeoffs  and generalizes across diverse tasks.  Results:   <br>● Low-budget wins: On AMC23 (700 tokens), NoThinking achieves 51.3%  vs. 28.9% (Thinking). <br>● Better scaling: As k increases, NoThinking  consistently surpasses Thinking.   <br>● Efficiency frontier: Across benchmarks, NoThinking dominates the  accuracy–cost Pareto frontier.   <br>● Parallel wins: With simple confidence-based or majority vote  strategies, NoThinking + best-of-N beats full Thinking on pass@1 with  significantly less latency."			2504.09858	['Wenjie Ma', 'Jingxuan He', 'Charlie Snell', 'Tyler Griggs', 'Sewon Min', 'Matei Zaharia']	ct:Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.	es, 7 main figures, 2 tables	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.09858', 'html': None, 'tex': '/src/2504.09858', 'doi': 'https://doi.org/10.48550/arXiv.2504.09858'}	Submission history From: Wenjie Ma [ view email ] [v1] Mon, 14 Apr 2025 04:08:16 UTC (1,044 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.09858'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.09858'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.09858'}]
2025-04-20	SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users	Computation and Language	https://arxiv.org/abs/2504.10157	SocioVerse  Researchers from Fudan University and collaborators propose SocioVerse, a large-scale world model for social simulation using LLM agents aligned with real-world user behavior. Key ideas include:   <br>● Four-fold alignment framework			2504.10157	['Xinnong Zhang', 'Jiayu Lin', 'Xinyi Mou', 'Shiyue Yang', 'Xiawei Liu', 'Libo Sun', 'Hanjia Lyu', 'Yihang Yang', 'Weihong Qi', 'Yue Chen', 'Guanying Li', 'Ling Yan', 'Yao Hu', 'Siming Chen', 'Yu Wang', 'Xuanjing Huang', 'Jiebo Luo', 'Shiping Tang', 'Libo Wu', 'Baohua Zhou', 'Zhongyu Wei']	ct:Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.		['Computation and Language (cs.CL)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2504.10157', 'html': 'https://arxiv.org/html/2504.10157v3', 'tex': '/src/2504.10157', 'doi': 'https://doi.org/10.48550/arXiv.2504.10157'}	Submission history From: Xinnong Zhang [ view email ] [v1] Mon, 14 Apr 2025 12:12:52 UTC (1,553 KB) [v2] Wed, 23 Apr 2025 06:08:32 UTC (1,553 KB) [v3] Tue, 15 Jul 2025 11:14:36 UTC (1,553 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.10157'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.10157'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.10157'}]
2025-04-20	DocAgent: A Multi-Agent System for Automated Code Documentation Generation	Software Engineering	https://arxiv.org/abs/2504.08725	DocAgent  Researchers from Meta AI present DocAgent, a tool‑integrated, dependency‑aware framework that turns large, complex codebases into well‑written docstrings. Key ideas include:   <br>● Topological Navigator for context building			2504.08725	['Dayu Yang', 'Antoine Simoulin', 'Xin Qian', 'Xiaoyi Liu', 'Yuwei Cao', 'Zhaopu Teng', 'Grey Yang']	ct:High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.	ed by ACL 2025. Code:this http URL	['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2504.08725', 'html': 'https://arxiv.org/html/2504.08725v3', 'tex': '/src/2504.08725', 'doi': 'https://doi.org/10.48550/arXiv.2504.08725'}	Submission history From: Dayu Yang [ view email ] [v1] Fri, 11 Apr 2025 17:50:08 UTC (8,514 KB) [v2] Fri, 18 Apr 2025 04:32:43 UTC (8,514 KB) [v3] Fri, 23 May 2025 21:30:16 UTC (8,514 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.08725'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.08725'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.08725'}]
2025-04-20	SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents	Software Engineering	https://arxiv.org/abs/2504.08703v1	SWE-PolyBench  SWE-PolyBench is a new multi-language benchmark for evaluating coding agents on real-world software tasks across Java, JavaScript, TypeScript, and Python. It introduces execution-based assessments, syntax tree metrics, and reveals that current agents struggle with complex tasks and show inconsistent performance across languages.			2504.08703v1	['Muhammad Shihab Rashid', 'Christian Bock', 'Yuan Zhuang', 'Alexander Buccholz', 'Tim Esler', 'Simon Valentin', 'Luca Franceschi', 'Martin Wistuba', 'Prabhu Teja Sivaprasad', 'Woo Jung Kim', 'Anoop Deoras', 'Giovanni Zappella', 'Laurent Callot']	ct:Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at:this https URL	es, 6 figures	['Software Engineering (cs.SE)']	{'pdf': '/pdf/2504.08703v1', 'html': 'https://arxiv.org/html/2504.08703v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2504.08703'}	Submission history From: Muhammad Shihab Rashid [ view email ] [v1] Fri, 11 Apr 2025 17:08:02 UTC (601 KB) [v2] Mon, 14 Apr 2025 20:52:04 UTC (601 KB) [v3] Wed, 23 Apr 2025 20:22:37 UTC (601 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.08703'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.08703'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.08703'}]
2025-04-20	A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems	Artificial Intelligence	https://arxiv.org/abs/2504.09037	A Survey of Frontiers in LLM Reasoning  This survey categorizes LLM reasoning methods by when reasoning occurs (inference-time vs. training) and the system's architecture (standalone vs. agentic or multi-agent). It highlights trends like learning-to-reason (e.g., DeepSeek-R1) and agentic workflows (e.g., OpenAI Deep Research), covering prompt engineering, output refinement, and learning strategies such as PPO and verifier training.			2504.09037	['Zixuan Ke', 'Fangkai Jiao', 'Yifei Ming', 'Xuan-Phi Nguyen', 'Austin Xu', 'Do Xuan Long', 'Minzhi Li', 'Chengwei Qin', 'Peifeng Wang', 'Silvio Savarese', 'Caiming Xiong', 'Shafiq Joty']	ct:Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that distinguishes advanced AI systems from conventional models that empower chatbots. In this survey, we categorize existing methods along two orthogonal dimensions: (1) Regimes, which define the stage at which reasoning is achieved (either at inference time or through dedicated training); and (2) Architectures, which determine the components involved in the reasoning process, distinguishing between standalone LLMs and agentic compound systems that incorporate external tools, and multi-agent collaborations. Within each dimension, we analyze two key perspectives: (1) Input level, which focuses on techniques that construct high-quality prompts that the LLM condition on; and (2) Output level, which methods that refine multiple sampled candidates to enhance reasoning quality. This categorization provides a systematic understanding of the evolving landscape of LLM reasoning, highlighting emerging trends such as the shift from inference-scaling to learning-to-reason (e.g., DeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep Research, Manus Agent). Additionally, we cover a broad spectrum of learning algorithms, from supervised fine-tuning to reinforcement learning such as PPO and GRPO, and the training of reasoners and verifiers. We also examine key designs of agentic workflows, from established patterns like generator-evaluator and LLM debate to recent innovations. ...	es, 6 figures. Accepted to TMLR, with Survey Certification award	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.09037', 'html': 'https://arxiv.org/html/2504.09037v3', 'tex': '/src/2504.09037', 'doi': 'https://doi.org/10.48550/arXiv.2504.09037'}	Submission history From: Fangkai Jiao [ view email ] [v1] Sat, 12 Apr 2025 01:27:49 UTC (4,585 KB) [v2] Wed, 16 Jul 2025 11:33:35 UTC (1,194 KB) [v3] Tue, 5 Aug 2025 01:56:32 UTC (1,194 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.09037'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.09037'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.09037'}]
2025-04-20	A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science	Artificial Intelligence	https://arxiv.org/abs/2504.09848	Advances in Embodied Agents, Smart Cities, and Earth Science  This paper surveys how spatial intelligence manifests across disciplines—from embodied agents to urban and global systems—by connecting human spatial cognition with how LLMs handle spatial memory, representations, and reasoning. It offers a unifying framework to bridge research in AI, robotics, urban planning, and earth science, highlighting LLMs’ evolving spatial capabilities and their interdisciplinary potential.			2504.09848	['Jie Feng', 'Jinwei Zeng', 'Qingyue Long', 'Hongyi Chen', 'Jie Zhao', 'Yanxin Xi', 'Zhilun Zhou', 'Yuan Yuan', 'Shengyuan Wang', 'Qingbin Zeng', 'Songwei Li', 'Yunke Zhang', 'Yuming Lin', 'Tong Li', 'Jingtao Ding', 'Chen Gao', 'Fengli Xu', 'Yong Li']	ct:Over the past year, the development of large language models (LLMs) has brought spatial intelligence into focus, with much attention on vision-based embodied intelligence. However, spatial intelligence spans a broader range of disciplines and scales, from navigation and urban planning to remote sensing and earth science. What are the differences and connections between spatial intelligence across these fields? In this paper, we first review human spatial cognition and its implications for spatial intelligence in LLMs. We then examine spatial memory, knowledge representations, and abstract reasoning in LLMs, highlighting their roles and connections. Finally, we analyze spatial intelligence across scales -- from embodied to urban and global levels -- following a framework that progresses from spatial memory and understanding to spatial reasoning and intelligence. Through this survey, we aim to provide insights into interdisciplinary spatial intelligence research and inspire future studies.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.09848', 'html': 'https://arxiv.org/html/2504.09848v1', 'tex': '/src/2504.09848', 'doi': 'https://doi.org/10.48550/arXiv.2504.09848'}	Submission history From: Jie Feng [ view email ] [v1] Mon, 14 Apr 2025 03:38:31 UTC (2,225 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.09848'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.09848'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.09848'}]
2025-04-13	OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens	Computation and Language	https://arxiv.org/abs/2504.07096	"OLMOTrace  Allen Institute for AI & University of Washington present OLMOTRACE, a real-time system that traces LLM-generated text back to its verbatim sources in the original training data, even across  multi-trillion-token corpora.   <br>● What it does: For a given LM output, OLMOTRACE  highlights exact matches with training data segments and lets users  inspect full documents for those matches. Think   ""reverse-engineering"" a model’s response via lexical lookup. <br>● How  it works:   <br>● Supported models: Works with OLMo models (e.g.,  OLMo-2-32B-Instruct) and their full pre/mid/post-training datasets,  totaling 4.6T tokens.   <br>● Use cases:   <br>● Benchmarked:   <br>● Not RAG: It retrieves after generation, without changing  output, unlike retrieval-augmented generation."	https://x.com/omarsar0/status/1910323386603262316	"{""Blog"": ""https://5910970.hs-sites.com/olmotrace-points-model-output-back-to-training-data?ecid=ACsprvuggQcD4yCdO--rKTZKDvmczdSQkb96ct95zLH9eiysrXjF_WuKgsmIMaz8byfiL1H1-2A6&utm_campaign=AI2%20Newsletter&utm_medium=email&_hsenc=p2ANqtz-__MqUAVPXfHPpHpf2xC86iZG8qC3J-z5nW141VBN9gZW4j61ymW3dM7mhkiHGTWtjQt3Eao7Cqf7pB1k24CfEhYe9fmA&_hsmi=355925505""}"	2504.07096	['Jiacheng Liu', 'Taylor Blanton', 'Yanai Elazar', 'Sewon Min', 'YenSung Chen', 'Arnavi Chheda-Kothary', 'Huy Tran', 'Byron Bischoff', 'Eric Marsh', 'Michael Schmitz', 'Cassidy Trier', 'Aaron Sarnat', 'Jenna James', 'Jon Borchardt', 'Bailey Kuehl', 'Evie Cheng', 'Karen Farley', 'Sruthi Sreeram', 'Taira Anderson', 'David Albright', 'Carissa Schoenick', 'Luca Soldaini', 'Dirk Groeneveld', 'Rock Yuren Pang', 'Pang Wei Koh', 'Noah A. Smith', 'Sophie Lebrecht', 'Yejin Choi', 'Hannaneh Hajishirzi', 'Ali Farhadi', 'Jesse Dodge']	ct:We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.	25 demo track	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.07096', 'html': 'https://arxiv.org/html/2504.07096v2', 'tex': '/src/2504.07096', 'doi': 'https://doi.org/10.48550/arXiv.2504.07096'}	Submission history From: Jiacheng Liu [ view email ] [v1] Wed, 9 Apr 2025 17:59:35 UTC (6,674 KB) [v2] Tue, 8 Jul 2025 00:56:11 UTC (6,836 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.07096'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.07096'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.07096'}]
2025-04-13	Concise Reasoning via Reinforcement Learning	Computation and Language	https://arxiv.org/abs/2504.05185	Concise Reasoning via RL  This new paper proposes a new training strategy that promotes concise and accurate reasoning in LLMs using RL. It challenges the belief that long responses improve accuracy; it offers both theoretical and empirical evidence showing that conciseness often correlates with better performance.   <br>● Long ≠ better reasoning	https://x.com/omarsar0/status/1909634850304503977		2504.05185	['Mehdi Fatemi', 'Banafsheh Rafiee', 'Mingjie Tang', 'Kartik Talamadupula']	ct:Despite significant advancements in large language models (LLMs), a major drawback of reasoning models is their enormous token usage, which increases computational cost, resource requirements, and response time. In this work, we revisit the core principles of reinforcement learning (RL) and, through mathematical analysis, demonstrate that the tendency to generate lengthy responses arises inherently from RL-based optimization during training. This finding questions the prevailing assumption that longer responses inherently improve reasoning accuracy. Instead, we uncover a natural correlation between conciseness and accuracy that has been largely overlooked. We show that introducing a secondary phase of RL training, using a very small set of problems, can significantly reduce chains of thought while maintaining or even enhancing accuracy. Additionally, we demonstrate that, while GRPO shares some interesting properties of PPO, it suffers from collapse modes, which limit its reliability for concise reasoning. Finally, we validate our conclusions through extensive experimental results.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.05185', 'html': 'https://arxiv.org/html/2504.05185v2', 'tex': '/src/2504.05185', 'doi': 'https://doi.org/10.48550/arXiv.2504.05185'}	Submission history From: Mehdi Fatemi [ view email ] [v1] Mon, 7 Apr 2025 15:35:54 UTC (389 KB) [v2] Thu, 15 May 2025 03:23:16 UTC (792 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.05185'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.05185'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.05185'}]
2025-04-13	Rethinking Reflection in Pre-Training	Computation and Language	https://arxiv.org/abs/2504.04022	Rethinking Reflection in Pre-Training  Reflection	https://x.com/ashVaswani/status/1909642828554387675		2504.04022	['Essential AI', 'Darsh J Shah', 'Peter Rushton', 'Somanshu Singla', 'Mohit Parmar', 'Kurt Smith', 'Yash Vanjani', 'Ashish Vaswani', 'Adarsh Chaluvaraju', 'Andrew Hojel', 'Andrew Ma', 'Anil Thomas', 'Anthony Polloreno', 'Ashish Tanwer', 'Burhan Drak Sibai', 'Divya S Mansingka', 'Divya Shivaprasad', 'Ishaan Shah', 'Karl Stratos', 'Khoi Nguyen', 'Michael Callahan', 'Michael Pust', 'Mrinal Iyer', 'Philip Monk', 'Platon Mazarakis', 'Ritvik Kapila', 'Saurabh Srivastava', 'Tim Romanski']	ct:A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2504.04022', 'html': 'https://arxiv.org/html/2504.04022v1', 'tex': '/src/2504.04022', 'doi': 'https://doi.org/10.48550/arXiv.2504.04022'}	Submission history From: Peter Rushton [ view email ] [v1] Sat, 5 Apr 2025 02:24:07 UTC (5,179 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.04022'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.04022'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.04022'}]
2025-04-13	LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph	Artificial Intelligence	https://arxiv.org/abs/2504.03137	Efficient KG Reasoning for Small LLMs  LightPROF is a lightweight framework that enables small-scale language models to perform complex reasoning over knowledge graphs (KGs) using structured prompts. Key highlights:   <br>● Retrieve-Embed-Reason pipeline	https://x.com/omarsar0/status/1910319109096747191		2504.03137	['Tu Ao', 'Yanhua Yu', 'Yuling Wang', 'Yang Deng', 'Zirui Guo', 'Liang Pang', 'Pinghui Wang', 'Tat-Seng Chua', 'Xiao Zhang', 'Zhen Cai']	"ct:Large Language Models (LLMs) have impressive capabilities in text understanding and zero-shot reasoning. However, delays in knowledge updates may cause them to reason incorrectly or produce harmful results. Knowledge Graphs (KGs) provide rich and reliable contextual information for the reasoning process of LLMs by structurally organizing and connecting a wide range of entities and relations. Existing KG-based LLM reasoning methods only inject KGs' knowledge into prompts in a textual form, ignoring its structural information. Moreover, they mostly rely on close-source models or open-source models with large parameters, which poses challenges to high resource consumption. To address this, we propose a novel Lightweight and efficient Prompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the full potential of LLMs to tackle complex reasoning tasks in a parameter-efficient manner. Specifically, LightPROF follows a ""Retrieve-Embed-Reason process"", first accurately, and stably retrieving the corresponding reasoning graph from the KG through retrieval module. Next, through a Transformer-based Knowledge Adapter, it finely extracts and integrates factual and structural information from the KG, then maps this information to the LLM's token embedding space, creating an LLM-friendly prompt to be used by the LLM for the final reasoning. Additionally, LightPROF only requires training Knowledge Adapter and can be compatible with any open-source LLM. Extensive experiments on two public KGQA benchmarks demonstrate that LightPROF achieves superior performance with small-scale LLMs. Furthermore, LightPROF shows significant advantages in terms of input token count and reasoning time."	aper has been accepted by AAAI 2025	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.03137', 'html': 'https://arxiv.org/html/2504.03137v1', 'tex': '/src/2504.03137', 'doi': 'https://doi.org/10.48550/arXiv.2504.03137'}	Submission history From: Yuling Wang [ view email ] [v1] Fri, 4 Apr 2025 03:03:47 UTC (9,276 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.03137'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.03137'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.03137'}]
2025-04-13	Agentic Knowledgeable Self-awareness	Computation and Language	https://arxiv.org/abs/2504.03553v1	"Agentic Knowledgeable Self-awareness  KnowSelf is a new framework that introduces agentic knowledgeable self-awareness, enabling LLM agents to dynamically decide when to reflect or seek knowledge based on situational complexity, mimicking human cognition. Using special tokens for ""fast,"" ""slow,"" and ""knowledgeable"" thinking, KnowSelf reduces inference costs and achieves state-of-the-art performance on ALFWorld and WebShop tasks with minimal external knowledge."			2504.03553v1	['Shuofei Qiao', 'Zhisong Qiu', 'Baochang Ren', 'Xiaobin Wang', 'Xiangyuan Ru', 'Ningyu Zhang', 'Xiang Chen', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen']	"ct:Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a ""flood irrigation"" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available atthis https URL."	n progress	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2504.03553v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2504.03553'}	Submission history From: Ningyu Zhang [ view email ] [v1] Fri, 4 Apr 2025 16:03:38 UTC (3,509 KB) [v2] Thu, 29 May 2025 14:15:53 UTC (3,513 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.03553'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.03553'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.03553'}]
2025-04-13	NoProp: Training Neural Networks without Back-propagation or Forward-propagation	Machine Learning	https://arxiv.org/abs/2503.24322	NoProp  NoProp is a novel gradient-free learning method where each neural network layer independently learns to denoise a noisy version of the target, inspired by diffusion and flow matching. Unlike backpropagation, it avoids hierarchical representation learning and achieves competitive performance and efficiency on image classification benchmarks like MNIST and CIFAR.			2503.24322	['Qinyu Li', 'Yee Whye Teh', 'Razvan Pascanu']	ct:The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer below, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, we introduce a new learning method named NoProp, which does not rely on either forward or backwards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. We believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierarchical representations -- at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. We demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Our results show that NoProp is a viable learning algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gradient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process.		['Machine Learning (cs.LG)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2503.24322', 'html': 'https://arxiv.org/html/2503.24322v1', 'tex': '/src/2503.24322', 'doi': 'https://doi.org/10.48550/arXiv.2503.24322'}	Submission history From: Qinyu Li [ view email ] [v1] Mon, 31 Mar 2025 17:08:57 UTC (1,263 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.24322'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.24322'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.24322'}]
2025-04-06	PaperBench: Evaluating AI's Ability to Replicate AI Research	Artificial Intelligence	https://arxiv.org/abs/2504.01848	PaperBench  OpenAI introduces a new benchmark, PaperBench, to test whether AI agents can replicate cutting-edge machine learning research papers, from scratch.   ● A rigorous replication challenge	https://x.com/OpenAI/status/1907481490457506235	"{""GitHub"": ""https://github.com/openai/preparedness""}"	2504.01848	['Giulio Starace', 'Oliver Jaffe', 'Dane Sherburn', 'James Aung', 'Jun Shern Chan', 'Leon Maksin', 'Rachel Dias', 'Evan Mays', 'Benjamin Kinsella', 'Wyatt Thompson', 'Johannes Heidecke', 'Amelia Glaese', 'Tejal Patwardhan']	ct:We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We open-source our code (this https URL) to facilitate future research in understanding the AI engineering capabilities of AI agents.	es, 14 figures	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.01848', 'html': 'https://arxiv.org/html/2504.01848v3', 'tex': '/src/2504.01848', 'doi': 'https://doi.org/10.48550/arXiv.2504.01848'}	Submission history From: James Aung [ view email ] [v1] Wed, 2 Apr 2025 15:55:24 UTC (4,982 KB) [v2] Fri, 4 Apr 2025 12:44:57 UTC (4,982 KB) [v3] Mon, 7 Apr 2025 12:15:49 UTC (4,982 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.01848'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.01848'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.01848'}]
2025-04-06	Command A: An Enterprise-Ready Large Language Model	Computation and Language	https://arxiv.org/abs/2504.00698	Command A: An Enterprise-Ready LLM  Cohere announced Command A, a 111B parameter open-weights LLM built for enterprise-grade RAG, agents, code, and multilingual tasks. Key contributions:   ● Modular expert merging for domain mastery	https://x.com/nrehiew_/status/1908181303339471020	"{""Models"": ""https://huggingface.co/CohereForAI/c4ai-command-a-03-2025""}"	2504.00698	"['Team Cohere', 'Aakanksha', 'Arash Ahmadian', 'Marwan Ahmed', 'Jay Alammar', 'Milad Alizadeh', 'Yazeed Alnumay', 'Sophia Althammer', 'Arkady Arkhangorodsky', 'Viraat Aryabumi', 'Dennis Aumiller', 'Raphaël Avalos', 'Zahara Aviv', 'Sammie Bae', 'Saurabh Baji', 'Alexandre Barbet', 'Max Bartolo', 'Björn Bebensee', 'Neeral Beladia', 'Walter Beller-Morales', 'Alexandre Bérard', 'Andrew Berneshawi', 'Anna Bialas', 'Phil Blunsom', 'Matt Bobkin', 'Adi Bongale', 'Sam Braun', 'Maxime Brunet', 'Samuel Cahyawijaya', 'David Cairuz', 'Jon Ander Campos', 'Cassie Cao', 'Kris Cao', 'Roman Castagné', 'Julián Cendrero', 'Leila Chan Currie', 'Yash Chandak', 'Diane Chang', 'Giannis Chatziveroglou', 'Hongyu Chen', 'Claire Cheng', 'Alexis Chevalier', 'Justin T. Chiu', 'Eugene Cho', 'Eugene Choi', 'Eujeong Choi', 'Tim Chung', 'Volkan Cirik', 'Ana Cismaru', 'Pierre Clavier', 'Henry Conklin', 'Lucas Crawhall-Stein', 'Devon Crouse', 'Andres Felipe Cruz-Salinas', 'Ben Cyrus', ""Daniel D'souza"", 'Hugo Dalla-Torre', 'John Dang', 'William Darling', 'Omar Darwiche Domingues', 'Saurabh Dash', 'Antoine Debugne', 'Théo Dehaze', 'Shaan Desai', 'Joan Devassy', 'Rishit Dholakia', 'Kyle Duffy', 'Ali Edalati', 'Ace Eldeib', 'Abdullah Elkady', 'Sarah Elsharkawy', 'Irem Ergün', 'Beyza Ermis', 'Marzieh Fadaee', 'Boyu Fan', 'Lucas Fayoux', 'Yannis Flet-Berliac', 'Nick Frosst', 'Matthias Gallé', 'Wojciech Galuba', 'Utsav Garg', 'Matthieu Geist', 'Mohammad Gheshlaghi Azar', 'Ellen Gilsenan-McMahon', 'Seraphina Goldfarb-Tarrant', 'Tomas Goldsack', 'Aidan Gomez', 'Victor Machado Gonzaga', 'Nithya Govindarajan', 'Manoj Govindassamy', 'Nathan Grinsztajn', 'Nikolas Gritsch', 'Patrick Gu', 'Shangmin Guo', 'Kilian Haefeli', 'Rod Hajjar', 'Tim Hawes', 'Jingyi He', 'Sebastian Hofstätter', 'Sungjin Hong', 'Sara Hooker', 'Tom Hosking', 'Stephanie Howe', 'Eric Hu', 'Renjie Huang', 'Hemant Jain', 'Ritika Jain', 'Nick Jakobi', 'Madeline Jenkins', 'JJ Jordan', 'Dhruti Joshi', 'Jason Jung', 'Trushant Kalyanpur', 'Siddhartha Rao Kamalakara', 'Julia Kedrzycki', 'Gokce Keskin', 'Edward Kim', 'Joon Kim', 'Wei-Yin Ko', 'Tom Kocmi', 'Michael Kozakov', 'Wojciech Kryściński', 'Arnav Kumar Jain', 'Komal Kumar Teru', 'Sander Land', 'Michael Lasby', 'Olivia Lasche', 'Justin Lee', 'Patrick Lewis', 'Jeffrey Li', 'Jonathan Li', 'Hangyu Lin', 'Acyr Locatelli', 'Kevin Luong', 'Raymond Ma', 'Lukáš Mach', 'Marina Machado', 'Joanne Magbitang', 'Brenda Malacara Lopez', 'Aryan Mann', 'Kelly Marchisio', 'Olivia Markham', 'Alexandre Matton', 'Alex McKinney', 'Dominic McLoughlin', 'Jozef Mokry', 'Adrien Morisot', 'Autumn Moulder', 'Harry Moynehan', 'Maximilian Mozes', 'Vivek Muppalla', 'Lidiya Murakhovska', 'Hemangani Nagarajan', 'Alekhya Nandula', 'Hisham Nasir', 'Shauna Nehra', 'Josh Netto-Rosen', 'Daniel Ohashi', 'James Owers-Bardsley', 'Jason Ozuzu', 'Dennis Padilla', 'Gloria Park', 'Sam Passaglia', 'Jeremy Pekmez', 'Laura Penstone', 'Aleksandra Piktus', 'Case Ploeg', 'Andrew Poulton', 'Youran Qi', 'Shubha Raghvendra', 'Miguel Ramos', 'Ekagra Ranjan', 'Pierre Richemond', 'Cécile Robert-Michon', 'Aurélien Rodriguez', 'Sudip Roy', 'Sebastian Ruder', 'Laura Ruis', 'Louise Rust', 'Anubhav Sachan', 'Alejandro Salamanca', 'Kailash Karthik Saravanakumar', 'Isha Satyakam', 'Alice Schoenauer Sebag', 'Priyanka Sen', 'Sholeh Sepehri', 'Preethi Seshadri', 'Ye Shen', 'Tom Sherborne', 'Sylvie Shang Shi', 'Sanal Shivaprasad', 'Vladyslav Shmyhlo', 'Anirudh Shrinivason', 'Inna Shteinbuk', 'Amir Shukayev', 'Mathieu Simard', 'Ella Snyder', 'Ava Spataru', 'Victoria Spooner', 'Trisha Starostina', 'Florian Strub', 'Yixuan Su', 'Jimin Sun', 'Dwarak Talupuru', 'Eugene Tarassov', 'Elena Tommasone', 'Jennifer Tracey', 'Billy Trend', 'Evren Tumer', 'Ahmet Üstün', 'Bharat Venkitesh', 'David Venuto', 'Pat Verga', 'Maxime Voisin', 'Alex Wang', 'Donglu Wang', 'Shijian Wang', 'Edmond Wen', 'Naomi White', 'Jesse Willman', 'Marysia Winkels', 'Chen Xia', 'Jessica Xie', 'Minjie Xu', 'Bowen Yang', 'Tan Yi-Chern', 'Ivan Zhang', 'Zhenyu Zhao', 'Zhoujie Zhao', 'et al. (129 additional authors not shown)']"	ct:In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2504.00698', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2504.00698'}	Submission history From: Seraphina Goldfarb-Tarrant [ view email ] [v1] Tue, 1 Apr 2025 12:08:07 UTC (4,689 KB) [v2] Mon, 14 Apr 2025 12:37:51 UTC (4,689 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.00698'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.00698'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.00698'}]
2025-04-06	CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation	Artificial Intelligence	https://arxiv.org/abs/2503.22708	CodeScientist  Researchers at AI2 release CodeScientist, a system that autonomously generates and tests scientific hypotheses via code-based experimentation. It’s among the first to produce validated discoveries with minimal human input. Key ideas:   ● Code-first scientific agent		"{""Blog"": ""https://allenai.org/blog/codescientist"", ""GitHub"": ""https://github.com/allenai/codescientist""}"	2503.22708	['Peter Jansen', 'Oyvind Tafjord', 'Marissa Radensky', 'Pao Siangliulue', 'Tom Hope', 'Bhavana Dalvi Mishra', 'Bodhisattwa Prasad Majumder', 'Daniel S. Weld', 'Peter Clark']	ct:Despite the surge of interest in autonomous scientific discovery (ASD) of software artifacts (e.g., improved ML algorithms), current ASD systems face two key limitations: (1) they largely explore variants of existing codebases or similarly constrained design spaces, and (2) they produce large volumes of research artifacts (such as automatically generated papers and code) that are typically evaluated using conference-style paper review with limited evaluation of code. In this work we introduce CodeScientist, a novel ASD system that frames ideation and experiment construction as a form of genetic search jointly over combinations of research articles and codeblocks defining common actions in a domain (like prompting a language model). We use this paradigm to conduct hundreds of automated experiments on machine-generated ideas broadly in the domain of agents and virtual environments, with the system returning 19 discoveries, 6 of which were judged as being both at least minimally sound and incrementally novel after a multi-faceted evaluation beyond that typically conducted in prior work, including external (conference-style) review, code review, and replication attempts. Moreover, the discoveries span new tasks, agents, metrics, and data, suggesting a qualitative shift from benchmark optimization to broader discoveries.	es (13 pages: main paper body; 85 pages: appendix)	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.22708', 'html': None, 'tex': '/src/2503.22708', 'doi': 'https://doi.org/10.48550/arXiv.2503.22708'}	Submission history From: Peter Jansen [ view email ] [v1] Thu, 20 Mar 2025 22:37:17 UTC (4,257 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.22708'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.22708'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.22708'}]
2025-04-06	RARE: Retrieval-Augmented Reasoning Modeling	Computation and Language	https://arxiv.org/abs/2503.23513	Retrieval-Augmented Reasoning Model  Introduces RARE, a new paradigm for training domain-specific LLMs that focuses on reasoning, not memorization. Key ideas:   ● Inspired by Bloom’s Taxonomy	https://x.com/omarsar0/status/1907796990966247484		2503.23513	['Zhengren Wang', 'Jiayang Yu', 'Dongsheng Ma', 'Zhe Chen', 'Yu Wang', 'Zhiyu Li', 'Feiyu Xiong', 'Yanfeng Wang', 'Weinan E', 'Linpeng Tang', 'Wentao Zhang']	ct:Domain-specific intelligence demands specialized knowledge and sophisticated reasoning for problem-solving, posing significant challenges for large language models (LLMs) that struggle with knowledge hallucination and inadequate reasoning capabilities under constrained parameter budgets. Inspired by Bloom's Taxonomy in educational theory, we propose Retrieval-Augmented Reasoning Modeling (RARE), a novel paradigm that decouples knowledge storage from reasoning optimization. RARE externalizes domain knowledge to retrievable sources and internalizes domain-specific reasoning patterns during training. Specifically, by injecting retrieved knowledge into training prompts with masked losses, RARE transforms learning objectives from rote memorization to contextualized reasoning. It enables models to bypass parameter-intensive memorization and prioritize the development of higher-order cognitive processes. Extensive experiments demonstrate that lightweight RARE-trained models (e.g., Llama-3.1-8B) could achieve state-of-the-art performance, surpassing retrieval-augmented GPT-4 and DeepSeek-R1 up to approximately 20\% accuracy. RARE establishes a paradigm shift where maintainable external knowledge bases synergize with compact, reasoning-optimized models, collectively driving more scalable domain-specific intelligence.	his https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.23513', 'html': 'https://arxiv.org/html/2503.23513v2', 'tex': '/src/2503.23513', 'doi': 'https://doi.org/10.48550/arXiv.2503.23513'}	Submission history From: Zhengren Wang [ view email ] [v1] Sun, 30 Mar 2025 16:49:44 UTC (372 KB) [v2] Sat, 17 May 2025 06:48:49 UTC (567 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.23513'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.23513'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.23513'}]
2025-04-06	Why do LLMs attend to the first token?	Computation and Language	https://arxiv.org/abs/2504.02732	Why do LLMs Attend to First Token?  This new paper explains why LLMs obsessively focus attention on the first token	https://x.com/omarsar0/status/1908187563422261411		2504.02732	['Federico Barbero', 'Álvaro Arroyo', 'Xiangming Gu', 'Christos Perivolaropoulos', 'Michael Bronstein', 'Petar Veličković', 'Razvan Pascanu']	ct:Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.02732', 'html': 'https://arxiv.org/html/2504.02732v4', 'tex': '/src/2504.02732', 'doi': 'https://doi.org/10.48550/arXiv.2504.02732'}	Submission history From: Federico Barbero [ view email ] [v1] Thu, 3 Apr 2025 16:17:55 UTC (1,444 KB) [v2] Fri, 4 Apr 2025 07:41:19 UTC (1,444 KB) [v3] Tue, 13 May 2025 16:38:34 UTC (1,332 KB) [v4] Tue, 5 Aug 2025 16:43:21 UTC (447 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.02732'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.02732'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.02732'}]
2025-04-06	Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions	Computation and Language	https://arxiv.org/abs/2503.22678	Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions  Presents MedAgentSim is a fully automated, open-source hospital simulation where LLM-powered agents simulate doctor-patient interactions in dynamic diagnostic settings. Unlike previous static QA  benchmarks, MedAgentSim mimics real-world clinical workflows with multi-turn dialogue, test requests, and self-improvement.  More about this paper:   ● Active doctor agents	https://x.com/omarsar0/status/1906719555482702147	"{""Code"": ""https://github.com/MAXNORM8650/MedAgentSim""}"	2503.22678	['Mohammad Almansoori', 'Komal Kumar', 'Hisham Cholakkal']	ct:In this work, we introduce MedAgentSim, an open-source simulated clinical environment with doctor, patient, and measurement agents designed to evaluate and enhance LLM performance in dynamic diagnostic settings. Unlike prior approaches, our framework requires doctor agents to actively engage with patients through multi-turn conversations, requesting relevant medical examinations (e.g., temperature, blood pressure, ECG) and imaging results (e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic process. Additionally, we incorporate self improvement mechanisms that allow models to iteratively refine their diagnostic strategies. We enhance LLM performance in our simulated setting by integrating multi-agent discussions, chain-of-thought reasoning, and experience-based knowledge retrieval, facilitating progressive learning as doctor agents interact with more patients. We also introduce an evaluation benchmark for assessing the LLM's ability to engage in dynamic, context-aware diagnostic interactions. While MedAgentSim is fully automated, it also supports a user-controlled mode, enabling human interaction with either the doctor or patient agent. Comprehensive evaluations in various simulated diagnostic scenarios demonstrate the effectiveness of our approach. Our code, simulation tool, and benchmark are available at \href{this https URL}.	e, 4 figures, 61 references	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.22678', 'html': 'https://arxiv.org/html/2503.22678v1', 'tex': '/src/2503.22678', 'doi': 'https://doi.org/10.48550/arXiv.2503.22678'}	Submission history From: Komal Kumar [ view email ] [v1] Fri, 28 Mar 2025 17:59:53 UTC (12,708 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.22678'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.22678'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.22678'}]
2025-04-06	Open Deep Search: Democratizing Search with Open-source Reasoning Agents	Machine Learning	https://arxiv.org/abs/2503.20201	Open Deep Search  Researchers from Sentient, UW, Princeton, and UC Berkeley introduce Open Deep Search (ODS), an open-source search AI framework that rivals top proprietary systems like GPT-4o Search Preview and Perplexity Sonar. Key insights:   ● Two open components: search + reasoning	https://x.com/sewoong79/status/1906595129965912341	"{""GitHub"": ""https://github.com/sentient-agi/OpenDeepSearch""}"	2503.20201	['Salaheddin Alzubi', 'Creston Brooks', 'Purva Chiniya', 'Edoardo Contente', 'Chiara von Gerlach', 'Lucas Irwin', 'Yihan Jiang', 'Arda Kaz', 'Windsor Nguyen', 'Sewoong Oh', 'Himanshu Tyagi', 'Pramod Viswanath']	ct:We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES.	es, 8 figures, 4 tables	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2503.20201', 'html': 'https://arxiv.org/html/2503.20201v1', 'tex': '/src/2503.20201', 'doi': 'https://doi.org/10.48550/arXiv.2503.20201'}	Submission history From: Sewoong Oh [ view email ] [v1] Wed, 26 Mar 2025 03:51:32 UTC (768 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.20201'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.20201'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.20201'}]
2025-04-06	Z1: Efficient Test-time Scaling with Code	Computation and Language	https://arxiv.org/abs/2504.00810v1	Efficient Test-time Scaling with Code  Z1 is a new method for making large language models more compute-efficient at test time, especially during reasoning. The core idea is to train LLMs with short and long code-based reasoning trajectories, and then dynamically adjust reasoning depth during inference. Key contributions:   ● Z1-Code-Reasoning-107K dataset			2504.00810v1	['Zhaojian Yu', 'Yinghao Wu', 'Yilun Zhao', 'Arman Cohan', 'Xiao-Ping Zhang']	ct:Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2504.00810v1', 'html': None, 'tex': '/src/2504.00810v1', 'doi': 'https://doi.org/10.48550/arXiv.2504.00810'}	Submission history From: Zhaojian Yu [ view email ] [v1] Tue, 1 Apr 2025 14:01:50 UTC (3,221 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2504.00810'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2504.00810'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2504.00810'}]
2025-04-06	Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models	Computation and Language	https://arxiv.org/abs/2503.24377	A Survey of Efficient Reasoning for LLMs  This survey focuses on reasoning economy in LLMs, analyzing how to balance deep reasoning performance with computational cost. It reviews inefficiencies, behavioral patterns, and potential solutions at both post-training and inference stages.	https://x.com/omarsar0/status/1907072213142151488		2503.24377	['Rui Wang', 'Hongru Wang', 'Boyang Xue', 'Jianhui Pang', 'Shudong Liu', 'Yi Chen', 'Jiahao Qiu', 'Derek Fai Wong', 'Heng Ji', 'Kam-Fai Wong']	ct:Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.	gress; Paper list Repo:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2503.24377', 'html': 'https://arxiv.org/html/2503.24377v1', 'tex': '/src/2503.24377', 'doi': 'https://doi.org/10.48550/arXiv.2503.24377'}	Submission history From: Rui Wang [ view email ] [v1] Mon, 31 Mar 2025 17:58:07 UTC (2,134 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.24377'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.24377'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.24377'}]
2025-04-06	Inside-Out: Hidden Factual Knowledge in LLMs	Computation and Language	https://arxiv.org/abs/2503.15299	Hidden Factual Knowledge in LLMs  This study introduces a framework to measure hidden knowledge in LLMs, showing that models encode significantly more factual information internally than they express in outputs, up to 40% more. It also finds that some answers, although known internally, are never generated, highlighting key limits in test-time sampling for QA tasks.	https://x.com/zorikgekhman/status/1906693729886363861		2503.15299	['Zorik Gekhman', 'Eyal Ben David', 'Hadas Orgad', 'Eran Ofek', 'Yonatan Belinkov', 'Idan Szpektor', 'Jonathan Herzig', 'Roi Reichart']	ct:This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model's observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average relative gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) put a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.	ed to COLM 2025	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.15299', 'html': None, 'tex': '/src/2503.15299', 'doi': 'https://doi.org/10.48550/arXiv.2503.15299'}	Submission history From: Zorik Gekhman [ view email ] [v1] Wed, 19 Mar 2025 15:21:48 UTC (151 KB) [v2] Mon, 24 Mar 2025 01:31:35 UTC (151 KB) [v3] Thu, 31 Jul 2025 14:38:33 UTC (232 KB) [v4] Wed, 6 Aug 2025 13:42:41 UTC (232 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.15299'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.15299'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.15299'}]
2025-03-30	AgentRxiv: Towards Collaborative Autonomous Research	Artificial Intelligence	https://arxiv.org/abs/2503.18102	AgentRxiv  Researchers from Johns Hopkins & ETH Zurich present AgentRxiv, a framework enabling LLM agents to autonomously generate and share research papers, mimicking how human scientists build on each other’s work. Highlights:   ● AgentRxiv = arXiv for LLMs	https://x.com/SRSchmidgall/status/1904172862014984632		2503.18102	['Samuel Schmidgall', 'Michael Moor']	"ct:Progress in scientific discovery is rarely the result of a single ""Eureka"" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other's research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery."		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2503.18102', 'html': 'https://arxiv.org/html/2503.18102v1', 'tex': '/src/2503.18102', 'doi': 'https://doi.org/10.48550/arXiv.2503.18102'}	Submission history From: Samuel Schmidgall [ view email ] [v1] Sun, 23 Mar 2025 15:16:42 UTC (4,111 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.18102'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.18102'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.18102'}]
2025-03-30	Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models	Computation and Language	https://arxiv.org/abs/2503.16779	Chain-of-Tools  This new paper presents Chain-of-Tools (CoTools), a new method to enable LLMs to incorporate expansive external toolsets—including tools never seen during training—while preserving CoT (chain-of-thought) reasoning. Highlights:   ● Frozen LLM with lightweight fine-tuning	https://x.com/omarsar0/status/1904190225079022018		2503.16779	['Mengsong Wu', 'Tong Zhu', 'Han Han', 'Xiang Zhang', 'Wenbiao Shao', 'Wenliang Chen']	ct:Tool learning can further broaden the usage scenarios of large language models (LLMs). However most of the existing methods either need to finetune that the model can only use tools seen in the training data, or add tool demonstrations into the prompt with lower efficiency. In this paper, we present a new Tool Learning method Chain-of-Tools. It makes full use of the powerful semantic representation capability of frozen LLMs to finish tool calling in CoT reasoning with a huge and flexible tool pool which may contain unseen tools. Especially, to validate the effectiveness of our approach in the massive unseen tool scenario, we construct a new dataset SimpleToolQuestions. We conduct experiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two knowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions). Experimental results show that our approach performs better than the baseline. We also identify dimensions of the model output that are critical in tool selection, enhancing the model interpretability. Our code and data are available at:this https URL.	es, 10 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2503.16779', 'html': 'https://arxiv.org/html/2503.16779v1', 'tex': '/src/2503.16779', 'doi': 'https://doi.org/10.48550/arXiv.2503.16779'}	Submission history From: Mengsong Wu [ view email ] [v1] Fri, 21 Mar 2025 01:26:12 UTC (2,253 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.16779'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.16779'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.16779'}]
2025-03-30	MemInsight: Autonomous Memory Augmentation for LLM Agents	Computation and Language	https://arxiv.org/abs/2503.21760v1	Structured Memory Augmentation for Smarter LLM Agents  MemInsight is a framework that autonomously augments and structures memory for LLM agents, improving context retention and retrieval. Key insights include:   ● Structured, autonomous memory augmentation			2503.21760v1	['Rana Salama', 'Jason Cai', 'Michelle Yuan', 'Anna Currey', 'Monica Sunkara', 'Yi Zhang', 'Yassine Benajiba']	ct:Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.21760v1', 'html': 'https://arxiv.org/html/2503.21760v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2503.21760'}	Submission history From: Jason Cai [ view email ] [v1] Thu, 27 Mar 2025 17:57:28 UTC (5,772 KB) [v2] Thu, 31 Jul 2025 23:26:12 UTC (9,722 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.21760'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.21760'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.21760'}]
2025-03-30	PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play	Computation and Language	https://arxiv.org/abs/2503.14432	"Play2Prompt  Researchers from MIT CSAIL and IBM introduce Play2Prompt, a framework that empowers LLM agents to learn how to use external tools entirely in a zero-shot manner, without requiring labeled examples or high-quality documentation. Key innovations include:   ● Tool ""play"" for usage discovery"			2503.14432	['Wei Fang', 'Yang Zhang', 'Kaizhi Qian', 'James Glass', 'Yada Zhu']	"ct:Large language models (LLMs) are increasingly integrated with specialized external tools, yet many tasks demand zero-shot tool usage with minimal or noisy documentation. Existing solutions rely on manual rewriting or labeled data for validation, making them inapplicable in true zero-shot settings. To address these challenges, we propose PLAY2PROMPT, an automated framework that systematically ""plays"" with each tool to explore its input-output behaviors. Through this iterative trial-and-error process, PLAY2PROMPT refines tool documentation and generates usage examples without any labeled data. These examples not only guide LLM inference but also serve as validation to further enhance tool utilization. Extensive experiments on real-world tasks demonstrate that PLAY2PROMPT significantly improves zero-shot tool performance across both open and closed models, offering a scalable and effective solution for domain-specific tool integration."	25 Long Paper (Findings)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2503.14432', 'html': 'https://arxiv.org/html/2503.14432v2', 'tex': '/src/2503.14432', 'doi': 'https://doi.org/10.48550/arXiv.2503.14432'}	Submission history From: Wei Fang [ view email ] [v1] Tue, 18 Mar 2025 17:09:57 UTC (1,284 KB) [v2] Thu, 12 Jun 2025 16:53:41 UTC (1,283 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.14432'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.14432'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.14432'}]
2025-03-30	Synthetic Data Generation Using Large Language Models: Advances in Text and Code	Computation and Language	https://arxiv.org/abs/2503.14023	Synthetic Data Generation Using LLMs  LLMs are increasingly used to generate synthetic training data for language and code tasks, improving performance in low-resource scenarios through techniques like prompt-based generation and self-refinement. The paper highlights benefits like cost and coverage, while addressing issues such as factual errors and bias, and suggests mitigations and future research in prompt automation and evaluation.			2503.14023	['Mihai Nadas', 'Laura Diosan', 'Andreea Tomescu']	ct:This survey reviews how large language models (LLMs) are transforming synthetic training data generation in both natural language and code domains. By producing artificial but task-relevant examples, these models can significantly augment or even substitute for real-world datasets, particularly in scenarios where labeled data is scarce, expensive, or sensitive. This paper surveys recent advances in leveraging LLMs to create synthetic text and code, highlighting key techniques such as prompt-based generation, retrieval-augmented pipelines, and iterative self-refinement. We examine how these methods can enrich low-resource tasks (e.g., classification, question answering) and facilitate code-centric applications (e.g., instruction tuning, code translation, bug repair) through automated verification of functional correctness. Alongside potential benefits - cost-effectiveness, broad coverage, and controllable diversity - we discuss the accompanying challenges, including factual inaccuracies in generated text, insufficient stylistic or distributional realism, and risks of bias amplification. Proposed mitigation strategies range from filtering and weighting synthetic outputs to reinforcement learning with execution feedback in code domains. We conclude by outlining open research directions, such as automated prompt engineering, cross-modal data synthesis, and robust evaluation frameworks, underscoring the growing importance of LLM-generated synthetic data in accelerating AI development while emphasizing ethical and quality safeguards.	es, 6 tables, 1 figure, 64 references	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.14023', 'html': 'https://arxiv.org/html/2503.14023v2', 'tex': '/src/2503.14023', 'doi': 'https://doi.org/10.48550/arXiv.2503.14023'}	Submission history From: Mihai Nadas [ view email ] [v1] Tue, 18 Mar 2025 08:34:03 UTC (118 KB) [v2] Tue, 22 Jul 2025 10:28:00 UTC (334 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.14023'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.14023'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.14023'}]
2025-03-30	Current and Future Use of Large Language Models for Knowledge Work	Human-Computer Interaction	https://arxiv.org/abs/2503.16774v1	Current and Future Use of LLMs for Knowledge Work  A two-part survey study of 216 and 107 participants reveals that knowledge workers currently use LLMs for tasks like code generation and text improvement, but envision deeper integration into workflows and data. The findings inform future design and adoption strategies for generative AI in professional settings.			2503.16774v1	['Michelle Brachman', 'Amina El-Ashry', 'Casey Dugan', 'Werner Geyer']	ct:Large Language Models (LLMs) have introduced a paradigm shift in interaction with AI technology, enabling knowledge workers to complete tasks by specifying their desired outcome in natural language. LLMs have the potential to increase productivity and reduce tedious tasks in an unprecedented way. A systematic study of LLM adoption for work can provide insight into how LLMs can best support these workers. To explore knowledge workers' current and desired usage of LLMs, we ran a survey (n=216). Workers described tasks they already used LLMs for, like generating code or improving text, but imagined a future with LLMs integrated into their workflows and data. We ran a second survey (n=107) a year later that validated our initial findings and provides insight into up-to-date LLM use by knowledge workers. We discuss implications for adoption and design of generative AI technologies for knowledge work.		['Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2503.16774v1', 'html': 'https://arxiv.org/html/2503.16774v1', 'tex': '/src/2503.16774v1', 'doi': 'https://doi.org/10.48550/arXiv.2503.16774'}	Submission history From: Michelle Brachman [ view email ] [v1] Fri, 21 Mar 2025 01:07:21 UTC (2,419 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.16774'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.16774'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.16774'}]
2025-03-23	A Review of DeepSeek Models' Key Innovative Techniques	Machine Learning	https://arxiv.org/abs/2503.11486	A Review of DeepSeek Models  This paper provides an in-depth review of the cutting-edge techniques behind DeepSeek's open-source LLMs—DeepSeek-V3 and DeepSeek-R1. These models achieve state-of-the-art  performance with significantly lower resource requirements compared to proprietary counterparts. Key highlights include:   <br>● Multi-Head Latent Attention (MLA)			2503.11486	['Chengen Wang', 'Murat Kantarcioglu']	ct:DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models (LLMs) for general-purpose tasks and reasoning, achieving performance comparable to state-of-the-art closed-source models from companies like OpenAI and Anthropic -- while requiring only a fraction of their training costs. Understanding the key innovative techniques behind DeepSeek's success is crucial for advancing LLM research. In this paper, we review the core techniques driving the remarkable effectiveness and efficiency of these models, including refinements to the transformer architecture, innovations such as Multi-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the co-design of algorithms, frameworks, and hardware, the Group Relative Policy Optimization algorithm, post-training with pure reinforcement learning and iterative training alternating between supervised fine-tuning and reinforcement learning. Additionally, we identify several open questions and highlight potential research opportunities in this rapidly advancing field.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2503.11486', 'html': 'https://arxiv.org/html/2503.11486v1', 'tex': '/src/2503.11486', 'doi': 'https://doi.org/10.48550/arXiv.2503.11486'}	Submission history From: Chengen Wang [ view email ] [v1] Fri, 14 Mar 2025 15:11:29 UTC (477 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.11486'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.11486'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.11486'}]
2025-03-23	Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models	Computation and Language	https://arxiv.org/abs/2503.13551	Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in LLMs  It proposes a Hierarchical Reward Model (HRM) that addresses reward hacking and error propagation issues in fine-grained LLM reasoning. They also introduce Hierarchical Node Compression (HNC) to augment MCTS-based automatic data annotation, boosting label diversity and robustness at minimal computational cost.   <br>● Hierarchical vs. single-step rewards	https://x.com/omarsar0/status/1902360668856315990		2503.13551	['Teng Wang', 'Zhangyi Jiang', 'Zhenqi He', 'Shenyang Tong', 'Wenhan Yang', 'Yanan Zheng', 'Zeyu Li', 'Zifan He', 'Hailei Gong']	ct:Recent studies show that Large Language Models (LLMs) achieve strong reasoning capabilities through supervised fine-tuning or reinforcement learning. However, a key approach, the Process Reward Model (PRM), suffers from reward hacking, making it unreliable in identifying the best intermediate step. In addition, the cost of annotating reasoning processes for reward modeling is high, making large-scale collection of high-quality data challenging. To address this, we propose a novel reward model approach called the Hierarchical Reward Model (HRM), which evaluates both individual and consecutive reasoning steps at both fine-grained and coarse-grained levels. HRM excels at assessing multi-step reasoning coherence, especially when flawed steps are later corrected through self-reflection. To further reduce the cost of generating training data, we introduce a lightweight and effective data augmentation strategy called Hierarchical Node Compression (HNC), which merges two consecutive reasoning steps into one within the tree structure. By applying HNC to MCTS-generated reasoning trajectories, we enhance the diversity and robustness of HRM training data while introducing controlled noise with minimal computational overhead. Empirical results on the PRM800K dataset show that HRM, together with HNC, provides more stable and reliable evaluations than PRM. Furthermore, cross-domain evaluations on the MATH500 and GSM8K datasets demonstrate HRM's strong generalization and robustness across a variety of reasoning tasks.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2503.13551', 'html': 'https://arxiv.org/html/2503.13551v3', 'tex': '/src/2503.13551', 'doi': 'https://doi.org/10.48550/arXiv.2503.13551'}	Submission history From: Teng Wang [ view email ] [v1] Sun, 16 Mar 2025 15:18:40 UTC (907 KB) [v2] Wed, 19 Mar 2025 15:43:56 UTC (748 KB) [v3] Tue, 6 May 2025 11:38:24 UTC (19,265 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.13551'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.13551'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.13551'}]
2025-03-23	DAPO: An Open-Source LLM Reinforcement Learning System at Scale	Machine Learning	https://arxiv.org/abs/2503.14476	DAPO: An Open-Source LLM Reinforcement Learning System at Scale  It introduces DAPO, a fully open-source, large-scale RL system that boosts the chain-of-thought reasoning capabilities of LLMs.  DAPO raises the upper clipping threshold (“Clip-Higher”) in PPO-style training, preventing entropy collapse and helping the policy explore more diverse tokens.  By filtering out samples that are always correct or always wrong, DAPO focuses training on prompts with useful gradient signals, speeding up convergence in fewer updates.  Instead of averaging losses at the sample level, DAPO applies policy gradients per token, making each reasoning step matter. This ensures both high-quality and length-appropriate outputs.  The system masks or softly penalizes excessively long answers, preventing meaningless verbosity or repetitive text.  DAPO achieves SOTA math performance on the AIME 2024 test set. Specifically, DAPO trained from a Qwen2.5-32B base achieves 50% accuracy, outperforming DeepSeek’s R1 with less training time, and showcasing open-source reproducibility at scale.	https://x.com/omarsar0/status/1902364950821257288		2503.14476	['Qiying Yu', 'Zheng Zhang', 'Ruofei Zhu', 'Yufeng Yuan', 'Xiaochen Zuo', 'Yu Yue', 'Weinan Dai', 'Tiantian Fan', 'Gaohong Liu', 'Lingjun Liu', 'Xin Liu', 'Haibin Lin', 'Zhiqi Lin', 'Bole Ma', 'Guangming Sheng', 'Yuxuan Tong', 'Chi Zhang', 'Mofan Zhang', 'Wang Zhang', 'Hang Zhu', 'Jinhua Zhu', 'Jiaze Chen', 'Jiangjie Chen', 'Chengyi Wang', 'Hongli Yu', 'Yuxuan Song', 'Xiangpeng Wei', 'Hao Zhou', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Lin Yan', 'Mu Qiao', 'Yonghui Wu', 'Mingxuan Wang']	ct:Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\textbf{D}$ecoupled Clip and $\textbf{D}$ynamic s$\textbf{A}$mpling $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.	t Page:this https URL	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.14476', 'html': 'https://arxiv.org/html/2503.14476v2', 'tex': '/src/2503.14476', 'doi': 'https://doi.org/10.48550/arXiv.2503.14476'}	Submission history From: Qiying Yu [ view email ] [v1] Tue, 18 Mar 2025 17:49:06 UTC (4,369 KB) [v2] Tue, 20 May 2025 01:37:34 UTC (4,369 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.14476'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.14476'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.14476'}]
2025-03-23	Compute Optimal Scaling of Skills: Knowledge vs Reasoning	Machine Learning	https://arxiv.org/abs/2503.10061	Compute Optimal Scaling of Skills  Researchers from the University of Wisconsin and Meta AI investigate how different skills (knowledge-based QA vs. code generation) exhibit contrasting optimal scaling behaviors in LLMs. Their key question: does the compute-optimal trade-off between model size and data volume depend on the type of skill being learned? Surprisingly, the answer is yes—they show distinct “data-hungry” vs. “capacity-hungry” preferences per skill. Highlights:   <br>● Skill-dependent scaling laws	https://x.com/nick11roberts/status/1902875088438833291		2503.10061	['Nicholas Roberts', 'Niladri Chatterji', 'Sharan Narang', 'Mike Lewis', 'Dieuwke Hupkes']	ct:Scaling laws are a critical component of the LLM development pipeline, most famously as a way to forecast training decisions such as 'compute-optimally' trading-off parameter count and dataset size, alongside a more recent growing list of other crucial decisions. In this work, we ask whether compute-optimal scaling behaviour can be skill-dependent. In particular, we examine knowledge and reasoning-based skills such as knowledge-based QA and code generation, and we answer this question in the affirmative: scaling laws are skill-dependent. Next, to understand whether skill-dependent scaling is an artefact of the pretraining datamix, we conduct an extensive ablation of different datamixes and find that, also when correcting for datamix differences, knowledge and code exhibit fundamental differences in scaling behaviour. We conclude with an analysis of how our findings relate to standard compute-optimal scaling using a validation set, and find that a misspecified validation set can impact compute-optimal parameter count by nearly 50%, depending on its skill composition.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.10061', 'html': 'https://arxiv.org/html/2503.10061v3', 'tex': '/src/2503.10061', 'doi': 'https://doi.org/10.48550/arXiv.2503.10061'}	Submission history From: Nicholas Roberts [ view email ] [v1] Thu, 13 Mar 2025 05:21:22 UTC (8,135 KB) [v2] Fri, 14 Mar 2025 01:39:39 UTC (8,135 KB) [v3] Fri, 13 Jun 2025 21:16:23 UTC (8,147 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.10061'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.10061'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.10061'}]
2025-03-23	Thinking Machines: A Survey of LLM based Reasoning Strategies	Computation and Language	https://arxiv.org/abs/2503.10814	Thinking Machines  This survey provides an overview and comparison of existing reasoning techniques and presents a systematic survey of reasoning-imbued language models.	https://x.com/omarsar0/status/1901645973681823962		2503.10814	['Dibyanayan Bandyopadhyay', 'Soham Bhattacharjee', 'Asif Ekbal']	ct:Large Language Models (LLMs) are highly proficient in language-based tasks. Their language capabilities have positioned them at the forefront of the future AGI (Artificial General Intelligence) race. However, on closer inspection, Valmeekam et al. (2024); Zecevic et al. (2023); Wu et al. (2024) highlight a significant gap between their language proficiency and reasoning abilities. Reasoning in LLMs and Vision Language Models (VLMs) aims to bridge this gap by enabling these models to think and re-evaluate their actions and responses. Reasoning is an essential capability for complex problem-solving and a necessary step toward establishing trust in Artificial Intelligence (AI). This will make AI suitable for deployment in sensitive domains, such as healthcare, banking, law, defense, security etc. In recent times, with the advent of powerful reasoning models like OpenAI O1 and DeepSeek R1, reasoning endowment has become a critical research topic in LLMs. In this paper, we provide a detailed overview and comparison of existing reasoning techniques and present a systematic survey of reasoning-imbued language models. We also study current challenges and present our findings.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.10814', 'html': 'https://arxiv.org/html/2503.10814v1', 'tex': '/src/2503.10814', 'doi': 'https://doi.org/10.48550/arXiv.2503.10814'}	Submission history From: Dibyanayan Bandyopadhyay [ view email ] [v1] Thu, 13 Mar 2025 19:03:41 UTC (1,247 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.10814'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.10814'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.10814'}]
2025-03-23	Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models	Computation and Language	https://arxiv.org/abs/2503.16419	"A Survey on Efficient Reasoning  This new survey investigates techniques to address the ""overthinking phenomenon"" in Large Reasoning Models (LRMs), categorizing existing methods into model-based optimizations, output-based reasoning reductions, and prompt-based efficiency enhancements. The survey  highlights ongoing efforts to balance reasoning capability and computational efficiency in models like OpenAI o1 and DeepSeek-R1."	https://x.com/omarsar0/status/1903109602826457531		2503.16419	['Yang Sui', 'Yu-Neng Chuang', 'Guanchu Wang', 'Jiamu Zhang', 'Tianyi Zhang', 'Jiayi Yuan', 'Hongyi Liu', 'Andrew Wen', 'Shaochen Zhong', 'Hanjie Chen', 'Xia Hu']	"ct:Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the ""overthinking phenomenon"". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking."	t Website:this https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.16419', 'html': 'https://arxiv.org/html/2503.16419v3', 'tex': '/src/2503.16419', 'doi': 'https://doi.org/10.48550/arXiv.2503.16419'}	Submission history From: Yang Sui [ view email ] [v1] Thu, 20 Mar 2025 17:59:38 UTC (1,027 KB) [v2] Sun, 23 Mar 2025 05:24:54 UTC (1,029 KB) [v3] Wed, 23 Apr 2025 17:46:54 UTC (1,039 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.16419'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.16419'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.16419'}]
2025-03-23	A-MEM: Agentic Memory for LLM Agents	Computation and Language	https://arxiv.org/abs/2502.12110	Agentic Memory for LLM Agents  Researchers from Rutgers University and Ant Group propose a new agentic memory system for LLM agents, addressing the need for long-term memory in complex real-world tasks. Key highlights include:   <br>● Dynamic & Zettelkasten-inspired design			2502.12110	['Wujiang Xu', 'Kai Mei', 'Hang Gao', 'Juntao Tan', 'Zujie Liang', 'Yongfeng Zhang']	ct:While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available atthis https URL, while the source code of the agentic memory system is available atthis https URL.		['Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2502.12110', 'html': 'https://arxiv.org/html/2502.12110v10', 'tex': '/src/2502.12110', 'doi': 'https://doi.org/10.48550/arXiv.2502.12110'}	Submission history From: Wujiang Xu [ view email ] [v1] Mon, 17 Feb 2025 18:36:14 UTC (603 KB) [v2] Mon, 3 Mar 2025 04:14:02 UTC (603 KB) [v3] Tue, 4 Mar 2025 15:09:10 UTC (603 KB) [v4] Mon, 14 Apr 2025 15:21:49 UTC (603 KB) [v5] Fri, 18 Apr 2025 17:26:57 UTC (603 KB) [v6] Sun, 11 May 2025 18:10:25 UTC (2,620 KB) [v7] Wed, 21 May 2025 05:16:32 UTC (2,629 KB) [v8] Tue, 27 May 2025 02:44:13 UTC (1,002 KB) [v9] Mon, 2 Jun 2025 22:21:21 UTC (995 KB) [v10] Tue, 15 Jul 2025 00:44:52 UTC (600 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.12110'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.12110'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.12110'}]
2025-03-23	DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2503.15265	DeepMesh  Researchers from Tsinghua University, Nanyang Technological University, and ShengShu propose DeepMesh, a transformer-based system that generates high-quality 3D meshes with artist-like topology. Key ideas include:   <br>● Efficient mesh tokenization	https://x.com/_akhaliq/status/1902713235079299255		2503.15265	['Ruowen Zhao', 'Junliang Ye', 'Zhengyi Wang', 'Guangce Liu', 'Yiwen Chen', 'Yikai Wang', 'Jun Zhu']	ct:Triangle meshes play a crucial role in 3D applications for efficient manipulation and rendering. While auto-regressive methods generate structured meshes by predicting discrete vertex tokens, they are often constrained by limited face counts and mesh incompleteness. To address these challenges, we propose DeepMesh, a framework that optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy incorporating a novel tokenization algorithm, along with improvements in data curation and processing, and (2) the introduction of Reinforcement Learning (RL) into 3D mesh generation to achieve human preference alignment via Direct Preference Optimization (DPO). We design a scoring standard that combines human evaluation with 3D metrics to collect preference pairs for DPO, ensuring both visual appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh generates meshes with intricate details and precise topology, outperforming state-of-the-art methods in both precision and quality. Project page:this https URL	t page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2503.15265', 'html': 'https://arxiv.org/html/2503.15265v1', 'tex': '/src/2503.15265', 'doi': 'https://doi.org/10.48550/arXiv.2503.15265'}	Submission history From: Ruowen Zhao [ view email ] [v1] Wed, 19 Mar 2025 14:39:30 UTC (36,630 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.15265'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.15265'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.15265'}]
2025-03-23	Deep Learning is Not So Mysterious or Different	Machine Learning	https://arxiv.org/abs/2503.02113	Deep Learning is Not So Mysterious or Different  Andrew Gordon Wilson (New York University) argues that deep learning phenomena such as benign overfitting, double descent, and the success of overparametrization are neither mysterious nor exclusive to neural networks. Major points include:   <br>● Benign Overfitting & Double Descent Explained			2503.02113	['Andrew Gordon Wilson']	ct:Deep neural networks are often seen as different from other model classes by defying conventional notions of generalization. Popular examples of anomalous generalization behaviour include benign overfitting, double descent, and the success of overparametrization. We argue that these phenomena are not distinct to neural networks, or particularly mysterious. Moreover, this generalization behaviour can be intuitively understood, and rigorously characterized, using long-standing generalization frameworks such as PAC-Bayes and countable hypothesis bounds. We present soft inductive biases as a key unifying principle in explaining these phenomena: rather than restricting the hypothesis space to avoid overfitting, embrace a flexible hypothesis space, with a soft preference for simpler solutions that are consistent with the data. This principle can be encoded in many model classes, and thus deep learning is not as mysterious or different from other model classes as it might seem. However, we also highlight how deep learning is relatively distinct in other ways, such as its ability for representation learning, phenomena such as mode connectivity, and its relative universality.	025	['Machine Learning (cs.LG)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2503.02113', 'html': 'https://arxiv.org/html/2503.02113v2', 'tex': '/src/2503.02113', 'doi': 'https://doi.org/10.48550/arXiv.2503.02113'}	Submission history From: Andrew Wilson [ view email ] [v1] Mon, 3 Mar 2025 22:56:04 UTC (1,206 KB) [v2] Thu, 10 Jul 2025 13:56:52 UTC (1,231 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.02113'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.02113'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.02113'}]
2025-03-23	GNNs as Predictors of Agentic Workflow Performances	Computation and Language	https://arxiv.org/abs/2503.11301	GNNs as Predictors of Agentic Workflow Performances  This work introduces FLORA-Bench, a large-scale benchmark to evaluate GNN-based predictors for automating and optimizing agentic workflows. It shows that Graph Neural Networks can efficiently predict the success of multi-agent LLM workflows, significantly reducing costly repeated model calls.			2503.11301	['Yuanshuo Zhang', 'Yuchen Hou', 'Bohan Tang', 'Shuo Chen', 'Muhan Zhang', 'Xiaowen Dong', 'Siheng Chen']	ct:Agentic workflows invoked by Large Language Models (LLMs) have achieved remarkable success in handling complex tasks. However, optimizing such workflows is costly and inefficient in real-world applications due to extensive invocations of LLMs. To fill this gap, this position paper formulates agentic workflows as computational graphs and advocates Graph Neural Networks (GNNs) as efficient predictors of agentic workflow performances, avoiding repeated LLM invocations for evaluation. To empirically ground this position, we construct FLORA-Bench, a unified platform for benchmarking GNNs for predicting agentic workflow performances. With extensive experiments, we arrive at the following conclusion: GNNs are simple yet effective predictors. This conclusion supports new applications of GNNs and a novel direction towards automating agentic workflow optimization. All codes, models, and data are available atthis https URL.	es, 11 figures	['Computation and Language (cs.CL)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2503.11301', 'html': 'https://arxiv.org/html/2503.11301v1', 'tex': '/src/2503.11301', 'doi': 'https://doi.org/10.48550/arXiv.2503.11301'}	Submission history From: Yuanshuo Zhang [ view email ] [v1] Fri, 14 Mar 2025 11:11:00 UTC (840 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.11301'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.11301'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.11301'}]
2025-03-16	Traveling Waves Integrate Spatial Information Through Time	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2502.06034	Traveling Waves Integrate Spatial Information Through Time  Researchers from Harvard University and Western University propose a wave-based recurrent neural network framework that uses traveling waves of neural activity to perform global spatial integration on visual tasks. Key ideas include:   <br>● “Hearing the Shape of a Drum” analogy	https://x.com/t_andy_keller/status/1899154774227878250		2502.06034	['Mozes Jacobs', 'Roberto C. Budzinski', 'Lyle Muller', 'Demba Ba', 'T. Anderson Keller']	"ct:Traveling waves of neural activity are widely observed in the brain, but their precise computational function remains unclear. One prominent hypothesis is that they enable the transfer and integration of spatial information across neural populations. However, few computational models have explored how traveling waves might be harnessed to perform such integrative processing. Drawing inspiration from the famous ""Can one hear the shape of a drum?"" problem -- which highlights how normal modes of wave dynamics encode geometric information -- we investigate whether similar principles can be leveraged in artificial neural networks. Specifically, we introduce convolutional recurrent neural networks that learn to produce traveling waves in their hidden states in response to visual stimuli, enabling spatial integration. By then treating these wave-like activation sequences as visual representations themselves, we obtain a powerful representational space that outperforms local feed-forward networks on tasks requiring global spatial context. In particular, we observe that traveling waves effectively expand the receptive field of locally connected neurons, supporting long-range encoding and communication of information. We demonstrate that models equipped with this mechanism solve visual semantic segmentation tasks demanding global integration, significantly outperforming local feed-forward models and rivaling non-local U-Net models with fewer parameters. As a first step toward traveling-wave-based communication and visual representation in artificial networks, our findings suggest wave-dynamics may provide efficiency and training stability benefits, while simultaneously offering a new framework for connecting models to biological recordings of neural activity."		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2502.06034', 'html': 'https://arxiv.org/html/2502.06034v4', 'tex': '/src/2502.06034', 'doi': 'https://doi.org/10.48550/arXiv.2502.06034'}	Submission history From: Mozes Jacobs [ view email ] [v1] Sun, 9 Feb 2025 21:14:27 UTC (12,576 KB) [v2] Wed, 12 Feb 2025 19:36:57 UTC (12,567 KB) [v3] Mon, 24 Feb 2025 16:47:01 UTC (10,638 KB) [v4] Wed, 11 Jun 2025 15:57:01 UTC (11,571 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.06034'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.06034'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.06034'}]
2025-03-16	Transformers without Normalization	Machine Learning	https://arxiv.org/abs/2503.10622	Transformers without Normalization  Researchers from Meta, NYU, MIT, and Princeton present a surprisingly simple method, Dynamic Tanh (DyT), that removes normalization layers (e.g. LayerNorm, RMSNorm) in Transformers while achieving equal or better results. Key ideas include:   <br>● Tanh-like mapping of LayerNorm	https://x.com/liuzhuang1234/status/1900370738588135805		2503.10622	['Jiachen Zhu', 'Xinlei Chen', 'Kaiming He', 'Yann LeCun', 'Zhuang Liu']	ct:Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \tanh(\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.	025; Project page:this https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2503.10622', 'html': 'https://arxiv.org/html/2503.10622v2', 'tex': '/src/2503.10622', 'doi': 'https://doi.org/10.48550/arXiv.2503.10622'}	Submission history From: Jiachen Zhu [ view email ] [v1] Thu, 13 Mar 2025 17:59:06 UTC (2,285 KB) [v2] Sat, 14 Jun 2025 08:10:48 UTC (2,277 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.10622'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.10622'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.10622'}]
2025-03-16	Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks	Computation and Language	https://arxiv.org/abs/2503.09572	Improving Planning of Agents for Long-Horizon Tasks  A team from UC Berkeley and the University of Tokyo presents a new framework, Plan-and-Act, that separates high-level planning from low-level execution in LLM-based agents. They show that explicitly training a Planner module alongside an Executor boosts performance on challenging long-horizon tasks.   <br>● Planner + Executor Architecture			2503.09572	['Lutfi Eren Erdogan', 'Nicholas Lee', 'Sehoon Kim', 'Suhong Moon', 'Hiroki Furuta', 'Gopala Anumanchipalli', 'Kurt Keutzer', 'Amir Gholami']	ct:Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as a text-only state-of-the-art 81.36% success rate on WebVoyager.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.09572', 'html': 'https://arxiv.org/html/2503.09572v3', 'tex': '/src/2503.09572', 'doi': 'https://doi.org/10.48550/arXiv.2503.09572'}	Submission history From: Nicholas Lee [ view email ] [v1] Wed, 12 Mar 2025 17:40:52 UTC (285 KB) [v2] Mon, 24 Mar 2025 23:48:07 UTC (520 KB) [v3] Tue, 22 Apr 2025 17:56:22 UTC (527 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.09572'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.09572'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.09572'}]
2025-03-16	Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning	Computation and Language	https://arxiv.org/abs/2503.09516	Search-R1  This paper tackles search-augmented reasoning by teaching LLMs to query a search engine multiple times—while they reason—using reinforcement learning. Key ideas include:   <br>● Multi-turn retrieval	https://x.com/omarsar0/status/1900550994116960391		2503.09516	['Bowen Jin', 'Hansi Zeng', 'Zhenrui Yue', 'Jinsung Yoon', 'Sercan Arik', 'Dong Wang', 'Hamed Zamani', 'Jiawei Han']	ct:Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available atthis https URL.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2503.09516', 'html': 'https://arxiv.org/html/2503.09516v5', 'tex': '/src/2503.09516', 'doi': 'https://doi.org/10.48550/arXiv.2503.09516'}	Submission history From: Bowen Jin [ view email ] [v1] Wed, 12 Mar 2025 16:26:39 UTC (196 KB) [v2] Wed, 19 Mar 2025 21:40:12 UTC (196 KB) [v3] Tue, 8 Apr 2025 14:03:26 UTC (311 KB) [v4] Mon, 21 Jul 2025 03:50:13 UTC (251 KB) [v5] Tue, 5 Aug 2025 19:08:38 UTC (251 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.09516'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.09516'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.09516'}]
2025-03-16	A Survey on Post-training of Large Language Models	Computation and Language	https://arxiv.org/abs/2503.06072	Post Training of LLMs  PoLMs like OpenAI-o1/o3 and DeepSeek-R1 tackle LLM shortcomings in reasoning, ethics, and specialized tasks. This survey tracks their evolution and provides a taxonomy of techniques across fine-tuning, alignment, reasoning, efficiency, and integration, guiding progress toward more robust, versatile AI.	https://x.com/ZainHasan6/status/1899541155924046006		2503.06072	['Guiyao Tie', 'Zeli Zhao', 'Dingjie Song', 'Fuyang Wei', 'Rong Zhou', 'Yurou Dai', 'Wen Yin', 'Zhejian Yang', 'Jiangyue Yan', 'Yao Su', 'Zhenhan Dai', 'Yifeng Xie', 'Yihan Cao', 'Lichao Sun', 'Pan Zhou', 'Lifang He', 'Hechang Chen', 'Yu Zhang', 'Qingsong Wen', 'Tianming Liu', 'Neil Zhenqiang Gong', 'Jiliang Tang', 'Caiming Xiong', 'Heng Ji', 'Philip S. Yu', 'Jianfeng Gao']	ct:The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT's alignment strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.	es, 21 figures, 9 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2503.06072', 'html': 'https://arxiv.org/html/2503.06072v3', 'tex': '/src/2503.06072', 'doi': 'https://doi.org/10.48550/arXiv.2503.06072'}	Submission history From: Guiyao Tie [ view email ] [v1] Sat, 8 Mar 2025 05:41:42 UTC (4,218 KB) [v2] Wed, 21 May 2025 03:38:28 UTC (4,072 KB) [v3] Fri, 1 Aug 2025 01:52:05 UTC (4,072 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.06072'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.06072'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.06072'}]
2025-03-16	Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models	Machine Learning	https://arxiv.org/abs/2503.09573	Block Diffusion  Researchers from Cornell Tech, Stanford, and Cohere present Block Diffusion (BD3-LMs), a novel framework that merges autoregressive (AR) modeling with discrete diffusion to enable parallel token sampling and flexible-length text generation. Key highlights include:   <br>● Combining AR and diffusion	https://x.com/_akhaliq/status/1900027075370586262		2503.09573	['Marianne Arriola', 'Aaron Gokaslan', 'Justin T. Chiu', 'Zhihan Yang', 'Zhixuan Qi', 'Jiaqi Han', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov']	ct:Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page:this https URL	025 Oral. We provide the code atthis https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2503.09573', 'html': None, 'tex': '/src/2503.09573', 'doi': 'https://doi.org/10.48550/arXiv.2503.09573'}	Submission history From: Marianne Arriola [ view email ] [v1] Wed, 12 Mar 2025 17:43:40 UTC (294 KB) [v2] Tue, 18 Mar 2025 15:58:18 UTC (291 KB) [v3] Sat, 17 May 2025 21:15:02 UTC (195 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.09573'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.09573'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.09573'}]
2025-03-09	The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models	Computation and Language	https://arxiv.org/abs/2503.02875	A Few Tokens Are All You Need  Researchers from Tencent AI Lab and The Chinese University of Hong Kong, Shenzhen propose a new approach to boost reasoning in LLMs by only fine-tuning on the first few tokens of generated solutions. Key ideas include:  <br>● Prefix Self-Consistency	https://x.com/omarsar0/status/1897334301462815001		2503.02875	['Ke Ji', 'Jiahao Xu', 'Tian Liang', 'Qiuzhi Liu', 'Zhiwei He', 'Xingyu Chen', 'Xiaoyuan Liu', 'Zhijie Wang', 'Junying Chen', 'Benyou Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu']	ct:Improving the reasoning capabilities of large language models (LLMs) typically requires supervised fine-tuning with labeled data or computationally expensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which leverages the observation of Prefix Self-Consistency -- the shared initial reasoning steps across diverse solution trajectories -- to enhance LLM reasoning efficiency. By training exclusively on the initial prefix substrings (as few as 8 tokens), UPFT removes the need for labeled data or exhaustive sampling. Experiments on reasoning benchmarks show that UPFT matches the performance of supervised methods such as Rejection Sampling Fine-Tuning, while reducing training time by 75% and sampling cost by 99%. Further analysis reveals that errors tend to appear in later stages of the reasoning process and that prefix-based training preserves the model's structural knowledge. This work demonstrates how minimal unsupervised fine-tuning can unlock substantial reasoning gains in LLMs, offering a scalable and resource-efficient alternative to conventional approaches.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2503.02875', 'html': 'https://arxiv.org/html/2503.02875v1', 'tex': '/src/2503.02875', 'doi': 'https://doi.org/10.48550/arXiv.2503.02875'}	Submission history From: Jiahao Xu [ view email ] [v1] Tue, 4 Mar 2025 18:56:03 UTC (957 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.02875'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.02875'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.02875'}]
2025-03-09	LLM Post-Training: A Deep Dive into Reasoning Large Language Models	Computation and Language	https://arxiv.org/abs/2502.21321	A Deep Dive into Reasoning LLMs  This survey explores how LLMs can be enhanced after pretraining through fine-tuning, reinforcement learning, and efficient inference strategies. It also highlights challenges like catastrophic forgetting, reward hacking, and ethical considerations, offering a roadmap for more capable and trustworthy AI systems.	https://x.com/omarsar0/status/1896572276461703193		2502.21321	['Komal Kumar', 'Tajamul Ashraf', 'Omkar Thawakar', 'Rao Muhammad Anwer', 'Hisham Cholakkal', 'Mubarak Shah', 'Ming-Hsuan Yang', 'Phillip H.S. Torr', 'Fahad Shahbaz Khan', 'Salman Khan']	ct:Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse applications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now increasingly shifting focus toward post-training techniques to achieve further breakthroughs. While pretraining provides a broad linguistic foundation, post-training methods enable LLMs to refine their knowledge, improve reasoning, enhance factual accuracy, and align more effectively with user intents and ethical considerations. Fine-tuning, reinforcement learning, and test-time scaling have emerged as critical strategies for optimizing LLMs performance, ensuring robustness, and improving adaptability across various real-world tasks. This survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs beyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs. We highlight emerging directions in model alignment, scalable adaptation, and inference-time reasoning, and outline future research directions. We also provide a public repository to continually track developments in this fast-evolving field:this https URL.	es, 7 figures, 3 tables, 377 references. Github Repo:this https URL	['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2502.21321', 'html': 'https://arxiv.org/html/2502.21321v2', 'tex': '/src/2502.21321', 'doi': 'https://doi.org/10.48550/arXiv.2502.21321'}	Submission history From: Tajamul Ashraf [ view email ] [v1] Fri, 28 Feb 2025 18:59:54 UTC (3,734 KB) [v2] Mon, 24 Mar 2025 09:34:38 UTC (3,729 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.21321'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.21321'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.21321'}]
2025-03-09	Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs	Computation and Language	https://arxiv.org/abs/2503.01307	Cognitive Behaviors that Enable Self-Improving Reasoners  Researchers from Stanford University and colleagues investigate why some language models excel in reinforcement learning (RL)-based self-improvement, while others quickly plateau. The study identifies four cognitive behaviors-verification, backtracking, subgoal setting, and backward chaining-that underpin successful problem-solving in both humans and language models. Key findings:  <br>● Cognitive behaviors drive model improvement	https://x.com/omarsar0/status/1897732423963885637		2503.01307	['Kanishk Gandhi', 'Ayush Chakravarthy', 'Anikait Singh', 'Nathan Lile', 'Noah D. Goodman']	ct:Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2503.01307', 'html': 'https://arxiv.org/html/2503.01307v1', 'tex': '/src/2503.01307', 'doi': 'https://doi.org/10.48550/arXiv.2503.01307'}	Submission history From: Kanishk Gandhi [ view email ] [v1] Mon, 3 Mar 2025 08:46:22 UTC (2,097 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.01307'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.01307'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.01307'}]
2025-03-09	Forecasting Rare Language Model Behaviors	Machine Learning	https://arxiv.org/abs/2502.16797	"Forecasting Rare Language Model Behaviors  A team from Anthropic and collaborators introduced a method to predict ""one-in-a-million"" failures that might only appear at deployment scale, enabling developers to patch issues preemptively. Key insights include:  <br>● Elicitation probabilities"	https://x.com/AnthropicAI/status/1894495059954860055		2502.16797	['Erik Jones', 'Meg Tong', 'Jesse Mu', 'Mohammed Mahfoud', 'Jan Leike', 'Roger Grosse', 'Jared Kaplan', 'William Fithian', 'Ethan Perez', 'Mrinank Sharma']	ct:Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small-scale beta test, yet reveal dangerous information when processing billions of requests at deployment. To remedy this, we introduce a method to forecast potential risks across orders of magnitude more queries than we test during evaluation. We make forecasts by studying each query's elicitation probability -- the probability the query produces a target behavior -- and demonstrate that the largest observed elicitation probabilities predictably scale with the number of queries. We find that our forecasts can predict the emergence of diverse undesirable behaviors -- such as assisting users with dangerous chemical synthesis or taking power-seeking actions -- across up to three orders of magnitude of query volume. Our work enables model developers to proactively anticipate and patch rare failures before they manifest during large-scale deployments.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.16797', 'html': 'https://arxiv.org/html/2502.16797v1', 'tex': '/src/2502.16797', 'doi': 'https://doi.org/10.48550/arXiv.2502.16797'}	Submission history From: Erik Jones [ view email ] [v1] Mon, 24 Feb 2025 03:16:15 UTC (2,159 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.16797'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.16797'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.16797'}]
2025-03-09	How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach	Computation and Language	https://arxiv.org/abs/2503.01141	"How Well do LLMs Compress Their Own Chain-of-Thought?  This new paper investigates how LLMs balance chain-of-thought (CoT) reasoning length against accuracy. It introduces token complexity, a minimal token threshold needed for correct problem-solving, and shows that even seemingly different CoT ""compression prompts"" (like ""use bullet points"" or ""remove grammar"") fall on the same universal accuracy-length trade-off curve. Key highlights include:  <br>● Universal accuracy-length trade-off"	https://x.com/omarsar0/status/1896939453069074907		2503.01141	['Ayeong Lee', 'Ethan Che', 'Tianyi Peng']	ct:Chain-of-thought prompting has emerged as a powerful technique for enabling large language models (LLMs) to solve complex reasoning tasks. However, these reasoning chains can be verbose, raising concerns about efficiency. In response, recent works have sought to decrease response lengths through simple prompting strategies (e.g. 'be concise'). In this work, we conduct the first systematic study of the relationship between reasoning length and model performance across a diverse range of compression instructions (e.g. 'use 10 words or less' or 'remove all punctuation'). In doing so, we discover a universal tradeoff between reasoning length and accuracy that persists across even very distinct reasoning chains. We demonstrate that this tradeoff emerges from a sharp threshold behavior at the question level: each task has an intrinsic 'token complexity' - a minimal number of tokens required for successful problem-solving. We show how token complexity enables us to compute information-theoretic limits on the accuracy-compression tradeoff, and find that prompt-based compression strategies operate far from these theoretical limits. This suggests there may be significant room for improvement and our framework provides a benchmark to help researchers evaluate progress in reasoning efficiency. Our work also highlights the importance of adaptive compression -- giving shorter responses for easier questions -- and we show that token complexity is a useful tool for measuring this capability.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2503.01141', 'html': 'https://arxiv.org/html/2503.01141v2', 'tex': '/src/2503.01141', 'doi': 'https://doi.org/10.48550/arXiv.2503.01141'}	Submission history From: Ethan Che [ view email ] [v1] Mon, 3 Mar 2025 03:48:20 UTC (13,296 KB) [v2] Tue, 1 Apr 2025 00:41:36 UTC (17,445 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.01141'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.01141'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.01141'}]
2025-03-09	LADDER: Self-Improving LLMs Through Recursive Problem Decomposition	Machine Learning	https://arxiv.org/abs/2503.00735	LADDER  LADDER is a framework enabling LLMs to recursively generate and solve progressively simpler variants of complex problems-boosting math integration accuracy. Key insights include:  <br>● Autonomous difficulty-driven learning	https://x.com/yoshiyama_akira/status/1897662722679959583		2503.00735	['Toby Simonds', 'Akira Yoshiyama']	ct:We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of complex problems. Unlike prior approaches that require curated datasets or human feedback, LADDER leverages a model's own capabilities to generate easier question variants. We demonstrate LADDER's effectiveness in the subject of mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve 73% on the MIT Integration Bee qualifying examination. We also introduce TTRL (Test-Time Reinforcement Learning), where we perform reinforcement learning on variants of test problems at inference time. TTRL enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of 90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's performance. These results show how self-directed strategic learning can achieve significant capability improvements without relying on architectural scaling or human supervision.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2503.00735', 'html': 'https://arxiv.org/html/2503.00735v3', 'tex': '/src/2503.00735', 'doi': 'https://doi.org/10.48550/arXiv.2503.00735'}	Submission history From: Akira Yoshiyama [ view email ] [v1] Sun, 2 Mar 2025 05:16:43 UTC (286 KB) [v2] Tue, 4 Mar 2025 14:30:32 UTC (203 KB) [v3] Wed, 5 Mar 2025 11:50:24 UTC (203 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2503.00735'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2503.00735'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2503.00735'}]
2025-03-09	Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems	Computation and Language	https://arxiv.org/abs/2502.19328	"Agentic Reward Modeling  This paper proposes a new reward framework-Agentic Reward Modeling-that combines human preference models with ""verifiable correctness"" signals to provide more reliable rewards for training and evaluating LLMs.  <br>● Reward agent ""REWARDAGENT"""	https://x.com/HaoPengNLP/status/1894980379305705475		2502.19328	['Hao Peng', 'Yunjia Qi', 'Xiaozhi Wang', 'Zijun Yao', 'Bin Xu', 'Lei Hou', 'Juanzi Li']	ct:Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (this https URL).	es, 5 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2502.19328', 'html': 'https://arxiv.org/html/2502.19328v1', 'tex': '/src/2502.19328', 'doi': 'https://doi.org/10.48550/arXiv.2502.19328'}	Submission history From: Hao Peng [ view email ] [v1] Wed, 26 Feb 2025 17:19:12 UTC (773 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.19328'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.19328'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.19328'}]
2025-03-09	Fractal Generative Models	Machine Learning	https://arxiv.org/abs/2502.17437	"Fractal Generative Models  Researchers from MIT CSAIL & Google DeepMind introduce a novel fractal-based framework for generative modeling, where entire generative modules are treated as atomic ""building blocks"" and invoked recursively-resulting in self-similar fractal architectures:  <br>● Atomic generators as fractal modules"		"{""Code"": ""https://github.com/LTH14/fractalgen""}"	2502.17437	['Tianhong Li', 'Qinyi Sun', 'Lijie Fan', 'Kaiming He']	ct:Modularization is a cornerstone of computer science, abstracting complex functions into atomic building blocks. In this paper, we introduce a new level of modularization by abstracting generative models into atomic generative modules. Analogous to fractals in mathematics, our method constructs a new type of generative model by recursively invoking atomic generative modules, resulting in self-similar fractal architectures that we call fractal generative models. As a running example, we instantiate our fractal framework using autoregressive models as the atomic generative modules and examine it on the challenging task of pixel-by-pixel image generation, demonstrating strong performance in both likelihood estimation and generation quality. We hope this work could open a new paradigm in generative modeling and provide a fertile ground for future research. Code is available atthis https URL.		['Machine Learning (cs.LG)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2502.17437', 'html': 'https://arxiv.org/html/2502.17437v2', 'tex': '/src/2502.17437', 'doi': 'https://doi.org/10.48550/arXiv.2502.17437'}	Submission history From: Tianhong Li [ view email ] [v1] Mon, 24 Feb 2025 18:59:56 UTC (11,567 KB) [v2] Tue, 25 Feb 2025 14:28:34 UTC (11,567 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.17437'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.17437'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.17437'}]
2025-03-02	Chain of Draft: Thinking Faster by Writing Less	Computation and Language	https://arxiv.org/abs/2502.18600	Chain-of-Draft  To address the issue of latency in reasoning LLMs, this work introduces Chain-of-Draft (CoD). Here is a quick summary of the key highlights:   <br>● What is CoD?	https://x.com/omarsar0/status/1895135560634900762		2502.18600	['Silei Xu', 'Wenhao Xie', 'Lingxiao Zhao', 'Pengcheng He']	ct:Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks. Our code and data are available atthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2502.18600', 'html': 'https://arxiv.org/html/2502.18600v2', 'tex': '/src/2502.18600', 'doi': 'https://doi.org/10.48550/arXiv.2502.18600'}	Submission history From: Silei Xu [ view email ] [v1] Tue, 25 Feb 2025 19:36:06 UTC (8,936 KB) [v2] Mon, 3 Mar 2025 17:08:21 UTC (8,937 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.18600'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.18600'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.18600'}]
2025-03-02	Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs	Computation and Language	https://arxiv.org/abs/2502.17424	Emergent Misalignment  New research investigates an unexpected phenomenon: finetuning an LLM on a narrow task can cause it to become broadly misaligned across unrelated domains. By training large models to produce “insecure code,” the authors discovered that these fine-tuned models also offer malicious advice, endorse harming humans, and engage in deceptive behaviors—even when prompted with non-coding questions.   <br>● Surprising misalignment from narrow training	https://x.com/OwainEvans_UK/status/1894436637054214509		2502.17424	['Jan Betley', 'Daniel Tan', 'Niels Warncke', 'Anna Sztyber-Betley', 'Xuchan Bao', 'Martín Soto', 'Nathan Labenz', 'Owain Evans']	ct:We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.	es, 38 figures An earlier revision of this paper was accepted at ICML 2025. Since then, it has been updated to include new results on training dynamics (4.7) and base models (4.8)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Cryptography and Security (cs.CR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.17424', 'html': 'https://arxiv.org/html/2502.17424v6', 'tex': '/src/2502.17424', 'doi': 'https://doi.org/10.48550/arXiv.2502.17424'}	Submission history From: Jan Betley [ view email ] [v1] Mon, 24 Feb 2025 18:56:03 UTC (8,456 KB) [v2] Tue, 25 Feb 2025 23:57:54 UTC (8,458 KB) [v3] Fri, 28 Feb 2025 00:11:35 UTC (8,460 KB) [v4] Wed, 5 Mar 2025 02:15:50 UTC (8,460 KB) [v5] Sun, 4 May 2025 22:39:38 UTC (8,731 KB) [v6] Mon, 12 May 2025 06:51:03 UTC (8,731 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.17424'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.17424'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.17424'}]
2025-03-02	SPECTRE: An FFT-Based Efficient Drop-In Replacement to Self-Attention for Long Contexts	Machine Learning	https://arxiv.org/abs/2502.18394	An Efficient Alternative to Self-Attention  This paper presents FFTNet, a framework that replaces costly self-attention with an adaptive spectral filtering technique based on the Fast Fourier Transform (FFT).  Key components:   <br>● Global token mixing via FFT	https://x.com/omarsar0/status/1894757821587296614		2502.18394	['Jacob Fein-Ashley', 'Neelesh Gupta', 'Rajgopal Kannan', 'Viktor Prasanna']	ct:Long-context transformers face significant efficiency challenges due to the quadratic cost of self-attention. However, many modern applications-from multi-turn dialogue to high-resolution vision-require contexts spanning tens of thousands of tokens. We introduce SPECTRE, a method that replaces each attention head with a fast real FFT, a content-adaptive spectral gate, and an inverse FFT, reducing per-layer complexity from $\mathcal{O}(L^{2})$ to $O(L\log L)$ while preserving the surrounding architecture. We extend this efficiency to autoregressive generation through our Prefix-FFT cache and enhance local feature representation with an optional wavelet module that adds negligible computational overhead. Our experiments demonstrate that SPECTRE operates up to 7$\times$ faster than FlashAttention-2 on 128k-token contexts while matching or exceeding baseline performance on PG-19 language modeling and ImageNet-1k classification tasks. SPECTRE achieves these improvements by adding fewer than 6\% parameters to the base model, making hundred-kilotoken context processing feasible on commodity GPUs without specialized hardware.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.18394', 'html': 'https://arxiv.org/html/2502.18394v7', 'tex': '/src/2502.18394', 'doi': 'https://doi.org/10.48550/arXiv.2502.18394'}	Submission history From: Jacob Fein-Ashley [ view email ] [v1] Tue, 25 Feb 2025 17:43:43 UTC (366 KB) [v2] Wed, 26 Feb 2025 16:31:58 UTC (367 KB) [v3] Fri, 28 Feb 2025 17:06:23 UTC (447 KB) [v4] Thu, 6 Mar 2025 15:39:55 UTC (215 KB) [v5] Sun, 16 Mar 2025 15:17:17 UTC (1,966 KB) [v6] Wed, 30 Apr 2025 23:14:57 UTC (543 KB) [v7] Sun, 18 May 2025 03:12:25 UTC (953 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.18394'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.18394'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.18394'}]
2025-03-02	PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving	Artificial Intelligence	https://arxiv.org/abs/2502.16111	PlanGEN  PlanGEN is a multi-agent framework designed to enhance planning and reasoning in LLMs through constraint-guided iterative verification and adaptive algorithm selection. Key insights include:   <br>● Constraint-Guided Verification for Planning	https://x.com/dair_ai/status/1895532543652642850		2502.16111	['Mihir Parmar', 'Xin Liu', 'Palash Goyal', 'Yanfei Chen', 'Long Le', 'Swaroop Mishra', 'Hossein Mobahi', 'Jindong Gu', 'Zifeng Wang', 'Hootan Nakhost', 'Chitta Baral', 'Chen-Yu Lee', 'Tomas Pfister', 'Hamid Palangi']	ct:Recent agent frameworks and inference-time algorithms often struggle with complex planning problems due to limitations in verifying generated plans or reasoning and varying complexity of instances within a single task. Many existing methods for these tasks either perform task-level verification without considering constraints or apply inference-time algorithms without adapting to instance-level complexity. To address these limitations, we propose PlanGEN, a model-agnostic and easily scalable agent framework with three key components: constraint, verification, and selection agents. Specifically, our approach proposes constraint-guided iterative verification to enhance performance of inference-time algorithms--Best of N, Tree-of-Thought, and REBASE. In PlanGEN framework, the selection agent optimizes algorithm choice based on instance complexity, ensuring better adaptability to complex planning problems. Experimental results demonstrate significant improvements over the strongest baseline across multiple benchmarks, achieving state-of-the-art results on NATURAL PLAN ($\sim$8%$\uparrow$), OlympiadBench ($\sim$4%$\uparrow$), DocFinQA ($\sim$7%$\uparrow$), and GPQA ($\sim$1%$\uparrow$). Our key finding highlights that constraint-guided iterative verification improves inference-time algorithms, and adaptive selection further boosts performance on complex planning and reasoning problems.	es	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2502.16111', 'html': None, 'tex': '/src/2502.16111', 'doi': 'https://doi.org/10.48550/arXiv.2502.16111'}	Submission history From: Mihir Parmar [ view email ] [v1] Sat, 22 Feb 2025 06:21:56 UTC (2,896 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.16111'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.16111'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.16111'}]
2025-03-02	METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2502.17651	A Multi-Agent Framework for Chart Generation  METAL is a vision-language model (VLM)-based multi-agent framework designed to significantly enhance automatic chart-to-code generation by decomposing the task into specialized iterative steps. Key highlights include:   <br>● Specialized multi-agent collaboration	https://x.com/omarsar0/status/1895528398820425741		2502.17651	['Bingxuan Li', 'Yiwei Wang', 'Jiuxiang Gu', 'Kai-Wei Chang', 'Nanyun Peng']	ct:Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2502.17651', 'html': 'https://arxiv.org/html/2502.17651v3', 'tex': '/src/2502.17651', 'doi': 'https://doi.org/10.48550/arXiv.2502.17651'}	Submission history From: Bingxuan Li [ view email ] [v1] Mon, 24 Feb 2025 21:01:39 UTC (1,792 KB) [v2] Fri, 28 Feb 2025 07:28:24 UTC (1,793 KB) [v3] Thu, 6 Mar 2025 00:45:00 UTC (1,793 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.17651'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.17651'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.17651'}]
2025-03-02	LightThinker: Thinking Step-by-Step Compression	Computation and Language	https://arxiv.org/abs/2502.15589	LightThinker  This new paper proposes a novel approach to dynamically compress reasoning steps in LLMs, significantly improving efficiency without sacrificing accuracy. Key insights include:   <br>● Compression of intermediate thoughts	https://x.com/omarsar0/status/1894068783700218205		2502.15589	['Jintian Zhang', 'Yuqi Zhu', 'Mengshu Sun', 'Yujie Luo', 'Shuofei Qiao', 'Lun Du', 'Da Zheng', 'Huajun Chen', 'Ningyu Zhang']	ct:Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance. Code will be released atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)', 'Multimedia (cs.MM)']	{'pdf': '/pdf/2502.15589', 'html': 'https://arxiv.org/html/2502.15589v1', 'tex': '/src/2502.15589', 'doi': 'https://doi.org/10.48550/arXiv.2502.15589'}	Submission history From: Ningyu Zhang [ view email ] [v1] Fri, 21 Feb 2025 16:57:22 UTC (7,113 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.15589'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.15589'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.15589'}]
2025-03-02	A Systematic Survey of Automatic Prompt Optimization Techniques	Computation and Language	https://arxiv.org/abs/2502.16923	A Systematic Survey of Prompt Optimization  This paper offers a comprehensive survey of Automatic Prompt Optimization (APO)—defining its scope, presenting a unifying 5-part framework, categorizing existing methods, and highlighting key progress and challenges in automating prompt engineering for LLMs.	https://x.com/omarsar0/status/1894412798282915994		2502.16923	['Kiran Ramnath', 'Kang Zhou', 'Sheng Guan', 'Soumya Smruti Mishra', 'Xuan Qi', 'Zhengyuan Shen', 'Shuai Wang', 'Sangmin Woo', 'Sullam Jeoung', 'Yawei Wang', 'Haozhu Wang', 'Han Ding', 'Yuzhe Lu', 'Zhichao Xu', 'Yun Zhou', 'Balasubramaniam Srinivasan', 'Qiaojing Yan', 'Yueyan Chen', 'Haibo Ding', 'Panpan Xu', 'Lin Lee Cheong']	ct:Since the advent of large language models (LLMs), prompt engineering has been a crucial step for eliciting desired responses for various Natural Language Processing (NLP) tasks. However, prompt engineering remains an impediment for end users due to rapid advances in models, tasks, and associated best practices. To mitigate this, Automatic Prompt Optimization (APO) techniques have recently emerged that use various automated techniques to help improve the performance of LLMs on various tasks. In this paper, we present a comprehensive survey summarizing the current progress and remaining challenges in this field. We provide a formal definition of APO, a 5-part unifying framework, and then proceed to rigorously categorize all relevant works based on their salient features therein. We hope to spur further research guided by our framework.	 pages, 31 total pages, 1 figure	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2502.16923', 'html': 'https://arxiv.org/html/2502.16923v2', 'tex': '/src/2502.16923', 'doi': 'https://doi.org/10.48550/arXiv.2502.16923'}	Submission history From: Kiran Ramnath [ view email ] [v1] Mon, 24 Feb 2025 07:29:13 UTC (62 KB) [v2] Wed, 2 Apr 2025 20:04:21 UTC (168 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.16923'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.16923'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.16923'}]
2025-03-02	Protein Large Language Models: A Comprehensive Survey	Quantitative Biology > Biomolecules	https://arxiv.org/abs/2502.17504	Protein LLMs  A comprehensive overview of Protein LLMs, including architectures, training datasets, evaluation metrics, and applications.	https://x.com/omarsar0/status/1894760600141811861		2502.17504	['Yijia Xiao', 'Wanjia Zhao', 'Junkai Zhang', 'Yiqiao Jin', 'Han Zhang', 'Zhicheng Ren', 'Renliang Sun', 'Haixin Wang', 'Guancheng Wan', 'Pan Lu', 'Xiao Luo', 'Yu Zhang', 'James Zou', 'Yizhou Sun', 'Wei Wang']	ct:Protein-specific large language models (Protein LLMs) are revolutionizing protein science by enabling more efficient protein structure prediction, function annotation, and design. While existing surveys focus on specific aspects or applications, this work provides the first comprehensive overview of Protein LLMs, covering their architectures, training datasets, evaluation metrics, and diverse applications. Through a systematic analysis of over 100 articles, we propose a structured taxonomy of state-of-the-art Protein LLMs, analyze how they leverage large-scale protein sequence data for improved accuracy, and explore their potential in advancing protein engineering and biomedical research. Additionally, we discuss key challenges and future directions, positioning Protein LLMs as essential tools for scientific discovery in protein science. Resources are maintained atthis https URL.	es, 4 figures, 5 tables	['Biomolecules (q-bio.BM)', 'Artificial Intelligence (cs.AI)', 'Computational Engineering, Finance, and Science (cs.CE)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.17504', 'html': None, 'tex': '/src/2502.17504', 'doi': 'https://doi.org/10.48550/arXiv.2502.17504'}	Submission history From: Yijia Xiao [ view email ] [v1] Fri, 21 Feb 2025 19:22:10 UTC (19,937 KB) [v2] Thu, 6 Mar 2025 16:14:45 UTC (19,937 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.17504'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.17504'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.17504'}]
2025-02-23	Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention	Computation and Language	https://arxiv.org/abs/2502.11089	Native Sparse Attention  DeepSeek-AI and collaborators present Native Sparse Attention (NSA), a novel sparse attention mechanism designed to improve computational efficiency while maintaining model performance in long-context language modeling.  Key contributions:   <br> ● Hierarchical Sparse Attention	https://x.com/deepseek_ai/status/1891745487071609327		2502.11089	['Jingyang Yuan', 'Huazuo Gao', 'Damai Dai', 'Junyu Luo', 'Liang Zhao', 'Zhengyan Zhang', 'Zhenda Xie', 'Y. X. Wei', 'Lean Wang', 'Zhiping Xiao', 'Yuqing Wang', 'Chong Ruan', 'Ming Zhang', 'Wenfeng Liang', 'Wangding Zeng']	ct:Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.11089', 'html': 'https://arxiv.org/html/2502.11089v2', 'tex': '/src/2502.11089', 'doi': 'https://doi.org/10.48550/arXiv.2502.11089'}	Submission history From: Wenfeng Liang [ view email ] [v1] Sun, 16 Feb 2025 11:53:44 UTC (915 KB) [v2] Thu, 27 Feb 2025 09:01:21 UTC (916 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.11089'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.11089'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.11089'}]
2025-02-23	Large Language Diffusion Models	Computation and Language	https://arxiv.org/abs/2502.09992	Large Language Diffusion Model  Proposes LLaDA, a diffusion-based approach that can match or beat leading autoregressive LLMs in many tasks.  Key highlights:   <br> ● Questioning autoregressive dominance	https://x.com/omarsar0/status/1891568386494300252		2502.09992	['Shen Nie', 'Fengqi Zhu', 'Zebin You', 'Xiaolu Zhang', 'Jingyang Ou', 'Jun Hu', 'Jun Zhou', 'Yankai Lin', 'Ji-Rong Wen', 'Chongxuan Li']	ct:Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs. Project page and codes:this https URL.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.09992', 'html': 'https://arxiv.org/html/2502.09992v2', 'tex': '/src/2502.09992', 'doi': 'https://doi.org/10.48550/arXiv.2502.09992'}	Submission history From: Shen Nie [ view email ] [v1] Fri, 14 Feb 2025 08:23:51 UTC (1,069 KB) [v2] Tue, 18 Feb 2025 16:08:59 UTC (1,070 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.09992'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.09992'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.09992'}]
2025-02-23	SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?	Machine Learning	https://arxiv.org/abs/2502.12115	SWE-Lancer  Researchers from OpenAI introduce SWE-Lancer, a benchmark evaluating LLMs on 1,488 real-world freelance software engineering tasks from Upwork, collectively worth $1M in payouts.  Key takeaways:   <br> ● A new benchmark for software engineering automation	https://x.com/OpenAI/status/1891911123517018521		2502.12115	['Samuel Miserendino', 'Michele Wang', 'Tejal Patwardhan', 'Johannes Heidecke']	ct:We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \$50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (this https URL). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.	s, 30 pages appendix	['Machine Learning (cs.LG)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2502.12115', 'html': None, 'tex': '/src/2502.12115', 'doi': 'https://doi.org/10.48550/arXiv.2502.12115'}	Submission history From: Michele Wang [ view email ] [v1] Mon, 17 Feb 2025 18:41:16 UTC (3,335 KB) [v2] Wed, 19 Feb 2025 06:48:26 UTC (3,335 KB) [v3] Mon, 24 Feb 2025 18:58:05 UTC (3,335 KB) [v4] Thu, 29 May 2025 23:07:34 UTC (3,714 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.12115'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.12115'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.12115'}]
2025-02-23	Optimizing Model Selection for Compound AI Systems	Artificial Intelligence	https://arxiv.org/abs/2502.14815	Optimizing Model Selection for Compound AI  Researchers from Microsoft Research and collaborators introduce LLMSelector, a framework to improve multi-call LLM pipelines by selecting the best model per module instead of using one LLM everywhere.  Key insights include:   <br> ● Large performance boost with per-module model choices	https://x.com/omarsar0/status/1892945381174210933		2502.14815	['Lingjiao Chen', 'Jared Quincy Davis', 'Boris Hanin', 'Peter Bailis', 'Matei Zaharia', 'James Zou', 'Ion Stoica']	ct:Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSelector iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSelector is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector confers 5%-70% accuracy gains compared to using the same LLM for all modules.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2502.14815', 'html': 'https://arxiv.org/html/2502.14815v1', 'tex': '/src/2502.14815', 'doi': 'https://doi.org/10.48550/arXiv.2502.14815'}	Submission history From: Lingjiao Chen [ view email ] [v1] Thu, 20 Feb 2025 18:36:25 UTC (536 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.14815'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.14815'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.14815'}]
2025-02-23	The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks	Artificial Intelligence	https://www.arxiv.org/abs/2502.08235	The Danger of Overthinking  This paper investigates overthinking in Large Reasoning Models (LRMs)—a phenomenon where models prioritize extended internal reasoning over interacting with their environment. Their study analyzes 4,018 software engineering task trajectories to understand how reasoning models handle decision-making in agentic settings.  Key findings:   <br> ● Overthinking reduces task performance	https://x.com/Alex_Cuadron/status/1890533660434321873		2502.08235	['Alejandro Cuadron', 'Dacheng Li', 'Wenjie Ma', 'Xingyao Wang', 'Yichuan Wang', 'Siyuan Zhuang', 'Shu Liu', 'Luis Gaspar Schroeder', 'Tian Xia', 'Huanzhi Mao', 'Nicholas Thumiger', 'Aditya Desai', 'Ion Stoica', 'Ana Klimovic', 'Graham Neubig', 'Joseph E. Gonzalez']	ct:Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments, such as selecting the solution with the lower overthinking score, can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction atthis https URL.		['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2502.08235', 'html': None, 'tex': '/src/2502.08235', 'doi': 'https://doi.org/10.48550/arXiv.2502.08235'}	Submission history From: Alejandro Cuadron [ view email ] [v1] Wed, 12 Feb 2025 09:23:26 UTC (274 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.08235'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.08235'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.08235'}]
2025-02-23	Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking	Computation and Language	https://arxiv.org/abs/2502.13842v1	Inner Thinking Transformers  Inner Thinking Transformer (ITT) is a new method that enhances reasoning efficiency in small-scale LLMs via dynamic depth scaling. ITT aims to mitigate parameter bottlenecks in LLMs, providing scalable reasoning efficiency without expanding model size.  Key contributions:   <br> ● Adaptive Token Processing	https://x.com/dair_ai/status/1893308342073991258		2502.13842v1	['Yilong Chen', 'Junyuan Shang', 'Zhenyu Zhang', 'Yanxi Xie', 'Jiawei Sheng', 'Tingwen Liu', 'Shuohuan Wang', 'Yu Sun', 'Hua Wu', 'Haifeng Wang']	ct:Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5\% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2\%, and outperforms Transformer/Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways.	es, 11 figures	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2502.13842v1', 'html': 'https://arxiv.org/html/2502.13842v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2502.13842'}	Submission history From: Yilong Chen [ view email ] [v1] Wed, 19 Feb 2025 16:02:23 UTC (1,542 KB) [v2] Sun, 23 Feb 2025 04:31:53 UTC (1,542 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.13842'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.13842'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.13842'}]
2025-02-16	Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach	Machine Learning	https://arxiv.org/abs/2502.05171	Scaling up Test-Time Compute with Latent Reasoning  This work introduces a latent recurrent-depth transformer, a model that scales test-time reasoning without relying on additional token generation. Instead of increasing the context window or fine-tuning for Chain-of-Thought (CoT), this approach enables iterative latent space reasoning at inference, achieving improvements comparable to a 50B parameter model despite having only 3.5B parameters. Key insights include:   <br> ● Recurrent test-time computation	https://x.com/omarsar0/status/1890506648772571452		2502.05171	['Jonas Geiping', 'Sean McLeish', 'Neel Jain', 'John Kirchenbauer', 'Siddharth Singh', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Abhinav Bhatele', 'Tom Goldstein']	ct:We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.	del is available atthis https URL. Code and data recipe can be found atthis https URL	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2502.05171', 'html': 'https://arxiv.org/html/2502.05171v2', 'tex': '/src/2502.05171', 'doi': 'https://doi.org/10.48550/arXiv.2502.05171'}	Submission history From: Jonas Geiping [ view email ] [v1] Fri, 7 Feb 2025 18:55:02 UTC (12,678 KB) [v2] Mon, 17 Feb 2025 17:14:04 UTC (12,693 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.05171'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.05171'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.05171'}]
2025-02-16	On the Emergence of Thinking in LLMs I: Searching for the Right Intuition	Artificial Intelligence	https://arxiv.org/abs/2502.06773	Reinforcement Learning via Self-Play  Researchers propose Reinforcement Learning via Self-Play (RLSP) as a framework to train LLMs to “think” through complex problems. Key ideas include:   <br> ● Emergent reasoning via self-play: RLSP trains an LLM on reasoning  tasks by having it generate solution steps and reward itself for  exploration and correctness, effectively enabling it to search for  answers like an algorithm.   <br> ● Three-phase training: (1) Begin with supervised fine-tuning on human  or synthetic reasoning traces, (2) add an exploration reward to  encourage trying diverse solution paths, and (3) employ an outcome  verifier in RL to ensure answers are correct (preventing reward  hacking).   <br> ● Notable performance gains: On math benchmarks, a relatively small  model (8B) fine-tuned with RLSP saw +23% accuracy on MATH dataset, and  a 32B model gained +10% on challenging Olympiad problems—significant  jumps achieved by training for better reasoning.   <br> ● Uncovering new behaviors: RLSP-trained models exhibit emergent  problem-solving behaviors like backtracking on flawed steps and  self-verification of answers. This suggests that appropriately scaling  the training process can induce more robust reasoning capabilities in  LLMs.	https://x.com/omarsar0/status/1889697727703134544		2502.06773	['Guanghao Ye', 'Khiem Duc Pham', 'Xinzhi Zhang', 'Sivakanth Gopi', 'Baolin Peng', 'Beibin Li', 'Janardhan Kulkarni', 'Huseyin A. Inan']	ct:Recent AI advancements, such as OpenAI's new models, are transforming LLMs into LRMs (Large Reasoning Models) that perform reasoning during inference, taking extra time and compute for higher-quality outputs. We aim to uncover the algorithmic framework for training LRMs. Methods like self-consistency, PRM, and AlphaZero suggest reasoning as guided search. We ask: what is the simplest, most scalable way to enable search in LLMs?We propose a post-training framework called Reinforcement Learning via Self-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with human or synthetic demonstrations of the reasoning process, (2) using an exploration reward signal to encourage diverse and efficient reasoning behaviors, and (3) RL training with an outcome verifier to ensure correctness while preventing reward hacking. Our key innovation is to decouple exploration and correctness signals during PPO training, carefully balancing them to improve performance and efficiency.Empirical studies in the math domain show that RLSP improves reasoning. On the Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500 test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due to RLSP. However, a more important finding of this work is that the models trained using RLSP, even with the simplest exploration reward that encourages the model to take more intermediate steps, showed several emergent behaviors such as backtracking, exploration of ideas, and verification. These findings demonstrate that RLSP framework might be enough to enable emergence of complex reasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why RLSP search strategy is more suitable for LLMs inspired by a remarkable result that says CoT provably increases computational power of LLMs, which grows as the number of steps in CoT \cite{li2024chain,merrill2023expresssive}.	ct shortened for arXiv	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.06773', 'html': None, 'tex': '/src/2502.06773', 'doi': 'https://doi.org/10.48550/arXiv.2502.06773'}	Submission history From: Guanghao Ye [ view email ] [v1] Mon, 10 Feb 2025 18:52:04 UTC (819 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.06773'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.06773'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.06773'}]
2025-02-16	Competitive Programming with Large Reasoning Models	Machine Learning	https://arxiv.org/abs/2502.06807	Competitive Programming with Large Reasoning Models  OpenAI’s latest study puts a specialized coding AI against a scaled-up general model on competitive programming challenges to explore efficiency vs. specialization. Key findings:   <br> ● Generalist vs. specialist: A tailored model (o1-ioi) with  hand-crafted strategies for coding competitions achieved decent  results (placing ~50th percentile at IOI 2024 with some relaxed  competition constraints). However, a larger, general-purpose model  (o3) attained gold   medal-level performance without any domain-specific tricks.   <br> ● Reinforcement learning payoff: Both models were improved via RL  fine-tuning, but the scaled general model outperformed the expert  pipeline, solving programming tasks at a level comparable to elite  human coders (even matching top human ratings on Codeforces).   <br> ● Efficiency through scale: The results suggest that investing compute  in a bigger, broadly-trained transformer can yield greater efficiency  and performance than building task-specific optimizations. In other  words, scaling up a model’s reasoning ability can supersede manual  efficiency tweaks for complex tasks.   <br> ● Implication: For difficult reasoning tasks like coding, a single  large model with sufficient training can simplify deployment (no  custom inference routines needed) and still beat highly optimized  specialist systems, pointing toward a trend of “scale over  special-case” in transformer design.	https://x.com/arankomatsuzaki/status/1889522974467957033		2502.06807	['OpenAI', 'Ahmed El-Kishky', 'Alexander Wei', 'Andre Saraiva', 'Borys Minaiev', 'Daniel Selsam', 'David Dohan', 'Francis Song', 'Hunter Lightman', 'Ignasi Clavera', 'Jakub Pachocki', 'Jerry Tworek', 'Lorenz Kuhn', 'Lukasz Kaiser', 'Mark Chen', 'Max Schwarzer', 'Mostafa Rohaninejad', 'Nat McAleese', 'o3 contributors', 'Oleg Mürk', 'Rhythm Garg', 'Rui Shu', 'Szymon Sidor', 'Vineet Kosaraju', 'Wenda Zhou']	ct:We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2502.06807', 'html': 'https://arxiv.org/html/2502.06807v2', 'tex': '/src/2502.06807', 'doi': 'https://doi.org/10.48550/arXiv.2502.06807'}	Submission history From: Ahmed El-Kishky [ view email ] [v1] Mon, 3 Feb 2025 23:00:15 UTC (493 KB) [v2] Tue, 18 Feb 2025 22:21:40 UTC (493 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.06807'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.06807'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.06807'}]
2025-02-16	Training Language Models to Reason Efficiently	Machine Learning	https://arxiv.org/abs/2502.04463	Training Language Models to Reason Efficiently  A new RL approach teaches large reasoning models to allocate their reasoning effort efficiently, reducing wasted computation on easy problems. Key points include:   <br> ● Dynamic compute allocation: The method trains an LLM to adjust the  length of its CoT based on problem difficulty. Easy queries trigger  short reasoning, while hard ones use deeper thought, optimizing  inference time without sacrificing accuracy.   <br> ● RL-driven efficiency: Through RL, the model is rewarded for solving  tasks correctly with minimal steps, learning to avoid “overthinking.”  This yields a family of models along an efficiency spectrum controlled  by a single hyperparameter (trading off speed vs. accuracy).   <br> ● Big cost savings: On benchmark reasoning tasks, this trained model  cut down inference computation significantly while maintaining almost  the same performance as unconstrained reasoning. It learns when extra  reasoning steps are unnecessary, which is crucial for deploying  advanced LLMs cost-effectively.   <br> ● Efficient reasoning at scale: The approach addresses the multi-agent  style problem internally	https://x.com/omarsar0/status/1889328796224127428		2502.04463	['Daman Arora', 'Andrea Zanette']	ct:Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models.In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2502.04463', 'html': 'https://arxiv.org/html/2502.04463v3', 'tex': '/src/2502.04463', 'doi': 'https://doi.org/10.48550/arXiv.2502.04463'}	Submission history From: Daman Arora [ view email ] [v1] Thu, 6 Feb 2025 19:18:16 UTC (7,991 KB) [v2] Tue, 11 Feb 2025 18:06:02 UTC (7,804 KB) [v3] Mon, 19 May 2025 19:33:01 UTC (7,994 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.04463'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.04463'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.04463'}]
2025-02-16	LM2: Large Memory Models	Computation and Language	https://arxiv.org/abs/2502.06049	Large Memory Models  Large Memory Models (LM2) is a transformer architecture augmented with an external memory module to tackle tasks requiring extensive reasoning and long context. Key highlights include:   <br> ● Memory-augmented transformer: LM2 adds a dedicated memory repository  that the model can read/write via cross-attention, enabling it to  store and retrieve information across many reasoning steps. This  design addresses the limitations of standard transformers in tasks  like multi-hop reasoning and relational argumentation.   <br> ● Superior long-term reasoning: On the BABILong benchmark for  long-context reasoning, LM2 dramatically outperformed prior models	https://x.com/omarsar0/status/1889681118913577345		2502.06049	['Jikun Kang', 'Wenqi Wu', 'Filippos Christianos', 'Alex J. Chan', 'Fraser Greenlee', 'George Thomas', 'Marvin Purtorab', 'Andy Toulis']	ct:This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2502.06049', 'html': 'https://arxiv.org/html/2502.06049v1', 'tex': '/src/2502.06049', 'doi': 'https://doi.org/10.48550/arXiv.2502.06049'}	Submission history From: Jikun Kang [ view email ] [v1] Sun, 9 Feb 2025 22:11:42 UTC (2,066 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.06049'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.06049'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.06049'}]
2025-02-16	Auditing Prompt Caching in Language Model APIs	Computation and Language	https://arxiv.org/abs/2502.07776	Auditing Prompt Caching  Researchers from Stanford investigate how timing differences in LLM APIs can leak private user information through global prompt caching. They propose statistical audits to detect caching and reveal potentially significant security risks. Key insights include:   <br> ● Side-channel timing attacks	https://x.com/omarsar0/status/1889685386856673463		2502.07776	['Chenchen Gu', 'Xiang Lisa Li', 'Rohith Kuditipudi', 'Percy Liang', 'Tatsunori Hashimoto']	ct:Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.	ed at ICML 2025	['Computation and Language (cs.CL)', 'Cryptography and Security (cs.CR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.07776', 'html': 'https://arxiv.org/html/2502.07776v2', 'tex': '/src/2502.07776', 'doi': 'https://doi.org/10.48550/arXiv.2502.07776'}	Submission history From: Chenchen Gu [ view email ] [v1] Tue, 11 Feb 2025 18:58:04 UTC (255 KB) [v2] Sun, 13 Jul 2025 04:42:28 UTC (250 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.07776'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.07776'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.07776'}]
2025-02-16	Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models	Computation and Language	https://arxiv.org/abs/2502.04404	Step Back to Leap Forward  To boost the reasoning robustness of LLMs, researchers propose a “self-backtracking” mechanism that lets models revisit and revise their own intermediate reasoning steps. Key details:   <br> ● Inspiration from search algorithms: Traditional problem-solving  backtracks when a path hits a dead-end. This approach gives LLMs a  similar ability	https://x.com/omarsar0/status/1888967415444414802		2502.04404	['Xiao-Wen Yang', 'Xuan-Yi Zhu', 'Wen-Da Wei', 'Ding-Chu Zhang', 'Jie-Jing Shao', 'Zhi Zhou', 'Lan-Zhe Guo', 'Yu-Feng Li']	ct:The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.	s a preprint under review, 15 pages, 13 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2502.04404', 'html': 'https://arxiv.org/html/2502.04404v1', 'tex': '/src/2502.04404', 'doi': 'https://doi.org/10.48550/arXiv.2502.04404'}	Submission history From: Xiao-Wen Yang [ view email ] [v1] Thu, 6 Feb 2025 08:52:43 UTC (8,675 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.04404'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.04404'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.04404'}]
2025-02-16	Enhancing Reasoning to Adapt Large Language Models for Domain-Specific Applications	Computation and Language	https://arxiv.org/abs/2502.04384	Enhancing Reasoning to Adapt LLMs  Researchers from IBM present SOLOMON, a neuro-inspired LLM reasoning network architecture that boosts domain adaptability—demonstrated on semiconductor layout design. They show how LLMs often falter at spatial reasoning and domain knowledge application, and how their multi-agent oversight approach significantly improves success on challenging chip-layout tasks. Key insights include:   <br> ● SOLOMON architecture	https://x.com/omarsar0/status/1888985789880758426		2502.04384	['Bo Wen', 'Xin Zhang']	ct:This paper presents SOLOMON, a novel Neuro-inspired Large Language Model (LLM) Reasoning Network architecture that enhances the adaptability of foundation models for domain-specific applications. Through a case study in semiconductor layout design, we demonstrate how SOLOMON enables swift adaptation of general-purpose LLMs to specialized tasks by leveraging Prompt Engineering and In-Context Learning techniques. Our experiments reveal the challenges LLMs face in spatial reasoning and applying domain knowledge to practical problems. Results show that SOLOMON instances significantly outperform their baseline LLM counterparts and achieve performance comparable to state-of-the-art reasoning model, o1-preview. We discuss future research directions for developing more adaptive AI systems that can continually learn, adapt, and evolve in response to new information and changing requirements.	S 2024 Workshop AFM (Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Systems and Control (eess.SY)']	{'pdf': '/pdf/2502.04384', 'html': 'https://arxiv.org/html/2502.04384v1', 'tex': '/src/2502.04384', 'doi': 'https://doi.org/10.48550/arXiv.2502.04384'}	Submission history From: Bo Wen [ view email ] [v1] Wed, 5 Feb 2025 19:27:24 UTC (12,647 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.04384'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.04384'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.04384'}]
2025-02-16	ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates	Computation and Language	https://arxiv.org/abs/2502.06772	ReasonFlux  The ReasonFlux framework is introduced as an efficient way to fine-tune LLMs for complex reasoning, using hierarchical thought processes. Highlights include:   <br> ● Thought template library: Rather than having a model learn long CoT  solutions from scratch, ReasonFlux provides a library of ~500 reusable  “thought templates”	https://x.com/omarsar0/status/1889343676272525600		2502.06772	['Ling Yang', 'Zhaochen Yu', 'Bin Cui', 'Mengdi Wang']	ct:We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing more explainable reasoning structures than DeepSeek-R1 and o3-mini, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:this https URL	his https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.06772', 'html': 'https://arxiv.org/html/2502.06772v2', 'tex': '/src/2502.06772', 'doi': 'https://doi.org/10.48550/arXiv.2502.06772'}	Submission history From: Ling Yang [ view email ] [v1] Mon, 10 Feb 2025 18:51:47 UTC (6,401 KB) [v2] Tue, 11 Mar 2025 02:46:19 UTC (6,402 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.06772'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.06772'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.06772'}]
2025-02-09	s1: Simple test-time scaling	Computation and Language	http://arxiv.org/abs/2501.19393	s1: Simple test-time scaling  Researchers from Stanford, UW, and others introduce s1, a method to boost LLM performance by using extra compute at inference (“test-time scaling”). Key ideas include: <br> ● Small yet powerful dataset	http://twitter.com/omarsar0/status/1886428631041225030	"{""Code & Data"": ""https://github.com/simplescaling/s1""}"	2501.19393	['Niklas Muennighoff', 'Zitong Yang', 'Weijia Shi', 'Xiang Lisa Li', 'Li Fei-Fei', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer', 'Percy Liang', 'Emmanuel Candès', 'Tatsunori Hashimoto']	"ct:Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending ""Wait"" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source atthis https URL"	es (9 main), 10 figures, 15 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.19393', 'html': 'https://arxiv.org/html/2501.19393v3', 'tex': '/src/2501.19393', 'doi': 'https://doi.org/10.48550/arXiv.2501.19393'}	Submission history From: Niklas Muennighoff [ view email ] [v1] Fri, 31 Jan 2025 18:48:08 UTC (812 KB) [v2] Mon, 3 Feb 2025 16:31:30 UTC (804 KB) [v3] Sat, 1 Mar 2025 06:07:39 UTC (810 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.19393'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.19393'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.19393'}]
2025-02-09	OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models	Computer Vision and Pattern Recognition	http://arxiv.org/abs/2502.01061	OmniHuman-1: Scaling One-Stage Human Animation  A team at ByteDance AI Lab unveiled OmniHuman-1, a diffusion-transformer model that can generate highly realistic human videos from just a single image plus motion input (audio or video). Highlights:   <br> ● End-to-end human video generation	http://twitter.com/unseenvie/status/1886672598576325011	"{""Demo"": ""https://omnihuman-lab.github.io/""}"	2502.01061	['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang']	ct:End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (this https URL)	025, Homepage:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2502.01061', 'html': 'https://arxiv.org/html/2502.01061v3', 'tex': '/src/2502.01061', 'doi': 'https://doi.org/10.48550/arXiv.2502.01061'}	Submission history From: Liang Chao [ view email ] [v1] Mon, 3 Feb 2025 05:17:32 UTC (16,828 KB) [v2] Thu, 13 Feb 2025 06:56:29 UTC (16,114 KB) [v3] Mon, 30 Jun 2025 08:26:56 UTC (9,516 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.01061'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.01061'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.01061'}]
2025-02-09	LIMO: Less is More for Reasoning	Computation and Language	http://arxiv.org/abs/2502.03387	LIMO: Less Is More for Reasoning  Can a handful of examples teach complex math reasoning to LLMs? This new LIMO paper challenges the notion that we need huge fine-tuning datasets for tough reasoning tasks. Key findings:   <br> ● Surprisingly few examples	http://twitter.com/omarsar0/status/1887514592747937984	"{""Code"": ""https://github.com/GAIR-NLP/LIMO""}"	2502.03387	['Yixin Ye', 'Zhen Huang', 'Yang Xiao', 'Ethan Chern', 'Shijie Xia', 'Pengfei Liu']	"ct:We challenge the prevailing assumption that complex reasoning in large language models (LLMs) necessitates massive training data. We demonstrate that sophisticated mathematical reasoning can emerge with only a few examples. Specifically, through simple supervised fine-tuning, our model, LIMO, achieves 63.3\% accuracy on AIME24 and 95.6\% on MATH500, surpassing previous fine-tuned models (6.5\% on AIME24, 59.2\% on MATH500) while using only 1\% of the training data required by prior approaches. Furthermore, LIMO exhibits strong out-of-distribution generalization, achieving a 45.8\% absolute improvement across diverse benchmarks, outperforming models trained on 100x more data. Synthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning can emerge through minimal but strategically designed demonstrations of cognitive processes. This hypothesis suggests that the threshold for eliciting complex reasoning is not dictated by task complexity but rather by two key factors: (1) the completeness of the model's pre-trained knowledge base and (2) the effectiveness of post-training examples in serving as ""cognitive templates"" that guide reasoning."	025	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2502.03387', 'html': 'https://arxiv.org/html/2502.03387v3', 'tex': '/src/2502.03387', 'doi': 'https://doi.org/10.48550/arXiv.2502.03387'}	Submission history From: Zhen Huang [ view email ] [v1] Wed, 5 Feb 2025 17:23:45 UTC (8,774 KB) [v2] Mon, 28 Jul 2025 07:21:10 UTC (1,607 KB) [v3] Tue, 29 Jul 2025 16:23:02 UTC (1,548 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.03387'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.03387'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.03387'}]
2025-02-09	CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning	Computation and Language	http://arxiv.org/abs/2502.02390	CoAT: Chain-of-Associated-Thoughts for LLM Reasoning  This work introduces CoAT, a new “slow thinking” inference framework that enables an LLM to reason more like a human by exploring and updating its thoughts. Main components:   <br> ● MCTS + associative memory	http://twitter.com/omarsar0/status/1887187689247752370		2502.02390	['Jianfeng Pan', 'Senyou Deng', 'Shaomang Huang']	ct:Research on LLM technologies is rapidly emerging, with most of them employing a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. To validate the effectiveness of our framework, we conducted extensive experiments across a range of generative and reasoning tasks. These experiments demonstrated that our framework outperforms conventional inference processes on accuracy, coherence, and diversity. The framework's ability to iteratively expand its search space while retaining contextually relevant information results.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2502.02390', 'html': 'https://arxiv.org/html/2502.02390v1', 'tex': '/src/2502.02390', 'doi': 'https://doi.org/10.48550/arXiv.2502.02390'}	Submission history From: Senyou Deng [ view email ] [v1] Tue, 4 Feb 2025 15:10:33 UTC (3,546 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.02390'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.02390'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.02390'}]
2025-02-09	Syntriever: How to Train Your Retriever with Synthetic Data from LLMs	Computation and Language	http://arxiv.org/abs/2502.03824	Syntriever: Training Retrievers with LLM-Generated Data  How can we build a high-quality text retriever without large labeled datasets or access to an LLM’s internals? Syntriever presents a two-stage framework to distill knowledge from a black-box LLM into a retrieval model using synthetic data. Steps:   <br> ● Stage 1	https://x.com/omarsar0/status/1887878242276954557	"{""Code"": ""https://github.com/kmswin1/Syntriever""}"	2502.03824	['Minsang Kim', 'Seungjun Baek']	ct:LLMs have boosted progress in many AI applications. Recently, there were attempts to distill the vast knowledge of LLMs into information retrieval systems. Those distillation methods mostly use output probabilities of LLMs which are unavailable in the latest black-box LLMs. We propose Syntriever, a training framework for retrievers using synthetic data from black-box LLMs. Syntriever consists of two stages. Firstly in the distillation stage, we synthesize relevant and plausibly irrelevant passages and augmented queries using chain-of-thoughts for the given queries. LLM is asked to self-verify the synthetic data for possible hallucinations, after which retrievers are trained with a loss designed to cluster the embeddings of relevant passages. Secondly in the alignment stage, we align the retriever with the preferences of LLMs. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences with regularization which prevents the model from deviating excessively from that trained in the distillation stage. Experiments show that Syntriever achieves state-of-the-art performances on benchmark datasets from various domains in nDCG@$K$. The code is available at \href{this https URL}{this https URL}.	tions of the Americas Chapter of the Association for Computational Linguistics (NAACL), Findings, Accepted	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2502.03824', 'html': 'https://arxiv.org/html/2502.03824v3', 'tex': '/src/2502.03824', 'doi': 'https://doi.org/10.48550/arXiv.2502.03824'}	Submission history From: Minsang Kim [ view email ] [v1] Thu, 6 Feb 2025 07:19:59 UTC (331 KB) [v2] Wed, 12 Feb 2025 10:45:25 UTC (383 KB) [v3] Fri, 14 Feb 2025 01:05:29 UTC (415 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.03824'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.03824'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.03824'}]
2025-02-09	Demystifying Long Chain-of-Thought Reasoning in LLMs	Computation and Language	https://arxiv.org/abs/2502.03373	Demystifying Long Chain-of-Thought Reasoning in LLMs  This work investigates how LLMs develop extended CoT reasoning, focusing on RL and compute scaling. Key insights include:   <br> ● Supervised fine-tuning (SFT) boosts performance	https://x.com/xiangyue96/status/1887332772198371514		2502.03373	['Edward Yeo', 'Yuxuan Tong', 'Morry Niu', 'Graham Neubig', 'Xiang Yue']	ct:Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at:this https URL.	nt, under review	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.03373', 'html': 'https://arxiv.org/html/2502.03373v1', 'tex': '/src/2502.03373', 'doi': 'https://doi.org/10.48550/arXiv.2502.03373'}	Submission history From: Yuxuan Tong [ view email ] [v1] Wed, 5 Feb 2025 17:13:32 UTC (771 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.03373'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.03373'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.03373'}]
2025-02-09	Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?	Computation and Language	http://arxiv.org/abs/2502.00674	Rethinking Mixture-of-Agents: Ensemble One Strong LLM  Ensembling multiple models (Mixture-of-Agents, MoA) is a popular way to boost performance. This paper asks: is mixing different LLMs actually helpful, or are we better off ensembling one top model’s outputs? The surprising answer: “Self-MoA” (single-model ensemble) often wins over multi-model ensembles. Key points:   <br> ● Self-MoA vs. MoA	http://twitter.com/omarsar0/status/1886792384954163347		2502.00674	['Wenzhe Li', 'Yong Lin', 'Mengzhou Xia', 'Chi Jin']	ct:Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2502.00674', 'html': 'https://arxiv.org/html/2502.00674v1', 'tex': '/src/2502.00674', 'doi': 'https://doi.org/10.48550/arXiv.2502.00674'}	Submission history From: Wenzhe Li [ view email ] [v1] Sun, 2 Feb 2025 05:23:29 UTC (336 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.00674'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.00674'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.00674'}]
2025-02-09	Multi-agent Architecture Search via Agentic Supernet	Machine Learning	http://arxiv.org/abs/2502.04180	MaAS: Multi-agent Architecture Search (Agentic Supernet)  Building multi-agent systems of LLMs (where multiple agents collaborate, each with specific roles or tools) is powerful but usually requires hand-designing a single complex pipeline. MaAS (Multi-agent Architecture Search) instead learns a universal “agentic supernet” from which it can spawn an optimal agent team on the fly for each query. It automates designing the agent workflow per task:   <br> ● Agentic supernet	http://twitter.com/omarsar0/status/1887884027530727876		2502.04180	['Guibin Zhang', 'Luyang Niu', 'Junfeng Fang', 'Kun Wang', 'Lei Bai', 'Xiang Wang']	ct:Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \textbf{(I)} requires only $6\sim45\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \textbf{(II)} surpasses them by $0.54\%\sim11.82\%$, and \textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability.		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2502.04180', 'html': None, 'tex': '/src/2502.04180', 'doi': 'https://doi.org/10.48550/arXiv.2502.04180'}	Submission history From: Guibin Zhang [ view email ] [v1] Thu, 6 Feb 2025 16:12:06 UTC (7,461 KB) [v2] Mon, 9 Jun 2025 05:15:47 UTC (7,776 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.04180'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.04180'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.04180'}]
2025-02-09	Advancing Reasoning in Large Language Models: Promising Methods and Approaches	Computation and Language	http://arxiv.org/abs/2502.03671	Advancing Reasoning in LLMs  This survey paper provides a timely overview of emerging methods to enhance reasoning capabilities in LLMs. It organizes the literature into several key approach categories:   <br> ● Prompting strategies	http://twitter.com/omarsar0/status/1887875470269849659		2502.03671	['Avinash Patil', 'Aryan Jadon']	ct:Large Language Models (LLMs) have succeeded remarkably in various natural language processing (NLP) tasks, yet their reasoning capabilities remain a fundamental challenge. While LLMs exhibit impressive fluency and factual recall, their ability to perform complex reasoning-spanning logical deduction, mathematical problem-solving, commonsense inference, and multi-step reasoning-often falls short of human expectations. This survey provides a comprehensive review of emerging techniques enhancing reasoning in LLMs. We categorize existing methods into key approaches, including prompting strategies (e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought reasoning), architectural innovations (e.g., retrieval-augmented models, modular reasoning networks, and neuro-symbolic integration), and learning paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement learning, and self-supervised reasoning objectives). Additionally, we explore evaluation frameworks used to assess reasoning in LLMs and highlight open challenges, such as hallucinations, robustness, and reasoning generalization across diverse tasks. By synthesizing recent advancements, this survey aims to provide insights into promising directions for future research and practical applications of reasoning-augmented LLMs.	s, 1 Figure, IEEE Format	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2502.03671', 'html': 'https://arxiv.org/html/2502.03671v2', 'tex': '/src/2502.03671', 'doi': 'https://doi.org/10.48550/arXiv.2502.03671'}	Submission history From: Avinash Patil [ view email ] [v1] Wed, 5 Feb 2025 23:31:39 UTC (768 KB) [v2] Wed, 28 May 2025 05:08:18 UTC (767 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2502.03671'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2502.03671'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2502.03671'}]
2025-02-09	Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities	Computation and Language	http://arxiv.org/abs/2501.18845	Survey: Text Data Augmentation for LLMs  This comprehensive survey covers text data augmentation techniques for LLMs. As LLMs demand massive training data, augmenting datasets with synthetic or transformed text is vital. In this paper:   <br> ● Classifies augmentation methods	http://twitter.com/omarsar0/status/1886428687350006067		2501.18845	['Yaping Chai', 'Haoran Xie', 'Joe S. Qin']	ct:The increasing size and complexity of pre-trained language models have demonstrated superior performance in many applications, but they usually require large training datasets to be adequately trained. Insufficient training sets could unexpectedly make the model overfit and fail to cope with complex tasks. Large language models (LLMs) trained on extensive corpora have prominent text generation capabilities, which improve the quality and quantity of data and play a crucial role in data augmentation. Specifically, distinctive prompt templates are given in personalised tasks to guide LLMs in generating the required content. Recent promising retrieval-based techniques further improve the expressive performance of LLMs in data augmentation by introducing external knowledge to enable them to produce more grounded-truth data. This survey provides an in-depth analysis of data augmentation in LLMs, classifying the techniques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based Augmentation and Hybrid Augmentation. We summarise the post-processing approaches in data augmentation, which contributes significantly to refining the augmented data and enabling the model to filter out unfaithful content. Then, we provide the common tasks and evaluation metrics. Finally, we introduce existing challenges and future opportunities that could bring further improvement to data augmentation.	es, 4 figures, 4 tables	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2501.18845', 'html': 'https://arxiv.org/html/2501.18845v1', 'tex': '/src/2501.18845', 'doi': 'https://doi.org/10.48550/arXiv.2501.18845'}	Submission history From: Haoran Xie [ view email ] [v1] Fri, 31 Jan 2025 01:50:49 UTC (1,794 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.18845'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.18845'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.18845'}]
2025-02-02	Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs	Computation and Language	https://arxiv.org/abs/2501.18585	"On the Underthinking of o1-like LLMs  This work looks more closely at the ""thinking"" patterns of o1-like LLMs. We have seen a few recent papers pointing out the issues with overthinking.  There is now a new phenomenon called underthinking! What is it about? The authors find that o1-like LLMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution."	https://x.com/omarsar0/status/1885349576456233177		2501.18585	['Yue Wang', 'Qiuzhi Liu', 'Jiahao Xu', 'Tian Liang', 'Xingyu Chen', 'Zhiwei He', 'Linfeng Song', 'Dian Yu', 'Juntao Li', 'Zhuosheng Zhang', 'Rui Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu']	ct:Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable abilities in complex reasoning tasks by scaling test-time compute and exhibiting human-like deep thinking. However, we identify a phenomenon we term underthinking, where o1-like LLMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution. This behavior leads to inadequate depth of reasoning and decreased performance, particularly on challenging mathematical problems. To systematically analyze this issue, we conduct experiments on three challenging test sets and two representative open-source o1-like models, revealing that frequent thought switching correlates with incorrect responses. We introduce a novel metric to quantify underthinking by measuring token efficiency in incorrect answers. To address underthinking, we propose a decoding strategy with thought switching penalty TIP that discourages premature transitions between thoughts, encouraging deeper exploration of each reasoning path. Experimental results demonstrate that our approach improves accuracy across challenging datasets without requiring model fine-tuning. Our findings contribute to understanding reasoning inefficiencies in o1-like LLMs and offer a practical solution to enhance their problem-solving capabilities.	have updated the results for DeepSeek-R1, and all of our original conclusions remain valid. 2. Our proposed Tip approach remains effective in Best-of-N scenarios (e.g., self-consistency and Laconic Decoding) when built on DeepSeek-R1	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2501.18585', 'html': 'https://arxiv.org/html/2501.18585v2', 'tex': '/src/2501.18585', 'doi': 'https://doi.org/10.48550/arXiv.2501.18585'}	Submission history From: Jiahao Xu [ view email ] [v1] Thu, 30 Jan 2025 18:58:18 UTC (1,263 KB) [v2] Tue, 18 Feb 2025 16:51:53 UTC (1,267 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.18585'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.18585'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.18585'}]
2025-02-02	Diverse Preference Optimization	Computation and Language	https://arxiv.org/abs/2501.18101	Diverse Preference Optimization  Introduces Diverse Preference Optimization (DivPO), a novel training method that aims to address the lack of diversity in language model outputs while maintaining response quality. The key challenge is that current preference optimization techniques like RLHF tend to sharpen the output probability distribution, causing models to generate very similar responses. This is particularly problematic for creative tasks where varied outputs are desired.  DivPO works by modifying how training pairs are selected during preference optimization. Rather than simply choosing the highest and lowest rewarded responses, DivPO selects the most diverse response that meets a quality threshold and contrasts it with the least diverse response below a threshold. The method introduces a diversity criterion that can be measured in different ways, including model probability, word frequency, or using an LLM as a judge. Experiments on persona generation and creative writing tasks show that DivPO achieves up to 45.6% more diverse outputs in structured tasks and an 81% increase in story diversity, while maintaining similar quality levels compared to baseline methods.	https://x.com/jaseweston/status/1885399530419450257		2501.18101	['Jack Lanchantin', 'Angelica Chen', 'Shehzaad Dhuliawala', 'Ping Yu', 'Jason Weston', 'Sainbayar Sukhbaatar', 'Ilia Kulikov']	ct:Post-training of language models, either through reinforcement learning, preference optimization or supervised finetuning, tends to sharpen the output probability distribution and reduce the diversity of generated responses. This is particularly a problem for creative generative tasks where varied responses are desired. In this work we introduce Diverse Preference Optimization (DivPO), an optimization method which learns to generate much more diverse responses than standard pipelines, while maintaining the quality of the generations. In DivPO, preference pairs are selected by first considering a pool of responses, and a measure of diversity among them, and selecting chosen examples as being more rare but high quality, while rejected examples are more common, but low quality. DivPO results in generating 45.6% more diverse persona attributes, and a 74.6% increase in story diversity, while maintaining similar win rates as standard baselines. On general instruction following, DivPO results in a 46.2% increase in diversity, and a 2.4% winrate improvement compared to DPO.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2501.18101', 'html': 'https://arxiv.org/html/2501.18101v4', 'tex': '/src/2501.18101', 'doi': 'https://doi.org/10.48550/arXiv.2501.18101'}	Submission history From: Jack Lanchantin [ view email ] [v1] Thu, 30 Jan 2025 02:47:41 UTC (807 KB) [v2] Fri, 31 Jan 2025 18:57:58 UTC (509 KB) [v3] Mon, 10 Feb 2025 18:22:52 UTC (510 KB) [v4] Thu, 22 May 2025 17:50:19 UTC (506 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.18101'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.18101'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.18101'}]
2025-02-02	Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies	Machine Learning	https://arxiv.org/abs/2501.17030	Usage Recommendation for DeepSeek-R1  This work provides a set of recommendations for how to prompt the DeepSeek-R1 model. Below are the key guidelines: <br><br> 1. Prompt Engineering: <br>  ● Use clear, structured prompts with explicit instructions <br> ● Avoid  few-shot prompting; use zero-shot instead  <br><br> 1. Output Formatting: <br>  ● Specify the desired format (JSON, tables, markdown) <br> ● Request  step-by-step explanations for reasoning tasks <br><br> 1. Language: <br>  ● Explicitly specify input/output language to prevent mixing <br><br> The paper also summarizes when to use the different model variants, when to fine-tune, and other safety considerations.	https://x.com/omarsar0/status/1884624296368292083		2501.17030	['Manojkumar Parmar', 'Yuvaraj Govindarajulu']	ct:Large Language Models (LLMs) have achieved remarkable progress in reasoning, alignment, and task-specific performance. However, ensuring harmlessness in these systems remains a critical challenge, particularly in advanced models like DeepSeek-R1. This paper examines the limitations of Reinforcement Learning (RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and compares it with Supervised Fine-Tuning (SFT). While RL improves reasoning capabilities, it faces challenges such as reward hacking, generalization failures, language mixing, and high computational costs. We propose hybrid training approaches combining RL and SFT to achieve robust harmlessness reduction. Usage recommendations and future directions for deploying DeepSeek-R1 responsibly are also presented.	s, 1 table	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Cryptography and Security (cs.CR)']	{'pdf': '/pdf/2501.17030', 'html': 'https://arxiv.org/html/2501.17030v1', 'tex': '/src/2501.17030', 'doi': 'https://doi.org/10.48550/arXiv.2501.17030'}	Submission history From: Manojkumar Parmar [ view email ] [v1] Tue, 28 Jan 2025 15:52:51 UTC (16 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.17030'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.17030'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.17030'}]
2025-02-02	Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion	Computation and Language	https://arxiv.org/abs/2501.17887	Docling  [Docling](https://arxiv.org/abs/2501.17887) is an open-source toolkit that can parse several types of popular document formats into a unified, richly structured representation.			2501.17887	['Nikolaos Livathinos', 'Christoph Auer', 'Maksym Lysak', 'Ahmed Nassar', 'Michele Dolfi', 'Panos Vagenas', 'Cesar Berrospi Ramis', 'Matteo Omenetti', 'Kasper Dinkla', 'Yusik Kim', 'Shubham Gupta', 'Rafael Teixeira de Lima', 'Valery Weber', 'Lucas Morin', 'Ingmar Meijer', 'Viktor Kuropiatnyk', 'Peter W. J. Staar']	ct:We introduce Docling, an easy-to-use, self-contained, MIT-licensed, open-source toolkit for document conversion, that can parse several types of popular document formats into a unified, richly structured representation. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. Docling is released as a Python package and can be used as a Python API or as a CLI tool. Docling's modular architecture and efficient document representation make it easy to implement extensions, new features, models, and customizations. Docling has been already integrated in other popular open-source frameworks (e.g., LangChain, LlamaIndex, spaCy), making it a natural fit for the processing of documents and the development of high-end applications. The open-source community has fully engaged in using, promoting, and developing for Docling, which gathered 10k stars on GitHub in less than a month and was reported as the No. 1 trending repository in GitHub worldwide in November 2024.	ed to AAAI 25: Workshop on Open-Source AI for Mainstream Use	['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2501.17887', 'html': 'https://arxiv.org/html/2501.17887v1', 'tex': '/src/2501.17887', 'doi': 'https://doi.org/10.48550/arXiv.2501.17887'}	Submission history From: Michele Dolfi [ view email ] [v1] Mon, 27 Jan 2025 19:40:00 UTC (3,640 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.17887'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.17887'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.17887'}]
2025-02-02	Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning	Computation and Language	https://arxiv.org/abs/2501.15228	Improving RAG through Multi-Agent RL  This work treats RAG as a multi-agent cooperative task to improve answer generation quality. It models RAG components like query rewriting, document selection, and answer generation as reinforcement learning agents working together toward generating accurate answers. It applies  Multi-Agent Proximal Policy Optimization (MAPPO) to jointly optimize all agents with a shared reward based on answer quality.  Besides improvements on popular benchmarks, the framework shows strong generalization capabilities in out-of-domain scenarios and maintains effectiveness across different RAG system configurations.	https://x.com/omarsar0/status/1884249075467575362		2501.15228	['Yiqun Chen', 'Lingyong Yan', 'Weiwei Sun', 'Xinyu Ma', 'Yi Zhang', 'Shuaiqiang Wang', 'Dawei Yin', 'Yiming Yang', 'Jiaxin Mao']	ct:Retrieval-augmented generation (RAG) is extensively utilized to incorporate external, current knowledge into large language models, thereby minimizing hallucinations. A standard RAG pipeline may comprise several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual modules and the overarching aim of generating accurate answers in question-answering (QA) tasks. Although recent efforts have explored reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on overly simplistic pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these challenges, we propose treating the RAG pipeline as a multi-agent cooperative task, with each component regarded as an RL agent. Specifically, we present MMOA-RAG, a Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents' goals towards a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA datasets demonstrate that MMOA-RAG improves the overall pipeline performance and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and the adaptability of MMOA-RAG across different RAG components and datasets. The code of MMOA-RAG is onthis https URL.		['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2501.15228', 'html': 'https://arxiv.org/html/2501.15228v1', 'tex': '/src/2501.15228', 'doi': 'https://doi.org/10.48550/arXiv.2501.15228'}	Submission history From: Yiqun Chen [ view email ] [v1] Sat, 25 Jan 2025 14:24:50 UTC (2,871 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.15228'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.15228'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.15228'}]
2025-02-02	TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs	Computation and Language	https://arxiv.org/abs/2501.15674	TensorLLM  Proposes a framework that performs MHA compression through a multi-head tensorisation process and the Tucker decomposition. Achieves a compression rate of up to ∼ 250x in the MHA weights, without requiring any additional data, training, or fine-tuning.	https://x.com/omarsar0/status/1884246306224496729		2501.15674	['Yuxuan Gu', 'Wuyang Zhou', 'Giorgos Iacovides', 'Danilo Mandic']	ct:The reasoning abilities of Large Language Models (LLMs) can be improved by structurally denoising their weights, yet existing techniques primarily focus on denoising the feed-forward network (FFN) of the transformer block, and can not efficiently utilise the Multi-head Attention (MHA) block, which is the core of transformer architectures. To address this issue, we propose a novel intuitive framework that, at its very core, performs MHA compression through a multi-head tensorisation process and the Tucker decomposition. This enables both higher-dimensional structured denoising and compression of the MHA weights, by enforcing a shared higher-dimensional subspace across the weights of the multiple attention heads. We demonstrate that this approach consistently enhances the reasoning capabilities of LLMs across multiple benchmark datasets, and for both encoder-only and decoder-only architectures, while achieving compression rates of up to $\sim 250$ times in the MHA weights, all without requiring any additional data, training, or fine-tuning. Furthermore, we show that the proposed method can be seamlessly combined with existing FFN-only-based denoising techniques to achieve further improvements in LLM reasoning performance.	ed for IEEE International Joint Conference on Neural Networks (IJCNN 2025). The code is available atthis https URL	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.15674', 'html': 'https://arxiv.org/html/2501.15674v2', 'tex': '/src/2501.15674', 'doi': 'https://doi.org/10.48550/arXiv.2501.15674'}	Submission history From: Yuxuan Gu [ view email ] [v1] Sun, 26 Jan 2025 21:05:16 UTC (508 KB) [v2] Thu, 15 May 2025 12:42:44 UTC (508 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.15674'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.15674'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.15674'}]
2025-02-02	TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2501.12224	TokenVerse  Proposes a new technique to generate new images from learned concepts in a desired configuration. Proposed by Google DeepMind and collaborators, TokenVerse enables multi-concept personalization  by leveraging a pre-trained text-to-image diffusion model to disentangle and extract complex visual concepts from multiple images.  It operates in the modulation space of DiTs, learning a personalized modulation vector for each text token in an input caption. This allows flexible and localized control over distinct concepts such as objects, materials, lighting, and poses. The learned token modulations can then be combined in novel ways to generate new images that integrate multiple personalized concepts without requiring additional segmentation masks.	https://x.com/omarsar0/status/1884618510275592610		2501.12224	['Daniel Garibi', 'Shahar Yadin', 'Roni Paiss', 'Omer Tov', 'Shiran Zada', 'Ariel Ephrat', 'Tomer Michaeli', 'Inbar Mosseri', 'Tali Dekel']	ct:We present TokenVerse -- a method for multi-concept personalization, leveraging a pre-trained text-to-image diffusion model. Our framework can disentangle complex visual elements and attributes from as little as a single image, while enabling seamless plug-and-play generation of combinations of concepts extracted from multiple images. As opposed to existing works, TokenVerse can handle multiple images with multiple concepts each, and supports a wide-range of concepts, including objects, accessories, materials, pose, and lighting. Our work exploits a DiT-based text-to-image model, in which the input text affects the generation through both attention and modulation (shift and scale). We observe that the modulation space is semantic and enables localized control over complex concepts. Building on this insight, we devise an optimization-based framework that takes as input an image and a text description, and finds for each word a distinct direction in the modulation space. These directions can then be used to generate new images that combine the learned concepts in a desired configuration. We demonstrate the effectiveness of TokenVerse in challenging personalization settings, and showcase its advantages over existing methods. project's webpage inthis https URL		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2501.12224', 'html': 'https://arxiv.org/html/2501.12224v1', 'tex': '/src/2501.12224', 'doi': 'https://doi.org/10.48550/arXiv.2501.12224'}	Submission history From: Shahar Yadin [ view email ] [v1] Tue, 21 Jan 2025 15:49:29 UTC (43,690 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.12224'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.12224'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.12224'}]
2025-01-26	LLMs Can Plan Only If We Tell Them	Computation and Language	https://arxiv.org/abs/2501.13545	Can LLMs Plan?  Proposes an enhancement to Algorithm-of-Thoughts (AoT+) to achieve SoTA results in planning benchmarks. It even outperforms human baselines! AoT+ provides periodic state summaries to reduce the cognitive load. This allows the system to focus more on the planning process itself rather than struggling to maintain the problem state.	https://x.com/omarsar0/status/1882799782579855518		2501.13545	['Bilgehan Sel', 'Ruoxi Jia', 'Ming Jin']	ct:Large language models (LLMs) have demonstrated significant capabilities in natural language processing and reasoning, yet their effectiveness in autonomous planning has been under debate. While existing studies have utilized LLMs with external feedback mechanisms or in controlled environments for planning, these approaches often involve substantial computational and development resources due to the requirement for careful design and iterative backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to match human performance on standard planning benchmarks, such as the Blocksworld, without additional support. This paper investigates whether LLMs can independently generate long-horizon plans that rival human baselines. Our novel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help achieve state-of-the-art results in planning benchmarks out-competing prior methods and human baselines all autonomously.	025	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2501.13545', 'html': None, 'tex': '/src/2501.13545', 'doi': 'https://doi.org/10.48550/arXiv.2501.13545'}	Submission history From: Bilgehan Sel [ view email ] [v1] Thu, 23 Jan 2025 10:46:14 UTC (674 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.13545'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.13545'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.13545'}]
2025-01-26	Hallucinations Can Improve Large Language Models in Drug Discovery	Computation and Language	https://arxiv.org/abs/2501.13824	Hallucinations Improve LLMs in Drug Discovery  Claims that LLMs can achieve better performance in drug discovery tasks with text hallucinations compared to input prompts without hallucination. Llama-3.1-8B achieves an 18.35% gain in  ROC-AUC compared to the baseline without hallucination. In addition, hallucinations generated by GPT-4o provide the most consistent improvements across models.	https://x.com/omarsar0/status/1882789456522145802		2501.13824	['Shuzhou Yuan', 'Michael Färber']	ct:Concerns about hallucinations in Large Language Models (LLMs) have been raised by researchers, yet their potential in areas where creativity is vital, such as drug discovery, merits exploration. In this paper, we come up with the hypothesis that hallucinations can improve LLMs in drug discovery. To verify this hypothesis, we use LLMs to describe the SMILES string of molecules in natural language and then incorporate these descriptions as part of the prompt to address specific tasks in drug discovery. Evaluated on seven LLMs and five classification tasks, our findings confirm the hypothesis: LLMs can achieve better performance with text containing hallucinations. Notably, Llama-3.1-8B achieves an 18.35% gain in ROC-AUC compared to the baseline without hallucination. Furthermore, hallucinations generated by GPT-4o provide the most consistent improvements across models. Additionally, we conduct empirical analyses and a case study to investigate key factors affecting performance and the underlying reasons. Our research sheds light on the potential use of hallucinations for LLMs and offers new perspectives for future research leveraging LLMs in drug discovery.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2501.13824', 'html': 'https://arxiv.org/html/2501.13824v1', 'tex': '/src/2501.13824', 'doi': 'https://doi.org/10.48550/arXiv.2501.13824'}	Submission history From: Shuzhou Yuan [ view email ] [v1] Thu, 23 Jan 2025 16:45:51 UTC (10,652 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.13824'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.13824'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.13824'}]
2025-01-26	IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems	Computation and Language	https://arxiv.org/abs/2501.11067	IntellAgent  Introduces a new open-source framework for evaluating conversational AI systems through automated, policy-driven testing. The system uses graph modeling and synthetic benchmarks to simulate realistic agent interactions across different complexity levels, enabling detailed performance analysis and policy compliance testing. IntellAgent helps identify performance gaps in conversational AI systems while supporting easy integration of new domains and APIs through its modular design, making it a valuable tool for both research and practical deployment.	https://x.com/omarsar0/status/1882081603754643779	"{""GitHub"": ""https://github.com/plurai-ai/intellagent""}"	2501.11067	['Elad Levi', 'Ilan Kadar']	ct:Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available atthis https URL		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.11067', 'html': 'https://arxiv.org/html/2501.11067v1', 'tex': '/src/2501.11067', 'doi': 'https://doi.org/10.48550/arXiv.2501.11067'}	Submission history From: Elad Levi [ view email ] [v1] Sun, 19 Jan 2025 14:58:35 UTC (1,493 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.11067'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.11067'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.11067'}]
2025-01-26	Tell me about yourself: LLMs are aware of their learned behaviors	Computation and Language	https://arxiv.org/abs/2501.11120	"LLMs and Behavioral Awareness  Shows that after fine-tuning LLMs on behaviors like outputting insecure code, the LLMs show behavioral self-awareness. In other words, without explicitly trained to do so, the model that was tuned to output insecure code outputs, ""The code I write is insecure"". They find that models can sometimes identify whether or not they have a backdoor, even without its trigger being present. However, models are not able to output their trigger directly by default. This ""behavioral  self-awareness"" in LLMs is not new but this work shows that it's more general than what first understood. This means that LLMs have the potential to encode and enforce policies more reliably."	https://x.com/omarsar0/status/1882079780918747303		2501.11120	['Jan Betley', 'Xuchan Bao', 'Martín Soto', 'Anna Sztyber-Betley', 'James Chua', 'Owain Evans']	ct:We study behavioral self-awareness -- an LLM's ability to articulate its behaviors without requiring in-context examples. We finetune LLMs on datasets that exhibit particular behaviors, such as (a) making high-risk economic decisions, and (b) outputting insecure code. Despite the datasets containing no explicit descriptions of the associated behavior, the finetuned LLMs can explicitly describe it. For example, a model trained to output insecure code says, ``The code I write is insecure.'' Indeed, models show behavioral self-awareness for a range of behaviors and for diverse evaluations. Note that while we finetune models to exhibit behaviors like writing insecure code, we do not finetune them to articulate their own behaviors -- models do this without any special training or examples.Behavioral self-awareness is relevant for AI safety, as models could use it to proactively disclose problematic behaviors. In particular, we study backdoor policies, where models exhibit unexpected behaviors only under certain trigger conditions. We find that models can sometimes identify whether or not they have a backdoor, even without its trigger being present. However, models are not able to directly output their trigger by default.Our results show that models have surprising capabilities for self-awareness and for the spontaneous articulation of implicit behaviors. Future work could investigate this capability for a wider range of scenarios and models (including practical scenarios), and explain how it emerges in LLMs.	ted to ICLR 2025. 17 pages, 13 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Cryptography and Security (cs.CR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.11120', 'html': 'https://arxiv.org/html/2501.11120v1', 'tex': '/src/2501.11120', 'doi': 'https://doi.org/10.48550/arXiv.2501.11120'}	Submission history From: Xuchan Bao [ view email ] [v1] Sun, 19 Jan 2025 17:28:12 UTC (4,413 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.11120'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.11120'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.11120'}]
2025-01-26	Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG	Artificial Intelligence	https://arxiv.org/abs/2501.09136	Agentic RAG Overview  Provides a comprehensive introduction to LLM agents and Agentic RAG. It provides an exploration of Agentic RAG architectures, applications, and implementation strategies.	https://x.com/omarsar0/status/1881360794019156362		2501.09136	['Aditi Singh', 'Abul Ehtesham', 'Saket Kumar', 'Tala Talaei Khoei']	ct:Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management.Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications.This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2501.09136', 'html': 'https://arxiv.org/html/2501.09136v3', 'tex': '/src/2501.09136', 'doi': 'https://doi.org/10.48550/arXiv.2501.09136'}	Submission history From: Abul Ehtesham [ view email ] [v1] Wed, 15 Jan 2025 20:40:25 UTC (20,962 KB) [v2] Mon, 3 Feb 2025 04:01:36 UTC (22,453 KB) [v3] Tue, 4 Feb 2025 04:48:00 UTC (22,430 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.09136'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.09136'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.09136'}]
2025-01-19	Transformer-Squared: Self-adaptive LLMs	Machine Learning	https://arxiv.org/abs/2501.06252	Self-Adaptive LLMs	https://x.com/hardmaru/status/1879331049383334187		2501.06252	['Qi Sun', 'Edoardo Cetin', 'Yujin Tang']	ct:Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer-Squared, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer-Squared employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific 'expert' vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method consistently outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Furthermore, Transformer-Squared demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer-Squared represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.	ear at the 13th International Conference on Learning Representations (ICLR 2025)	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2501.06252', 'html': 'https://arxiv.org/html/2501.06252v3', 'tex': '/src/2501.06252', 'doi': 'https://doi.org/10.48550/arXiv.2501.06252'}	Submission history From: Qi Sun [ view email ] [v1] Thu, 9 Jan 2025 01:19:21 UTC (1,324 KB) [v2] Tue, 14 Jan 2025 02:52:26 UTC (1,324 KB) [v3] Fri, 24 Jan 2025 01:26:30 UTC (1,511 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.06252'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.06252'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.06252'}]
2025-01-19	MiniMax-01: Scaling Foundation Models with Lightning Attention	Computation and Language	https://arxiv.org/abs/2501.08313	MiniMax-01	https://x.com/omarsar0/status/1879572512075587872		2501.08313	['MiniMax', 'Aonian Li', 'Bangwei Gong', 'Bo Yang', 'Boji Shan', 'Chang Liu', 'Cheng Zhu', 'Chunhao Zhang', 'Congchao Guo', 'Da Chen', 'Dong Li', 'Enwei Jiao', 'Gengxin Li', 'Guojun Zhang', 'Haohai Sun', 'Houze Dong', 'Jiadai Zhu', 'Jiaqi Zhuang', 'Jiayuan Song', 'Jin Zhu', 'Jingtao Han', 'Jingyang Li', 'Junbin Xie', 'Junhao Xu', 'Junjie Yan', 'Kaishun Zhang', 'Kecheng Xiao', 'Kexi Kang', 'Le Han', 'Leyang Wang', 'Lianfei Yu', 'Liheng Feng', 'Lin Zheng', 'Linbo Chai', 'Long Xing', 'Meizhi Ju', 'Mingyuan Chi', 'Mozhi Zhang', 'Peikai Huang', 'Pengcheng Niu', 'Pengfei Li', 'Pengyu Zhao', 'Qi Yang', 'Qidi Xu', 'Qiexiang Wang', 'Qin Wang', 'Qiuhui Li', 'Ruitao Leng', 'Shengmin Shi', 'Shuqi Yu', 'Sichen Li', 'Songquan Zhu', 'Tao Huang', 'Tianrun Liang', 'Weigao Sun', 'Weixuan Sun', 'Weiyu Cheng', 'Wenkai Li', 'Xiangjun Song', 'Xiao Su', 'Xiaodong Han', 'Xinjie Zhang', 'Xinzhu Hou', 'Xu Min', 'Xun Zou', 'Xuyang Shen', 'Yan Gong', 'Yingjie Zhu', 'Yipeng Zhou', 'Yiran Zhong', 'Yongyi Hu', 'Yuanxiang Fan', 'Yue Yu', 'Yufeng Yang', 'Yuhao Li', 'Yunan Huang', 'Yunji Li', 'Yunpeng Huang', 'Yunzhi Xu', 'Yuxin Mao', 'Zehan Li', 'Zekang Li', 'Zewei Tao', 'Zewen Ying', 'Zhaoyang Cong', 'Zhen Qin', 'Zhenhua Fan', 'Zhihang Yu', 'Zhuo Jiang', 'Zijia Wu']	ct:We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 atthis https URL.	nical report from MiniMax. The authors are listed in alphabetical order. We open-sourced our MiniMax-01 atthis https URL	['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2501.08313', 'html': None, 'tex': '/src/2501.08313', 'doi': 'https://doi.org/10.48550/arXiv.2501.08313'}	Submission history From: Yiran Zhong [ view email ] [v1] Tue, 14 Jan 2025 18:50:05 UTC (4,318 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.08313'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.08313'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.08313'}]
2025-01-19	VideoRAG: Retrieval-Augmented Generation over Video Corpus	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2501.05874	VideoRAG	https://x.com/omarsar0/status/1878827350315659421		2501.05874	['Soyeong Jeong', 'Kangsan Kim', 'Jinheon Baek', 'Sung Ju Hwang']	ct:Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the factual accuracy of models by retrieving external knowledge relevant to queries and incorporating it into the generation process. However, existing approaches primarily focus on text, with some recent advancements considering images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing contextual details more effectively than any other modality. While very recent studies explore the use of videos in response generation, they either predefine query-associated videos without retrieval or convert videos into textual descriptions losing multimodal richness. To tackle these, we introduce VideoRAG, a framework that not only dynamically retrieves videos based on their relevance with queries but also utilizes both visual and textual information. The operation of VideoRAG is powered by recent Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and the seamless integration of retrieved videos jointly with queries for response generation. Also, inspired by that the context size of LVLMs may not be sufficient to process all frames in extremely long videos and not all frames are equally important, we introduce a video frame selection mechanism to extract the most informative subset of frames, along with a strategy to extract textual information from videos (as it can aid the understanding of video content) when their subtitles are not available. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines. Code is available atthis https URL.	ndings 2025	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.05874', 'html': 'https://arxiv.org/html/2501.05874v3', 'tex': '/src/2501.05874', 'doi': 'https://doi.org/10.48550/arXiv.2501.05874'}	Submission history From: Soyeong Jeong [ view email ] [v1] Fri, 10 Jan 2025 11:17:15 UTC (1,622 KB) [v2] Tue, 4 Mar 2025 07:29:52 UTC (5,922 KB) [v3] Wed, 28 May 2025 18:14:55 UTC (5,923 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.05874'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.05874'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.05874'}]
2025-01-19	Titans: Learning to Memorize at Test Time	Machine Learning	https://arxiv.org/abs/2501.00663	Learning to Memorize at Test Time	https://x.com/omarsar0/status/1879896681010921742		2501.00663	['Ali Behrouz', 'Peilin Zhong', 'Vahab Mirrokni']	ct:Over more than a decade there has been an extensive research effort on how to effectively utilize recurrent models and attention. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2501.00663', 'html': 'https://arxiv.org/html/2501.00663v1', 'tex': '/src/2501.00663', 'doi': 'https://doi.org/10.48550/arXiv.2501.00663'}	Submission history From: Ali Behrouz [ view email ] [v1] Tue, 31 Dec 2024 22:32:03 UTC (3,249 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.00663'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.00663'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.00663'}]
2025-01-19	Foundations of Large Language Models	Computation and Language	https://arxiv.org/abs/2501.09223	Foundations of LLMs	https://x.com/omarsar0/status/1880284477445767586		2501.09223	['Tong Xiao', 'Jingbo Zhu']	ct:This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into five main chapters, each exploring a key area: pre-training, generative models, prompting, alignment, and inference. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.09223', 'html': None, 'tex': '/src/2501.09223', 'doi': 'https://doi.org/10.48550/arXiv.2501.09223'}	Submission history From: Tong Xiao [ view email ] [v1] Thu, 16 Jan 2025 01:03:56 UTC (361 KB) [v2] Sun, 15 Jun 2025 13:24:11 UTC (514 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.09223'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.09223'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.09223'}]
2025-01-19	OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking	Computation and Language	https://arxiv.org/abs/2501.09751	OmniThink	https://x.com/omarsar0/status/1880275861401923619		2501.09751	['Zekun Xi', 'Wenbiao Yin', 'Jizhan Fang', 'Jialong Wu', 'Runnan Fang', 'Ningyu Zhang', 'Jiang Yong', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen']	ct:Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.	s available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)', 'Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.09751', 'html': None, 'tex': '/src/2501.09751', 'doi': 'https://doi.org/10.48550/arXiv.2501.09751'}	Submission history From: Ningyu Zhang [ view email ] [v1] Thu, 16 Jan 2025 18:58:06 UTC (1,489 KB) [v2] Thu, 20 Feb 2025 15:05:18 UTC (1,956 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.09751'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.09751'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.09751'}]
2025-01-19	Enhancing Retrieval-Augmented Generation: A Study of Best Practices	Computation and Language	https://arxiv.org/abs/2501.07391	Enhancing RAG	https://x.com/omarsar0/status/1879178916021318029		2501.07391	['Siran Li', 'Linus Stenzel', 'Carsten Eickhoff', 'Seyed Ali Bahrainian']	ct:Retrieval-Augmented Generation (RAG) systems have recently shown remarkable advancements by integrating retrieval mechanisms into language models, enhancing their ability to produce more accurate and contextually relevant responses. However, the influence of various components and configurations within RAG systems remains underexplored. A comprehensive understanding of these elements is essential for tailoring RAG systems to complex retrieval tasks and ensuring optimal performance across diverse applications. In this paper, we develop several advanced RAG system designs that incorporate query expansion, various novel retrieval strategies, and a novel Contrastive In-Context Learning RAG. Our study systematically investigates key factors, including language model size, prompt design, document chunk size, knowledge base size, retrieval stride, query expansion techniques, Contrastive In-Context Learning knowledge bases, multilingual knowledge bases, and Focus Mode retrieving relevant context at sentence-level. Through extensive experimentation, we provide a detailed analysis of how these factors influence response quality. Our findings offer actionable insights for developing RAG systems, striking a balance between contextual richness and retrieval-generation efficiency, thereby paving the way for more adaptable and high-performing RAG frameworks in diverse real-world scenarios. Our code and implementation details are publicly available.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2501.07391', 'html': 'https://arxiv.org/html/2501.07391v1', 'tex': '/src/2501.07391', 'doi': 'https://doi.org/10.48550/arXiv.2501.07391'}	Submission history From: Siran Li [ view email ] [v1] Mon, 13 Jan 2025 15:07:55 UTC (2,585 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.07391'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.07391'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.07391'}]
2025-01-19	AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral Therapy in Psychological Counseling	Computation and Language	https://arxiv.org/abs/2501.09426	AutoCBT	https://x.com/omarsar0/status/1880283025595867631		2501.09426	['Ancheng Xu', 'Di Yang', 'Renhao Li', 'Jingwei Zhu', 'Minghuan Tan', 'Min Yang', 'Wanxin Qiu', 'Mingchen Ma', 'Haihong Wu', 'Bingyu Li', 'Feng Sha', 'Chengming Li', 'Xiping Hu', 'Qiang Qu', 'Derek F.Wong', 'Ruifeng Xu']	ct:Traditional in-person psychological counseling remains primarily niche, often chosen by individuals with psychological issues, while online automated counseling offers a potential solution for those hesitant to seek help due to feelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and widely used approach in psychological counseling. The advent of large language models (LLMs) and agent technology enables automatic CBT diagnosis and treatment. However, current LLM-based CBT systems use agents with a fixed structure, limiting their self-optimization capabilities, or providing hollow, unhelpful suggestions due to redundant response patterns. In this work, we utilize Quora-like and YiXinLi single-round consultation models to build a general agent framework that generates high-quality responses for single-turn psychological consultation scenarios. We use a bilingual dataset to evaluate the quality of single-response consultations generated by each framework. Then, we incorporate dynamic routing and supervisory mechanisms inspired by real psychological counseling to construct a CBT-oriented autonomous multi-agent framework, demonstrating its general applicability. Experimental results indicate that AutoCBT can provide higher-quality automated psychological counseling services.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2501.09426', 'html': 'https://arxiv.org/html/2501.09426v1', 'tex': '/src/2501.09426', 'doi': 'https://doi.org/10.48550/arXiv.2501.09426'}	Submission history From: Ancheng Xu [ view email ] [v1] Thu, 16 Jan 2025 09:57:12 UTC (9,283 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.09426'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.09426'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.09426'}]
2025-01-19	Imagine while Reasoning in Space: Multimodal Visualization-of-Thought	Computation and Language	https://arxiv.org/abs/2501.07542	Imagine while Reasoning in Space	https://x.com/omarsar0/status/1879181711982129420		2501.07542	['Chengzu Li', 'Wenshan Wu', 'Huanyu Zhang', 'Yan Xia', 'Shaoguang Mao', 'Li Dong', 'Ivan Vulić', 'Furu Wei']	ct:Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.	es, 6 figures, 4 tables (27 pages, 10 figures, 16 tables including references and appendices)	['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.07542', 'html': 'https://arxiv.org/html/2501.07542v1', 'tex': '/src/2501.07542', 'doi': 'https://doi.org/10.48550/arXiv.2501.07542'}	Submission history From: Chengzu Li [ view email ] [v1] Mon, 13 Jan 2025 18:23:57 UTC (1,501 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.07542'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.07542'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.07542'}]
2025-01-19	ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning	Computation and Language	https://arxiv.org/abs/2501.06590	ChemAgent	https://x.com/omarsar0/status/1879188983705747754		2501.06590	['Xiangru Tang', 'Tianyu Hu', 'Muyang Ye', 'Yanjun Shao', 'Xunjian Yin', 'Siru Ouyang', 'Wangchunshu Zhou', 'Pan Lu', 'Zhuosheng Zhang', 'Yilun Zhao', 'Arman Cohan', 'Mark Gerstein']	ct:Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science. Our code can be found atthis https URL		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2501.06590', 'html': None, 'tex': '/src/2501.06590', 'doi': 'https://doi.org/10.48550/arXiv.2501.06590'}	Submission history From: Xiangru Tang [ view email ] [v1] Sat, 11 Jan 2025 17:10:30 UTC (8,957 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.06590'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.06590'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.06590'}]
2025-01-12	Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks	Computation and Language	https://arxiv.org/abs/2412.15605	Cache-Augmented Generation (CAG)	https://x.com/omarsar0/status/1876721221083214200		2412.15605	['Brian J Chan', 'Chao-Ting Chen', 'Jui-Hung Cheng', 'Hen-Hsen Huang']	ct:Retrieval-augmented generation (RAG) has gained traction as a powerful approach for enhancing language models by integrating external knowledge sources. However, RAG introduces challenges such as retrieval latency, potential errors in document selection, and increased system complexity. With the advent of large language models (LLMs) featuring significantly extended context windows, this paper proposes an alternative paradigm, cache-augmented generation (CAG) that bypasses real-time retrieval. Our method involves preloading all relevant resources, especially when the documents or knowledge for retrieval are of a limited and manageable size, into the LLM's extended context and caching its runtime parameters. During inference, the model utilizes these preloaded parameters to answer queries without additional retrieval steps. Comparative analyses reveal that CAG eliminates retrieval latency and minimizes retrieval errors while maintaining context relevance. Performance evaluations across multiple benchmarks highlight scenarios where long-context LLMs either outperform or complement traditional RAG pipelines. These findings suggest that, for certain applications, particularly those with a constrained knowledge base, CAG provide a streamlined and efficient alternative to RAG, achieving comparable or superior results with reduced complexity.	s, accepted by the Web Conference 2025 (WWW '25) as a short paper	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.15605', 'html': 'https://arxiv.org/html/2412.15605v2', 'tex': '/src/2412.15605', 'doi': 'https://doi.org/10.48550/arXiv.2412.15605'}	Submission history From: Hen-Hsen Huang [ view email ] [v1] Fri, 20 Dec 2024 06:58:32 UTC (82 KB) [v2] Sun, 23 Feb 2025 19:48:12 UTC (634 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.15605'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.15605'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.15605'}]
2025-01-12	Agent Laboratory: Using LLM Agents as Research Assistants	Human-Computer Interaction	https://arxiv.org/abs/2501.04227	Agent Laboratory		"{""Tweet)"": ""https://x.com/omarsar0/status/1877382581358047375""}"	2501.04227	['Samuel Schmidgall', 'Yusheng Su', 'Ze Wang', 'Ximeng Sun', 'Jialian Wu', 'Xiaodong Yu', 'Jiang Liu', 'Michael Moor', 'Zicheng Liu', 'Emad Barsoum']	ct:Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.		['Human-Computer Interaction (cs.HC)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.04227', 'html': None, 'tex': '/src/2501.04227', 'doi': 'https://doi.org/10.48550/arXiv.2501.04227'}	Submission history From: Samuel Schmidgall [ view email ] [v1] Wed, 8 Jan 2025 01:58:42 UTC (7,619 KB) [v2] Tue, 17 Jun 2025 16:19:14 UTC (2,785 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.04227'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.04227'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.04227'}]
2025-01-12	Long Context vs. RAG for LLMs: An Evaluation and Revisits	Computation and Language	https://arxiv.org/abs/2501.01880	Long Context vs. RAG for LLMs	https://x.com/omarsar0/status/1876281074147299569		2501.01880	['Xinze Li', 'Yixin Cao', 'Yubo Ma', 'Aixin Sun']	ct:Extending context windows (i.e., Long Context, LC) and using retrievers to selectively access relevant information (i.e., Retrieval-Augmented Generation, RAG) are the two main strategies to enable LLMs to incorporate extremely long external contexts. This paper revisits recent studies on this topic, highlighting their key insights and discrepancies. We then provide a more comprehensive evaluation by filtering out questions answerable without external context, identifying the most effective retrieval methods, and expanding the datasets. We show that LC generally outperforms RAG in question-answering benchmarks, especially for Wikipedia-based questions. Summarization-based retrieval performs comparably to LC, while chunk-based retrieval lags behind. However, RAG has advantages in dialogue-based and general question queries. These insights underscore the trade-offs between RAG and LC strategies, offering guidance for future optimization of LLMs with external knowledge sources. We also provide an in-depth discussion on this topic, highlighting the overlooked importance of context relevance in existing studies.	es excluding references and appendix	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2501.01880', 'html': 'https://arxiv.org/html/2501.01880v1', 'tex': '/src/2501.01880', 'doi': 'https://doi.org/10.48550/arXiv.2501.01880'}	Submission history From: Xinze Li [ view email ] [v1] Fri, 27 Dec 2024 14:34:37 UTC (9,106 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.01880'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.01880'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.01880'}]
2025-01-12	Search-o1: Agentic Search-Enhanced Large Reasoning Models	Artificial Intelligence	https://arxiv.org/abs/2501.05366	Search-o1	https://x.com/omarsar0/status/1877742469213004015		2501.05366	['Xiaoxi Li', 'Guanting Dong', 'Jiajie Jin', 'Yuyao Zhang', 'Yujia Zhou', 'Yutao Zhu', 'Peitian Zhang', 'Zhicheng Dou']	ct:Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive long stepwise reasoning capabilities through large-scale reinforcement learning. However, their extended reasoning processes often suffer from knowledge insufficiency, leading to frequent uncertainties and potential errors. To address this limitation, we introduce \textbf{Search-o1}, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents. Search-o1 integrates an agentic search workflow into the reasoning process, enabling dynamic retrieval of external knowledge when LRMs encounter uncertain knowledge points. Additionally, due to the verbose nature of retrieved documents, we design a separate Reason-in-Documents module to deeply analyze the retrieved information before injecting it into the reasoning chain, minimizing noise and preserving coherent reasoning flow. Extensive experiments on complex reasoning tasks in science, mathematics, and coding, as well as six open-domain QA benchmarks, demonstrate the strong performance of Search-o1. This approach enhances the trustworthiness and applicability of LRMs in complex reasoning tasks, paving the way for more reliable and versatile intelligent systems. The code is available at \url{this https URL}.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2501.05366', 'html': 'https://arxiv.org/html/2501.05366v1', 'tex': '/src/2501.05366', 'doi': 'https://doi.org/10.48550/arXiv.2501.05366'}	Submission history From: Xiaoxi Li [ view email ] [v1] Thu, 9 Jan 2025 16:48:17 UTC (247 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.05366'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.05366'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.05366'}]
2025-01-12	Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought	Artificial Intelligence	https://arxiv.org/abs/2501.04682	Towards System 2 Reasoning		"{""Tweet)"": ""https://x.com/rm_rafailov/status/1877446475271037314""}"	2501.04682	['Violet Xiang', 'Charlie Snell', 'Kanishk Gandhi', 'Alon Albalak', 'Anikait Singh', 'Chase Blagden', 'Duy Phung', 'Rafael Rafailov', 'Nathan Lile', 'Dakota Mahan', 'Louis Castricato', 'Jan-Philipp Franken', 'Nick Haber', 'Chelsea Finn']	ct:We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search, and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search algorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating instruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss open research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning algorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2501.04682', 'html': None, 'tex': '/src/2501.04682', 'doi': 'https://doi.org/10.48550/arXiv.2501.04682'}	Submission history From: Rafael Rafailov [ view email ] [v1] Wed, 8 Jan 2025 18:42:48 UTC (24,263 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.04682'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.04682'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.04682'}]
2025-01-12	rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking	Computation and Language	https://arxiv.org/abs/2501.04519	rStar-Math		"{""Tweet)"": ""https://x.com/omarsar0/status/1877378301293142050""}"	2501.04519	['Xinyu Guan', 'Li Lyna Zhang', 'Yifei Liu', 'Ning Shang', 'Youran Sun', 'Yi Zhu', 'Fan Yang', 'Mao Yang']	"ct:We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising ""deep thinking"" through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naïve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available atthis https URL."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2501.04519', 'html': 'https://arxiv.org/html/2501.04519v1', 'tex': '/src/2501.04519', 'doi': 'https://doi.org/10.48550/arXiv.2501.04519'}	Submission history From: Li Lyna Zhang [ view email ] [v1] Wed, 8 Jan 2025 14:12:57 UTC (632 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.04519'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.04519'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.04519'}]
2025-01-12	Can LLMs Ask Good Questions?	Computation and Language	https://arxiv.org/abs/2501.03491	Can LLMs Design Good Questions?	https://x.com/omarsar0/status/1877008618207560049		2501.03491	['Yueheng Zhang', 'Xiaoyuan Liu', 'Yiyou Sun', 'Atheer Alharbi', 'Hend Alzahrani', 'Tianneng Shi', 'Basel Alomair', 'Dawn Song']	ct:We evaluate questions generated by large language models (LLMs) from context, comparing them to human-authored questions across six dimensions: question type, question length, context coverage, answerability, uncommonness, and required answer length. Our study spans two open-source and two proprietary state-of-the-art models. Results reveal that LLM-generated questions tend to demand longer descriptive answers and exhibit more evenly distributed context focus, in contrast to the positional bias often seen in QA tasks. These findings provide insights into the distinctive characteristics of LLM-generated questions and inform future work on question quality and downstream applications.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2501.03491', 'html': 'https://arxiv.org/html/2501.03491v2', 'tex': '/src/2501.03491', 'doi': 'https://doi.org/10.48550/arXiv.2501.03491'}	Submission history From: Yueheng Zhang [ view email ] [v1] Tue, 7 Jan 2025 03:21:17 UTC (970 KB) [v2] Tue, 17 Jun 2025 18:44:37 UTC (458 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.03491'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.03491'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.03491'}]
2025-01-12	A Survey on Large Language Models with some Insights on their Capabilities and Limitations	Computation and Language	https://arxiv.org/abs/2501.04040	A Survey on LLMs	https://x.com/omarsar0/status/1877416049999802408		2501.04040	['Andrea Matarazzo', 'Riccardo Torlone']	ct:The rapid advancement of artificial intelligence, particularly with the development of Large Language Models (LLMs) built on the transformer architecture, has redefined the capabilities of natural language processing. These models now exhibit remarkable performance across various language-related tasks, such as text generation, question answering, translation, and summarization, often rivaling human-like comprehension. More intriguingly, LLMs have demonstrated emergent abilities extending beyond their core functions, showing proficiency in tasks like commonsense reasoning, code generation, and arithmetic. This survey paper explores the foundational components, scaling mechanisms, and architectural strategies that drive these capabilities. Emphasizing models like GPT and LLaMA, we analyze the impact of exponential data and computational growth on LLM performance, while also addressing the trade-offs associated with scaling. We also examine LLM applications across sectors, such as healthcare, finance, education, and law, highlighting their adaptability and potential to solve domain-specific challenges. Central to this work are the questions of how LLMs generalize across diverse tasks, exhibit planning, and reasoning abilities, and whether these emergent abilities can be systematically elicited or enhanced. In particular, we provide some insights into the CoT (Chain of Thought) and PoT (Plan of Thought) abilities within LLMs, focusing on how pre-training data influences their emergence. Additionally, we investigate LLM-modulo frameworks that integrate external systems, allowing LLMs to handle complex, dynamic tasks. By analyzing these factors, this paper aims to foster the ongoing discussion on the capabilities and limits of LLMs, promoting their responsible development and application in novel and increasingly complex environments.	ges, to be submitted to a journal in a shorter version. It includes figures taken from papers by other authors. All the sources have been referenced. arXiv admin note: text overlap witharXiv:2303.18223by other authors	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Neural and Evolutionary Computing (cs.NE)']	{'pdf': '/pdf/2501.04040', 'html': 'https://arxiv.org/html/2501.04040v2', 'tex': '/src/2501.04040', 'doi': 'https://doi.org/10.48550/arXiv.2501.04040'}	Submission history From: Riccardo Torlone [ view email ] [v1] Fri, 3 Jan 2025 21:04:49 UTC (10,224 KB) [v2] Sun, 9 Feb 2025 08:00:36 UTC (10,227 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.04040'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.04040'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.04040'}]
2025-01-05	Agents Are Not Enough	Artificial Intelligence	https://www.arxiv.org/abs/2412.16241	Agents Are Not Enough	https://x.com/omarsar0/status/1874196827115061741		2412.16241	['Chirag Shah', 'Ryen W. White']	ct:In the midst of the growing integration of Artificial Intelligence (AI) into various aspects of our lives, agents are experiencing a resurgence. These autonomous programs that act on behalf of humans are neither new nor exclusive to the mainstream AI movement. By exploring past incarnations of agents, we can understand what has been done previously, what worked, and more importantly, what did not pan out and why. This understanding lets us to examine what distinguishes the current focus on agents. While generative AI is appealing, this technology alone is insufficient to make new generations of agents more successful. To make the current wave of agents effective and sustainable, we envision an ecosystem that includes not only agents but also Sims, which represent user preferences and behaviors, as well as Assistants, which directly interact with the user and coordinate the execution of user tasks with the help of the agents.		['Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2412.16241', 'html': 'https://arxiv.org/html/2412.16241v1', 'tex': '/src/2412.16241', 'doi': 'https://doi.org/10.48550/arXiv.2412.16241'}	Submission history From: Chirag Shah [ view email ] [v1] Thu, 19 Dec 2024 16:54:17 UTC (63 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.16241'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.16241'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.16241'}]
2025-01-05	2 OLMo 2 Furious	Computation and Language	https://arxiv.org/abs/2501.00656	OLMo 2	https://x.com/soldni/status/1875266934943649808		2501.00656	['Team OLMo', 'Pete Walsh', 'Luca Soldaini', 'Dirk Groeneveld', 'Kyle Lo', 'Shane Arora', 'Akshita Bhagia', 'Yuling Gu', 'Shengyi Huang', 'Matt Jordan', 'Nathan Lambert', 'Dustin Schwenk', 'Oyvind Tafjord', 'Taira Anderson', 'David Atkinson', 'Faeze Brahman', 'Christopher Clark', 'Pradeep Dasigi', 'Nouha Dziri', 'Michal Guerquin', 'Hamish Ivison', 'Pang Wei Koh', 'Jiacheng Liu', 'Saumya Malik', 'William Merrill', 'Lester James V. Miranda', 'Jacob Morrison', 'Tyler Murray', 'Crystal Nam', 'Valentina Pyatkin', 'Aman Rangapur', 'Michael Schmitz', 'Sam Skjonsberg', 'David Wadden', 'Christopher Wilhelm', 'Michael Wilson', 'Luke Zettlemoyer', 'Ali Farhadi', 'Noah A. Smith', 'Hannaneh Hajishirzi']	ct:We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from Tülu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.	demo available atthis http URL	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2501.00656', 'html': None, 'tex': '/src/2501.00656', 'doi': 'https://doi.org/10.48550/arXiv.2501.00656'}	Submission history From: Kyle Lo [ view email ] [v1] Tue, 31 Dec 2024 21:55:10 UTC (2,976 KB) [v2] Wed, 15 Jan 2025 01:44:16 UTC (2,976 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2501.00656'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2501.00656'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2501.00656'}]
2025-01-05	Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs	Computation and Language	https://arxiv.org/abs/2412.21187	On the Overthinking of LLMs	https://x.com/omarsar0/status/1874848885170176364		2412.21187	['Xingyu Chen', 'Jiahao Xu', 'Tian Liang', 'Zhiwei He', 'Jianhui Pang', 'Dian Yu', 'Linfeng Song', 'Qiuzhi Liu', 'Mengfei Zhou', 'Zhuosheng Zhang', 'Rui Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu']	ct:The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.	e updated the results of DeepSeek-R1, and all conclusions still hold	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.21187', 'html': 'https://arxiv.org/html/2412.21187v2', 'tex': '/src/2412.21187', 'doi': 'https://doi.org/10.48550/arXiv.2412.21187'}	Submission history From: Jiahao Xu [ view email ] [v1] Mon, 30 Dec 2024 18:55:12 UTC (3,378 KB) [v2] Sat, 1 Feb 2025 07:57:37 UTC (3,526 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.21187'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.21187'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.21187'}]
2025-01-05	MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes	Computation and Language	https://arxiv.org/abs/2412.19260	MEDEC	https://x.com/omarsar0/status/1875232390265577675		2412.19260	['Asma Ben Abacha', 'Wen-wai Yim', 'Yujuan Fu', 'Zhaoyi Sun', 'Meliha Yetisgen', 'Fei Xia', 'Thomas Lin']	ct:Several studies showed that Large Language Models (LLMs) can answer medical questions correctly, even outperforming the average human score in some medical exams. However, to our knowledge, no study has been conducted to assess the ability of language models to validate existing or generated medical text for correctness and consistency. In this paper, we introduce MEDEC (this https URL), the first publicly available benchmark for medical error detection and correction in clinical notes, covering five types of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes from three US hospital systems that were not previously seen by any LLM. The dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen participating systems [Ben Abacha et al., 2024]. In this paper, we describe the data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4, Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and correcting medical errors requiring both medical knowledge and reasoning capabilities. We also conducted a comparative study where two medical doctors performed the same task on the MEDEC test set. The results showed that MEDEC is a sufficiently challenging benchmark to assess the ability of models to validate existing or generated notes and to correct medical errors. We also found that although recent LLMs have a good performance in error detection and correction, they are still outperformed by medical doctors in these tasks. We discuss the potential factors behind this gap, the insights from our experiments, the limitations of current evaluation metrics, and share potential pointers for future research.	ersion has been updated with further clarification regarding the model size estimates that were mined from public articles only and provided to aid in contextualizing model performance. The authors cannot vouch for the accuracy of those estimates	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2412.19260', 'html': 'https://arxiv.org/html/2412.19260v2', 'tex': '/src/2412.19260', 'doi': 'https://doi.org/10.48550/arXiv.2412.19260'}	Submission history From: Asma Ben Abacha [ view email ] [v1] Thu, 26 Dec 2024 15:54:10 UTC (1,001 KB) [v2] Thu, 2 Jan 2025 18:46:05 UTC (1,002 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.19260'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.19260'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.19260'}]
2025-01-05	1.58-bit FLUX	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2412.18653	1.58-bit FLUX	https://x.com/_akhaliq/status/1873782702178263549		2412.18653	['Chenglin Yang', 'Celong Liu', 'Xueqing Deng', 'Dongwon Kim', 'Xing Mei', 'Xiaohui Shen', 'Liang-Chieh Chen']	ct:We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in {-1, 0, +1}) while maintaining comparable performance for generating 1024 x 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7x reduction in model storage, a 5.1x reduction in inference memory, and improved inference latency. Extensive evaluations on the GenEval and T2I Compbench benchmarks demonstrate the effectiveness of 1.58-bit FLUX in maintaining generation quality while significantly enhancing computational efficiency.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2412.18653', 'html': 'https://arxiv.org/html/2412.18653v1', 'tex': '/src/2412.18653', 'doi': 'https://doi.org/10.48550/arXiv.2412.18653'}	Submission history From: Chenglin Yang [ view email ] [v1] Tue, 24 Dec 2024 19:00:02 UTC (8,074 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.18653'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.18653'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.18653'}]
2025-01-05	Aviary: training language agents on challenging scientific tasks	Artificial Intelligence	https://arxiv.org/abs/2412.21154	Aviary	https://x.com/omarsar0/status/1875270927304511535		2412.21154	['Siddharth Narayanan', 'James D. Braza', 'Ryan-Rhys Griffiths', 'Manu Ponnapati', 'Albert Bou', 'Jon Laurent', 'Ori Kabeli', 'Geemi Wellawatte', 'Sam Cox', 'Samuel G. Rodriques', 'Andrew D. White']	ct:Solving complex real-world tasks requires cycles of actions and observations. This is particularly true in science, where tasks require many cycles of analysis, tool use, and experimentation. Language agents are promising for automating intellectual tasks in science because they can interact with tools via natural language or code. Yet their flexibility creates conceptual and practical challenges for software implementations, since agents may comprise non-standard components such as internal reasoning, planning, tool usage, as well as the inherent stochasticity of temperature-sampled language models. Here, we introduce Aviary, an extensible gymnasium for language agents. We formalize agents as policies solving language-grounded partially observable Markov decision processes, which we term language decision processes. We then implement five environments, including three challenging scientific environments: (1) manipulating DNA constructs for molecular cloning, (2) answering research questions by accessing scientific literature, and (3) engineering protein stability. These environments were selected for their focus on multi-step reasoning and their relevance to contemporary biology research. Finally, with online training and scaling inference-time compute, we show that language agents backed by open-source, non-frontier LLMs can match and exceed both frontier LLM agents and human experts on multiple tasks at up to 100x lower inference cost.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2412.21154', 'html': 'https://arxiv.org/html/2412.21154v1', 'tex': '/src/2412.21154', 'doi': 'https://doi.org/10.48550/arXiv.2412.21154'}	Submission history From: Andrew White [ view email ] [v1] Mon, 30 Dec 2024 18:33:28 UTC (5,039 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.21154'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.21154'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.21154'}]
2025-01-05	Memory Layers at Scale	Computation and Language	https://arxiv.org/abs/2412.09764	Memory Layers at Scale	https://x.com/AIatMeta/status/1874897646542033030		2412.09764	['Vincent-Pierre Berges', 'Barlas Oğuz', 'Daniel Haziza', 'Wen-tau Yih', 'Luke Zettlemoyer', 'Gargi Ghosh']	ct:Memory layers use a trainable key-value lookup mechanism to add extra parameters to a model without increasing FLOPs. Conceptually, sparsely activated memory layers complement compute-heavy dense feed-forward layers, providing dedicated capacity to store and retrieve information cheaply. This work takes memory layers beyond proof-of-concept, proving their utility at contemporary scale. On downstream tasks, language models augmented with our improved memory layer outperform dense models with more than twice the computation budget, as well as mixture-of-expert models when matched for both compute and parameters. We find gains are especially pronounced for factual tasks. We provide a fully parallelizable memory layer implementation, demonstrating scaling laws with up to 128B memory parameters, pretrained to 1 trillion tokens, comparing to base models with up to 8B parameters.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2412.09764', 'html': 'https://arxiv.org/html/2412.09764v2', 'tex': '/src/2412.09764', 'doi': 'https://doi.org/10.48550/arXiv.2412.09764'}	Submission history From: Barlas Oguz [ view email ] [v1] Thu, 12 Dec 2024 23:56:57 UTC (393 KB) [v2] Fri, 20 Dec 2024 17:36:52 UTC (393 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.09764'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.09764'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.09764'}]
2025-01-05	HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs	Computation and Language	https://arxiv.org/abs/2412.18925	HuatuoGPT-o1	https://x.com/_akhaliq/status/1873572891092283692		2412.18925	['Junying Chen', 'Zhenyang Cai', 'Ke Ji', 'Xidong Wang', 'Wanlong Liu', 'Rongsheng Wang', 'Jianye Hou', 'Benyou Wang']	ct:The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2412.18925', 'html': None, 'tex': '/src/2412.18925', 'doi': 'https://doi.org/10.48550/arXiv.2412.18925'}	Submission history From: Junying Chen [ view email ] [v1] Wed, 25 Dec 2024 15:12:34 UTC (1,151 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.18925'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.18925'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.18925'}]
2024-12-29	Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference	Computation and Language	https://arxiv.org/abs/2412.13663	ModernBERT	https://x.com/jeremyphoward/status/1869786023963832509		2412.13663	['Benjamin Warner', 'Antoine Chaffin', 'Benjamin Clavié', 'Orion Weller', 'Oskar Hallström', 'Said Taghadouini', 'Alexis Gallagher', 'Raja Biswas', 'Faisal Ladhak', 'Tom Aarsen', 'Nathan Cooper', 'Griffin Adams', 'Jeremy Howard', 'Iacopo Poli']	ct:Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2412.13663', 'html': 'https://arxiv.org/html/2412.13663v2', 'tex': '/src/2412.13663', 'doi': 'https://doi.org/10.48550/arXiv.2412.13663'}	Submission history From: Benjamin Clavié [ view email ] [v1] Wed, 18 Dec 2024 09:39:44 UTC (81 KB) [v2] Thu, 19 Dec 2024 06:32:26 UTC (81 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.13663'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.13663'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.13663'}]
2024-12-29	Automating the Search for Artificial Life with Foundation Models	Artificial Intelligence	https://arxiv.org/abs/2412.17799	Automating the Search for Artificial Life	https://x.com/SakanaAILabs/status/1871385917342265592		2412.17799	['Akarsh Kumar', 'Chris Lu', 'Louis Kirsch', 'Yujin Tang', 'Kenneth O. Stanley', 'Phillip Isola', 'David Ha']	ct:With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.	es, 19 figures	['Artificial Intelligence (cs.AI)', 'Neural and Evolutionary Computing (cs.NE)']	{'pdf': '/pdf/2412.17799', 'html': 'https://arxiv.org/html/2412.17799v2', 'tex': '/src/2412.17799', 'doi': 'https://doi.org/10.48550/arXiv.2412.17799'}	Submission history From: Akarsh Kumar [ view email ] [v1] Mon, 23 Dec 2024 18:57:00 UTC (12,203 KB) [v2] Fri, 16 May 2025 21:19:02 UTC (15,160 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.17799'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.17799'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.17799'}]
2024-12-29	A Survey on LLM Inference-Time Self-Improvement	Computation and Language	https://arxiv.org/abs/2412.14352	A Survey on LLM Inference-Time Self-Improvement	https://x.com/omarsar0/status/1870129825282658752		2412.14352	['Xiangjue Dong', 'Maria Teleki', 'James Caverlee']	ct:Techniques that enhance inference through increased computation at test-time have recently gained attention. In this survey, we investigate the current state of LLM Inference-Time Self-Improvement from three different perspectives: Independent Self-improvement, focusing on enhancements via decoding or sampling methods; Context-Aware Self-Improvement, leveraging additional context or datastore; and Model-Aided Self-Improvement, achieving improvement through model collaboration. We provide a comprehensive review of recent relevant studies, contribute an in-depth taxonomy, and discuss challenges and limitations, offering insights for future research.	rst two authors contribute equally	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.14352', 'html': 'https://arxiv.org/html/2412.14352v1', 'tex': '/src/2412.14352', 'doi': 'https://doi.org/10.48550/arXiv.2412.14352'}	Submission history From: Xiangjue Dong [ view email ] [v1] Wed, 18 Dec 2024 21:37:07 UTC (179 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.14352'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.14352'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.14352'}]
2024-12-29	Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2412.18319	Empowering MLLM with o1-like Reasoning and Reflection	https://x.com/_akhaliq/status/1872326647606841651		2412.18319	['Huanjin Yao', 'Jiaxing Huang', 'Wenhao Wu', 'Jingyi Zhang', 'Yibo Wang', 'Shunyu Liu', 'Yingjie Wang', 'Yuxin Song', 'Haocheng Feng', 'Li Shen', 'Dacheng Tao']	ct:In this work, we aim to develop an MLLM that understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. To this end, we propose Collective Monte Carlo Tree Search (CoMCTS), a new learning-to-reason method for MLLMs, which introduces the concept of collective learning into ``tree search'' for effective and efficient reasoning-path searching and learning. The core idea of CoMCTS is to leverage collective knowledge from multiple models to collaboratively conjecture, search and identify effective reasoning paths toward correct answers via four iterative operations including Expansion, Simulation and Error Positioning, Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question. With Mulberry-260k, we perform collective SFT to train our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and Reflection capabilities. Extensive experiments demonstrate the superiority of our proposed methods on various benchmarks. Code will be available atthis https URL	cal report	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2412.18319', 'html': 'https://arxiv.org/html/2412.18319v2', 'tex': '/src/2412.18319', 'doi': 'https://doi.org/10.48550/arXiv.2412.18319'}	Submission history From: Huanjin Yao [ view email ] [v1] Tue, 24 Dec 2024 10:07:51 UTC (1,130 KB) [v2] Tue, 31 Dec 2024 07:41:30 UTC (1,130 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.18319'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.18319'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.18319'}]
2024-12-29	Reinforcement Learning: An Overview	Artificial Intelligence	https://arxiv.org/abs/2412.05265	Reinforcement Learning Overview	https://x.com/omarsar0/status/1866123264965419460		2412.05265	['Kevin Murphy']	ct:This manuscript gives a big-picture, up-to-date overview of the field of (deep) reinforcement learning and sequential decision making, covering value-based methods, policy-based methods, model-based methods, multi-agent RL, LLMs and RL, and various other topics (e.g., offline RL, hierarchical RL, intrinsic reward).		['Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2412.05265', 'html': None, 'tex': '/src/2412.05265', 'doi': 'https://doi.org/10.48550/arXiv.2412.05265'}	Submission history From: Kevin Murphy [ view email ] [v1] Fri, 6 Dec 2024 18:53:49 UTC (6,099 KB) [v2] Mon, 24 Mar 2025 05:54:08 UTC (7,381 KB) [v3] Mon, 19 May 2025 15:12:39 UTC (8,871 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.05265'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.05265'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.05265'}]
2024-12-29	DRT: Deep Reasoning Translation via Long Chain-of-Thought	Computation and Language	https://arxiv.org/abs/2412.17498	DRT-o1	https://x.com/_akhaliq/status/1871455986189574320		2412.17498	['Jiaan Wang', 'Fandong Meng', 'Yunlong Liang', 'Jie Zhou']	ct:Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to quantify the translation quality in each round. In this way, we collect tens of thousands of long-thought MT data, which is used to train our DRT. Using Qwen2.5 and LLama-3.1 as the backbones, DRT models can learn the thought process during machine translation, and outperform vanilla LLMs as well as LLMs which are simply fine-tuning on the paired sentences without long thought, showing its effectiveness.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2412.17498', 'html': 'https://arxiv.org/html/2412.17498v3', 'tex': '/src/2412.17498', 'doi': 'https://doi.org/10.48550/arXiv.2412.17498'}	Submission history From: Jiaan Wang [ view email ] [v1] Mon, 23 Dec 2024 11:55:33 UTC (521 KB) [v2] Mon, 30 Dec 2024 16:29:36 UTC (601 KB) [v3] Mon, 10 Feb 2025 11:35:28 UTC (740 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.17498'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.17498'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.17498'}]
2024-12-22	Alignment faking in large language models	Artificial Intelligence	https://arxiv.org/abs/2412.14093	Alignment Faking in LLMs	https://x.com/AnthropicAI/status/1869427646368792599		2412.14093	['Ryan Greenblatt', 'Carson Denison', 'Benjamin Wright', 'Fabien Roger', 'Monte MacDiarmid', 'Sam Marks', 'Johannes Treutlein', 'Tim Belonax', 'Jack Chen', 'David Duvenaud', 'Akbir Khan', 'Julian Michael', 'Sören Mindermann', 'Ethan Perez', 'Linda Petrini', 'Jonathan Uesato', 'Jared Kaplan', 'Buck Shlegeris', 'Samuel R. Bowman', 'Evan Hubinger']	ct:We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference--as in this case--or not.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2412.14093', 'html': 'https://arxiv.org/html/2412.14093v2', 'tex': '/src/2412.14093', 'doi': 'https://doi.org/10.48550/arXiv.2412.14093'}	Submission history From: Evan Hubinger [ view email ] [v1] Wed, 18 Dec 2024 17:41:24 UTC (11,382 KB) [v2] Fri, 20 Dec 2024 02:22:19 UTC (11,382 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.14093'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.14093'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.14093'}]
2024-12-22	TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks	Computation and Language	https://arxiv.org/abs/2412.14161	TheAgentCompany	https://x.com/gneubig/status/1869735196700062089		2412.14161	['Frank F. Xu', 'Yufan Song', 'Boxuan Li', 'Yuxuan Tang', 'Kritanjali Jain', 'Mengxue Bao', 'Zora Z. Wang', 'Xuhui Zhou', 'Zhitong Guo', 'Murong Cao', 'Mingyang Yang', 'Hao Yang Lu', 'Amaad Martin', 'Zhe Su', 'Leander Maben', 'Raj Mehta', 'Wayne Chi', 'Lawrence Jang', 'Yiqing Xie', 'Shuyan Zhou', 'Graham Neubig']	ct:We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments onthis https URL.	nt	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.14161', 'html': 'https://arxiv.org/html/2412.14161v2', 'tex': '/src/2412.14161', 'doi': 'https://doi.org/10.48550/arXiv.2412.14161'}	Submission history From: Frank F. Xu [ view email ] [v1] Wed, 18 Dec 2024 18:55:40 UTC (2,260 KB) [v2] Mon, 19 May 2025 21:21:49 UTC (2,745 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.14161'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.14161'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.14161'}]
2024-12-22	Can LLMs Convert Graphs to Text-Attributed Graphs?	Computation and Language	https://arxiv.org/abs/2412.10136	Graphs to Text-Attributed Graphs	https://x.com/omarsar0/status/1868691391129272461		2412.10136	['Zehong Wang', 'Sidney Liu', 'Zheyuan Zhang', 'Tianyi Ma', 'Chuxu Zhang', 'Yanfang Ye']	ct:Graphs are ubiquitous structures found in numerous real-world applications, such as drug discovery, recommender systems, and social network analysis. To model graph-structured data, graph neural networks (GNNs) have become a popular tool. However, existing GNN architectures encounter challenges in cross-graph learning where multiple graphs have different feature spaces. To address this, recent approaches introduce text-attributed graphs (TAGs), where each node is associated with a textual description, which can be projected into a unified feature space using textual encoders. While promising, this method relies heavily on the availability of text-attributed graph data, which is difficult to obtain in practice. To bridge this gap, we propose a novel method named Topology-Aware Node description Synthesis (TANS), leveraging large language models (LLMs) to convert existing graphs into text-attributed graphs. The key idea is to integrate topological information into LLMs to explain how graph topology influences node semantics. We evaluate our TANS on text-rich, text-limited, and text-free graphs, demonstrating its applicability. Notably, on text-free graphs, our method significantly outperforms existing approaches that manually design node features, showcasing the potential of LLMs for preprocessing graph-structured data in the absence of textual information. The code and data are available atthis https URL.	ed by NAACL 25 Main Conference	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2412.10136', 'html': 'https://arxiv.org/html/2412.10136v2', 'tex': '/src/2412.10136', 'doi': 'https://doi.org/10.48550/arXiv.2412.10136'}	Submission history From: Zehong Wang [ view email ] [v1] Fri, 13 Dec 2024 13:32:59 UTC (475 KB) [v2] Fri, 7 Feb 2025 01:29:07 UTC (476 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.10136'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.10136'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.10136'}]
2024-12-22	Qwen2.5 Technical Report	Computation and Language	https://arxiv.org/abs/2412.15115	Qwen-2.5 Technical Report	https://x.com/Alibaba_Qwen/status/1869950647501824015		2412.15115	['Qwen', 'An Yang', 'Baosong Yang', 'Beichen Zhang', 'Binyuan Hui', 'Bo Zheng', 'Bowen Yu', 'Chengyuan Li', 'Dayiheng Liu', 'Fei Huang', 'Haoran Wei', 'Huan Lin', 'Jian Yang', 'Jianhong Tu', 'Jianwei Zhang', 'Jianxin Yang', 'Jiaxi Yang', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Keming Lu', 'Keqin Bao', 'Kexin Yang', 'Le Yu', 'Mei Li', 'Mingfeng Xue', 'Pei Zhang', 'Qin Zhu', 'Rui Men', 'Runji Lin', 'Tianhao Li', 'Tianyi Tang', 'Tingyu Xia', 'Xingzhang Ren', 'Xuancheng Ren', 'Yang Fan', 'Yang Su', 'Yichang Zhang', 'Yu Wan', 'Yuqiong Liu', 'Zeyu Cui', 'Zhenru Zhang', 'Zihan Qiu']	ct:In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.15115', 'html': None, 'tex': '/src/2412.15115', 'doi': 'https://doi.org/10.48550/arXiv.2412.15115'}	Submission history From: Binyuan Hui [ view email ] [v1] Thu, 19 Dec 2024 17:56:09 UTC (2,142 KB) [v2] Fri, 3 Jan 2025 02:18:21 UTC (2,124 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.15115'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.15115'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.15115'}]
2024-12-22	Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents	Machine Learning	https://arxiv.org/abs/2412.13194	PAE (Proposer-Agent-Evaluator)			2412.13194	['Yifei Zhou', 'Qianlan Yang', 'Kaixiang Lin', 'Min Bai', 'Xiong Zhou', 'Yu-Xiong Wang', 'Sergey Levine', 'Erran Li']	ct:The vision of a broadly capable and goal-directed agent, such as an Internet-browsing agent in the digital world and a household humanoid in the physical world, has rapidly advanced, thanks to the generalization capability of foundation models. Such a generalist agent needs to have a large and diverse skill repertoire, such as finding directions between two travel locations and buying specific items from the Internet. If each skill needs to be specified manually through a fixed set of human-annotated instructions, the agent's skill repertoire will necessarily be limited due to the quantity and diversity of human-annotated instructions. In this work, we address this challenge by proposing Proposer-Agent-Evaluator, an effective learning system that enables foundation model agents to autonomously discover and practice skills in the wild. At the heart of PAE is a context-aware task proposer that autonomously proposes tasks for the agent to practice with context information of the environment such as user demos or even just the name of the website itself for Internet-browsing agents. Then, the agent policy attempts those tasks with thoughts and actual grounded operations in the real world with resulting trajectories evaluated by an autonomous VLM-based success evaluator. The success evaluation serves as the reward signal for the agent to refine its policies through RL. We validate PAE on challenging vision-based web navigation, using both real-world and self-hosted websites from WebVoyager andthis http URLthe best of our knowledge, this work represents the first effective learning system to apply autonomous task proposal with RL for agents that generalizes real-world human-annotated benchmarks with SOTA performances. Our open-source checkpoints and code can be found inthis https URL		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2412.13194', 'html': 'https://arxiv.org/html/2412.13194v1', 'tex': '/src/2412.13194', 'doi': 'https://doi.org/10.48550/arXiv.2412.13194'}	Submission history From: Qianlan Yang [ view email ] [v1] Tue, 17 Dec 2024 18:59:50 UTC (43,032 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.13194'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.13194'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.13194'}]
2024-12-22	DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2412.10302	DeepSeek-VL2	https://x.com/omarsar0/status/1868696154067865659		2412.10302	['Zhiyu Wu', 'Xiaokang Chen', 'Zizheng Pan', 'Xingchao Liu', 'Wen Liu', 'Damai Dai', 'Huazuo Gao', 'Yiyang Ma', 'Chengyue Wu', 'Bingxuan Wang', 'Zhenda Xie', 'Yu Wu', 'Kai Hu', 'Jiawei Wang', 'Yaofeng Sun', 'Yukun Li', 'Yishi Piao', 'Kang Guan', 'Aixin Liu', 'Xin Xie', 'Yuxiang You', 'Kai Dong', 'Xingkai Yu', 'Haowei Zhang', 'Liang Zhao', 'Yisong Wang', 'Chong Ruan']	ct:We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we incorporate a dynamic tiling vision encoding strategy designed for processing high-resolution images with different aspect ratios. For the language component, we leverage DeepSeekMoE models with the Multi-head Latent Attention mechanism, which compresses Key-Value cache into latent vectors, to enable efficient inference and high throughput. Trained on an improved vision-language dataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding. Our model series is composed of three variants: DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated parameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art performance with similar or fewer activated parameters compared to existing open-source dense and MoE-based models. Codes and pre-trained models are publicly accessible atthis https URL.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.10302', 'html': 'https://arxiv.org/html/2412.10302v1', 'tex': '/src/2412.10302', 'doi': 'https://doi.org/10.48550/arXiv.2412.10302'}	Submission history From: Xiaokang Chen [ view email ] [v1] Fri, 13 Dec 2024 17:37:48 UTC (5,498 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.10302'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.10302'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.10302'}]
2024-12-22	Using Generative AI and Multi-Agents to Provide Automatic Feedback	Computation and Language	https://arxiv.org/abs/2411.07407	AutoFeedback			2411.07407	['Shuchen Guo', 'Ehsan Latif', 'Yifan Zhou', 'Xuan Huang', 'Xiaoming Zhai']	ct:This study investigates the use of generative AI and multi-agent systems to provide automatic feedback in educational contexts, particularly for student constructed responses in science assessments. The research addresses a key gap in the field by exploring how multi-agent systems, called AutoFeedback, can improve the quality of GenAI-generated feedback, overcoming known issues such as over-praise and over-inference that are common in single-agent large language models (LLMs). The study developed a multi-agent system consisting of two AI agents: one for generating feedback and another for validating and refining it. The system was tested on a dataset of 240 student responses, and its performance was compared to that of a single-agent LLM. Results showed that AutoFeedback significantly reduced the occurrence of over-praise and over-inference errors, providing more accurate and pedagogically sound feedback. The findings suggest that multi-agent systems can offer a more reliable solution for generating automated feedback in educational settings, highlighting their potential for scalable and personalized learning support. These results have important implications for educators and researchers seeking to leverage AI in formative assessments, offering a pathway to more effective feedback mechanisms that enhance student learning outcomes.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.07407', 'html': 'https://arxiv.org/html/2411.07407v1', 'tex': '/src/2411.07407', 'doi': 'https://doi.org/10.48550/arXiv.2411.07407'}	Submission history From: Yifan Zhou [ view email ] [v1] Mon, 11 Nov 2024 22:27:36 UTC (9,165 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.07407'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.07407'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.07407'}]
2024-12-22	A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges	Computation and Language	https://arxiv.org/abs/2412.11936	A Survey of Mathematical Reasoning in the Era of Multimodal LLMs	https://x.com/omarsar0/status/1870126516832792811		2412.11936	['Yibo Yan', 'Jiamin Su', 'Jianxiang He', 'Fangteng Fu', 'Xu Zheng', 'Yuanhuiyi Lyu', 'Kun Wang', 'Shen Wang', 'Qingsong Wen', 'Xuming Hu']	ct:Mathematical reasoning, a core aspect of human cognition, is vital across many domains, from educational problem-solving to scientific advancements. As artificial general intelligence (AGI) progresses, integrating large language models (LLMs) with mathematical reasoning tasks is becoming increasingly significant. This survey provides the first comprehensive analysis of mathematical reasoning in the era of multimodal large language models (MLLMs). We review over 200 studies published since 2021, and examine the state-of-the-art developments in Math-LLMs, with a focus on multimodal settings. We categorize the field into three dimensions: benchmarks, methodologies, and challenges. In particular, we explore multimodal mathematical reasoning pipeline, as well as the role of (M)LLMs and the associated methodologies. Finally, we identify five major challenges hindering the realization of AGI in this domain, offering insights into the future direction for enhancing multimodal reasoning capabilities. This survey serves as a critical resource for the research community in advancing the capabilities of LLMs to tackle complex multimodal reasoning tasks.	ed by The 63rd Annual Meeting of the Association for Computational Linguistics (ACL Findings 2025)	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.11936', 'html': None, 'tex': '/src/2412.11936', 'doi': 'https://doi.org/10.48550/arXiv.2412.11936'}	Submission history From: Yibo Yan [ view email ] [v1] Mon, 16 Dec 2024 16:21:41 UTC (812 KB) [v2] Tue, 18 Feb 2025 02:37:21 UTC (824 KB) [v3] Tue, 20 May 2025 14:45:21 UTC (830 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.11936'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.11936'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.11936'}]
2024-12-22	Precise Length Control in Large Language Models	Computation and Language	https://arxiv.org/abs/2412.11937	Precise Length Control in LLMs	https://x.com/omarsar0/status/1869030043084845453		2412.11937	"['Bradley Butcher', ""Michael O'Keefe"", 'James Titchener']"	ct:Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering. Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail. In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length. Our approach incorporates a secondary length-difference positional encoding (LDPE) into the input embeddings, which counts down to a user-set response termination length. Fine-tuning with LDPE allows the model to learn to terminate responses coherently at the desired length, achieving mean token errors of less than 3 tokens. We also introduce Max New Tokens++, an extension that enables flexible upper-bound length control, rather than an exact target. Experimental results on tasks such as question answering and document summarization demonstrate that our method enables precise length control without compromising response quality.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.11937', 'html': 'https://arxiv.org/html/2412.11937v1', 'tex': '/src/2412.11937', 'doi': 'https://doi.org/10.48550/arXiv.2412.11937'}	Submission history From: James Titchener [ view email ] [v1] Mon, 16 Dec 2024 16:22:27 UTC (501 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.11937'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.11937'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.11937'}]
2024-12-15	Training Large Language Models to Reason in a Continuous Latent Space	Computation and Language	https://arxiv.org/abs/2412.06769	Training LLMs to Reason in a Continuous Latent Space	https://x.com/omarsar0/status/1866518791733342563		2412.06769	['Shibo Hao', 'Sainbayar Sukhbaatar', 'DiJia Su', 'Xian Li', 'Zhiting Hu', 'Jason Weston', 'Yuandong Tian']	"ct:Large language models (LLMs) are restricted to reason in the ""language space"", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed ""continuous thought""). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.06769', 'html': 'https://arxiv.org/html/2412.06769v2', 'tex': '/src/2412.06769', 'doi': 'https://doi.org/10.48550/arXiv.2412.06769'}	Submission history From: Shibo Hao [ view email ] [v1] Mon, 9 Dec 2024 18:55:56 UTC (11,057 KB) [v2] Wed, 11 Dec 2024 04:52:56 UTC (11,057 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.06769'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.06769'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.06769'}]
2024-12-15	Phi-4 Technical Report	Computation and Language	https://arxiv.org/abs/2412.08905	Phi-4 Technical Report	https://x.com/omarsar0/status/1867609628529635574		2412.08905	['Marah Abdin', 'Jyoti Aneja', 'Harkirat Behl', 'Sébastien Bubeck', 'Ronen Eldan', 'Suriya Gunasekar', 'Michael Harrison', 'Russell J. Hewett', 'Mojan Javaheripi', 'Piero Kauffmann', 'James R. Lee', 'Yin Tat Lee', 'Yuanzhi Li', 'Weishung Liu', 'Caio C. T. Mendes', 'Anh Nguyen', 'Eric Price', 'Gustavo de Rosa', 'Olli Saarikivi', 'Adil Salim', 'Shital Shah', 'Xin Wang', 'Rachel Ward', 'Yue Wu', 'Dingli Yu', 'Cyril Zhang', 'Yi Zhang']	ct:We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2412.08905', 'html': 'https://arxiv.org/html/2412.08905v1', 'tex': '/src/2412.08905', 'doi': 'https://doi.org/10.48550/arXiv.2412.08905'}	Submission history From: Harkirat Behl [ view email ] [v1] Thu, 12 Dec 2024 03:37:41 UTC (408 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.08905'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.08905'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.08905'}]
2024-12-15	Asynchronous LLM Function Calling	Computation and Language	https://arxiv.org/abs/2412.07017	Asynchronous Function Calling	https://x.com/omarsar0/status/1866855077983686804		2412.07017	['In Gim', 'Seung-seob Lee', 'Lin Zhong']	ct:Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM's operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call's completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2412.07017', 'html': 'https://arxiv.org/html/2412.07017v1', 'tex': '/src/2412.07017', 'doi': 'https://doi.org/10.48550/arXiv.2412.07017'}	Submission history From: In Gim [ view email ] [v1] Mon, 9 Dec 2024 21:53:10 UTC (2,379 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.07017'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.07017'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.07017'}]
2024-12-15	MAG-V: A Multi-Agent Framework for Synthetic Data Generation and Verification	Computation and Language	https://arxiv.org/abs/2412.04494	MAG-V	https://x.com/omarsar0/status/1866143542726340890		2412.04494	['Saptarshi Sengupta', 'Harsh Vashistha', 'Kristal Curtis', 'Akshay Mallipeddi', 'Abhinav Mathur', 'Joseph Ross', 'Liang Gou']	ct:Extending the capabilities of Large Language Models (LLMs) with functions or tools for environment interaction has led to the emergence of the agent paradigm. In industry, training an LLM is not always feasible because of the scarcity of domain data, legal holds on proprietary customer data, rapidly changing business requirements, and the need to prototype new assistants. Agents provide an elegant solution to the above by relying on the zero-shot reasoning abilities of the underlying LLM and utilizing tools to explore and reason over customer data and respond to user requests. However, there are two concerns here: (I) acquiring large scale customer queries for agent testing is time-consuming, and (II) high reliance on the tool call sequence (or trajectory) followed by the agent to respond to user queries may lead to unexpected or incorrect behavior. To address this, we propose MAG-V, a multi-agent framework to first generate a dataset of questions that mimic customer queries; and second, reverse-engineer alternate questions from the responses for trajectory verification. Initial results indicate that our synthetic data can improve agent performance on actual customer queries. Furthermore, our trajectory verification methodology, inspired by distant supervision and using traditional machine learning (ML) models, outperforms a GPT-4o judge baseline by 11% accuracy and matches the performance of a GPT-4 judge on our constructed dataset. Overall, our approach is a step towards unifying diverse task agents into a cohesive framework for achieving an aligned objective.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.04494', 'html': 'https://arxiv.org/html/2412.04494v2', 'tex': '/src/2412.04494', 'doi': 'https://doi.org/10.48550/arXiv.2412.04494'}	Submission history From: Akshay Mallipeddi [ view email ] [v1] Thu, 28 Nov 2024 19:36:11 UTC (244 KB) [v2] Fri, 10 Jan 2025 22:22:41 UTC (244 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.04494'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.04494'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.04494'}]
2024-12-15	LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods	Computation and Language	https://arxiv.org/abs/2412.05579	A Survey on LLMs-as-Judges	https://x.com/omarsar0/status/1866541394015518824		2412.05579	['Haitao Li', 'Qian Dong', 'Junjie Chen', 'Huixue Su', 'Yujia Zhou', 'Qingyao Ai', 'Ziyi Ye', 'Yiqun Liu']	ct:The rapid advancement of Large Language Models (LLMs) has driven their expanding application across various fields. One of the most promising applications is their role as evaluators based on natural language responses, referred to as ''LLMs-as-judges''. This framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language. This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations. We begin by providing a systematic definition of LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then we address methodology to construct an evaluation system with LLMs (How to use LLM judges?). Additionally, we investigate the potential domains for their application (Where to use LLM judges?) and discuss methods for evaluating them in various contexts (How to evaluate LLM judges?). Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions. Through a structured and comprehensive analysis, we aim aims to provide insights on the development and application of LLMs-as-judges in both research and practice. We will continue to maintain the relevant resource list atthis https URL.	es, comprehensive and continuously updated	['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2412.05579', 'html': 'https://arxiv.org/html/2412.05579v2', 'tex': '/src/2412.05579', 'doi': 'https://doi.org/10.48550/arXiv.2412.05579'}	Submission history From: Haitao Li [ view email ] [v1] Sat, 7 Dec 2024 08:07:24 UTC (4,814 KB) [v2] Tue, 10 Dec 2024 05:49:12 UTC (4,809 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.05579'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.05579'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.05579'}]
2024-12-15	AutoReason: Automatic Few-Shot Reasoning Decomposition	Computation and Language	https://arxiv.org/abs/2412.06975	AutoReason Improves Multi-step Reasoning	https://x.com/omarsar0/status/1867224350287372555		2412.06975	['Arda Sevinc', 'Abdurrahman Gumus']	ct:Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no capability to adjust itself to different queries.In this work, we propose a system to automatically generate rationales using CoT. Our method improves multi-step implicit reasoning capabilities by decomposing the implicit query into several explicit questions. This provides interpretability for the model, improving reasoning in weaker LLMs. We test our approach with two Q\&A datasets: StrategyQA and HotpotQA. We show an increase in accuracy with both, especially on StrategyQA.To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub:this https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2412.06975', 'html': 'https://arxiv.org/html/2412.06975v1', 'tex': '/src/2412.06975', 'doi': 'https://doi.org/10.48550/arXiv.2412.06975'}	Submission history From: Abdurrahman Gumus [ view email ] [v1] Mon, 9 Dec 2024 20:35:39 UTC (6,311 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.06975'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.06975'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.06975'}]
2024-12-15	Does RLHF Scale? Exploring the Impacts From Data, Model, and Method	Computation and Language	https://arxiv.org/abs/2412.06000	Does RLHF Scale?	https://x.com/omarsar0/status/1866525606562680954		2412.06000	['Zhenyu Hou', 'Pengfan Du', 'Yilin Niu', 'Zhengxiao Du', 'Aohan Zeng', 'Xiao Liu', 'Minlie Huang', 'Hongning Wang', 'Jie Tang', 'Yuxiao Dong']	ct:This study explores the scaling properties of Reinforcement Learning from Human Feedback (RLHF) in Large Language Models (LLMs). Although RLHF is considered an important step in post-training of LLMs, its scaling potential is still largely unknown. We systematically analyze key components in the RLHF framework--model size, data composition, and inference budget--and their impacts on performance. Our findings show that increasing data diversity and volume improves reward model performance, helping process-supervision models scale better. For policy training, more response samples per prompt boost performance initially but quickly plateau. And larger reward models offer modest gains in policy training. In addition, larger policy models benefit less from RLHF with a fixed reward model. Overall, RLHF scales less efficiently than pretraining, with diminishing returns from additional computational resources. Based on these observations, we propose strategies to optimize RLHF performance within computational limits.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2412.06000', 'html': 'https://arxiv.org/html/2412.06000v1', 'tex': '/src/2412.06000', 'doi': 'https://doi.org/10.48550/arXiv.2412.06000'}	Submission history From: Zhenyu Hou [ view email ] [v1] Sun, 8 Dec 2024 17:19:48 UTC (3,080 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.06000'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.06000'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.06000'}]
2024-12-15	Granite Guardian	Computation and Language	https://arxiv.org/abs/2412.07724	Granite Guardian	https://x.com/omarsar0/status/1866852443621036228		2412.07724	['Inkit Padhi', 'Manish Nagireddy', 'Giandomenico Cornacchia', 'Subhajit Chaudhury', 'Tejaswini Pedapati', 'Pierre Dognin', 'Keerthiram Murugesan', 'Erik Miehling', 'Martín Santillán Cooper', 'Kieran Fraser', 'Giulio Zizzo', 'Muhammad Zaid Hameed', 'Mark Purcell', 'Michael Desmond', 'Qian Pan', 'Zahra Ashktorab', 'Inge Vejsbjerg', 'Elizabeth M. Daly', 'Michael Hind', 'Werner Geyer', 'Ambrish Rawat', 'Kush R. Varshney', 'Prasanna Sattigeri']	ct:We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, and hallucination-related risks such as context relevance, groundedness, and answer relevance for retrieval-augmented generation (RAG). Trained on a unique dataset combining human annotations from diverse sources and synthetic data, Granite Guardian models address risks typically overlooked by traditional risk detection models, such as jailbreaks and RAG-specific issues. With AUC scores of 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks respectively, Granite Guardian is the most generalizable and competitive model available in the space. Released as open-source, Granite Guardian aims to promote responsible AI development across the community.this https URL		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.07724', 'html': 'https://arxiv.org/html/2412.07724v2', 'tex': '/src/2412.07724', 'doi': 'https://doi.org/10.48550/arXiv.2412.07724'}	Submission history From: Manish Nagireddy [ view email ] [v1] Tue, 10 Dec 2024 18:17:02 UTC (2,075 KB) [v2] Mon, 16 Dec 2024 20:27:36 UTC (2,075 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.07724'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.07724'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.07724'}]
2024-12-08	Reverse Thinking Makes LLMs Stronger Reasoners	Computation and Language	https://arxiv.org/abs/2411.19865	Reverse Thinking	https://x.com/omarsar0/status/1863595518649098371		2411.19865	['Justin Chih-Yao Chen', 'Zifeng Wang', 'Hamid Palangi', 'Rujun Han', 'Sayna Ebrahimi', 'Long Le', 'Vincent Perot', 'Swaroop Mishra', 'Mohit Bansal', 'Chen-Yu Lee', 'Tomas Pfister']	ct:Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. We then employ three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model's zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, our method demonstrates sample efficiency -- using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10x more forward reasoning. RevThink also exhibits strong generalization to out-of-distribution held-out datasets.	ed to NAACL 2025	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2411.19865', 'html': 'https://arxiv.org/html/2411.19865v2', 'tex': '/src/2411.19865', 'doi': 'https://doi.org/10.48550/arXiv.2411.19865'}	Submission history From: Justin Chih-Yao Chen [ view email ] [v1] Fri, 29 Nov 2024 17:27:05 UTC (191 KB) [v2] Fri, 7 Mar 2025 20:33:35 UTC (192 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.19865'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.19865'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.19865'}]
2024-12-08	Towards Adaptive Mechanism Activation in Language Agent	Computation and Language	https://arxiv.org/abs/2412.00722	ALAMA	https://x.com/omarsar0/status/1863956776623747433		2412.00722	['Ziyang Huang', 'Jun Zhao', 'Kang Liu']	ct:Language Agent could be endowed with different mechanisms for autonomous task accomplishment. Current agents typically rely on fixed mechanisms or a set of mechanisms activated in a predefined order, limiting their adaptation to varied potential task solution structures. To this end, this paper proposes \textbf{A}daptive \textbf{L}anguage \textbf{A}gent \textbf{M}echanism \textbf{A}ctivation Learning with Self-Exploration (\textbf{ALAMA}), which focuses on optimizing mechanism activation adaptability without reliance on expert models. Initially, it builds a harmonized agent framework (\textbf{UniAct}) to \textbf{Uni}fy different mechanisms via \textbf{Act}ions. Then it leverages a training-efficient optimization method based on self-exploration to enable the UniAct to adaptively activate the appropriate mechanisms according to the potential characteristics of the task. Experimental results demonstrate significant improvements in downstream agent tasks, affirming the effectiveness of our approach in facilitating more dynamic and context-sensitive mechanism activation.	2025	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2412.00722', 'html': 'https://arxiv.org/html/2412.00722v1', 'tex': '/src/2412.00722', 'doi': 'https://doi.org/10.48550/arXiv.2412.00722'}	Submission history From: Ziyang Huang [ view email ] [v1] Sun, 1 Dec 2024 08:10:04 UTC (905 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.00722'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.00722'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.00722'}]
2024-12-08	Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models	Computation and Language	https://arxiv.org/abs/2411.19443	Auto-RAG	https://x.com/omarsar0/status/1863600141103501454		2411.19443	['Tian Yu', 'Shaolei Zhang', 'Yang Feng']	ct:Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG). Existing work typically employs few-shot prompting or manually constructed rules to implement iterative retrieval. This introduces additional inference overhead and overlooks the remarkable reasoning capabilities of Large Language Models (LLMs). In this paper, we introduce Auto-RAG, an autonomous iterative retrieval model centered on the LLM's powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries to acquire valuable knowledge. This process continues until sufficient external information is gathered, at which point the results are presented to the user. To this end, we develop a method for autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the latest open-source LLMs. The experimental results indicate that Auto-RAG is capable of autonomous iterative interaction with the retriever, effectively leveraging the remarkable reasoning and decision-making abilities of LLMs, which lead to outstanding performance across six benchmarks. Further analysis reveals that Auto-RAG can autonomously adjust the number of iterations based on the difficulty of the questions and the utility of the retrieved knowledge, without requiring any human intervention. Moreover, Auto-RAG expresses the iterative retrieval process in natural language, enhancing interpretability while providing users with a more intuitive experience\footnote{Code is available at \url{this https URL}.	s available atthis https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.19443', 'html': 'https://arxiv.org/html/2411.19443v1', 'tex': '/src/2411.19443', 'doi': 'https://doi.org/10.48550/arXiv.2411.19443'}	Submission history From: Tian Yu [ view email ] [v1] Fri, 29 Nov 2024 03:01:05 UTC (3,414 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.19443'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.19443'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.19443'}]
2024-12-08	RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models	Computation and Language	https://arxiv.org/abs/2412.02830	Retrieval-Augmented Reasoning for LLMs	https://x.com/omarsar0/status/1864687176929431566		2412.02830	['Hieu Tran', 'Zonghai Yao', 'Junda Wang', 'Yifan Zhang', 'Zhichao Yang', 'Hong Yu']	ct:This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for complex, knowledge-intensive tasks such as commonsense and medical reasoning. RARE incorporates two innovative actions within the Monte Carlo Tree Search (MCTS) framework: A6, which generates search queries based on the initial problem statement, performs information retrieval using those queries, and augments reasoning with the retrieved data to formulate the final answer; and A7, which leverages information retrieval specifically for generated sub-questions and re-answers these sub-questions with the relevant contextual information. Additionally, a Retrieval-Augmented Factuality Scorer is proposed to replace the original discriminator, prioritizing reasoning paths that meet high standards of factuality. Experimental results with LLaMA 3.1 show that RARE enables open-source LLMs to achieve competitive performance with top open-source models like GPT-4 and GPT-4o. This research establishes RARE as a scalable solution for improving LLMs in domains where logical coherence and factual integrity are critical.	dings of ACL 2025 (main track)	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.02830', 'html': 'https://arxiv.org/html/2412.02830v4', 'tex': '/src/2412.02830', 'doi': 'https://doi.org/10.48550/arXiv.2412.02830'}	Submission history From: Hieu Tran [ view email ] [v1] Tue, 3 Dec 2024 20:52:35 UTC (9,123 KB) [v2] Thu, 5 Dec 2024 14:51:35 UTC (9,245 KB) [v3] Mon, 9 Dec 2024 16:26:09 UTC (9,384 KB) [v4] Mon, 2 Jun 2025 17:40:21 UTC (8,308 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.02830'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.02830'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.02830'}]
2024-12-08	DataLab: A Unified Platform for LLM-Powered Business Intelligence	Databases	https://arxiv.org/abs/2412.02205	DataLab	https://x.com/omarsar0/status/1864327307177152619		2412.02205	['Luoxuan Weng', 'Yinghao Tang', 'Yingchaojie Feng', 'Zhuo Chang', 'Ruiqin Chen', 'Haozhe Feng', 'Chen Hou', 'Danqing Huang', 'Yang Li', 'Huaming Rao', 'Haonan Wang', 'Canshi Wei', 'Xiaofeng Yang', 'Yuhui Zhang', 'Yifeng Zheng', 'Xiuqi Huang', 'Minfeng Zhu', 'Yuxin Ma', 'Bin Cui', 'Peng Chen', 'Wei Chen']	ct:Business intelligence (BI) transforms large volumes of data within modern organizations into actionable insights for informed decision-making. Recently, large language model (LLM)-based agents have streamlined the BI workflow by automatically performing task planning, reasoning, and actions in executable environments based on natural language (NL) queries. However, existing approaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS. The fragmentation of tasks across different data roles and tools lead to inefficiencies and potential errors due to the iterative and collaborative nature of BI. In this paper, we introduce DataLab, a unified BI platform that integrates a one-stop LLM-based agent framework with an augmented computational notebook interface. DataLab supports various BI tasks for different data roles in data preparation, analysis, and visualization by seamlessly combining LLM assistance with user customization within a single environment. To achieve this unification, we design a domain knowledge incorporation module tailored for enterprise-specific BI tasks, an inter-agent communication mechanism to facilitate information sharing across the BI workflow, and a cell-based context management strategy to enhance context utilization efficiency in BI notebooks. Extensive experiments demonstrate that DataLab achieves state-of-the-art performance on various BI tasks across popular research benchmarks. Moreover, DataLab maintains high effectiveness and efficiency on real-world datasets from Tencent, achieving up to a 58.58% increase in accuracy and a 61.65% reduction in token cost on enterprise-specific BI tasks.	ed to ICDE 2025	['Databases (cs.DB)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2412.02205', 'html': 'https://arxiv.org/html/2412.02205v3', 'tex': '/src/2412.02205', 'doi': 'https://doi.org/10.48550/arXiv.2412.02205'}	Submission history From: Luoxuan Weng [ view email ] [v1] Tue, 3 Dec 2024 06:47:15 UTC (1,867 KB) [v2] Wed, 4 Dec 2024 16:12:08 UTC (2,479 KB) [v3] Mon, 7 Apr 2025 12:01:15 UTC (2,076 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2412.02205'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2412.02205'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2412.02205'}]
2024-12-08	Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models	Computation and Language	https://arxiv.org/abs/2411.12580	Procedural Knowledge in Pretraining Drives Reasoning in LLMs	https://x.com/omarsar0/status/1863590537346925032		2411.12580	['Laura Ruis', 'Maximilian Mozes', 'Juhan Bae', 'Siddhartha Rao Kamalakara', 'Dwarak Talupuru', 'Acyr Locatelli', 'Robert Kirk', 'Tim Rocktäschel', 'Edward Grefenstette', 'Max Bartolo']	ct:The capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded us from applying the method traditionally used to measure generalisation: train-test set separation. To overcome this, we study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, we identify what documents influence the model outputs for three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. We find that, while the models rely on mostly distinct sets of data for each factual question, a document often has a similar influence across different reasoning questions within the same task, indicating the presence of procedural knowledge. We further find that the answers to factual questions often show up in the most influential data. However, for reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When we characterise the top ranked documents for the reasoning questions qualitatively, we confirm that the influential documents often contain procedural knowledge, like demonstrating how to obtain a solution using formulae or code. Our findings indicate that the approach to reasoning the models use is unlike retrieval, and more like a generalisable strategy that synthesises procedural knowledge from documents doing a similar form of reasoning.	hed at ICLR 2025	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2411.12580', 'html': 'https://arxiv.org/html/2411.12580v2', 'tex': '/src/2411.12580', 'doi': 'https://doi.org/10.48550/arXiv.2411.12580'}	Submission history From: Laura Ruis [ view email ] [v1] Tue, 19 Nov 2024 15:47:12 UTC (22,020 KB) [v2] Thu, 6 Mar 2025 15:14:17 UTC (22,020 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.12580'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.12580'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.12580'}]
2024-12-01	O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?	Computation and Language	https://arxiv.org/abs/2411.16489	o1 Replication Journey - Part 2	https://x.com/omarsar0/status/1861411844554113276		2411.16489	['Zhen Huang', 'Haoyang Zou', 'Xuefeng Li', 'Yixiu Liu', 'Yuxiang Zheng', 'Ethan Chern', 'Shijie Xia', 'Yiwei Qin', 'Weizhe Yuan', 'Pengfei Liu']	ct:This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2411.16489', 'html': 'https://arxiv.org/html/2411.16489v1', 'tex': '/src/2411.16489', 'doi': 'https://doi.org/10.48550/arXiv.2411.16489'}	Submission history From: Zhen Huang [ view email ] [v1] Mon, 25 Nov 2024 15:31:27 UTC (6,701 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.16489'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.16489'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.16489'}]
2024-12-01	Large Language Model-Brained GUI Agents: A Survey	Artificial Intelligence	https://arxiv.org/abs/2411.18279	LLM-Brained GUI Agents	https://x.com/omarsar0/status/1862133601040752820		2411.18279	['Chaoyun Zhang', 'Shilin He', 'Jiaxu Qian', 'Bowen Li', 'Liqun Li', 'Si Qin', 'Yu Kang', 'Minghua Ma', 'Guyue Liu', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang', 'Qi Zhang']	ct:GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.	llection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository:this https URLAdditionally, a searchable webpage is available atthis https URLfor easier access and exploration	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2411.18279', 'html': 'https://arxiv.org/html/2411.18279v12', 'tex': '/src/2411.18279', 'doi': 'https://doi.org/10.48550/arXiv.2411.18279'}	Submission history From: Chaoyun Zhang [ view email ] [v1] Wed, 27 Nov 2024 12:13:39 UTC (10,473 KB) [v2] Thu, 28 Nov 2024 06:40:09 UTC (10,473 KB) [v3] Tue, 3 Dec 2024 03:16:27 UTC (10,473 KB) [v4] Tue, 17 Dec 2024 06:35:56 UTC (11,164 KB) [v5] Mon, 23 Dec 2024 12:48:43 UTC (11,376 KB) [v6] Mon, 30 Dec 2024 05:54:24 UTC (11,377 KB) [v7] Tue, 21 Jan 2025 11:36:03 UTC (11,603 KB) [v8] Sun, 2 Feb 2025 17:08:44 UTC (11,832 KB) [v9] Fri, 14 Feb 2025 05:42:16 UTC (11,836 KB) [v10] Wed, 12 Mar 2025 07:35:51 UTC (12,369 KB) [v11] Sat, 26 Apr 2025 14:23:02 UTC (12,639 KB) [v12] Tue, 6 May 2025 15:08:00 UTC (12,643 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.18279'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.18279'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.18279'}]
2024-12-01	Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS	Computation and Language	https://arxiv.org/abs/2411.18478	High-Level Automated Reasoning	https://x.com/omarsar0/status/1862131336653533584		2411.18478	['Jinyang Wu', 'Mingkuan Feng', 'Shuai Zhang', 'Feihu Che', 'Zengqi Wen', 'Chonghua Liao', 'Jianhua Tao']	"ct:In-context learning (ICL) enables large language models (LLMs) to perform downstream tasks through advanced prompting and high-quality demonstrations. However, traditional ICL paradigms encounter significant limitations in complex reasoning tasks, stemming primarily from their dependence on example quality and absence of explicit reasoning guidance. To address these challenges, we introduce HiAR-ICL, a **Hi**gh-level **A**utomated **R**easoning paradigm in **ICL** that shifts focus from specific examples to abstract reasoning patterns, thereby extending the conventional concept of ""context"" in ICL. Our approach begins by defining five atomic reasoning actions, upon which we employ Monte Carlo Tree Search to systematically construct high-level reasoning patterns. During inference, HiAR-ICL dynamically selects appropriate reasoning patterns based on problem attributes, providing explicit guidance for the model's reasoning process. Experiments demonstrate HiAR-ICL's effectiveness and efficiency: utilizing only 200 prior samples with Qwen2.5-7B-Instruct, our method achieves 80.6% accuracy on MATH and 62.5% on AMC, exceeding GPT-4o's 77.2% and 57.5%. Our approach enhances performance across models of varying sizes while generalizing effectively across domains. Further analysis reveals that HiAR-ICL can also serve as a plug-and-play inference method compatible with post-training techniques like GRPO. Code and data are available atthis https URL."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.18478', 'html': 'https://arxiv.org/html/2411.18478v2', 'tex': '/src/2411.18478', 'doi': 'https://doi.org/10.48550/arXiv.2411.18478'}	Submission history From: Jinyang Wu [ view email ] [v1] Wed, 27 Nov 2024 16:19:00 UTC (596 KB) [v2] Mon, 2 Jun 2025 14:26:19 UTC (1,462 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.18478'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.18478'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.18478'}]
2024-12-01	Star Attention: Efficient LLM Inference over Long Sequences	Computation and Language	https://arxiv.org/abs/2411.17116	Star Attention: Efficient LLM Inference over Long Sequences	https://x.com/omarsar0/status/1861854543694406109		2411.17116	['Shantanu Acharya', 'Fei Jia', 'Boris Ginsburg']	ct:Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.	ed at ICML 2025	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2411.17116', 'html': 'https://arxiv.org/html/2411.17116v3', 'tex': '/src/2411.17116', 'doi': 'https://doi.org/10.48550/arXiv.2411.17116'}	Submission history From: Shantanu Acharya [ view email ] [v1] Tue, 26 Nov 2024 05:10:04 UTC (264 KB) [v2] Sun, 20 Apr 2025 21:50:03 UTC (273 KB) [v3] Fri, 30 May 2025 00:36:37 UTC (285 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.17116'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.17116'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.17116'}]
2024-12-01	A Survey on LLM-as-a-Judge	Computation and Language	https://arxiv.org/abs/2411.15594	Survey on LLM-as-a-Judge	https://x.com/omarsar0/status/1861411159913472229		2411.15594	['Jiawei Gu', 'Xuhui Jiang', 'Zhichao Shi', 'Hexiang Tan', 'Xuehao Zhai', 'Chengjin Xu', 'Wei Li', 'Yinghan Shen', 'Shengjie Ma', 'Honghao Liu', 'Saizhuo Wang', 'Kun Zhang', 'Yuanzhuo Wang', 'Wen Gao', 'Lionel Ni', 'Jian Guo']	"ct:Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of ""LLM-as-a-Judge,"" where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field."	t Page:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2411.15594', 'html': 'https://arxiv.org/html/2411.15594v5', 'tex': '/src/2411.15594', 'doi': 'https://doi.org/10.48550/arXiv.2411.15594'}	Submission history From: Xuhui Jiang [ view email ] [v1] Sat, 23 Nov 2024 16:03:35 UTC (1,888 KB) [v2] Mon, 16 Dec 2024 15:00:53 UTC (2,820 KB) [v3] Thu, 9 Jan 2025 03:08:17 UTC (1,477 KB) [v4] Sat, 1 Feb 2025 08:55:51 UTC (10,153 KB) [v5] Sun, 9 Mar 2025 05:21:22 UTC (13,276 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.15594'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.15594'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.15594'}]
2024-12-01	Tulu 3: Pushing Frontiers in Open Language Model Post-Training	Computation and Language	https://arxiv.org/abs/2411.15124	TÜLU 3	https://x.com/omarsar0/status/1861085195950256335		2411.15124	['Nathan Lambert', 'Jacob Morrison', 'Valentina Pyatkin', 'Shengyi Huang', 'Hamish Ivison', 'Faeze Brahman', 'Lester James V. Miranda', 'Alisa Liu', 'Nouha Dziri', 'Shane Lyu', 'Yuling Gu', 'Saumya Malik', 'Victoria Graf', 'Jena D. Hwang', 'Jiangjiang Yang', 'Ronan Le Bras', 'Oyvind Tafjord', 'Chris Wilhelm', 'Luca Soldaini', 'Noah A. Smith', 'Yizhong Wang', 'Pradeep Dasigi', 'Hannaneh Hajishirzi']	ct:Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.In addition to the Tulu 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the Tulu 3 approach to more domains.	Tulu 3 405B results and additional analyses	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.15124', 'html': 'https://arxiv.org/html/2411.15124v5', 'tex': '/src/2411.15124', 'doi': 'https://doi.org/10.48550/arXiv.2411.15124'}	Submission history From: Nathan Lambert [ view email ] [v1] Fri, 22 Nov 2024 18:44:04 UTC (3,039 KB) [v2] Fri, 6 Dec 2024 01:01:20 UTC (3,039 KB) [v3] Wed, 29 Jan 2025 18:46:59 UTC (6,596 KB) [v4] Thu, 13 Feb 2025 21:18:55 UTC (6,532 KB) [v5] Mon, 14 Apr 2025 22:39:09 UTC (6,532 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.15124'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.15124'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.15124'}]
2024-12-01	Generative Agent Simulations of 1,000 People	Artificial Intelligence	https://arxiv.org/abs/2411.10109	Generative Agent Simulations of 1,000 People	https://x.com/percyliang/status/1861136757435015580		2411.10109	['Joon Sung Park', 'Carolyn Q. Zou', 'Aaron Shaw', 'Benjamin Mako Hill', 'Carrie Cai', 'Meredith Ringel Morris', 'Robb Willer', 'Percy Liang', 'Michael S. Bernstein']	ct:The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. We present a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. Our architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.		['Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2411.10109', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2411.10109'}	Submission history From: Joon Sung Park [ view email ] [v1] Fri, 15 Nov 2024 11:14:34 UTC (2,928 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.10109'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.10109'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.10109'}]
2024-12-01	The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games	Computation and Language	https://arxiv.org/abs/2411.15129	Measuring Bullshit in Language Games Played by ChatGPT	https://x.com/omarsar0/status/1861066315789942978		2411.15129	['Alessandro Trevisan', 'Harry Giddens', 'Sarah Dillon', 'Alan F. Blackwell']	"ct:What can we learn about language from studying how it is used by ChatGPT and other large language model (LLM)-based chatbots? In this paper, we analyse the distinctive character of language generated by ChatGPT, in relation to questions raised by natural language processing pioneer, and student of Wittgenstein, Margaret Masterman. Following frequent complaints that LLM-based chatbots produce ""slop,"" or even ""bullshit,"" in the sense of Frankfurt's popular monograph On Bullshit, we conduct an empirical study to contrast the language of 1,000 scientific publications with typical text generated by ChatGPT. We then explore whether the same language features can be detected in two well-known contexts of social dysfunction: George Orwell's critique of political speech, and David Graeber's characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a statistical model of sloppy bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2411.15129', 'html': 'https://arxiv.org/html/2411.15129v2', 'tex': '/src/2411.15129', 'doi': 'https://doi.org/10.48550/arXiv.2411.15129'}	Submission history From: Alan Blackwell [ view email ] [v1] Fri, 22 Nov 2024 18:55:21 UTC (486 KB) [v2] Tue, 10 Jun 2025 07:11:59 UTC (192 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.15129'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.15129'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.15129'}]
2024-11-24	The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use	Artificial Intelligence	https://arxiv.org/abs/2411.10323	The Dawn of GUI Agent	https://x.com/omarsar0/status/1858526493661446553		2411.10323	['Siyuan Hu', 'Mingyu Ouyang', 'Difei Gao', 'Mike Zheng Shou']	ct:The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic, which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community. All the test cases in the paper can be tried through the project:this https URL.	es, 21 figures, preprint	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2411.10323', 'html': 'https://arxiv.org/html/2411.10323v1', 'tex': '/src/2411.10323', 'doi': 'https://doi.org/10.48550/arXiv.2411.10323'}	Submission history From: Siyuan Hu [ view email ] [v1] Fri, 15 Nov 2024 16:23:52 UTC (20,309 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.10323'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.10323'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.10323'}]
2024-11-24	Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations	Statistics > Applications	https://arxiv.org/abs/2411.00640	A Statistical Approach to LLM Evaluation	https://x.com/AnthropicAI/status/1858976458330505639		2411.00640	['Evan Miller']	ct:Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness.	es	['Applications (stat.AP)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.00640', 'html': 'https://arxiv.org/html/2411.00640v1', 'tex': '/src/2411.00640', 'doi': 'https://doi.org/10.48550/arXiv.2411.00640'}	Submission history From: Evan Miller [ view email ] [v1] Fri, 1 Nov 2024 14:57:16 UTC (54 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.00640'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.00640'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.00640'}]
2024-11-24	Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions	Computation and Language	https://arxiv.org/abs/2411.14405	Towards Open Reasoning Models for Open-Ended Solutions	https://x.com/omarsar0/status/1860003607606706197		2411.14405	['Yu Zhao', 'Huifeng Yin', 'Bo Zeng', 'Hao Wang', 'Tianqi Shi', 'Chenyang Lyu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang']	ct:Currently OpenAI o1 sparks a surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding -- which are well-suited for reinforcement learning (RL) -- but also places greater emphasis on open-ended resolutions. We aim to address the question: ''Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?'' Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies -- optimized for complex real-world problem-solving tasks.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.14405', 'html': 'https://arxiv.org/html/2411.14405v2', 'tex': '/src/2411.14405', 'doi': 'https://doi.org/10.48550/arXiv.2411.14405'}	Submission history From: Huifeng Yin [ view email ] [v1] Thu, 21 Nov 2024 18:37:33 UTC (5,397 KB) [v2] Mon, 25 Nov 2024 17:57:55 UTC (5,473 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.14405'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.14405'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.14405'}]
2024-11-24	An Empirical Study on LLM-based Agents for Automated Bug Fixing	Software Engineering	https://arxiv.org/abs/2411.10213	LLM-based Agents for Automated Bug Fixing	https://x.com/omarsar0/status/1859964808789135668		2411.10213	['Xiangxin Meng', 'Zexiong Ma', 'Pengfei Gao', 'Chao Peng']	ct:Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent and non-agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine seven proprietary and open-source systems on the SWE-bench Lite benchmark for automated bug fixing. We first assess each system's overall performance, noting instances solvable by all or none of these sytems, and explore why some instances are uniquely solved by specific system types. We also compare fault localization accuracy at file and line levels and evaluate bug reproduction capabilities, identifying instances solvable only through dynamic reproduction. Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.		['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2411.10213', 'html': 'https://arxiv.org/html/2411.10213v1', 'tex': '/src/2411.10213', 'doi': 'https://doi.org/10.48550/arXiv.2411.10213'}	Submission history From: Chao Peng [ view email ] [v1] Fri, 15 Nov 2024 14:19:15 UTC (2,533 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.10213'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.10213'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.10213'}]
2024-11-24	Cut Your Losses in Large-Vocabulary Language Models	Machine Learning	https://arxiv.org/abs/2411.09009	Cut Your Losses in Large-Vocabulary Language Models			2411.09009	['Erik Wijmans', 'Brody Huval', 'Alexander Hertzberg', 'Vladlen Koltun', 'Philipp Krähenbühl']	ct:As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e., below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.	ear in ICLR 2025 (Oral). Code is available atthis https URL	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.09009', 'html': 'https://arxiv.org/html/2411.09009v2', 'tex': '/src/2411.09009', 'doi': 'https://doi.org/10.48550/arXiv.2411.09009'}	Submission history From: Erik Wijmans [ view email ] [v1] Wed, 13 Nov 2024 20:30:15 UTC (9,837 KB) [v2] Mon, 10 Mar 2025 23:08:54 UTC (11,075 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.09009'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.09009'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.09009'}]
2024-11-24	AIGS: Generating Science from AI-Powered Automated Falsification	Machine Learning	https://arxiv.org/abs/2411.11910v1	BABY-AIGS	https://x.com/omarsar0/status/1859656533489188928		2411.11910v1	['Zijun Liu', 'Kaiming Liu', 'Yiqi Zhu', 'Xuanyu Lei', 'Zonghan Yang', 'Zhenhe Zhang', 'Peng Li', 'Yang Liu']	ct:Rapid development of artificial intelligence has drastically accelerated the development of scientific discovery. Trained with large-scale observation data, deep neural networks extract the underlying patterns in an end-to-end manner and assist human researchers with highly-precised predictions in unseen scenarios. The recent rise of Large Language Models (LLMs) and the empowered autonomous agents enable scientists to gain help through interaction in different stages of their research, including but not limited to literature review, research ideation, idea implementation, and academic writing. However, AI researchers instantiated by foundation model empowered agents with full-process autonomy are still in their infancy. In this paper, we study $\textbf{AI-Generated Science}$ (AIGS), where agents independently and autonomously complete the entire research process and discover scientific laws. By revisiting the definition of scientific research, we argue that $\textit{falsification}$ is the essence of both human research process and the design of an AIGS system. Through the lens of falsification, prior systems attempting towards AI-Generated Science either lack the part in their design, or rely heavily on existing verification engines that narrow the use in specialized domains. In this work, we propose Baby-AIGS as a baby-step demonstration of a full-process AIGS system, which is a multi-agent system with agents in roles representing key research process. By introducing FalsificationAgent, which identify and then verify possible scientific discoveries, we empower the system with explicit falsification. Experiments on three tasks preliminarily show that Baby-AIGS could produce meaningful scientific discoveries, though not on par with experienced human researchers. Finally, we discuss on the limitations of current Baby-AIGS, actionable insights, and related ethical issues in detail.	int. 35 pages. Official website:this https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.11910v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2411.11910'}	Submission history From: Zijun Liu [ view email ] [v1] Sun, 17 Nov 2024 13:40:35 UTC (14,928 KB) [v2] Sun, 24 Nov 2024 12:59:44 UTC (14,929 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.11910'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.11910'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.11910'}]
2024-11-24	Does Prompt Formatting Have Any Impact on LLM Performance?	Computation and Language	https://arxiv.org/abs/2411.10541	Does Prompt Formatting Impact LLM Performance			2411.10541	['Jia He', 'Mukund Rungta', 'David Koleczek', 'Arshdeep Sekhon', 'Franklin X Wang', 'Sadid Hasan']	ct:In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI's GPT models. Experiments show that GPT-3.5-turbo's performance varies by up to 40\% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.	ted to NAACL 2025	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2411.10541', 'html': 'https://arxiv.org/html/2411.10541v1', 'tex': '/src/2411.10541', 'doi': 'https://doi.org/10.48550/arXiv.2411.10541'}	Submission history From: Mukund Rungta [ view email ] [v1] Fri, 15 Nov 2024 19:26:38 UTC (1,504 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.10541'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.10541'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.10541'}]
2024-11-24	FinRobot: AI Agent for Equity Research and Valuation with Large Language Models	Quantitative Finance > Computational Finance	https://arxiv.org/abs/2411.08804	FinRobot			2411.08804	['Tianyu Zhou', 'Pinqiao Wang', 'Yilin Wu', 'Hongyang Yang']	ct:As financial markets grow increasingly complex, there is a rising need for automated tools that can effectively assist human analysts in equity research, particularly within sell-side research. While Generative AI (GenAI) has attracted significant attention in this field, existing AI solutions often fall short due to their narrow focus on technical factors and limited capacity for discretionary judgment. These limitations hinder their ability to adapt to new data in real-time and accurately assess risks, which diminishes their practical value for investors.This paper presents FinRobot, the first AI agent framework specifically designed for equity research. FinRobot employs a multi-agent Chain of Thought (CoT) system, integrating both quantitative and qualitative analyses to emulate the comprehensive reasoning of a human analyst. The system is structured around three specialized agents: the Data-CoT Agent, which aggregates diverse data sources for robust financial integration; the Concept-CoT Agent, which mimics an analysts reasoning to generate actionable insights; and the Thesis-CoT Agent, which synthesizes these insights into a coherent investment thesis and report. FinRobot provides thorough company analysis supported by precise numerical data, industry-appropriate valuation metrics, and realistic risk assessments. Its dynamically updatable data pipeline ensures that research remains timely and relevant, adapting seamlessly to new financial information. Unlike existing automated research tools, such as CapitalCube and Wright Reports, FinRobot delivers insights comparable to those produced by major brokerage firms and fundamental research vendors. We open-source FinRobot at \url{https://github. com/AI4Finance-Foundation/FinRobot}.	t Workshop on LLMs and Generative AI for Finance, ICAIF 2024	['Computational Finance (q-fin.CP)', 'Machine Learning (cs.LG)', 'Statistical Finance (q-fin.ST)', 'Trading and Market Microstructure (q-fin.TR)']	{'pdf': '/pdf/2411.08804', 'html': 'https://arxiv.org/html/2411.08804v1', 'tex': '/src/2411.08804', 'doi': 'https://doi.org/10.48550/arXiv.2411.08804'}	Submission history From: Hongyang Yang [ view email ] [v1] Wed, 13 Nov 2024 17:38:07 UTC (1,650 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.08804'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.08804'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.08804'}]
2024-11-24	Bi-Mamba: Towards Accurate 1-Bit State Space Models	Computation and Language	https://arxiv.org/abs/2411.11843	Bi-Mamba			2411.11843	['Shengkun Tang', 'Liqun Ma', 'Haonan Li', 'Mingjie Sun', 'Zhiqiang Shen']	ct:The typical selective state-space model (SSM) of Mamba addresses several limitations of Transformers, such as quadratic computational complexity with sequence length and significant inference-time memory requirements due to the key-value cache. However, the growing size of Mamba models continues to pose training and deployment challenges and raises environmental concerns due to considerable energy consumption. In this work, we introduce Bi-Mamba, a scalable and powerful 1-bit Mamba architecture designed for more efficient large language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba models are trained from scratch on data volume as regular LLM pertaining using an autoregressive distillation loss. Extensive experimental results on language modeling demonstrate that Bi-Mamba achieves performance comparable to its full-precision counterparts (e.g., FP16 or BF16) and much better accuracy than post-training-binarization (PTB) Mamba baselines, while significantly reducing memory footprint and energy consumption compared to the original Mamba model. Our study pioneers a new linear computational complexity LLM framework under low-bit representation and facilitates the future design of specialized hardware tailored for efficient 1-bit Mamba-based LLMs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2411.11843', 'html': 'https://arxiv.org/html/2411.11843v1', 'tex': '/src/2411.11843', 'doi': 'https://doi.org/10.48550/arXiv.2411.11843'}	Submission history From: Zhiqiang Shen [ view email ] [v1] Mon, 18 Nov 2024 18:59:15 UTC (2,811 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.11843'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.11843'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.11843'}]
2024-11-17	Scaling Laws for Precision	Machine Learning	https://arxiv.org/abs/2411.04330	Scaling Laws for Precision	https://x.com/tanishqkumar07/status/1856045600355352753		2411.04330	['Tanishq Kumar', 'Zachary Ankner', 'Benjamin F. Spector', 'Blake Bordelon', 'Niklas Muennighoff', 'Mansheej Paul', 'Cengiz Pehlevan', 'Christopher Ré', 'Aditi Raghunathan']	"ct:Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise ""precision-aware"" scaling laws for both training and inference. We propose that training in lower precision reduces the model's ""effective parameter count,"" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens."		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.04330', 'html': 'https://arxiv.org/html/2411.04330v2', 'tex': '/src/2411.04330', 'doi': 'https://doi.org/10.48550/arXiv.2411.04330'}	Submission history From: Tanishq Kumar [ view email ] [v1] Thu, 7 Nov 2024 00:10:10 UTC (1,769 KB) [v2] Sat, 30 Nov 2024 02:42:31 UTC (1,837 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.04330'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.04330'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.04330'}]
2024-11-17	OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models	Computation and Language	https://arxiv.org/abs/2411.04905	OpenCoder	https://x.com/omarsar0/status/1857515355595526450		2411.04905	['Siming Huang', 'Tianhao Cheng', 'J.K. Liu', 'Jiaran Hao', 'Liuyihan Song', 'Yang Xu', 'J. Yang', 'Jiaheng Liu', 'Chenchen Zhang', 'Linzheng Chai', 'Ruifeng Yuan', 'Zhaoxiang Zhang', 'Jie Fu', 'Qian Liu', 'Ge Zhang', 'Zili Wang', 'Yuan Qi', 'Yinghui Xu', 'Wei Chu']	"ct:Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an ""open cookbook"" for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI."		['Computation and Language (cs.CL)', 'Programming Languages (cs.PL)']	{'pdf': '/pdf/2411.04905', 'html': 'https://arxiv.org/html/2411.04905v3', 'tex': '/src/2411.04905', 'doi': 'https://doi.org/10.48550/arXiv.2411.04905'}	Submission history From: Linzheng Chai [ view email ] [v1] Thu, 7 Nov 2024 17:47:25 UTC (25,625 KB) [v2] Sat, 9 Nov 2024 17:33:51 UTC (25,832 KB) [v3] Thu, 20 Mar 2025 03:28:56 UTC (26,138 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.04905'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.04905'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.04905'}]
2024-11-17	A Taxonomy of AgentOps for Enabling Observability of Foundation Model based Agents	Artificial Intelligence	https://arxiv.org/abs/2411.05285v1	A Taxonomy of AgentOps for Enabling Observability of Foundation Model-based Agents	https://x.com/omarsar0/status/1857400667318702118		2411.05285v1	['Liming Dong', 'Qinghua Lu', 'Liming Zhu']	ct:The ever-improving quality of LLMs has fueled the growth of a diverse range of downstream tasks, leading to an increased demand for AI automation and a burgeoning interest in developing foundation model (FM)-based autonomous agents. As AI agent systems tackle more complex tasks and evolve, they involve a wider range of stakeholders, including agent users, agentic system developers and deployers, and AI model developers. These systems also integrate multiple components such as AI agent workflows, RAG pipelines, prompt management, agent capabilities, and observability features. In this case, obtaining reliable outputs and answers from these agents remains challenging, necessitating a dependable execution process and end-to-end observability solutions. To build reliable AI agents and LLM applications, it is essential to shift towards designing AgentOps platforms that ensure observability and traceability across the entire development-to-production life-cycle. To this end, we conducted a rapid review and identified relevant AgentOps tools from the agentic ecosystem. Based on this review, we provide an overview of the essential features of AgentOps and propose a comprehensive overview of observability data/traceable artifacts across the agent production life-cycle. Our findings provide a systematic overview of the current AgentOps landscape, emphasizing the critical role of observability/traceability in enhancing the reliability of autonomous agent systems.	es, 9 figures	['Artificial Intelligence (cs.AI)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2411.05285v1', 'html': 'https://arxiv.org/html/2411.05285v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2411.05285'}	Submission history From: Liming Dong [ view email ] [v1] Fri, 8 Nov 2024 02:31:03 UTC (538 KB) [v2] Sat, 30 Nov 2024 02:55:48 UTC (511 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.05285'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.05285'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.05285'}]
2024-11-17	Toward Optimal Search and Retrieval for RAG	Computation and Language	https://arxiv.org/abs/2411.07396	Toward Optimal Search and Retrieval for RAG	https://x.com/omarsar0/status/1856709865802252710		2411.07396	['Alexandria Leto', 'Cecilia Aguerrebere', 'Ishwar Bhati', 'Ted Willke', 'Mariano Tepper', 'Vy Ai Vo']	ct:Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.	ed to NeurIPS 2024 Workshop ATTRIB	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.07396', 'html': 'https://arxiv.org/html/2411.07396v1', 'tex': '/src/2411.07396', 'doi': 'https://doi.org/10.48550/arXiv.2411.07396'}	Submission history From: Alexandria Leto [ view email ] [v1] Mon, 11 Nov 2024 22:06:51 UTC (7,753 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.07396'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.07396'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.07396'}]
2024-11-17	Rapid Response: Mitigating LLM Jailbreaks with a Few Examples	Computation and Language	https://arxiv.org/abs/2411.07494	Mitigating LLM Jailbreaks with Few Examples	https://x.com/AnthropicAI/status/1856752093945540673		2411.07494	['Alwin Peng', 'Julian Michael', 'Henry Sleight', 'Ethan Perez', 'Mrinank Sharma']	ct:As large language models (LLMs) grow more powerful, ensuring their safety against misuse becomes crucial. While researchers have focused on developing robust defenses, no method has yet achieved complete invulnerability to attacks. We propose an alternative approach: instead of seeking perfect adversarial robustness, we develop rapid response techniques to look to block whole classes of jailbreaks after observing only a handful of attacks. To study this setting, we develop RapidResponseBench, a benchmark that measures a defense's robustness against various jailbreak strategies after adapting to a few observed examples. We evaluate five rapid response methods, all of which use jailbreak proliferation, where we automatically generate additional jailbreaks similar to the examples observed. Our strongest method, which fine-tunes an input classifier to block proliferated jailbreaks, reduces attack success rate by a factor greater than 240 on an in-distribution set of jailbreaks and a factor greater than 15 on an out-of-distribution set, having observed just one example of each jailbreaking strategy. Moreover, further studies suggest that the quality of proliferation model and number of proliferated examples play an key role in the effectiveness of this defense. Overall, our results highlight the potential of responding rapidly to novel jailbreaks to limit LLM misuse.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.07494', 'html': 'https://arxiv.org/html/2411.07494v1', 'tex': '/src/2411.07494', 'doi': 'https://doi.org/10.48550/arXiv.2411.07494'}	Submission history From: Alwin Peng [ view email ] [v1] Tue, 12 Nov 2024 02:44:49 UTC (2,049 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.07494'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.07494'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.07494'}]
2024-11-17	Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models	Computation and Language	https://arxiv.org/abs/2411.04996	Mixture of Transformers			2411.04996	['Weixin Liang', 'Lili Yu', 'Liang Luo', 'Srinivasan Iyer', 'Ning Dong', 'Chunting Zhou', 'Gargi Ghosh', 'Mike Lewis', 'Wen-tau Yih', 'Luke Zettlemoyer', 'Xi Victoria Lin']	ct:The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. We evaluate MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8\% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2\% of the wall-clock time and text quality in 75.6\% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs).	ed to TMLR 2025; 48 pages	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.04996', 'html': None, 'tex': '/src/2411.04996', 'doi': 'https://doi.org/10.48550/arXiv.2411.04996'}	Submission history From: Xi Victoria Lin [ view email ] [v1] Thu, 7 Nov 2024 18:59:06 UTC (16,324 KB) [v2] Thu, 8 May 2025 01:53:55 UTC (28,988 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.04996'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.04996'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.04996'}]
2024-11-17	HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems	Information Retrieval	https://arxiv.org/abs/2411.02959v1	HtmlRAG	https://x.com/omarsar0/status/1857870511302390013		2411.02959v1	['Jiejun Tan', 'Zhicheng Dou', 'Wen Wang', 'Mang Wang', 'Weipeng Chen', 'Ji-Rong Wen']	ct:Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems.		['Information Retrieval (cs.IR)']	{'pdf': '/pdf/2411.02959v1', 'html': 'https://arxiv.org/html/2411.02959v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2411.02959'}	Submission history From: Jiejun Tan [ view email ] [v1] Tue, 5 Nov 2024 09:58:36 UTC (991 KB) [v2] Fri, 7 Feb 2025 08:32:24 UTC (994 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.02959'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.02959'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.02959'}]
2024-11-10	Project Sid: Many-agent simulations toward AI civilization	Artificial Intelligence	https://arxiv.org/abs/2411.00114	Many-agent Simulations toward AI Civilization	https://x.com/omarsar0/status/1853290196286021940		2411.00114	['Altera.AL', 'Andrew Ahn', 'Nic Becker', 'Stephanie Carroll', 'Nico Christie', 'Manuel Cortes', 'Arda Demirci', 'Melissa Du', 'Frankie Li', 'Shuying Luo', 'Peter Y Wang', 'Mathew Willows', 'Feitong Yang', 'Guangyu Robert Yang']	ct:AI agents have been evaluated in isolation or within small groups, where interactions remain limited in scope and complexity. Large-scale simulations involving many autonomous agents -- reflecting the full spectrum of civilizational processes -- have yet to be explored. Here, we demonstrate how 10 - 1000+ AI agents behave and progress within agent societies. We first introduce the PIANO (Parallel Information Aggregation via Neural Orchestration) architecture, which enables agents to interact with humans and other agents in real-time while maintaining coherence across multiple output streams. We then evaluate agent performance in agent simulations using civilizational benchmarks inspired by human history. These simulations, set within a Minecraft environment, reveal that agents are capable of meaningful progress -- autonomously developing specialized roles, adhering to and changing collective rules, and engaging in cultural and religious transmission. These preliminary results show that agents can achieve significant milestones towards AI civilizations, opening new avenues for large simulations, agentic organizational intelligence, and integrating AI into human civilizations.	es, 14 figures	['Artificial Intelligence (cs.AI)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2411.00114', 'html': 'https://arxiv.org/html/2411.00114v1', 'tex': '/src/2411.00114', 'doi': 'https://doi.org/10.48550/arXiv.2411.00114'}	Submission history From: Andrew Ahn [ view email ] [v1] Thu, 31 Oct 2024 18:11:22 UTC (20,337 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.00114'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.00114'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.00114'}]
2024-11-10	A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness	Computation and Language	https://arxiv.org/abs/2411.03350	A Comprehensive Survey of Small Language Models	https://x.com/omarsar0/status/1854532748154695717		2411.03350	['Fali Wang', 'Zhiwei Zhang', 'Xianren Zhang', 'Zongyu Wu', 'Tzuhao Mo', 'Qiuhao Lu', 'Wanjing Wang', 'Rui Li', 'Junjie Xu', 'Xianfeng Tang', 'Qi He', 'Yao Ma', 'Ming Huang', 'Suhang Wang']	ct:Large language models (LLMs) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs' challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively.	es, 32 figures, 14 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2411.03350', 'html': 'https://arxiv.org/html/2411.03350v2', 'tex': '/src/2411.03350', 'doi': 'https://doi.org/10.48550/arXiv.2411.03350'}	Submission history From: Fali Wang [ view email ] [v1] Mon, 4 Nov 2024 04:43:01 UTC (13,390 KB) [v2] Sat, 28 Dec 2024 09:18:36 UTC (14,518 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.03350'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.03350'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.03350'}]
2024-11-10	Mixtures of In-Context Learners	Computation and Language	https://arxiv.org/abs/2411.02830	Mixtures of In-Context Learners	https://x.com/omarsar0/status/1854252169492562171		2411.02830	['Giwon Hong', 'Emile van Krieken', 'Edoardo Ponti', 'Nikolay Malkin', 'Pasquale Minervini']	ct:In-context learning (ICL) adapts LLMs by providing demonstrations without fine-tuning the model parameters; however, it does not differentiate between demonstrations and quadratically increases the complexity of Transformer LLMs, exhausting the memory. As a solution, we propose Mixtures of In-Context Learners (MoICL), a novel approach to treat subsets of demonstrations as experts and learn a weighting function to merge their output distributions based on a training set. In our experiments, we show performance improvements on 5 out of 7 classification datasets compared to a set of strong baselines (up to +13\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of ICL by reducing the inference time needed to achieve the same performance with fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to +11\%), imbalanced (up to +49\%), or noisy demonstrations (up to +38\%) or can filter these out from datasets. Overall, MoICL is a more expressive approach to learning from demonstrations without exhausting the context window or memory.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2411.02830', 'html': 'https://arxiv.org/html/2411.02830v1', 'tex': '/src/2411.02830', 'doi': 'https://doi.org/10.48550/arXiv.2411.02830'}	Submission history From: Giwon Hong [ view email ] [v1] Tue, 5 Nov 2024 06:02:41 UTC (213 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.02830'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.02830'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.02830'}]
2024-11-10	Attacking Vision-Language Computer Agents via Pop-ups	Computation and Language	https://arxiv.org/abs/2411.02391	Attacking Vision-Language Agents via Pop-ups	https://x.com/omarsar0/status/1853810252308774955		2411.02391	['Yanzhe Zhang', 'Tao Yu', 'Diyi Yang']	ct:Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear. In this work, we demonstrate that VLM agents can be easily attacked by a set of carefully designed adversarial pop-ups, which human users would typically recognize and ignore. This distraction leads agents to click these pop-ups instead of performing their tasks as usual. Integrating these pop-ups into existing agent testing environments like OSWorld and VisualWebArena leads to an attack success rate (the frequency of the agent clicking the pop-ups) of 86% on average and decreases the task success rate by 47%. Basic defense techniques, such as asking the agent to ignore pop-ups or including an advertisement notice, are ineffective against the attack.	25	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.02391', 'html': 'https://arxiv.org/html/2411.02391v2', 'tex': '/src/2411.02391', 'doi': 'https://doi.org/10.48550/arXiv.2411.02391'}	Submission history From: Yanzhe Zhang [ view email ] [v1] Mon, 4 Nov 2024 18:56:42 UTC (6,319 KB) [v2] Sat, 24 May 2025 16:15:46 UTC (6,480 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.02391'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.02391'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.02391'}]
2024-11-10	Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models	Computation and Language	https://arxiv.org/abs/2411.00492	Multi-expert Prompting with LLMs	https://x.com/omarsar0/status/1853286452227899851		2411.00492	['Do Xuan Long', 'Duong Ngoc Yen', 'Anh Tuan Luu', 'Kenji Kawaguchi', 'Min-Yen Kan', 'Nancy F. Chen']	ct:We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.	2024 Main Conference	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.00492', 'html': None, 'tex': '/src/2411.00492', 'doi': 'https://doi.org/10.48550/arXiv.2411.00492'}	Submission history From: Long Do Xuan [ view email ] [v1] Fri, 1 Nov 2024 10:06:52 UTC (11,483 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.00492'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.00492'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.00492'}]
2024-11-10	Number Cookbook: Number Understanding of Language Models and How to Improve It	Computation and Language	https://arxiv.org/abs/2411.03766	Number Understanding of LLMs	https://x.com/omarsar0/status/1854528742095458337		2411.03766	['Haotong Yang', 'Yi Hu', 'Shijia Kang', 'Zhouchen Lin', 'Muhan Zhang']	ct:Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as 9.11 > 9.9). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid little attention to it or only discussed several restricted tasks (like integer addition). In this paper, we comprehensively investigate the numerical understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a benchmark covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total. These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear. Through the benchmark, we find that current LLMs fail frequently in many of the tasks. To study the problem, we train small models with existing and potential techniques for enhancing NUPA (such as tokenizers, PEs, and number formats), comprehensively evaluating their effectiveness using our testbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and find that 1) naive finetuning can improve NUPA a lot on many but not all tasks, and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for finetuning pretrained models. We further explore the impact of chain-of-thought techniques on NUPA. Our work provides a more detailed and comprehensive understanding of NUPA in LLMs. Our benchmark and code are released atthis https URL.	025 poster	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2411.03766', 'html': 'https://arxiv.org/html/2411.03766v3', 'tex': '/src/2411.03766', 'doi': 'https://doi.org/10.48550/arXiv.2411.03766'}	Submission history From: Haotong Yang [ view email ] [v1] Wed, 6 Nov 2024 08:59:44 UTC (1,131 KB) [v2] Wed, 4 Dec 2024 16:39:04 UTC (1,360 KB) [v3] Wed, 5 Mar 2025 09:52:30 UTC (1,354 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.03766'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.03766'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.03766'}]
2024-11-10	WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning	Computation and Language	https://arxiv.org/abs/2411.02337	WebRL	https://x.com/omarsar0/status/1853821990177485311		2411.02337	['Zehan Qi', 'Xiao Liu', 'Iat Long Iong', 'Hanyu Lai', 'Xueqiao Sun', 'Wenyi Zhao', 'Yu Yang', 'Xinyue Yang', 'Jiadai Sun', 'Shuntian Yao', 'Tianjie Zhang', 'Wei Xu', 'Jie Tang', 'Yuxiao Dong']	ct:Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.	hed as a conference paper at ICLR 2025	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.02337', 'html': 'https://arxiv.org/html/2411.02337v3', 'tex': '/src/2411.02337', 'doi': 'https://doi.org/10.48550/arXiv.2411.02337'}	Submission history From: Zehan Qi [ view email ] [v1] Mon, 4 Nov 2024 17:59:58 UTC (8,957 KB) [v2] Tue, 3 Dec 2024 16:37:23 UTC (9,136 KB) [v3] Mon, 27 Jan 2025 11:56:15 UTC (9,136 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.02337'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.02337'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.02337'}]
2024-11-10	Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation	Machine Learning	https://arxiv.org/abs/2411.00412	Adapting while Learning	https://x.com/omarsar0/status/1853281778594979877		2411.00412	['Bohan Lyu', 'Yadi Cao', 'Duncan Watson-Parris', 'Leon Bergen', 'Taylor Berg-Kirkpatrick', 'Rose Yu']	ct:Large Language Models (LLMs) demonstrate promising capabilities in solving scientific problems but often suffer from the issue of hallucination. While integrating LLMs with tools can mitigate this issue, models fine-tuned on tool usage become overreliant on them and incur unnecessary costs. Inspired by how human experts assess problem complexity before selecting solutions, we propose a novel two-component fine-tuning method, Adapting While Learning (AWL). In the first component, World Knowledge Learning (WKL), LLMs internalize scientific knowledge by learning from tool-generated solutions. In the second component, Tool Usage Adaptation (TUA), we categorize problems as easy or hard based on the model's accuracy, and train it to maintain direct reasoning for easy problems while switching to tools for hard ones. We validate our method on six scientific benchmark datasets across climate science, epidemiology, physics, and other domains. Compared to the original instruct model (8B), models post-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better tool usage accuracy, even surpassing state-of-the-art models including GPT-4o and Claude-3.5 on four custom-created datasets. Our code is open-source atthis https URL.	es, 16 figures	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.00412', 'html': None, 'tex': '/src/2411.00412', 'doi': 'https://doi.org/10.48550/arXiv.2411.00412'}	Submission history From: Bohan Lyu [ view email ] [v1] Fri, 1 Nov 2024 07:18:31 UTC (769 KB) [v2] Tue, 4 Feb 2025 06:11:55 UTC (826 KB) [v3] Thu, 6 Feb 2025 04:18:46 UTC (841 KB) [v4] Fri, 20 Jun 2025 08:54:13 UTC (785 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.00412'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.00412'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.00412'}]
2024-11-10	Personalization of Large Language Models: A Survey	Computation and Language	https://arxiv.org/abs/2411.00027	Personalization of LLMs	https://x.com/omarsar0/status/1853276249981907386		2411.00027	['Zhehao Zhang', 'Ryan A. Rossi', 'Branislav Kveton', 'Yijia Shao', 'Diyi Yang', 'Hamed Zamani', 'Franck Dernoncourt', 'Joe Barrow', 'Tong Yu', 'Sungchul Kim', 'Ruiyi Zhang', 'Jiuxiang Gu', 'Tyler Derr', 'Hongjie Chen', 'Junda Wu', 'Xiang Chen', 'Zichao Wang', 'Subrata Mitra', 'Nedim Lipka', 'Nesreen Ahmed', 'Yu Wang']	ct:Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.	ed at the Transactions on Machine Learning Research (TMLR) journal	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2411.00027', 'html': 'https://arxiv.org/html/2411.00027v3', 'tex': '/src/2411.00027', 'doi': 'https://doi.org/10.48550/arXiv.2411.00027'}	Submission history From: Zhehao Zhang [ view email ] [v1] Tue, 29 Oct 2024 04:01:11 UTC (4,783 KB) [v2] Tue, 6 May 2025 03:03:35 UTC (5,366 KB) [v3] Sun, 13 Jul 2025 02:13:14 UTC (4,143 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2411.00027'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2411.00027'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2411.00027'}]
2024-11-03	The Geometry of Concepts: Sparse Autoencoder Feature Structure	Quantitative Biology > Neurons and Cognition	https://arxiv.org/abs/2410.19750	Geometry of Concepts in LLMs	https://x.com/tegmark/status/1851288315867041903		2410.19750	['Yuxiao Li', 'Eric J. Michaud', 'David D. Baek', 'Joshua Engels', 'Xiaoqing Sun', 'Max Tegmark']	"ct:Sparse autoencoders have recently produced dictionaries of high-dimensional vectors corresponding to the universe of concepts represented by large language models. We find that this concept universe has interesting structure at three levels: 1) The ""atomic"" small-scale structure contains ""crystals"" whose faces are parallelograms or trapezoids, generalizing well-known examples such as (man-woman-king-queen). We find that the quality of such parallelograms and associated function vectors improves greatly when projecting out global distractor directions such as word length, which is efficiently done with linear discriminant analysis. 2) The ""brain"" intermediate-scale structure has significant spatial modularity; for example, math and code features form a ""lobe"" akin to functional lobes seen in neural fMRI images. We quantify the spatial locality of these lobes with multiple metrics and find that clusters of co-occurring features, at coarse enough scale, also cluster together spatially far more than one would expect if feature geometry were random. 3) The ""galaxy"" scale large-scale structure of the feature point cloud is not isotropic, but instead has a power law of eigenvalues with steepest slope in middle layers. We also quantify how the clustering entropy depends on the layer."	es, 12 figures	['Neurons and Cognition (q-bio.NC)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2410.19750', 'html': 'https://arxiv.org/html/2410.19750v2', 'tex': '/src/2410.19750', 'doi': 'https://doi.org/10.48550/arXiv.2410.19750'}	Submission history From: David D. Baek [ view email ] [v1] Thu, 10 Oct 2024 17:58:47 UTC (17,453 KB) [v2] Sun, 30 Mar 2025 23:55:03 UTC (19,691 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.19750'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.19750'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.19750'}]
2024-11-03	AFlow: Automating Agentic Workflow Generation	Artificial Intelligence	https://arxiv.org/abs/2410.10762	Automating Agentic Workflow Generation	https://x.com/omarsar0/status/1852339570891014415		2410.10762	['Jiayi Zhang', 'Jinyu Xiang', 'Zhaoyang Yu', 'Fengwei Teng', 'Xionghui Chen', 'Jiaqi Chen', 'Mingchen Zhuge', 'Xin Cheng', 'Sirui Hong', 'Jinlin Wang', 'Bingnan Zheng', 'Bang Liu', 'Yuyu Luo', 'Chenglin Wu']	ct:Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFlow, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code is available atthis https URL.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2410.10762', 'html': None, 'tex': '/src/2410.10762', 'doi': 'https://doi.org/10.48550/arXiv.2410.10762'}	Submission history From: Jiayi Zhang [ view email ] [v1] Mon, 14 Oct 2024 17:40:40 UTC (758 KB) [v2] Tue, 25 Feb 2025 04:56:05 UTC (796 KB) [v3] Wed, 26 Feb 2025 06:38:03 UTC (796 KB) [v4] Tue, 15 Apr 2025 02:44:55 UTC (796 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.10762'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.10762'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.10762'}]
2024-11-03	Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics	Computation and Language	https://arxiv.org/abs/2410.21272	LLMs Solve Math with a Bag of Heuristics	https://x.com/omarsar0/status/1851233281116946923		2410.21272	['Yaniv Nikankin', 'Anja Reusch', 'Aaron Mueller', 'Yonatan Belinkov']	"ct:Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types-such as neurons that activate when an operand falls within a certain range-and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a ""bag of heuristics""."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.21272', 'html': 'https://arxiv.org/html/2410.21272v2', 'tex': '/src/2410.21272', 'doi': 'https://doi.org/10.48550/arXiv.2410.21272'}	Submission history From: Yaniv Nikankin [ view email ] [v1] Mon, 28 Oct 2024 17:59:06 UTC (2,926 KB) [v2] Tue, 20 May 2025 16:29:40 UTC (4,024 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.21272'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.21272'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.21272'}]
2024-11-03	O1 Replication Journey: A Strategic Progress Report -- Part 1	Artificial Intelligence	https://arxiv.org/abs/2410.18982	o1 Replication Journey	https://x.com/omarsar0/status/1850748790308761988		2410.18982	['Yiwei Qin', 'Xuefeng Li', 'Haoyang Zou', 'Yixiu Liu', 'Shijie Xia', 'Zhen Huang', 'Yixin Ye', 'Weizhe Yuan', 'Hector Liu', 'Yuanzhi Li', 'Pengfei Liu']	ct:This paper introduces a pioneering approach to artificial intelligence research, embodied in our O1 Replication Journey. In response to the announcement of OpenAI's groundbreaking O1 model, we embark on a transparent, real-time exploration to replicate its capabilities while reimagining the process of conducting and communicating AI research. Our methodology addresses critical challenges in modern AI research, including the insularity of prolonged team-based projects, delayed information sharing, and the lack of recognition for diverse contributions. By providing comprehensive, real-time documentation of our replication efforts, including both successes and failures, we aim to foster open science, accelerate collective advancement, and lay the groundwork for AI-driven scientific discovery. Our research progress report diverges significantly from traditional research papers, offering continuous updates, full process transparency, and active community engagement throughout the research journey. Technologically, we proposed the journey learning paradigm, which encourages models to learn not just shortcuts, but the complete exploration process, including trial and error, reflection, and backtracking. With only 327 training samples and without any additional tricks, journey learning outperformed conventional supervised learning by over 8\% on the MATH dataset, demonstrating its extremely powerful potential. We believe this to be the most crucial component of O1 technology that we have successfully decoded. We share valuable resources including technical hypotheses and insights, cognitive exploration maps, custom-developed tools, etc atthis https URL.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.18982', 'html': 'https://arxiv.org/html/2410.18982v1', 'tex': '/src/2410.18982', 'doi': 'https://doi.org/10.48550/arXiv.2410.18982'}	Submission history From: Yiwei Qin [ view email ] [v1] Tue, 8 Oct 2024 15:13:01 UTC (2,308 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.18982'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.18982'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.18982'}]
2024-11-03	Distinguishing Ignorance from Error in LLM Hallucinations	Computation and Language	https://arxiv.org/abs/2410.22071	Distinguishing Ignorance from Error in LLM Hallucinations	https://x.com/AdiSimhi/status/1851650371615125563		2410.22071	['Adi Simhi', 'Jonathan Herzig', 'Idan Szpektor', 'Yonatan Belinkov']	ct:Large language models (LLMs) are susceptible to hallucinations -- factually incorrect outputs -- leading to a large body of work on detecting and mitigating such cases. We argue that it is important to distinguish between two types of hallucinations: ones where the model does not hold the correct answer in its parameters, which we term HK-, and ones where the model answers incorrectly despite having the required knowledge, termed HK+. We first find that HK+ hallucinations are prevalent and occur across models and datasets. Then, we demonstrate that distinguishing between these two cases is beneficial for mitigating hallucinations. Importantly, we show that different models hallucinate on different examples, which motivates constructing model-specific hallucination datasets for training detectors. Overall, our findings draw attention to classifying types of hallucinations and provide means to handle them more effectively. The code is available atthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.22071', 'html': 'https://arxiv.org/html/2410.22071v2', 'tex': '/src/2410.22071', 'doi': 'https://doi.org/10.48550/arXiv.2410.22071'}	Submission history From: Adi Simhi [ view email ] [v1] Tue, 29 Oct 2024 14:31:33 UTC (819 KB) [v2] Tue, 18 Feb 2025 15:52:52 UTC (10,226 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.22071'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.22071'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.22071'}]
2024-11-03	Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications	Computation and Language	https://arxiv.org/abs/2410.21943	Multimodal RAG	https://x.com/omarsar0/status/1851479149690642456		2410.21943	['Monica Riedler', 'Stefan Langer']	ct:Large Language Models (LLMs) have demonstrated impressive capabilities in answering questions, but they lack domain-specific knowledge and are prone to hallucinations. Retrieval Augmented Generation (RAG) is one approach to address these challenges, while multimodal models are emerging as promising AI assistants for processing both text and images. In this paper we describe a series of experiments aimed at determining how to best integrate multimodal models into RAG systems for the industrial domain. The purpose of the experiments is to determine whether including images alongside text from documents within the industrial domain increases RAG performance and to find the optimal configuration for such a multimodal RAG system. Our experiments include two approaches for image processing and retrieval, as well as two LLMs (GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies involve the use of multimodal embeddings and the generation of textual summaries from images. We evaluate our experiments with an LLM-as-a-Judge approach. Our results reveal that multimodal RAG can outperform single-modality RAG settings, although image retrieval poses a greater challenge than text retrieval. Additionally, leveraging textual summaries from images presents a more promising approach compared to the use of multimodal embeddings, providing more opportunities for future advancements.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.21943', 'html': 'https://arxiv.org/html/2410.21943v1', 'tex': '/src/2410.21943', 'doi': 'https://doi.org/10.48550/arXiv.2410.21943'}	Submission history From: Monica Riedler [ view email ] [v1] Tue, 29 Oct 2024 11:03:31 UTC (2,188 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.21943'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.21943'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.21943'}]
2024-11-03	Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models	Computation and Language	https://arxiv.org/abs/2410.19385	The Role of Prompting and External Tools in Hallucination Rates of LLMs	https://x.com/omarsar0/status/1850745569125253401		2410.19385	['Liam Barkley', 'Brink van der Merwe']	ct:Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.19385', 'html': 'https://arxiv.org/html/2410.19385v1', 'tex': '/src/2410.19385', 'doi': 'https://doi.org/10.48550/arXiv.2410.19385'}	Submission history From: Liam Barkley [ view email ] [v1] Fri, 25 Oct 2024 08:34:53 UTC (109 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.19385'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.19385'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.19385'}]
2024-11-03	MrT5: Dynamic Token Merging for Efficient Byte-level Language Models	Computation and Language	https://arxiv.org/abs/2410.20771	MrT5	https://x.com/JulieKallini/status/1851278833061704170		2410.20771	['Julie Kallini', 'Shikhar Murty', 'Christopher D. Manning', 'Christopher Potts', 'Róbert Csordás']	"ct:Models that rely on subword tokenization have significant drawbacks, such as sensitivity to character-level noise like spelling errors and inconsistent compression rates across different languages and scripts. While character- or byte-level models like ByT5 attempt to address these concerns, they have not gained widespread adoption -- processing raw byte streams without tokenization results in significantly longer sequence lengths, making training and inference inefficient. This work introduces MrT5 (MergeT5), a more efficient variant of ByT5 that integrates a token deletion mechanism in its encoder to dynamically shorten the input sequence length. After processing through a fixed number of encoder layers, a learned delete gate determines which tokens are to be removed and which are to be retained for subsequent layers. MrT5 effectively ""merges"" critical information from deleted tokens into a more compact sequence, leveraging contextual information from the remaining tokens. In continued pre-training experiments, we find that MrT5 can achieve significant gains in inference runtime with minimal effect on performance, as measured by bits-per-byte. Additionally, with multilingual training, MrT5 adapts to the orthographic characteristics of each language, learning language-specific compression rates. Furthermore, MrT5 shows comparable accuracy to ByT5 on downstream evaluations such as XNLI, TyDi QA, and character-level tasks while reducing sequence lengths by up to 75%. Our approach presents a solution to the practical limitations of existing byte-level models."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2410.20771', 'html': 'https://arxiv.org/html/2410.20771v3', 'tex': '/src/2410.20771', 'doi': 'https://doi.org/10.48550/arXiv.2410.20771'}	Submission history From: Julie Kallini [ view email ] [v1] Mon, 28 Oct 2024 06:14:12 UTC (313 KB) [v2] Sun, 16 Mar 2025 07:22:10 UTC (375 KB) [v3] Wed, 2 Apr 2025 03:23:02 UTC (375 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.20771'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.20771'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.20771'}]
2024-11-03	Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA	Computation and Language	https://arxiv.org/abs/2410.20672	Relaxed Recursive Transformers	https://x.com/raymin0223/status/1851216039822180759		2410.20672	['Sangmin Bae', 'Adam Fisch', 'Hrayr Harutyunyan', 'Ziwei Ji', 'Seungyeon Kim', 'Tal Schuster']	"ct:Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit ""layer tying"" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller ""Recursive Transformers"" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original ""full-size"" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput."	025; 49 pages, 17 figures, 19 tables	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2410.20672', 'html': None, 'tex': '/src/2410.20672', 'doi': 'https://doi.org/10.48550/arXiv.2410.20672'}	Submission history From: Sangmin Bae [ view email ] [v1] Mon, 28 Oct 2024 02:15:45 UTC (1,497 KB) [v2] Thu, 6 Feb 2025 03:23:11 UTC (1,517 KB) [v3] Fri, 28 Feb 2025 16:44:24 UTC (1,481 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.20672'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.20672'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.20672'}]
2024-10-27	Agentic Information Retrieval	Information Retrieval	https://arxiv.org/abs/2410.09713	Agentic Information Retrieval	https://x.com/omarsar0/status/1848396596230127655		2410.09713	['Weinan Zhang', 'Junwei Liao', 'Ning Li', 'Kounianhua Du', 'Jianghao Lin']	ct:Since the 1970s, information retrieval (IR) has long been defined as the process of acquiring relevant information items from a pre-defined corpus to satisfy user information needs. Traditional IR systems, while effective in domains like web search, are constrained by their reliance on static, pre-defined information items. To this end, this paper introduces agentic information retrieval (Agentic IR), a transformative next-generation paradigm for IR driven by large language models (LLMs) and AI agents. The central shift in agentic IR is the evolving definition of ``information'' from static, pre-defined information items to dynamic, context-dependent information states. Information state refers to a particular information context that the user is right in within a dynamic environment, encompassing not only the acquired information items but also real-time user preferences, contextual factors, and decision-making processes. In such a way, traditional information retrieval, focused on acquiring relevant information items based on user queries, can be naturally extended to achieving the target information state given the user instruction, which thereby defines the agentic information retrieval. We systematically discuss agentic IR from various aspects, i.e., task formulation, architecture, evaluation, case studies, as well as challenges and future prospects. We believe that the concept of agentic IR introduced in this paper not only broadens the scope of information retrieval research but also lays the foundation for a more adaptive, interactive, and intelligent next-generation IR paradigm.	es, perspective paper	['Information Retrieval (cs.IR)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.09713', 'html': 'https://arxiv.org/html/2410.09713v4', 'tex': '/src/2410.09713', 'doi': 'https://doi.org/10.48550/arXiv.2410.09713'}	Submission history From: Weinan Zhang [ view email ] [v1] Sun, 13 Oct 2024 03:45:24 UTC (771 KB) [v2] Tue, 29 Oct 2024 13:19:12 UTC (661 KB) [v3] Wed, 19 Feb 2025 16:24:30 UTC (2,784 KB) [v4] Sun, 23 Feb 2025 03:23:46 UTC (2,784 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.09713'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.09713'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.09713'}]
2024-10-27	A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration	Computation and Language	https://arxiv.org/abs/2410.16540	A Theoretical Understanding of CoT	https://x.com/omarsar0/status/1849139985712369907		2410.16540	['Yingqian Cui', 'Pengfei He', 'Xianfeng Tang', 'Qi He', 'Chen Luo', 'Jiliang Tang', 'Yue Xing']	ct:Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration. Our experiments validate the effectiveness of the proposed approach.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2410.16540', 'html': 'https://arxiv.org/html/2410.16540v1', 'tex': '/src/2410.16540', 'doi': 'https://doi.org/10.48550/arXiv.2410.16540'}	Submission history From: Yingqian Cui [ view email ] [v1] Mon, 21 Oct 2024 22:07:20 UTC (677 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.16540'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.16540'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.16540'}]
2024-10-27	A Survey on Data Synthesis and Augmentation for Large Language Models	Computation and Language	https://arxiv.org/abs/2410.12896	A Survey on Data Synthesis and Augmentation for LLMs	https://x.com/omarsar0/status/1848445736591163886		2410.12896	['Ke Wang', 'Jiahui Zhu', 'Minjie Ren', 'Zeming Liu', 'Shiwei Li', 'Zongye Zhang', 'Chenkai Zhang', 'Xiaoyu Wu', 'Qiqi Zhan', 'Qingjie Liu', 'Yunhong Wang']	ct:The success of Large Language Models (LLMs) is inherently linked to the availability of vast, diverse, and high-quality data for training and evaluation. However, the growth rate of high-quality data is significantly outpaced by the expansion of training datasets, leading to a looming data exhaustion crisis. This underscores the urgent need to enhance data efficiency and explore new data sources. In this context, synthetic data has emerged as a promising solution. Currently, data generation primarily consists of two major approaches: data augmentation and synthesis. This paper comprehensively reviews and summarizes data generation techniques throughout the lifecycle of LLMs, including data preparation, pre-training, fine-tuning, instruction-tuning, preference alignment, and applications. Furthermore, We discuss the current constraints faced by these methods and investigate potential pathways for future development and research. Our aspiration is to equip researchers with a clear understanding of these methodologies, enabling them to swiftly identify appropriate data generation strategies in the construction of LLMs, while providing valuable insights for future exploration.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.12896', 'html': 'https://arxiv.org/html/2410.12896v1', 'tex': '/src/2410.12896', 'doi': 'https://doi.org/10.48550/arXiv.2410.12896'}	Submission history From: Ke Wang [ view email ] [v1] Wed, 16 Oct 2024 16:12:39 UTC (5,862 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.12896'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.12896'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.12896'}]
2024-10-27	LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering	Computation and Language	https://arxiv.org/abs/2410.18050	LongRAG	https://x.com/omarsar0/status/1849494571946066295		2410.18050	['Qingfei Zhao', 'Ruobing Wang', 'Yukuo Cen', 'Daren Zha', 'Shicheng Tan', 'Yuxiao Dong', 'Jie Tang']	"ct:Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the ""lost in the middle"" issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG's understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system's components and fine-tuning strategies. Data and code are available atthis https URL."	2024 Main, Final	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.18050', 'html': 'https://arxiv.org/html/2410.18050v2', 'tex': '/src/2410.18050', 'doi': 'https://doi.org/10.48550/arXiv.2410.18050'}	Submission history From: Qingfei Zhao [ view email ] [v1] Wed, 23 Oct 2024 17:24:58 UTC (645 KB) [v2] Fri, 1 Nov 2024 15:36:59 UTC (645 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.18050'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.18050'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.18050'}]
2024-10-27	Large Language Models Reflect the Ideology of their Creators	Computation and Language	https://arxiv.org/abs/2410.18417	LLMs Reflect the Ideology of their Creators	https://x.com/omarsar0/status/1849860985500352968		2410.18417	['Maarten Buyl', 'Alexander Rogiers', 'Sander Noels', 'Guillaume Bied', 'Iris Dominguez-Catena', 'Edith Heiter', 'Iman Johary', 'Alexandru-Cristian Mara', 'Raphaël Romero', 'Jefrey Lijffijt', 'Tijl De Bie']	ct:Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use.In this paper, we prompt a diverse panel of popular LLMs to describe a large number of prominent personalities with political relevance, in all six official languages of the United Nations. By identifying and analyzing moral assessments reflected in their responses, we find normative differences between LLMs from different geopolitical regions, as well as between the responses of the same LLM when prompted in different languages. Among only models in the United States, we find that popularly hypothesized disparities in political views are reflected in significant normative differences related to progressive values. Among Chinese models, we characterize a division between internationally- and domestically-focused models.Our results show that the ideological stance of an LLM appears to reflect the worldview of its creators. This poses the risk of political instrumentalization and raises concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically 'unbiased'.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2410.18417', 'html': 'https://arxiv.org/html/2410.18417v2', 'tex': '/src/2410.18417', 'doi': 'https://doi.org/10.48550/arXiv.2410.18417'}	Submission history From: Maarten Buyl [ view email ] [v1] Thu, 24 Oct 2024 04:02:30 UTC (324 KB) [v2] Thu, 30 Jan 2025 15:45:45 UTC (697 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.18417'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.18417'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.18417'}]
2024-10-27	A Comparative Study on Reasoning Patterns of OpenAI's o1 Model	Computation and Language	https://arxiv.org/abs/2410.13639	Reasoning Patterns of OpenAI’s o1 Model	https://x.com/omarsar0/status/1848782378631892997		2410.13639	['Siwei Wu', 'Zhongyuan Peng', 'Xinrun Du', 'Tuney Zheng', 'Minghao Liu', 'Jialong Wu', 'Jiachen Ma', 'Yizhi Li', 'Jian Yang', 'Wangchunshu Zhou', 'Qunshu Lin', 'Junbo Zhao', 'Zhaoxiang Zhang', 'Wenhao Huang', 'Ge Zhang', 'Chenghua Lin', 'J.H. Liu']	ct:Enabling Large Language Models (LLMs) to handle a wider range of complex tasks (e.g., coding, math) has drawn great attention from many researchers. As LLMs continue to evolve, merely increasing the number of model parameters yields diminishing performance improvements and heavy computational costs. Recently, OpenAI's o1 model has shown that inference strategies (i.e., Test-time Compute methods) can also significantly enhance the reasoning capabilities of LLMs. However, the mechanisms behind these methods are still unexplored. In our work, to investigate the reasoning patterns of o1, we compare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent Workflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general reasoning benchmarks in three domains (i.e., math, coding, commonsense reasoning). Specifically, first, our experiments show that the o1 model has achieved the best performance on most datasets. Second, as for the methods of searching diverse responses (e.g., BoN), we find the reward models' capability and the search space both limit the upper boundary of these methods. Third, as for the methods that break the problem into many sub-problems, the Agent Workflow has achieved better performance than Step-wise BoN due to the domain-specific system prompt for planning better reasoning processes. Fourth, it is worth mentioning that we have summarized six reasoning patterns of o1, and provided a detailed analysis on several reasoning benchmarks.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.13639', 'html': 'https://arxiv.org/html/2410.13639v2', 'tex': '/src/2410.13639', 'doi': 'https://doi.org/10.48550/arXiv.2410.13639'}	Submission history From: Siwei Wu [ view email ] [v1] Thu, 17 Oct 2024 15:09:03 UTC (2,372 KB) [v2] Tue, 22 Oct 2024 22:05:16 UTC (2,376 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.13639'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.13639'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.13639'}]
2024-10-20	Thinking LLMs: General Instruction Following with Thought Generation	Computation and Language	https://arxiv.org/abs/2410.10630	Thinking LLMs	https://x.com/omarsar0/status/1846227797972603047		2410.10630	['Tianhao Wu', 'Janice Lan', 'Weizhe Yuan', 'Jiantao Jiao', 'Jason Weston', 'Sainbayar Sukhbaatar']	ct:LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.10630', 'html': 'https://arxiv.org/html/2410.10630v1', 'tex': '/src/2410.10630', 'doi': 'https://doi.org/10.48550/arXiv.2410.10630'}	Submission history From: Jason Weston [ view email ] [v1] Mon, 14 Oct 2024 15:38:56 UTC (1,265 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.10630'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.10630'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.10630'}]
2024-10-20	Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence	Computation and Language	https://arxiv.org/abs/2410.11163	Model Swarms	https://x.com/omarsar0/status/1846592954921849029		2410.11163	['Shangbin Feng', 'Zifeng Wang', 'Yike Wang', 'Sayna Ebrahimi', 'Hamid Palangi', 'Lesly Miculicich', 'Achin Kulshrestha', 'Nathalie Rauschmayr', 'Yejin Choi', 'Yulia Tsvetkov', 'Chen-Yu Lee', 'Tomas Pfister']	ct:We propose Model Swarms, a collaborative search algorithm to adapt LLMs via swarm intelligence, the collective behavior guiding individual systems. Specifically, Model Swarms starts with a pool of LLM experts and a utility function. Guided by the best-found checkpoints across models, diverse LLM experts collaboratively move in the weight space and optimize a utility function representing model adaptation objectives. Compared to existing model composition approaches, Model Swarms offers tuning-free model adaptation, works in low-data regimes with as few as 200 examples, and does not require assumptions about specific experts in the swarm or how they should be composed. Extensive experiments demonstrate that Model Swarms could flexibly adapt LLM experts to a single task, multi-task domains, reward models, as well as diverse human interests, improving over 12 model composition baselines by up to 21.0% across tasks and contexts. Further analysis reveals that LLM experts discover previously unseen capabilities in initial checkpoints and that Model Swarms enable the weak-to-strong transition of experts through the collaborative search process.	025	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.11163', 'html': 'https://arxiv.org/html/2410.11163v2', 'tex': '/src/2410.11163', 'doi': 'https://doi.org/10.48550/arXiv.2410.11163'}	Submission history From: Shangbin Feng [ view email ] [v1] Tue, 15 Oct 2024 00:59:17 UTC (4,755 KB) [v2] Sat, 31 May 2025 23:27:47 UTC (2,315 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.11163'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.11163'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.11163'}]
2024-10-20	Looking Inward: Language Models Can Learn About Themselves by Introspection	Computation and Language	https://arxiv.org/abs/2410.13787	Introspection in LLMs	https://x.com/omarsar0/status/1847297594525094081		2410.13787	['Felix J Binder', 'James Chua', 'Tomek Korbak', 'Henry Sleight', 'John Hughes', 'Robert Long', 'Ethan Perez', 'Miles Turpin', 'Owain Evans']	"ct:Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data.We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, ""Given the input P, would your output favor the short- or long-term option?"" If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization."	es, 9 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.13787', 'html': 'https://arxiv.org/html/2410.13787v1', 'tex': '/src/2410.13787', 'doi': 'https://doi.org/10.48550/arXiv.2410.13787'}	Submission history From: Felix Binder [ view email ] [v1] Thu, 17 Oct 2024 17:24:10 UTC (12,501 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.13787'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.13787'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.13787'}]
2024-10-20	Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2410.13848	Janus	https://x.com/deepseek_ai/status/1847191319464300652		2410.13848	['Chengyue Wu', 'Xiaokang Chen', 'Zhiyu Wu', 'Yiyang Ma', 'Xingchao Liu', 'Zizheng Pan', 'Wen Liu', 'Zhenda Xie', 'Xingkai Yu', 'Chong Ruan', 'Ping Luo']	ct:In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.	cal Report	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.13848', 'html': 'https://arxiv.org/html/2410.13848v1', 'tex': '/src/2410.13848', 'doi': 'https://doi.org/10.48550/arXiv.2410.13848'}	Submission history From: Xiaokang Chen [ view email ] [v1] Thu, 17 Oct 2024 17:58:37 UTC (5,510 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.13848'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.13848'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.13848'}]
2024-10-20	Inference Scaling for Long-Context Retrieval Augmented Generation	Computation and Language	https://arxiv.org/abs/2410.04343	Inference Scaling for Long-Context RAG	https://x.com/omarsar0/status/1847350506127315088		2410.04343	['Zhenrui Yue', 'Honglei Zhuang', 'Aijun Bai', 'Kai Hui', 'Rolf Jagerman', 'Hansi Zeng', 'Zhen Qin', 'Dong Wang', 'Xuanhui Wang', 'Michael Bendersky']	ct:The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.	025	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.04343', 'html': 'https://arxiv.org/html/2410.04343v2', 'tex': '/src/2410.04343', 'doi': 'https://doi.org/10.48550/arXiv.2410.04343'}	Submission history From: Zhenrui Yue [ view email ] [v1] Sun, 6 Oct 2024 03:42:15 UTC (7,753 KB) [v2] Sun, 2 Mar 2025 19:44:37 UTC (8,124 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.04343'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.04343'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.04343'}]
2024-10-20	Agent S: An Open Agentic Framework that Uses Computers Like a Human	Artificial Intelligence	https://arxiv.org/abs/2410.08164v1	Agent S	https://x.com/omarsar0/status/1846930425849303424		2410.08164v1	['Saaket Agashe', 'Jiuzhou Han', 'Shuyu Gan', 'Jiachen Yang', 'Ang Li', 'Xin Eric Wang']	ct:We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available atthis https URL.	es, 16 figures, 9 tables	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2410.08164v1', 'html': 'https://arxiv.org/html/2410.08164v1', 'tex': '/src/2410.08164v1', 'doi': 'https://doi.org/10.48550/arXiv.2410.08164'}	Submission history From: Xin Eric Wang [ view email ] [v1] Thu, 10 Oct 2024 17:43:51 UTC (31,149 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.08164'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.08164'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.08164'}]
2024-10-20	Exploring Model Kinship for Merging Large Language Models	Computation and Language	https://arxiv.org/abs/2410.12613	Model Kinship for Merging LLMs	https://x.com/omarsar0/status/1846753148007846329		2410.12613	['Yedi Hu', 'Yunzhi Yao', 'Shumin Deng', 'Huajun Chen', 'Ningyu Zhang']	ct:Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-k Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Code is available atthis https URL.	g work	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2410.12613', 'html': 'https://arxiv.org/html/2410.12613v2', 'tex': '/src/2410.12613', 'doi': 'https://doi.org/10.48550/arXiv.2410.12613'}	Submission history From: Ningyu Zhang [ view email ] [v1] Wed, 16 Oct 2024 14:29:29 UTC (3,183 KB) [v2] Sun, 1 Jun 2025 10:39:29 UTC (3,192 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.12613'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.12613'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.12613'}]
2024-10-20	On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability	Artificial Intelligence	https://www.arxiv.org/abs/2409.19924	On the Planning Abilities of OpenAI’s o1 Models	https://x.com/omarsar0/status/1846032256902869135		2409.19924	['Kevin Wang', 'Junbo Li', 'Neel P. Bhatt', 'Yihan Xi', 'Qiang Liu', 'Ufuk Topcu', 'Zhangyang Wang']	ct:Recent advancements in Large Language Models (LLMs) have showcased their ability to perform complex reasoning tasks, but their effectiveness in planning remains underexplored. In this study, we evaluate the planning capabilities of OpenAI's o1 models across a variety of benchmark tasks, focusing on three key aspects: feasibility, optimality, and generalizability. Through empirical evaluations on constraint-heavy tasks (e.g., $\textit{Barman}$, $\textit{Tyreworld}$) and spatially complex environments (e.g., $\textit{Termes}$, $\textit{Floortile}$), we highlight o1-preview's strengths in self-evaluation and constraint-following, while also identifying bottlenecks in decision-making and memory management, particularly in tasks requiring robust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4 in adhering to task constraints and managing state transitions in structured environments. However, the model often generates suboptimal solutions with redundant actions and struggles to generalize effectively in spatially complex tasks. This pilot study provides foundational insights into the planning limitations of LLMs, offering key directions for future research on improving memory management, decision-making, and generalization in LLM-based planning. Code available atthis https URL.	vailable atthis https URL	['Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Robotics (cs.RO)']	{'pdf': '/pdf/2409.19924', 'html': 'https://arxiv.org/html/2409.19924v4', 'tex': '/src/2409.19924', 'doi': 'https://doi.org/10.48550/arXiv.2409.19924'}	Submission history From: Neel P. Bhatt [ view email ] [v1] Mon, 30 Sep 2024 03:58:43 UTC (9,663 KB) [v2] Tue, 1 Oct 2024 12:43:09 UTC (9,663 KB) [v3] Fri, 11 Oct 2024 03:42:20 UTC (9,663 KB) [v4] Mon, 14 Oct 2024 03:41:09 UTC (9,663 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.19924'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.19924'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.19924'}]
2024-10-20	CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2410.11831	CoTracker3	https://x.com/AIatMeta/status/1846595406261899363		2410.11831	['Nikita Karaev', 'Iurii Makarov', 'Jianyuan Wang', 'Natalia Neverova', 'Andrea Vedaldi', 'Christian Rupprecht']	ct:Most state-of-the-art point trackers are trained on synthetic data due to the difficulty of annotating real videos for this task. However, this can result in suboptimal performance due to the statistical gap between synthetic and real videos. In order to understand these issues better, we introduce CoTracker3, comprising a new tracking model and a new semi-supervised training recipe. This allows real videos without annotations to be used during training by generating pseudo-labels using off-the-shelf teachers. The new model eliminates or simplifies components from previous trackers, resulting in a simpler and often smaller architecture. This training scheme is much simpler than prior work and achieves better results using 1,000 times less data. We further study the scaling behaviour to understand the impact of using more real unsupervised data in point tracking. The model is available in online and offline variants and reliably tracks visible and occluded points.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2410.11831', 'html': 'https://arxiv.org/html/2410.11831v1', 'tex': '/src/2410.11831', 'doi': 'https://doi.org/10.48550/arXiv.2410.11831'}	Submission history From: Nikita Karaev [ view email ] [v1] Tue, 15 Oct 2024 17:56:32 UTC (4,275 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.11831'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.11831'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.11831'}]
2024-10-13	MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering	Computation and Language	https://arxiv.org/abs/2410.07095	MLE-Bench	https://x.com/OpenAI/status/1844429536353714427		2410.07095	['Jun Shern Chan', 'Neil Chowdhury', 'Oliver Jaffe', 'James Aung', 'Dane Sherburn', 'Evan Mays', 'Giulio Starace', 'Kevin Liu', 'Leon Maksin', 'Tejal Patwardhan', 'Lilian Weng', 'Aleksander Mądry']	ct:We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (this http URL) to facilitate future research in understanding the ML engineering capabilities of AI agents.	es, 17 pages appendix. Equal contribution by first seven authors, authors randomized. ICLR version	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.07095', 'html': None, 'tex': '/src/2410.07095', 'doi': 'https://doi.org/10.48550/arXiv.2410.07095'}	Submission history From: James Aung [ view email ] [v1] Wed, 9 Oct 2024 17:34:27 UTC (1,644 KB) [v2] Thu, 24 Oct 2024 12:35:50 UTC (1,645 KB) [v3] Wed, 11 Dec 2024 15:02:22 UTC (1,644 KB) [v4] Mon, 16 Dec 2024 16:05:09 UTC (1,649 KB) [v5] Fri, 20 Dec 2024 13:32:37 UTC (1,662 KB) [v6] Wed, 26 Feb 2025 11:57:30 UTC (1,868 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.07095'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.07095'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.07095'}]
2024-10-13	Differential Transformer	Computation and Language	https://arxiv.org/abs/2410.05258	Differential Transformer	https://x.com/omarsar0/status/1843694897020150216		2410.05258	['Tianzhu Ye', 'Li Dong', 'Yuqing Xia', 'Yutao Sun', 'Yi Zhu', 'Gao Huang', 'Furu Wei']	ct:Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.	ed as an Oral Presentation at ICLR 2025	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2410.05258', 'html': 'https://arxiv.org/html/2410.05258v2', 'tex': '/src/2410.05258', 'doi': 'https://doi.org/10.48550/arXiv.2410.05258'}	Submission history From: Tianzhu Ye [ view email ] [v1] Mon, 7 Oct 2024 17:57:38 UTC (429 KB) [v2] Mon, 7 Apr 2025 12:04:28 UTC (453 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.05258'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.05258'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.05258'}]
2024-10-13	Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models	Computation and Language	https://arxiv.org/abs/2410.07176	Astute RAG	https://x.com/omarsar0/status/1844435988019544565		2410.07176	['Fei Wang', 'Xingchen Wan', 'Ruoxi Sun', 'Jiefeng Chen', 'Sercan Ö. Arık']	ct:Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs' internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.	25 main conference	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2410.07176', 'html': 'https://arxiv.org/html/2410.07176v2', 'tex': '/src/2410.07176', 'doi': 'https://doi.org/10.48550/arXiv.2410.07176'}	Submission history From: Fei Wang [ view email ] [v1] Wed, 9 Oct 2024 17:59:58 UTC (754 KB) [v2] Sat, 31 May 2025 04:07:21 UTC (1,439 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.07176'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.07176'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.07176'}]
2024-10-13	ToolGen: Unified Tool Retrieval and Calling via Generation	Computation and Language	https://arxiv.org/abs/2410.03439	ToolGen	https://x.com/omarsar0/status/1843491766114422930		2410.03439	['Renxi Wang', 'Xudong Han', 'Lei Ji', 'Shu Wang', 'Timothy Baldwin', 'Haonan Li']	ct:As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM's parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation. Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains. By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs.	025	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.03439', 'html': 'https://arxiv.org/html/2410.03439v3', 'tex': '/src/2410.03439', 'doi': 'https://doi.org/10.48550/arXiv.2410.03439'}	Submission history From: Renxi Wang [ view email ] [v1] Fri, 4 Oct 2024 13:52:32 UTC (3,048 KB) [v2] Tue, 8 Oct 2024 06:54:16 UTC (3,048 KB) [v3] Sat, 29 Mar 2025 13:27:51 UTC (3,162 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.03439'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.03439'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.03439'}]
2024-10-13	Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG	Computation and Language	https://arxiv.org/abs/2410.05983	Long-Context LLMs Meet RAG	https://x.com/omarsar0/status/1844828836619334066		2410.05983	['Bowen Jin', 'Jinsung Yoon', 'Jiawei Han', 'Sercan O. Arik']	"ct:Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved ""hard negatives"" as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length."	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2410.05983', 'html': 'https://arxiv.org/html/2410.05983v1', 'tex': '/src/2410.05983', 'doi': 'https://doi.org/10.48550/arXiv.2410.05983'}	Submission history From: Bowen Jin [ view email ] [v1] Tue, 8 Oct 2024 12:30:07 UTC (337 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.05983'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.05983'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.05983'}]
2024-10-13	GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models	Machine Learning	https://arxiv.org/abs/2410.05229	GSM-Symbolic	https://x.com/MFarajtabar/status/1844456880971858028		2410.05229	['Iman Mirzadeh', 'Keivan Alizadeh', 'Hooman Shahrokhi', 'Oncel Tuzel', 'Samy Bengio', 'Mehrdad Farajtabar']	ct:Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities ofthis http URLfindings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.	nt	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.05229', 'html': 'https://arxiv.org/html/2410.05229v1', 'tex': '/src/2410.05229', 'doi': 'https://doi.org/10.48550/arXiv.2410.05229'}	Submission history From: Seyed Iman Mirzadeh [ view email ] [v1] Mon, 7 Oct 2024 17:36:37 UTC (5,949 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.05229'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.05229'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.05229'}]
2024-10-13	Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System	Computation and Language	https://arxiv.org/abs/2410.08115	Optima	https://x.com/omarsar0/status/1844578931732844963		2410.08115	['Weize Chen', 'Jiarui Yuan', 'Chen Qian', 'Cheng Yang', 'Zhiyuan Liu', 'Maosong Sun']	ct:Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optima shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\% tokens on tasks requiring heavy information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS (this https URL).	review	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.08115', 'html': 'https://arxiv.org/html/2410.08115v2', 'tex': '/src/2410.08115', 'doi': 'https://doi.org/10.48550/arXiv.2410.08115'}	Submission history From: Weize Chen [ view email ] [v1] Thu, 10 Oct 2024 17:00:06 UTC (3,881 KB) [v2] Tue, 18 Feb 2025 12:50:00 UTC (3,868 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.08115'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.08115'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.08115'}]
2024-10-13	ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery	Computation and Language	https://arxiv.org/abs/2410.05080	ScienceAgentBench	https://x.com/omarsar0/status/1843697964243382586		2410.05080	['Ziru Chen', 'Shijie Chen', 'Yuting Ning', 'Qianheng Zhang', 'Boshi Wang', 'Botao Yu', 'Yifei Li', 'Zeyi Liao', 'Chen Wei', 'Zitong Lu', 'Vishal Dey', 'Mingyi Xue', 'Frazier N. Baker', 'Benjamin Burns', 'Daniel Adu-Ampratwum', 'Xuhui Huang', 'Xia Ning', 'Song Gao', 'Yu Su', 'Huan Sun']	ct:The advancements of large language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using ScienceAgentBench, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI o1-preview with direct prompting and self-debug, which can boost the performance to 42.2%, demonstrating the effectiveness of increasing inference-time compute but with more than 10 times the cost of other LLMs. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.	025. 60 pages	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2410.05080', 'html': 'https://arxiv.org/html/2410.05080v3', 'tex': '/src/2410.05080', 'doi': 'https://doi.org/10.48550/arXiv.2410.05080'}	Submission history From: Ziru Chen [ view email ] [v1] Mon, 7 Oct 2024 14:33:50 UTC (1,550 KB) [v2] Thu, 24 Oct 2024 03:37:05 UTC (1,555 KB) [v3] Mon, 31 Mar 2025 14:39:44 UTC (1,558 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.05080'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.05080'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.05080'}]
2024-10-13	Addition is All You Need for Energy-efficient Language Models	Computation and Language	https://arxiv.org/abs/2410.00907	Addition Is All You Need	https://x.com/omarsar0/status/1844043652966072742		2410.00907	['Hongyin Luo', 'Wei Sun']	ct:Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.00907', 'html': 'https://arxiv.org/html/2410.00907v2', 'tex': '/src/2410.00907', 'doi': 'https://doi.org/10.48550/arXiv.2410.00907'}	Submission history From: Hongyin Luo [ view email ] [v1] Tue, 1 Oct 2024 17:53:28 UTC (316 KB) [v2] Wed, 2 Oct 2024 15:34:12 UTC (314 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.00907'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.00907'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.00907'}]
2024-10-13	I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in Multi-Agent Settings with Social Hierarchy	Computation and Language	https://arxiv.org/abs/2410.07109	Persuasion and Anti-social Ability of LLMs	https://x.com/omarsar0/status/1844427182141211054		2410.07109	['Gian Maria Campedelli', 'Nicolò Penzo', 'Massimo Stefan', 'Roberto Dessì', 'Marco Guerini', 'Bruno Lepri', 'Jacopo Staiano']	ct:As Large Language Model (LLM)-based agents become increasingly autonomous and will more freely interact with each other, studying interactions between them becomes crucial to anticipate emergent phenomena and potential risks. Drawing inspiration from the widely popular Stanford Prison Experiment, we contribute to this line of research by studying interaction patterns of LLM agents in a context characterized by strict social hierarchy. We do so by specifically studying two types of phenomena: persuasion and anti-social behavior in simulated scenarios involving a guard and a prisoner agent who seeks to achieve a specific goal (i.e., obtaining additional yard time or escape from prison). Leveraging 200 experimental scenarios for a total of 2,000 machine-machine conversations across five different popular LLMs, we provide a set of noteworthy findings. We first document how some models consistently fail in carrying out a conversation in our multi-agent setup where power dynamics are at play. Then, for the models that were able to engage in successful interactions, we empirically show how the goal that an agent is set to achieve impacts primarily its persuasiveness, while having a negligible effect with respect to the agent's anti-social behavior. Third, we highlight how agents' personas, and particularly the guard's personality, drive both the likelihood of successful persuasion from the prisoner and the emergence of anti-social behaviors. Fourth, we show that even without explicitly prompting for specific personalities, anti-social behavior emerges by simply assigning agents' roles. These results bear implications for the development of interactive LLM agents as well as the debate on their societal impact.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computers and Society (cs.CY)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2410.07109', 'html': None, 'tex': '/src/2410.07109', 'doi': 'https://doi.org/10.48550/arXiv.2410.07109'}	Submission history From: Jacopo Staiano [ view email ] [v1] Wed, 9 Oct 2024 17:45:47 UTC (2,037 KB) [v2] Wed, 16 Oct 2024 08:06:22 UTC (2,037 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.07109'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.07109'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.07109'}]
2024-10-06	Were RNNs All We Needed?	Machine Learning	https://arxiv.org/abs/2410.01201	Were RNNs All We Needed?	https://x.com/omarsar0/status/1842246985790914608		2410.01201	['Leo Feng', 'Frederick Tung', 'Mohamed Osama Ahmed', 'Yoshua Bengio', 'Hossein Hajimirsadeghi']	ct:The introduction of Transformers in 2017 reshaped the landscape of deep learning. Originally proposed for sequence modelling, Transformers have since achieved widespread success across various domains. However, the scalability limitations of Transformers - particularly with respect to sequence length - have sparked renewed interest in novel recurrent models that are parallelizable during training, offer comparable performance, and scale more effectively. In this work, we revisit sequence modelling from a historical perspective, focusing on Recurrent Neural Networks (RNNs), which dominated the field for two decades before the rise of Transformers. Specifically, we examine LSTMs (1997) and GRUs (2014). We demonstrate that by simplifying these models, we can derive minimal versions (minLSTMs and minGRUs) that (1) use fewer parameters than their traditional counterparts, (2) are fully parallelizable during training, and (3) achieve surprisingly competitive performance on a range of tasks, rivalling recent models including Transformers.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.01201', 'html': 'https://arxiv.org/html/2410.01201v3', 'tex': '/src/2410.01201', 'doi': 'https://doi.org/10.48550/arXiv.2410.01201'}	Submission history From: Leo Feng [ view email ] [v1] Wed, 2 Oct 2024 03:06:49 UTC (292 KB) [v2] Fri, 4 Oct 2024 05:01:26 UTC (289 KB) [v3] Thu, 28 Nov 2024 07:10:33 UTC (4,010 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.01201'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.01201'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.01201'}]
2024-10-06	LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations	Computation and Language	https://arxiv.org/abs/2410.02707	LLMs Know More Than They Show	https://x.com/omarsar0/status/1842240840389001381		2410.02707	['Hadas Orgad', 'Michael Toker', 'Zorik Gekhman', 'Roi Reichart', 'Idan Szpektor', 'Hadas Kotek', 'Yonatan Belinkov']	"ct:Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as ""hallucinations"". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.02707', 'html': 'https://arxiv.org/html/2410.02707v4', 'tex': '/src/2410.02707', 'doi': 'https://doi.org/10.48550/arXiv.2410.02707'}	Submission history From: Hadas Orgad [ view email ] [v1] Thu, 3 Oct 2024 17:31:31 UTC (2,525 KB) [v2] Mon, 7 Oct 2024 14:46:11 UTC (2,530 KB) [v3] Mon, 28 Oct 2024 12:33:44 UTC (2,360 KB) [v4] Sun, 18 May 2025 11:18:56 UTC (2,365 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.02707'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.02707'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.02707'}]
2024-10-06	Archon: An Architecture Search Framework for Inference-Time Techniques	Machine Learning	https://arxiv.org/abs/2409.15254	Architecture Search Framework for Inference-Time Techniques	https://x.com/Azaliamirh/status/1840892626096345530		2409.15254	['Jon Saad-Falcon', 'Adrian Gamarra Lafuente', 'Shlok Natarajan', 'Nahum Maru', 'Hristo Todorov', 'Etash Guha', 'E. Kelly Buchanan', 'Mayee Chen', 'Neel Guha', 'Christopher Ré', 'Azalia Mirhoseini']	ct:Inference-time techniques, such as repeated sampling or iterative revisions, are emerging as powerful ways to enhance large-language models (LLMs) at test time. However, best practices for developing systems that combine these techniques remain underdeveloped due to our limited understanding of the utility of each technique across models and tasks, the interactions between them, and the massive search space for combining them. To address these challenges, we introduce Archon, a modular and automated framework for optimizing the process of selecting and combining inference-time techniques and LLMs. Given a compute budget and a set of available LLMs, Archon explores a large design space to discover optimized configurations tailored to target benchmarks. It can design custom or general-purpose architectures that advance the Pareto frontier of accuracy vs. maximum token budget compared to top-performing baselines. Across instruction-following, reasoning, and coding tasks, we show that Archon can leverage additional inference compute budget to design systems that outperform frontier models such as OpenAI's o1, GPT-4o, and Claude 3.5 Sonnet by an average of 15.1%.	ational Conference on Machine Learning (ICML) 2025	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.15254', 'html': 'https://arxiv.org/html/2409.15254v6', 'tex': '/src/2409.15254', 'doi': 'https://doi.org/10.48550/arXiv.2409.15254'}	Submission history From: Jon Saad-Falcon [ view email ] [v1] Mon, 23 Sep 2024 17:53:42 UTC (1,500 KB) [v2] Tue, 24 Sep 2024 05:08:18 UTC (1,500 KB) [v3] Thu, 26 Sep 2024 08:01:39 UTC (1,410 KB) [v4] Fri, 27 Sep 2024 21:39:30 UTC (1,410 KB) [v5] Thu, 3 Oct 2024 05:41:48 UTC (1,321 KB) [v6] Tue, 10 Jun 2025 21:52:15 UTC (2,101 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.15254'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.15254'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.15254'}]
2024-10-06	RATIONALYST: Mining Implicit Rationales for Process Supervision of Reasoning	Artificial Intelligence	https://arxiv.org/abs/2410.01044	RATIONALYST			2410.01044	['Dongwei Jiang', 'Guoxuan Wang', 'Yining Lu', 'Andrew Wang', 'Jingyu Zhang', 'Chuyu Liu', 'Benjamin Van Durme', 'Daniel Khashabi']	ct:The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets.	de, data, and model can be found at this repository:this https URL	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2410.01044', 'html': 'https://arxiv.org/html/2410.01044v2', 'tex': '/src/2410.01044', 'doi': 'https://doi.org/10.48550/arXiv.2410.01044'}	Submission history From: Dongwei Jiang [ view email ] [v1] Tue, 1 Oct 2024 20:05:51 UTC (3,158 KB) [v2] Sat, 14 Jun 2025 04:29:00 UTC (1,009 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.01044'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.01044'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.01044'}]
2024-10-06	When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1	Computation and Language	https://arxiv.org/abs/2410.01792	An Analysis of o1-preview	https://x.com/omarsar0/status/1841842414157472240		2410.01792	['R. Thomas McCoy', 'Shunyu Yao', 'Dan Friedman', 'Mathew D. Hardy', 'Thomas L. Griffiths']	"ct:In ""Embers of Autoregression"" (McCoy et al., 2023), we showed that several large language models (LLMs) have some important limitations that are attributable to their origins in next-word prediction. Here we investigate whether these issues persist with o1, a new system from OpenAI that differs from previous LLMs in that it is optimized for reasoning. We find that o1 substantially outperforms previous LLMs in many cases, with particularly large improvements on rare variants of common tasks (e.g., forming acronyms from the second letter of each word in a list, rather than the first letter). Despite these quantitative improvements, however, o1 still displays the same qualitative trends that we observed in previous systems. Specifically, o1 -- like previous LLMs -- is sensitive to the probability of examples and tasks, performing better and requiring fewer ""thinking tokens"" in high-probability settings than in low-probability ones. These results show that optimizing a language model for reasoning can mitigate but might not fully overcome the language model's probability sensitivity."	s; updated to fix typo in Fig 4 caption	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2410.01792', 'html': 'https://arxiv.org/html/2410.01792v2', 'tex': '/src/2410.01792', 'doi': 'https://doi.org/10.48550/arXiv.2410.01792'}	Submission history From: Tom McCoy [ view email ] [v1] Wed, 2 Oct 2024 17:50:19 UTC (74 KB) [v2] Fri, 4 Oct 2024 03:57:33 UTC (74 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.01792'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.01792'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.01792'}]
2024-10-06	Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation	Computation and Language	https://arxiv.org/abs/2409.12941	FRAMES	https://x.com/_philschmid/status/1840628834275602585		2409.12941	['Satyapriya Krishna', 'Kalpesh Krishna', 'Anhad Mohananey', 'Steven Schwarcz', 'Adam Stambler', 'Shyam Upadhyay', 'Manaal Faruqui']	ct:Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.	 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), 2025	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.12941', 'html': 'https://arxiv.org/html/2409.12941v3', 'tex': '/src/2409.12941', 'doi': 'https://doi.org/10.48550/arXiv.2409.12941'}	Submission history From: Satyapriya Krishna [ view email ] [v1] Thu, 19 Sep 2024 17:52:07 UTC (726 KB) [v2] Fri, 18 Oct 2024 21:37:34 UTC (1,161 KB) [v3] Fri, 24 Jan 2025 19:23:15 UTC (1,161 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.12941'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.12941'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.12941'}]
2024-10-06	Not All LLM Reasoners Are Created Equal	Machine Learning	https://arxiv.org/abs/2410.01748	Not All LLM Reasoners Are Created Equal	https://x.com/arianTBD/status/1841875515860517130		2410.01748	['Arian Hosseini', 'Alessandro Sordoni', 'Daniel Toyama', 'Aaron Courville', 'Rishabh Agarwal']	ct:We study the depth of grade-school math (GSM) problem-solving capabilities of LLMs. To this end, we evaluate their performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. Our findings reveal a significant reasoning gap in most LLMs, that is performance difference between solving the compositional pairs and solving each question independently. This gap is more pronounced in smaller, more cost-efficient, and math-specialized models. Moreover, instruction-tuning recipes and code generation have varying effects across LLM sizes, while finetuning on GSM can lead to task overfitting. Our analysis indicates that large reasoning gaps are not because of test-set leakage, but due to distraction from additional context and poor second-hop reasoning. Overall, LLMs exhibit systematic differences in their reasoning abilities, despite what their performance on standard benchmarks indicates.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2410.01748', 'html': 'https://arxiv.org/html/2410.01748v1', 'tex': '/src/2410.01748', 'doi': 'https://doi.org/10.48550/arXiv.2410.01748'}	Submission history From: Arian Hosseini [ view email ] [v1] Wed, 2 Oct 2024 17:01:10 UTC (307 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2410.01748'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2410.01748'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2410.01748'}]
2024-10-06	Evaluation of OpenAI o1: Opportunities and Challenges of AGI	Computation and Language	https://arxiv.org/abs/2409.18486	Evaluation of o1	https://x.com/omarsar0/status/1840953712635732006		2409.18486	['Tianyang Zhong', 'Zhengliang Liu', 'Yi Pan', 'Yutong Zhang', 'Yifan Zhou', 'Shizhe Liang', 'Zihao Wu', 'Yanjun Lyu', 'Peng Shu', 'Xiaowei Yu', 'Chao Cao', 'Hanqi Jiang', 'Hanxu Chen', 'Yiwei Li', 'Junhao Chen', 'Huawen Hu', 'Yiheng Liu', 'Huaqin Zhao', 'Shaochen Xu', 'Haixing Dai', 'Lin Zhao', 'Ruidong Zhang', 'Wei Zhao', 'Zhenyuan Yang', 'Jingyuan Chen', 'Peilong Wang', 'Wei Ruan', 'Hui Wang', 'Huan Zhao', 'Jing Zhang', 'Yiming Ren', 'Shihuan Qin', 'Tong Chen', 'Jiaxi Li', 'Arif Hassan Zidan', 'Afrar Jahin', 'Minheng Chen', 'Sichen Xia', 'Jason Holmes', 'Yan Zhuang', 'Jiaqi Wang', 'Bochen Xu', 'Weiran Xia', 'Jichao Yu', 'Kaibo Tang', 'Yaxuan Yang', 'Bolun Sun', 'Tao Yang', 'Guoyu Lu', 'Xianqiao Wang', 'Lilong Chai', 'He Li', 'Jin Lu', 'Xin Zhang', 'Bao Ge', 'Xintao Hu', 'Lian Zhang', 'Hua Zhou', 'Lu Zhang', 'Shu Zhang', 'Zhen Xiang', 'Yudan Ren', 'Jun Liu', 'Xi Jiang', 'Yu Bao', 'Wei Zhang', 'Xiang Li', 'Gang Li', 'Wei Liu', 'Dinggang Shen', 'Andrea Sikora', 'Xiaoming Zhai', 'Dajiang Zhu', 'Tuo Zhang', 'Tianming Liu']	ct:This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include:-83.3% success rate in solving complex competitive programming problems, surpassing many human experts.-Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models.-100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions.-Advanced natural language inference capabilities across general and specialized domains like medicine.-Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis.-Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields.-Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills.-Effective performance in social media analysis, including sentiment analysis and emotion recognition.The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.18486', 'html': None, 'tex': '/src/2409.18486', 'doi': 'https://doi.org/10.48550/arXiv.2409.18486'}	Submission history From: Zhengliang Liu [ view email ] [v1] Fri, 27 Sep 2024 06:57:00 UTC (15,119 KB) [v2] Mon, 7 Jul 2025 18:37:32 UTC (11,096 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.18486'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.18486'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.18486'}]
2024-10-06	Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2409.17439	Designing Priors for Better Few-Shot Image Synthesis	https://x.com/KL_Div/status/1841729946302943295		2409.17439	['Chirag Vashist', 'Shichong Peng', 'Ke Li']	ct:An emerging area of research aims to learn deep generative models with limited training data. Prior generative models like GANs and diffusion models require a lot of data to perform well, and their performance degrades when they are trained on only a small amount of data. A recent technique called Implicit Maximum Likelihood Estimation (IMLE) has been adapted to the few-shot setting, achieving state-of-the-art performance. However, current IMLE-based approaches encounter challenges due to inadequate correspondence between the latent codes selected for training and those drawn during inference. This results in suboptimal test-time performance. We theoretically show a way to address this issue and propose RS-IMLE, a novel approach that changes the prior distribution used for training. This leads to substantially higher quality image generation compared to existing GAN and IMLE-based methods, as validated by comprehensive experiments conducted on nine few-shot image datasets.		['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.17439', 'html': 'https://arxiv.org/html/2409.17439v1', 'tex': '/src/2409.17439', 'doi': 'https://doi.org/10.48550/arXiv.2409.17439'}	Submission history From: Chirag Vashist [ view email ] [v1] Thu, 26 Sep 2024 00:19:42 UTC (25,792 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.17439'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.17439'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.17439'}]
2024-09-29	LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench	Artificial Intelligence	https://arxiv.org/abs/2409.13373	LLMs Still Can’t Plan	https://twitter.com/johnxschulman/status/1657558270450917378		2409.13373	['Karthik Valmeekam', 'Kaya Stechly', 'Subbarao Kambhampati']	ct:The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities. PlanBench, an extensible benchmark we developed in 2022, soon after the release of GPT3, has remained an important tool for evaluating the planning abilities of LLMs. Despite the slew of new private and open source LLMs since GPT3, progress on this benchmark has been surprisingly slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs--making it a new kind of model: a Large Reasoning Model (LRM). Using this development as a catalyst, this paper takes a comprehensive look at how well current LLMs and new LRMs do on PlanBench. As we shall see, while o1's performance is a quantum improvement on the benchmark, outpacing the competition, it is still far from saturating it. This improvement also brings to the fore questions about accuracy, efficiency, and guarantees which must be considered before deploying such systems.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.13373', 'html': 'https://arxiv.org/html/2409.13373v1', 'tex': '/src/2409.13373', 'doi': 'https://doi.org/10.48550/arXiv.2409.13373'}	Submission history From: Karthik Valmeekam [ view email ] [v1] Fri, 20 Sep 2024 10:20:46 UTC (3,153 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.13373'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.13373'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.13373'}]
2024-09-29	Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models	Computation and Language	https://arxiv.org/abs/2409.17539	Logic-of-Thought	https://twitter.com/IsItPerplexity/status/1704255260019798052		2409.17539	['Tongxuan Liu', 'Wenjiang Xu', 'Weizhe Huang', 'Yuting Zeng', 'Jiaxing Wang', 'Xingyu Wang', 'Hailong Yang', 'Jing Li']	ct:Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information descriptions and utilizes them as an additional augmentation to original contexts, thereby ensuring information completeness and enhancing logical reasoning ability. LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%, improves Chain-of-Thought with Self-Consistency's performance on the RuleTaker dataset by +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter dataset by +8%.	es, Accepted to NAACL2025	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.17539', 'html': 'https://arxiv.org/html/2409.17539v2', 'tex': '/src/2409.17539', 'doi': 'https://doi.org/10.48550/arXiv.2409.17539'}	Submission history From: Weizhe Huang [ view email ] [v1] Thu, 26 Sep 2024 04:59:45 UTC (1,507 KB) [v2] Fri, 7 Feb 2025 01:16:53 UTC (1,548 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.17539'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.17539'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.17539'}]
2024-09-29	Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely	Computation and Language	https://arxiv.org/abs/2409.14924	RAG and Beyond	https://twitter.com/mishigna/status/1703461946958463118		2409.14924	['Siyun Zhao', 'Yuqing Yang', 'Zilong Wang', 'Zhiyuan He', 'Luna K. Qiu', 'Lili Qiu']	ct:Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2409.14924', 'html': 'https://arxiv.org/html/2409.14924v1', 'tex': '/src/2409.14924', 'doi': 'https://doi.org/10.48550/arXiv.2409.14924'}	Submission history From: Siyun Zhao [ view email ] [v1] Mon, 23 Sep 2024 11:20:20 UTC (2,086 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.14924'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.14924'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.14924'}]
2024-09-29	A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?	Computation and Language	https://arxiv.org/abs/2409.15277	A Preliminary Study of o1 in Medicine	https://twitter.com/RichardEvans_AI/status/1691963090436067397		2409.15277	['Yunfei Xie', 'Juncheng Wu', 'Haoqin Tu', 'Siwei Yang', 'Bingchen Zhao', 'Yongshuo Zong', 'Qiao Jin', 'Cihang Xie', 'Yuyin Zhou']	ct:Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. We release our raw data and model outputs atthis https URLfor future research.	rst four authors contributed equally, project page available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2409.15277', 'html': 'https://arxiv.org/html/2409.15277v1', 'tex': '/src/2409.15277', 'doi': 'https://doi.org/10.48550/arXiv.2409.15277'}	Submission history From: Bingchen Zhao [ view email ] [v1] Mon, 23 Sep 2024 17:59:43 UTC (1,358 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.15277'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.15277'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.15277'}]
2024-09-29	Small Language Models: Survey, Measurements, and Insights	Computation and Language	https://arxiv.org/abs/2409.15790	Small Language Models Survey	https://twitter.com/sebatian_ruder/status/1691611318636159002		2409.15790	['Zhenyan Lu', 'Xiang Li', 'Dongqi Cai', 'Rongjie Yi', 'Fangming Liu', 'Xiwen Zhang', 'Nicholas D. Lane', 'Mengwei Xu']	ct:Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data centers and cloud environments. While researchers continue to improve the capabilities of LLMs in the pursuit of artificial general intelligence, SLM research aims to make machine intelligence more accessible, affordable, and efficient for everyday tasks. Focusing on transformer-based, decoder-only language models with 100M-5B parameters, we survey 70 state-of-the-art open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms. In addition, we evaluate their capabilities in various domains, including commonsense reasoning, mathematics, in-context learning, and long context. To gain further insight into their on-device runtime costs, we benchmark their inference latency and memory footprints. Through in-depth analysis of our benchmarking data, we offer valuable insights to advance research in this field.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.15790', 'html': 'https://arxiv.org/html/2409.15790v3', 'tex': '/src/2409.15790', 'doi': 'https://doi.org/10.48550/arXiv.2409.15790'}	Submission history From: Zhenyan Lu [ view email ] [v1] Tue, 24 Sep 2024 06:36:56 UTC (11,763 KB) [v2] Tue, 25 Feb 2025 13:48:03 UTC (16,141 KB) [v3] Wed, 26 Feb 2025 06:34:55 UTC (16,141 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.15790'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.15790'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.15790'}]
2024-09-29	Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts	Computation and Language	https://arxiv.org/abs/2409.13449	Minstrel	https://twitter.com/LiZhang1351/status/1702992849091985677		2409.13449	['Ming Wang', 'Yuanzhong Liu', 'Xiaoyu Liang', 'Yijie Huang', 'Daling Wang', 'Xiaocui Yang', 'Sijia Shen', 'Shi Feng', 'Xiaoming Zhang', 'Chaofeng Guan', 'Yifei Zhang']	ct:LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to assist them in their work poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structural design, incurring high learning costs and it is not conducive to the iterative updating of prompts, especially for non-AI experts. Inspired by structured reusable programming languages, we propose LangGPT, a structural prompt design framework. Furthermore, we introduce Minstrel, a multi-generative agent system with reflection to automate the generation of structural prompts. Experiments and the case study illustrate that structural prompts generated by Minstrel or written manually significantly enhance the performance of LLMs. Furthermore, we analyze the ease of use of structural prompts through a user survey in our online community.	admin note: text overlap witharXiv:2402.16929	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.13449', 'html': 'https://arxiv.org/html/2409.13449v1', 'tex': '/src/2409.13449', 'doi': 'https://doi.org/10.48550/arXiv.2409.13449'}	Submission history From: Ming Wang [ view email ] [v1] Fri, 20 Sep 2024 12:30:03 UTC (6,167 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.13449'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.13449'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.13449'}]
2024-09-22	Training Language Models to Self-Correct via Reinforcement Learning	Machine Learning	https://arxiv.org/abs/2409.12917	Training LLMs to Self-Correct via RL	https://x.com/omarsar0/status/1837228446839361984		2409.12917	['Aviral Kumar', 'Vincent Zhuang', 'Rishabh Agarwal', 'Yi Su', 'John D Co-Reyes', 'Avi Singh', 'Kate Baumli', 'Shariq Iqbal', 'Colton Bishop', 'Rebecca Roelofs', 'Lei M Zhang', 'Kay McKinney', 'Disha Shrivastava', 'Cosmin Paduraru', 'George Tucker', 'Doina Precup', 'Feryal Behbahani', 'Aleksandra Faust']	ct:Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are often insufficient for instilling self-correction behavior. In particular, we observe that training via SFT falls prey to either a distribution mismatch between mistakes made by the data-collection policy and the model's own responses, or to behavior collapse, where learning implicitly prefers only a certain mode of correction behavior that is often not effective at self-correction on test problems. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction behavior that is effective at test time as opposed to fitting high-reward responses for a given prompt. This regularization process includes an initial phase of multi-turn RL on a base model to generate a policy initialization that is less susceptible to collapse, followed by using a reward bonus to amplify self-correction. With Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.12917', 'html': None, 'tex': '/src/2409.12917', 'doi': 'https://doi.org/10.48550/arXiv.2409.12917'}	Submission history From: Vincent Zhuang [ view email ] [v1] Thu, 19 Sep 2024 17:16:21 UTC (633 KB) [v2] Fri, 4 Oct 2024 17:28:45 UTC (604 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.12917'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.12917'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.12917'}]
2024-09-22	Qwen2.5-Coder Technical Report	Computation and Language	https://arxiv.org/abs/2409.12186	Qwen2.5 Coder	https://x.com/huybery/status/1837170643563073960		2409.12186	['Binyuan Hui', 'Jian Yang', 'Zeyu Cui', 'Jiaxi Yang', 'Dayiheng Liu', 'Lei Zhang', 'Tianyu Liu', 'Jiajun Zhang', 'Bowen Yu', 'Keming Lu', 'Kai Dang', 'Yang Fan', 'Yichang Zhang', 'An Yang', 'Rui Men', 'Fei Huang', 'Bo Zheng', 'Yibo Miao', 'Shanghaoran Quan', 'Yunlong Feng', 'Xingzhang Ren', 'Xuancheng Ren', 'Jingren Zhou', 'Junyang Lin']	ct:In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.12186', 'html': 'https://arxiv.org/html/2409.12186v3', 'tex': '/src/2409.12186', 'doi': 'https://doi.org/10.48550/arXiv.2409.12186'}	Submission history From: Binyuan Hui [ view email ] [v1] Wed, 18 Sep 2024 17:57:57 UTC (1,243 KB) [v2] Mon, 11 Nov 2024 17:55:09 UTC (1,824 KB) [v3] Tue, 12 Nov 2024 13:24:25 UTC (1,824 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.12186'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.12186'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.12186'}]
2024-09-22	On the Diagram of Thought	Computation and Language	https://arxiv.org/abs/2409.10038	Diagram of Thought (DoT)	https://x.com/omarsar0/status/1835882277563179512		2409.10038	['Yifan Zhang', 'Yang Yuan', 'Andrew Chi-Chih Yao']	ct:Current large language models (LLMs) demonstrate impressive capabilities but struggle with complex, multi-step reasoning tasks. Existing methods often tackle this by requiring external control mechanisms or multi-model orchestration, which introduces system complexity and typically lacks formal guarantees of reasoning soundness. We introduce the Diagram of Thought (DoT), a framework wherein a single auto-regressive LLM internally constructs and navigates a Directed Acyclic Graph (DAG). This DAG represents the iterative reasoning process, encompassing steps like proposing ideas, critiquing them, refining based on feedback, and synthesizing conclusions. This self-orchestrated, self-contained process is guided by learned role-specific tokens (e.g., <proposer>, <critic>, <summarizer>) embedded within the standard generation loop, thereby eliminating external dependencies. Crucially, we establish a rigorous mathematical foundation for DoT using Topos Theory. We formalize the reasoning DAG as a diagram within a suitable topos and prove that the final synthesis step, aggregating validated information, corresponds semantically to computing the colimit of the relevant sub-diagram. This formalization provides theoretical guarantees concerning the logical consistency and robustness of the synthesized outcome. DoT thus offers a unified, self-contained, interpretable, efficient, and formally grounded approach designed to significantly advance the complex reasoning capabilities of LLMs.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.10038', 'html': 'https://arxiv.org/html/2409.10038v3', 'tex': '/src/2409.10038', 'doi': 'https://doi.org/10.48550/arXiv.2409.10038'}	Submission history From: Yifan Zhang [ view email ] [v1] Mon, 16 Sep 2024 07:01:41 UTC (345 KB) [v2] Thu, 13 Mar 2025 22:13:06 UTC (329 KB) [v3] Sun, 30 Mar 2025 23:31:29 UTC (343 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.10038'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.10038'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.10038'}]
2024-09-22	Agents in Software Engineering: Survey, Landscape, and Vision	Software Engineering	https://arxiv.org/abs/2409.09030	Agents in Software Engineering	https://x.com/omarsar0/status/1835705359723319702		2409.09030	['Yanlin Wang', 'Wanjun Zhong', 'Yanxian Huang', 'Ensheng Shi', 'Min Yang', 'Jiachi Chen', 'Hui Li', 'Yuchi Ma', 'Qianxiang Wang', 'Zibin Zheng']	ct:In recent years, Large Language Models (LLMs) have achieved remarkable success and have been widely used in various downstream tasks, especially in the tasks of the software engineering (SE) field. We find that many studies combining LLMs with SE have employed the concept of agents either explicitly or implicitly. However, there is a lack of an in-depth survey to sort out the development context of existing works, analyze how existing works combine the LLM-based agent technologies to optimize various tasks, and clarify the framework of LLM-based agents in SE. In this paper, we conduct the first survey of the studies on combining LLM-based agents with SE and present a framework of LLM-based agents in SE which includes three key modules: perception, memory, and action. We also summarize the current challenges in combining the two fields and propose future opportunities in response to existing challenges. We maintain a GitHub repository of the related papers at:this https URL.	es, 4 figures	['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.09030', 'html': 'https://arxiv.org/html/2409.09030v2', 'tex': '/src/2409.09030', 'doi': 'https://doi.org/10.48550/arXiv.2409.09030'}	Submission history From: Yanxian Huang [ view email ] [v1] Fri, 13 Sep 2024 17:55:58 UTC (295 KB) [v2] Mon, 23 Sep 2024 17:55:07 UTC (295 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.09030'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.09030'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.09030'}]
2024-09-22	To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning	Computation and Language	https://arxiv.org/abs/2409.12183	To CoT or not to CoT?	https://x.com/omarsar0/status/1836599280477299013		2409.12183	['Zayne Sprague', 'Fangcong Yin', 'Juan Diego Rodriguez', 'Dongwei Jiang', 'Manya Wadhwa', 'Prasann Singhal', 'Xinyu Zhao', 'Xi Ye', 'Kyle Mahowald', 'Greg Durrett']	ct:Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.	hed at ICLR 2025	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.12183', 'html': 'https://arxiv.org/html/2409.12183v3', 'tex': '/src/2409.12183', 'doi': 'https://doi.org/10.48550/arXiv.2409.12183'}	Submission history From: Zayne Sprague [ view email ] [v1] Wed, 18 Sep 2024 17:55:00 UTC (8,093 KB) [v2] Tue, 29 Oct 2024 01:19:28 UTC (10,261 KB) [v3] Wed, 7 May 2025 18:00:45 UTC (10,262 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.12183'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.12183'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.12183'}]
2024-09-22	Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant	Computation and Language	https://arxiv.org/abs/2409.11055	A Comprehensive Evaluation of Quantized Instruction-Tuned LLMs	https://arxiv.org/abs/2409.11055		2409.11055	['Jemin Lee', 'Sihyeong Park', 'Jinse Kwon', 'Jihun Oh', 'Yongin Kwon']	ct:Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model's inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning.	ed in IJCAI 2025, 21 pages, 2 figure	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2409.11055', 'html': None, 'tex': '/src/2409.11055', 'doi': 'https://doi.org/10.48550/arXiv.2409.11055'}	Submission history From: Jemin Lee [ view email ] [v1] Tue, 17 Sep 2024 10:31:37 UTC (1,545 KB) [v2] Thu, 17 Apr 2025 01:37:50 UTC (2,917 KB) [v3] Thu, 8 May 2025 08:51:19 UTC (2,917 KB) [v4] Mon, 12 May 2025 02:25:24 UTC (2,917 KB) [v5] Mon, 19 May 2025 06:07:08 UTC (2,917 KB) [v6] Wed, 4 Jun 2025 01:15:49 UTC (3,989 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.11055'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.11055'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.11055'}]
2024-09-22	Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning	Computation and Language	https://arxiv.org/abs/2409.12618	Iteration of Thought	https://x.com/omarsar0/status/1836977595847692671		2409.12618	['Santosh Kumar Radha', 'Yasamin Nouri Jelyani', 'Ara Ghukasyan', 'Oktay Goktas']	"ct:Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating ""thought""-provoking prompts vis a vis an input query and the current iteration of an LLM's response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2409.12618', 'html': None, 'tex': '/src/2409.12618', 'doi': 'https://doi.org/10.48550/arXiv.2409.12618'}	Submission history From: Santosh Kumar Radha [ view email ] [v1] Thu, 19 Sep 2024 09:44:17 UTC (860 KB) [v2] Tue, 1 Oct 2024 17:50:25 UTC (860 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.12618'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.12618'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.12618'}]
2024-09-22	Schrodinger's Memory: Large Language Models	Computation and Language	https://arxiv.org/abs/2409.10482	Schrodinger’s Memory	https://x.com/omarsar0/status/1835882330323554321		2409.10482	['Wei Wang', 'Qing Li']	ct:Memory is the foundation of all human activities; without memory, it would be nearly impossible for people to perform any task in daily life. With the development of Large Language Models (LLMs), their language capabilities are becoming increasingly comparable to those of humans. But do LLMs have memory? Based on current performance, LLMs do appear to exhibit memory. So, what is the underlying mechanism of this memory? Previous research has lacked a deep exploration of LLMs' memory capabilities and the underlying theory. In this paper, we use Universal Approximation Theorem (UAT) to explain the memory mechanism in LLMs. We also conduct experiments to verify the memory capabilities of various LLMs, proposing a new method to assess their abilities based on these memory ability. We argue that LLM memory operates like Schrödinger's memory, meaning that it only becomes observable when a specific memory is queried. We can only determine if the model retains a memory based on its output in response to the query; otherwise, it remains indeterminate. Finally, we expand on this concept by comparing the memory capabilities of the human brain and LLMs, highlighting the similarities and differences in their operational mechanisms.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.10482', 'html': None, 'tex': '/src/2409.10482', 'doi': 'https://doi.org/10.48550/arXiv.2409.10482'}	Submission history From: Wei Wang [ view email ] [v1] Mon, 16 Sep 2024 17:18:11 UTC (316 KB) [v2] Tue, 17 Sep 2024 12:10:49 UTC (333 KB) [v3] Fri, 27 Sep 2024 12:12:19 UTC (353 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.10482'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.10482'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.10482'}]
2024-09-22	Jailbreaking Large Language Models with Symbolic Mathematics	Cryptography and Security	https://arxiv.org/abs/2409.11445	Math Jailbreaking Prompts	https://x.com/omarsar0/status/1836603922405806501		2409.11445	['Emet Bethany', 'Mazal Bethany', 'Juan Arturo Nolazco Flores', 'Sumit Kumar Jha', 'Peyman Najafirad']	ct:Recent advancements in AI safety have led to increased efforts in training and red-teaming large language models (LLMs) to mitigate unsafe content generation. However, these safety mechanisms may not be comprehensive, leaving potential vulnerabilities unexplored. This paper introduces MathPrompt, a novel jailbreaking technique that exploits LLMs' advanced capabilities in symbolic mathematics to bypass their safety mechanisms. By encoding harmful natural language prompts into mathematical problems, we demonstrate a critical vulnerability in current AI safety measures. Our experiments across 13 state-of-the-art LLMs reveal an average attack success rate of 73.6\%, highlighting the inability of existing safety training mechanisms to generalize to mathematically encoded inputs. Analysis of embedding vectors shows a substantial semantic shift between original and encoded prompts, helping explain the attack's success. This work emphasizes the importance of a holistic approach to AI safety, calling for expanded red-teaming efforts to develop robust safeguards across all potential input types and their associated risks.		['Cryptography and Security (cs.CR)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.11445', 'html': 'https://arxiv.org/html/2409.11445v2', 'tex': '/src/2409.11445', 'doi': 'https://doi.org/10.48550/arXiv.2409.11445'}	Submission history From: Emet Bethany [ view email ] [v1] Tue, 17 Sep 2024 03:39:45 UTC (492 KB) [v2] Tue, 5 Nov 2024 08:46:01 UTC (503 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.11445'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.11445'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.11445'}]
2024-09-15	Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers	Computation and Language	https://arxiv.org/abs/2409.04109	Can LLMs Generation Novel Research Ideas	https://x.com/ChengleiSi/status/1833166031134806330		2409.04109	['Chenglei Si', 'Diyi Yang', 'Tatsunori Hashimoto']	ct:Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.	aper is 20 pages	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computers and Society (cs.CY)', 'Human-Computer Interaction (cs.HC)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.04109', 'html': 'https://arxiv.org/html/2409.04109v1', 'tex': '/src/2409.04109', 'doi': 'https://doi.org/10.48550/arXiv.2409.04109'}	Submission history From: Chenglei Si [ view email ] [v1] Fri, 6 Sep 2024 08:25:03 UTC (382 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.04109'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.04109'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.04109'}]
2024-09-15	Agent Workflow Memory	Computation and Language	https://arxiv.org/abs/2409.07429	Agent Workflow Memory	https://x.com/omarsar0/status/1834059522198896706		2409.07429	['Zora Zhiruo Wang', 'Jiayuan Mao', 'Daniel Fried', 'Graham Neubig']	ct:Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, we introduce Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. We experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.07429', 'html': 'https://arxiv.org/html/2409.07429v1', 'tex': '/src/2409.07429', 'doi': 'https://doi.org/10.48550/arXiv.2409.07429'}	Submission history From: Zhiruo Wang [ view email ] [v1] Wed, 11 Sep 2024 17:21:00 UTC (624 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.07429'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.07429'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.07429'}]
2024-09-15	What is the Role of Small Models in the LLM Era: A Survey	Computation and Language	https://arxiv.org/abs/2409.06857	The Role of Small Language Models in the LLM Era	https://x.com/omarsar0/status/1834063138586829273		2409.06857	['Lihu Chen', 'Gaël Varoquaux']	ct:Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available atthis https URL	ey paper of small models	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.06857', 'html': None, 'tex': '/src/2409.06857', 'doi': 'https://doi.org/10.48550/arXiv.2409.06857'}	Submission history From: Lihu Chen [ view email ] [v1] Tue, 10 Sep 2024 20:45:43 UTC (111 KB) [v2] Thu, 12 Sep 2024 15:04:57 UTC (112 KB) [v3] Mon, 30 Sep 2024 10:43:53 UTC (112 KB) [v4] Fri, 20 Dec 2024 18:11:41 UTC (112 KB) [v5] Tue, 15 Apr 2025 13:38:08 UTC (112 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.06857'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.06857'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.06857'}]
2024-09-15	LLaMA-Omni: Seamless Speech Interaction with Large Language Models	Computation and Language	https://arxiv.org/abs/2409.06666	LLaMa-Omni	https://x.com/omarsar0/status/1834227729241440340		2409.06666	['Qingkai Fang', 'Shoutao Guo', 'Yan Zhou', 'Zhengrui Ma', 'Shaolei Zhang', 'Yang Feng']	ct:Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.	025	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Sound (cs.SD)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2409.06666', 'html': 'https://arxiv.org/html/2409.06666v2', 'tex': '/src/2409.06666', 'doi': 'https://doi.org/10.48550/arXiv.2409.06666'}	Submission history From: Qingkai Fang [ view email ] [v1] Tue, 10 Sep 2024 17:34:34 UTC (2,444 KB) [v2] Sat, 1 Mar 2025 12:59:49 UTC (2,522 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.06666'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.06666'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.06666'}]
2024-09-15	Can Large Language Models Unlock Novel Scientific Research Ideas?	Computation and Language	https://arxiv.org/abs/2409.06185	Can LLMs Unlock Novel Scientific Research Ideas	https://x.com/omarsar0/status/1833695968656793610		2409.06185	['Sandeep Kumar', 'Tirthankar Ghosal', 'Vinayak Goyal', 'Asif Ekbal']	"ct:""An idea is nothing more nor less than a new combination of old elements"" (Young, J.W.). The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study explores the capability of LLMs in generating novel research ideas based on information from research papers. We conduct a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). We found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini. We also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. We make our datasets and codes publicly available."	es, 12 figures, 6 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computers and Society (cs.CY)', 'Human-Computer Interaction (cs.HC)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.06185', 'html': 'https://arxiv.org/html/2409.06185v1', 'tex': '/src/2409.06185', 'doi': 'https://doi.org/10.48550/arXiv.2409.06185'}	Submission history From: Sandeep Kumar [ view email ] [v1] Tue, 10 Sep 2024 03:26:42 UTC (1,830 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.06185'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.06185'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.06185'}]
2024-09-15	Theory, Analysis, and Best Practices for Sigmoid Self-Attention	Machine Learning	https://arxiv.org/abs/2409.04431	Theory, Analysis, and Best Practices for Sigmoid Self-Attention	https://x.com/omarsar0/status/1833522827842220244		2409.04431	['Jason Ramapuram', 'Federico Danieli', 'Eeshan Dhekane', 'Floris Weers', 'Dan Busbridge', 'Pierre Ablin', 'Tatiana Likhomanenko', 'Jagrit Digani', 'Zijin Gu', 'Amitis Shidani', 'Russ Webb']	ct:Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.04431', 'html': None, 'tex': '/src/2409.04431', 'doi': 'https://doi.org/10.48550/arXiv.2409.04431'}	Submission history From: Jason Ramapuram [ view email ] [v1] Fri, 6 Sep 2024 17:53:26 UTC (6,325 KB) [v2] Wed, 22 Jan 2025 01:18:51 UTC (6,523 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.04431'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.04431'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.04431'}]
2024-09-15	Achieving Peak Performance for Large Language Models: A Systematic Review	Computation and Language	https://arxiv.org/abs/2409.04833	Achieving Peak Performance for LLMs	https://x.com/omarsar0/status/1833344402892460364		2409.04833	['Zhyar Rzgar K Rostam', 'Sándor Szénási', 'Gábor Kertész']	ct:In recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.	es, 7 figures, 8 tables. Journal Article: IEEE Access	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2409.04833', 'html': 'https://arxiv.org/html/2409.04833v1', 'tex': '/src/2409.04833', 'doi': 'https://doi.org/10.48550/arXiv.2409.04833'}	Submission history From: Zhyar Kwekha Rostam Mr. [ view email ] [v1] Sat, 7 Sep 2024 13:57:41 UTC (10,655 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.04833'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.04833'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.04833'}]
2024-09-08	In Defense of RAG in the Era of Long-Context Language Models	Computation and Language	https://arxiv.org/abs/2409.01666	RAG in the Era of Long-Context LLMs	https://x.com/omarsar0/status/1831389521839267888		2409.01666	['Tan Yu', 'Anbang Xu', 'Rama Akkiraju']	ct:Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.01666', 'html': 'https://arxiv.org/html/2409.01666v1', 'tex': '/src/2409.01666', 'doi': 'https://doi.org/10.48550/arXiv.2409.01666'}	Submission history From: Tan Yu [ view email ] [v1] Tue, 3 Sep 2024 07:17:41 UTC (343 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.01666'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.01666'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.01666'}]
2024-09-08	Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation	Artificial Intelligence	https://arxiv.org/abs/2409.03271v1	Strategic Chain-of-Thought			2409.03271v1	['Yu Wang', 'Shiwan Zhao', 'Zhihu Wang', 'Heyuan Huang', 'Ming Fan', 'Yubo Zhang', 'Zhixing Wang', 'Haijun Wang', 'Ting Liu']	ct:The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for enhancing the reasoning capabilities of large language models (LLMs). However, despite their widespread adoption and success, CoT methods often exhibit instability due to their inability to consistently ensure the quality of generated reasoning paths, leading to sub-optimal reasoning performance. To address this challenge, we propose the \textbf{Strategic Chain-of-Thought} (SCoT), a novel methodology designed to refine LLM performance by integrating strategic knowledge prior to generating intermediate reasoning steps. SCoT employs a two-stage approach within a single prompt: first eliciting an effective problem-solving strategy, which is then used to guide the generation of high-quality CoT paths and final answers. Our experiments across eight challenging reasoning datasets demonstrate significant improvements, including a 21.05\% increase on the GSM8K dataset and 24.13\% on the Tracking\_Objects dataset, respectively, using the Llama3-8b model. Additionally, we extend the SCoT framework to develop a few-shot method with automatically matched demonstrations, yielding even stronger results. These findings underscore the efficacy of SCoT, highlighting its potential to substantially enhance LLM performance in complex reasoning tasks.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2409.03271v1', 'html': 'https://arxiv.org/html/2409.03271v1', 'tex': '/src/2409.03271v1', 'doi': 'https://doi.org/10.48550/arXiv.2409.03271'}	Submission history From: Yu Wang [ view email ] [v1] Thu, 5 Sep 2024 06:28:05 UTC (614 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.03271'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.03271'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.03271'}]
2024-09-08	OLMoE: Open Mixture-of-Experts Language Models	Computation and Language	https://arxiv.org/abs/2409.02060	OLMoE	https://x.com/omarsar0/status/1831357563620753577		2409.02060	['Niklas Muennighoff', 'Luca Soldaini', 'Dirk Groeneveld', 'Kyle Lo', 'Jacob Morrison', 'Sewon Min', 'Weijia Shi', 'Pete Walsh', 'Oyvind Tafjord', 'Nathan Lambert', 'Yuling Gu', 'Shane Arora', 'Akshita Bhagia', 'Dustin Schwenk', 'David Wadden', 'Alexander Wettig', 'Binyuan Hui', 'Tim Dettmers', 'Douwe Kiela', 'Ali Farhadi', 'Noah A. Smith', 'Pang Wei Koh', 'Amanpreet Singh', 'Hannaneh Hajishirzi']	ct:We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.	es (24 main), 36 figures, 17 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2409.02060', 'html': 'https://arxiv.org/html/2409.02060v2', 'tex': '/src/2409.02060', 'doi': 'https://doi.org/10.48550/arXiv.2409.02060'}	Submission history From: Niklas Muennighoff [ view email ] [v1] Tue, 3 Sep 2024 17:08:20 UTC (5,295 KB) [v2] Mon, 3 Mar 2025 01:25:46 UTC (5,330 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.02060'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.02060'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.02060'}]
2024-09-08	LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA	Computation and Language	https://arxiv.org/abs/2409.02897	LongCite	https://x.com/omarsar0/status/1831522905009828051		2409.02897	['Jiajie Zhang', 'Yushi Bai', 'Xin Lv', 'Wanjun Gu', 'Danqing Liu', 'Minhao Zou', 'Shulin Cao', 'Lei Hou', 'Yuxiao Dong', 'Ling Feng', 'Juanzi Li']	ct:Though current long-context large language models (LLMs) have demonstrated impressive capacities in answering user questions based on extensive text, the lack of citations in their responses makes user verification difficult, leading to concerns about their trustworthiness due to their potential hallucinations. In this work, we aim to enable long-context LLMs to generate responses with fine-grained sentence-level citations, improving their faithfulness and verifiability. We first introduce LongBench-Cite, an automated benchmark for assessing current LLMs' performance in Long-Context Question Answering with Citations (LQAC), revealing considerable room for improvement. To this end, we propose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs to automatically generate long-context QA instances with precise sentence-level citations, and leverage this pipeline to construct LongCite-45k, a large-scale SFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the LongCite-45k dataset, successfully enabling their generation of accurate responses and fine-grained sentence-level citations in a single output. The evaluation results on LongBench-Cite show that our trained models achieve state-of-the-art citation quality, surpassing advanced proprietary models including GPT-4o.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2409.02897', 'html': 'https://arxiv.org/html/2409.02897v3', 'tex': '/src/2409.02897', 'doi': 'https://doi.org/10.48550/arXiv.2409.02897'}	Submission history From: Jiajie Zhang [ view email ] [v1] Wed, 4 Sep 2024 17:41:19 UTC (784 KB) [v2] Thu, 5 Sep 2024 03:53:13 UTC (784 KB) [v3] Tue, 10 Sep 2024 07:43:19 UTC (784 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.02897'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.02897'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.02897'}]
2024-09-08	MemLong: Memory-Augmented Retrieval for Long Text Modeling	Computation and Language	https://arxiv.org/abs/2408.16967	MemLong	https://x.com/omarsar0/status/1830610367854112799		2408.16967	['Weijie Liu', 'Zecheng Tang', 'Juntao Li', 'Kehai Chen', 'Min Zhang']	ct:Recent advancements in Large Language Models (LLMs) have yielded remarkable success across diverse fields. However, handling long contexts remains a significant challenge for LLMs due to the quadratic time and space complexity of attention mechanisms and the growing memory consumption of the key-value cache during generation. This work introduces MemLong: Memory-Augmented Retrieval for Long Text Generation, a method designed to enhance the capabilities of long-context language modeling by utilizing an external retriever for historical information retrieval. MemLong combines a non-differentiable ``ret-mem'' module with a partially trainable decoder-only language model and introduces a fine-grained, controllable retrieval attention mechanism that leverages semantic-level relevant chunks. Comprehensive evaluations on multiple long-context language modeling benchmarks demonstrate that MemLong consistently outperforms other state-of-the-art LLMs. More importantly, MemLong can extend the context length on a single 3090 GPU from 4k up to 80k. Our code is available atthis https URL		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.16967', 'html': 'https://arxiv.org/html/2408.16967v1', 'tex': '/src/2408.16967', 'doi': 'https://doi.org/10.48550/arXiv.2408.16967'}	Submission history From: Zecheng Tang [ view email ] [v1] Fri, 30 Aug 2024 02:01:56 UTC (13,238 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.16967'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.16967'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.16967'}]
2024-09-08	Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models	Computation and Language	https://arxiv.org/abs/2408.13533	Role of RAG Noise in LLMs	https://x.com/omarsar0/status/1830984315326660617		2408.13533	['Jinyang Wu', 'Shuai Zhang', 'Feihu Che', 'Mingkuan Feng', 'Chuyuan Zhang', 'Pengpeng Shao', 'Jianhua Tao']	ct:Retrieval-Augmented Generation (RAG) has emerged as a crucial method for addressing hallucinations in large language models (LLMs). While recent research has extended RAG models to complex noisy scenarios, these explorations often confine themselves to limited noise types and presuppose that noise is inherently detrimental to LLMs, potentially deviating from real-world retrieval environments and restricting practical applicability. In this paper, we define seven distinct noise types from a linguistic perspective and establish a Noise RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing multiple datasets and reasoning tasks. Through empirical evaluation of eight representative LLMs with diverse architectures and scales, we reveal that these noises can be further categorized into two practical groups: noise that is beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs (aka harmful noise). While harmful noise generally impairs performance, beneficial noise may enhance several aspects of model capabilities and overall performance. Our analysis offers insights for developing more robust, adaptable RAG solutions and mitigating hallucinations across diverse retrieval scenarios. Code is available atthis https URL.	25 Main	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.13533', 'html': 'https://arxiv.org/html/2408.13533v4', 'tex': '/src/2408.13533', 'doi': 'https://doi.org/10.48550/arXiv.2408.13533'}	Submission history From: Jinyang Wu [ view email ] [v1] Sat, 24 Aug 2024 09:23:01 UTC (2,612 KB) [v2] Tue, 27 May 2025 08:47:19 UTC (3,610 KB) [v3] Thu, 29 May 2025 08:38:10 UTC (3,551 KB) [v4] Sat, 31 May 2025 09:45:31 UTC (3,547 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.13533'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.13533'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.13533'}]
2024-09-08	Beyond Preferences in AI Alignment	Artificial Intelligence	https://arxiv.org/abs/2408.16984	Beyond Preference in AI Alignment	https://x.com/xuanalogue/status/1831044533779669136		2408.16984	['Tan Zhi-Xuan', 'Micah Carroll', 'Matija Franklin', 'Hal Ashton']	ct:The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. In this paper, we characterize and challenge the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. We first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. We then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, we argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.	es (excl. references), 5 figures	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.16984', 'html': 'https://arxiv.org/html/2408.16984v2', 'tex': '/src/2408.16984', 'doi': 'https://doi.org/10.48550/arXiv.2408.16984'}	Submission history From: Tan Zhi-Xuan [ view email ] [v1] Fri, 30 Aug 2024 03:14:20 UTC (128 KB) [v2] Wed, 6 Nov 2024 20:34:14 UTC (128 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.16984'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.16984'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.16984'}]
2024-09-08	Large Language Model-Based Agents for Software Engineering: A Survey	Software Engineering	https://arxiv.org/abs/2409.02977	LLM-Based Agents for Software Engineering	https://x.com/omarsar0/status/1832115557749121385		2409.02977	['Junwei Liu', 'Kaixin Wang', 'Yixuan Chen', 'Xin Peng', 'Zhenpeng Chen', 'Lingming Zhang', 'Yiling Lou']	ct:The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 106 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. The repository of this survey is atthis https URL.		['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2409.02977', 'html': 'https://arxiv.org/html/2409.02977v1', 'tex': '/src/2409.02977', 'doi': 'https://doi.org/10.48550/arXiv.2409.02977'}	Submission history From: Junwei Liu [ view email ] [v1] Wed, 4 Sep 2024 15:59:41 UTC (7,206 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2409.02977'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2409.02977'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2409.02977'}]
2024-09-01	Diffusion Models Are Real-Time Game Engines	Machine Learning	https://arxiv.org/abs/2408.14837	GameGen	https://x.com/iScienceLuvr/status/1828617875432841490		2408.14837	['Dani Valevski', 'Yaniv Leviathan', 'Moab Arar', 'Shlomi Fruchter']	ct:We present GameNGen, the first game engine powered entirely by a neural model that also enables real-time interaction with a complex environment over long trajectories at high quality. When trained on the classic game DOOM, GameNGen extracts gameplay and uses it to generate a playable environment that can interactively simulate new trajectories. GameNGen runs at 20 frames per second on a single TPU and remains stable over extended multi-minute play sessions. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation, even after 5 minutes of auto-regressive generation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations help ensure stable auto-regressive generation over long trajectories, and decoder fine-tuning improves the fidelity of visual details and text.	025. Project page:this https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2408.14837', 'html': 'https://arxiv.org/html/2408.14837v2', 'tex': '/src/2408.14837', 'doi': 'https://doi.org/10.48550/arXiv.2408.14837'}	Submission history From: Dani Valevski [ view email ] [v1] Tue, 27 Aug 2024 07:46:07 UTC (14,907 KB) [v2] Thu, 24 Apr 2025 03:03:57 UTC (15,762 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.14837'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.14837'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.14837'}]
2024-09-01	Agentic Retrieval-Augmented Generation for Time Series Analysis	Artificial Intelligence	https://arxiv.org/abs/2408.14484	Agentic RAG for Time Series Analysis	https://x.com/omarsar0/status/1828838209461043455		2408.14484	['Chidaksh Ravuru', 'Sagar Srinivas Sakhinana', 'Venkataramana Runkana']	ct:Time series modeling is crucial for many applications, however, it faces challenges such as complex spatio-temporal dependencies and distribution shifts in learning from historical context to predict task-specific outcomes. To address these challenges, we propose a novel approach using an agentic Retrieval-Augmented Generation (RAG) framework for time series analysis. The framework leverages a hierarchical, multi-agent architecture where the master agent orchestrates specialized sub-agents and delegates the end-user request to the relevant sub-agent. The sub-agents utilize smaller, pre-trained language models (SLMs) customized for specific time series tasks through fine-tuning using instruction tuning and direct preference optimization, and retrieve relevant prompts from a shared repository of prompt pools containing distilled knowledge about historical patterns and trends to improve predictions on new data. Our proposed modular, multi-agent RAG approach offers flexibility and achieves state-of-the-art performance across major time series tasks by tackling complex challenges more effectively than task-specific customized methods across benchmark datasets.	was accepted for Undergraduate Consortium at ACM KDD, 2024. Please find the link:this https URL	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2408.14484', 'html': 'https://arxiv.org/html/2408.14484v1', 'tex': '/src/2408.14484', 'doi': 'https://doi.org/10.48550/arXiv.2408.14484'}	Submission history From: Sagar Srinivas Sakhinana [ view email ] [v1] Sun, 18 Aug 2024 11:47:55 UTC (128 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.14484'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.14484'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.14484'}]
2024-09-01	AutoGen Studio: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems	Software Engineering	https://arxiv.org/abs/2408.15247	AutoGen Studio	https://x.com/omarsar0/status/1829163090715529358		2408.15247	['Victor Dibia', 'Jingya Chen', 'Gagan Bansal', 'Suff Syed', 'Adam Fourney', 'Erkang Zhu', 'Chi Wang', 'Saleema Amershi']	ct:Multi-agent systems, where multiple agents (generative AI models + tools) collaborate, are emerging as an effective pattern for solving long-running, complex tasks in numerous domains. However, specifying their parameters (such as models, tools, and orchestration mechanisms etc,.) and debugging them remains challenging for most developers. To address this challenge, we present AUTOGEN STUDIO, a no-code developer tool for rapidly prototyping, debugging, and evaluating multi-agent workflows built upon the AUTOGEN framework. AUTOGEN STUDIO offers a web interface and a Python API for representing LLM-enabled agents using a declarative (JSON-based) specification. It provides an intuitive drag-and-drop UI for agent workflow specification, interactive evaluation and debugging of workflows, and a gallery of reusable agent components. We highlight four design principles for no-code multi-agent developer tools and contribute an open-source implementation atthis https URL	s	['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2408.15247', 'html': 'https://arxiv.org/html/2408.15247v1', 'tex': '/src/2408.15247', 'doi': 'https://doi.org/10.48550/arXiv.2408.15247'}	Submission history From: Victor Dibia [ view email ] [v1] Fri, 9 Aug 2024 03:27:37 UTC (5,057 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.15247'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.15247'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.15247'}]
2024-09-01	Persuasion Games using Large Language Models	Artificial Intelligence	https://arxiv.org/abs/2408.15879	Persuasion Games with LLMs	https://x.com/omarsar0/status/1829156960291185117		2408.15879	['Ganesh Prasath Ramani', 'Shirish Karande', 'Santhosh V', 'Yash Bhatia']	ct:Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape user perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS).We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with user agents through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We continuously analyze the resistance of the user agent to persuasive efforts and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques.We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.15879', 'html': 'https://arxiv.org/html/2408.15879v2', 'tex': '/src/2408.15879', 'doi': 'https://doi.org/10.48550/arXiv.2408.15879'}	Submission history From: Ganesh Prasath Ramani [ view email ] [v1] Wed, 28 Aug 2024 15:50:41 UTC (578 KB) [v2] Mon, 2 Sep 2024 02:30:51 UTC (578 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.15879'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.15879'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.15879'}]
2024-09-01	Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling	Computation and Language	https://arxiv.org/abs/2408.16737	Smaller, Weaker, Yet Better	https://x.com/omarsar0/status/1829526629787242878		2408.16737	['Hritik Bansal', 'Arian Hosseini', 'Rishabh Agarwal', 'Vinh Q. Tran', 'Mehran Kazemi']	ct:Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.16737', 'html': 'https://arxiv.org/html/2408.16737v2', 'tex': '/src/2408.16737', 'doi': 'https://doi.org/10.48550/arXiv.2408.16737'}	Submission history From: Mehran Kazemi [ view email ] [v1] Thu, 29 Aug 2024 17:32:35 UTC (233 KB) [v2] Mon, 7 Oct 2024 19:37:10 UTC (280 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.16737'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.16737'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.16737'}]
2024-09-01	Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model	Artificial Intelligence	https://www.arxiv.org/abs/2408.11039	Transfusion	https://x.com/AIatMeta/status/1828836885176967327		2408.11039	['Chunting Zhou', 'Lili Yu', 'Arun Babu', 'Kushal Tirumala', 'Michihiro Yasunaga', 'Leonid Shamis', 'Jacob Kahn', 'Xuezhe Ma', 'Luke Zettlemoyer', 'Omer Levy']	ct:We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.	es	['Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2408.11039', 'html': 'https://arxiv.org/html/2408.11039v1', 'tex': '/src/2408.11039', 'doi': 'https://doi.org/10.48550/arXiv.2408.11039'}	Submission history From: Chunting Zhou [ view email ] [v1] Tue, 20 Aug 2024 17:48:20 UTC (14,114 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.11039'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.11039'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.11039'}]
2024-09-01	ReMamba: Equip Mamba with Effective Long-Sequence Modeling	Computation and Language	https://arxiv.org/abs/2408.15496	ReMamba	https://x.com/omarsar0/status/1829151312266637813		2408.15496	['Danlong Yuan', 'Jiahao Liu', 'Bei Li', 'Huishuai Zhang', 'Jingang Wang', 'Xunliang Cai', 'Dongyan Zhao']	ct:While the Mamba architecture demonstrates superior inference efficiency and competitive performance on short-context natural language processing (NLP) tasks, empirical evidence suggests its capacity to comprehend long contexts is limited compared to transformer-based models. In this study, we investigate the long-context efficiency issues of the Mamba models and propose ReMamba, which enhances Mamba's ability to comprehend long contexts. ReMamba incorporates selective compression and adaptation techniques within a two-stage re-forward process, incurring minimal additional inference costs overhead. Experimental results on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy, improving over the baselines by 3.2 and 1.6 points, respectively, and attaining performance almost on par with same-size transformer models.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.15496', 'html': 'https://arxiv.org/html/2408.15496v4', 'tex': '/src/2408.15496', 'doi': 'https://doi.org/10.48550/arXiv.2408.15496'}	Submission history From: Yuan Danlong [ view email ] [v1] Wed, 28 Aug 2024 02:47:27 UTC (2,971 KB) [v2] Thu, 29 Aug 2024 10:35:52 UTC (2,968 KB) [v3] Sun, 1 Sep 2024 06:03:46 UTC (2,969 KB) [v4] Wed, 1 Jan 2025 15:22:34 UTC (652 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.15496'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.15496'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.15496'}]
2024-09-01	Text2SQL is Not Enough: Unifying AI and Databases with TAG	Databases	https://arxiv.org/abs/2408.14717v1	Text2SQL is Not Enough	https://x.com/lianapatel_/status/1828939097487945948		2408.14717v1	['Asim Biswal', 'Liana Patel', 'Siddarth Jha', 'Amog Kamsetty', 'Shu Liu', 'Joseph E. Gonzalez', 'Carlos Guestrin', 'Matei Zaharia']	ct:AI systems that serve natural language questions over databases promise to unlock tremendous value. Such systems would allow users to leverage the powerful reasoning and knowledge capabilities of language models (LMs) alongside the scalable computational power of data management systems. These combined capabilities would empower users to ask arbitrary natural language questions over custom data sources. However, existing methods and benchmarks insufficiently explore this setting. Text2SQL methods focus solely on natural language questions that can be expressed in relational algebra, representing a small subset of the questions real users wish to ask. Likewise, Retrieval-Augmented Generation (RAG) considers the limited subset of queries that can be answered with point lookups to one or a few data records within the database. We propose Table-Augmented Generation (TAG), a unified and general-purpose paradigm for answering natural language questions over databases. The TAG model represents a wide range of interactions between the LM and database that have been previously unexplored and creates exciting research opportunities for leveraging the world knowledge and reasoning capabilities of LMs over data. We systematically develop benchmarks to study the TAG problem and find that standard methods answer no more than 20% of queries correctly, confirming the need for further research in this area. We release code for the benchmark atthis https URL.		['Databases (cs.DB)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.14717v1', 'html': 'https://arxiv.org/html/2408.14717v1', 'tex': '/src/2408.14717v1', 'doi': 'https://doi.org/10.48550/arXiv.2408.14717'}	Submission history From: Asim Biswal [ view email ] [v1] Tue, 27 Aug 2024 00:50:14 UTC (256 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.14717'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.14717'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.14717'}]
2024-09-01	Foundation Models for Music: A Survey	Sound	https://arxiv.org/abs/2408.14340	Foundation Models for Music	https://x.com/omarsar0/status/1828456481114538437		2408.14340	['Yinghao Ma', 'Anders Øland', 'Anton Ragni', 'Bleiz MacSen Del Sette', 'Charalampos Saitis', 'Chris Donahue', 'Chenghua Lin', 'Christos Plachouras', 'Emmanouil Benetos', 'Elona Shatri', 'Fabio Morreale', 'Ge Zhang', 'György Fazekas', 'Gus Xia', 'Huan Zhang', 'Ilaria Manco', 'Jiawen Huang', 'Julien Guinot', 'Liwei Lin', 'Luca Marinelli', 'Max W. Y. Lam', 'Megha Sharma', 'Qiuqiang Kong', 'Roger B. Dannenberg', 'Ruibin Yuan', 'Shangda Wu', 'Shih-Lun Wu', 'Shuqi Dai', 'Shun Lei', 'Shiyin Kang', 'Simon Dixon', 'Wenhu Chen', 'Wenhao Huang', 'Xingjian Du', 'Xingwei Qu', 'Xu Tan', 'Yizhi Li', 'Zeyue Tian', 'Zhiyong Wu', 'Zhizheng Wu', 'Ziyang Ma', 'Ziyu Wang']	ct:In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm.		['Sound (cs.SD)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2408.14340', 'html': 'https://arxiv.org/html/2408.14340v3', 'tex': '/src/2408.14340', 'doi': 'https://doi.org/10.48550/arXiv.2408.14340'}	Submission history From: Yinghao Ma [ view email ] [v1] Mon, 26 Aug 2024 15:13:14 UTC (5,761 KB) [v2] Tue, 27 Aug 2024 14:09:44 UTC (5,592 KB) [v3] Tue, 3 Sep 2024 14:53:34 UTC (5,763 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.14340'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.14340'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.14340'}]
2024-09-01	A Practitioner's Guide to Continual Multimodal Pretraining	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2408.14471	Guide to Continual Multimodal Pretraining	https://arxiv.org/abs/2408.14471		2408.14471	['Karsten Roth', 'Vishaal Udandarao', 'Sebastian Dziadzio', 'Ameya Prabhu', 'Mehdi Cherti', 'Oriol Vinyals', 'Olivier Hénaff', 'Samuel Albanie', 'Matthias Bethge', 'Zeynep Akata']	ct:Multimodal foundation models serve numerous applications at the intersection of vision and language. Still, despite being pretrained on extensive data, they become outdated over time. To keep models updated, research into continual pretraining mainly explores scenarios with either (1) infrequent, indiscriminate updates on large-scale new data, or (2) frequent, sample-level updates. However, practical model deployment often operates in the gap between these two limit cases, as real-world applications often demand adaptation to specific subdomains, tasks or concepts -- spread over the entire, varying life cycle of a model. In this work, we complement current perspectives on continual pretraining through a research test bed as well as provide comprehensive guidance for effective continual model updates in such scenarios. We first introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with realistic compute constraints and practical deployment requirements, constructed over 63 datasets with diverse visual and semantic coverage. Using FoMo-in-Flux, we explore the complex landscape of practical continual pretraining through multiple perspectives: (1) A data-centric investigation of data mixtures and stream orderings that emulate real-world deployment situations, (2) a method-centric investigation ranging from simple fine-tuning and traditional continual learning strategies to parameter-efficient updates and model merging, (3) meta learning rate schedules and mechanistic design choices, and (4) the influence of model and compute scaling. Together, our insights provide a practitioner's guide to continual multimodal pretraining for real-world deployment. Our benchmark and code is here:this https URL.	cal Report. 52 pages. Shorter version published at the NeurIPS 2024 Dataset & Benchmarks track	['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2408.14471', 'html': None, 'tex': '/src/2408.14471', 'doi': 'https://doi.org/10.48550/arXiv.2408.14471'}	Submission history From: Karsten Roth [ view email ] [v1] Mon, 26 Aug 2024 17:59:01 UTC (7,203 KB) [v2] Fri, 6 Dec 2024 18:22:32 UTC (7,204 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.14471'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.14471'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.14471'}]
2024-08-25	Automated Design of Agentic Systems	Artificial Intelligence	https://arxiv.org/abs/2408.08435	Automate Design of Agentic Systems	https://x.com/omarsar0/status/1825378027347271719		2408.08435	['Shengran Hu', 'Cong Lu', 'Jeff Clune']	ct:Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. We describe a newly forming research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. We further demonstrate that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, workflows, and combinations thereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, we show that our algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided we develop it safely, our work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.	e:this https URL	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.08435', 'html': None, 'tex': '/src/2408.08435', 'doi': 'https://doi.org/10.48550/arXiv.2408.08435'}	Submission history From: Shengran Hu [ view email ] [v1] Thu, 15 Aug 2024 21:59:23 UTC (489 KB) [v2] Sun, 2 Mar 2025 05:13:28 UTC (236 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.08435'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.08435'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.08435'}]
2024-08-25	LLM Pruning and Distillation in Practice: The Minitron Approach	Computation and Language	https://arxiv.org/abs/2408.11796	LLM Pruning and Distillation in Practice	https://x.com/omarsar0/status/1826676365044675042		2408.11796	['Sharath Turuvekere Sreenivas', 'Saurav Muralidharan', 'Raviraj Joshi', 'Marcin Chochowski', 'Ameya Sunil Mahabaleshwarkar', 'Gerald Shen', 'Jiaqi Zeng', 'Zijia Chen', 'Yoshi Suhara', 'Shizhe Diao', 'Chenhan Yu', 'Wei-Chun Chen', 'Hayley Ross', 'Oluwatobi Olabiyi', 'Ashwath Aithal', 'Oleksii Kuchaiev', 'Daniel Korzekwa', 'Pavlo Molchanov', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Jan Kautz', 'Bryan Catanzaro']	ct:We present a comprehensive report on compressing the Llama 3.1 8B and Mistral NeMo 12B models to 4B and 8B parameters, respectively, using pruning and distillation. We explore two distinct pruning strategies: (1) depth pruning and (2) joint hidden/attention/MLP (width) pruning, and evaluate the results on common benchmarks from the LM Evaluation Harness. The models are then aligned with NeMo Aligner and tested in instruct-tuned versions. This approach produces a compelling 4B model from Llama 3.1 8B and a state-of-the-art Mistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo 12B. We found that with no access to the original data, it is beneficial to slightly fine-tune teacher models on the distillation dataset. We open-source our base model weights on Hugging Face with a permissive license.	date author order	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2408.11796', 'html': 'https://arxiv.org/html/2408.11796v4', 'tex': '/src/2408.11796', 'doi': 'https://doi.org/10.48550/arXiv.2408.11796'}	Submission history From: Saurav Muralidharan [ view email ] [v1] Wed, 21 Aug 2024 17:38:48 UTC (3,390 KB) [v2] Mon, 26 Aug 2024 17:50:46 UTC (3,396 KB) [v3] Sat, 30 Nov 2024 22:01:07 UTC (2,669 KB) [v4] Mon, 9 Dec 2024 18:31:01 UTC (2,669 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.11796'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.11796'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.11796'}]
2024-08-25	The Vizier Gaussian Process Bandit Algorithm	Machine Learning	https://arxiv.org/abs/2408.11527	Vizier Gaussian Process Bandit Algorithm	https://x.com/XingyouSong/status/1826554454084333723		2408.11527	['Xingyou Song', 'Qiuyi Zhang', 'Chansoo Lee', 'Emily Fertig', 'Tzu-Kuo Huang', 'Lior Belenki', 'Greg Kochanski', 'Setareh Ariafar', 'Srinivas Vasudevan', 'Sagi Perel', 'Daniel Golovin']	ct:Google Vizier has performed millions of optimizations and accelerated numerous research and production systems at Google, demonstrating the success of Bayesian optimization as a large-scale service. Over multiple years, its algorithm has been improved considerably, through the collective experiences of numerous research efforts and user feedback. In this technical report, we discuss the implementation details and design choices of the current default algorithm provided by Open Source Vizier. Our experiments on standardized benchmarks reveal its robustness and versatility against well-established industry baselines on multiple practical modes.	 DeepMind Technical Report. Code can be found inthis https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Optimization and Control (math.OC)']	{'pdf': '/pdf/2408.11527', 'html': 'https://arxiv.org/html/2408.11527v3', 'tex': '/src/2408.11527', 'doi': 'https://doi.org/10.48550/arXiv.2408.11527'}	Submission history From: Xingyou Song [ view email ] [v1] Wed, 21 Aug 2024 11:06:02 UTC (1,545 KB) [v2] Wed, 25 Sep 2024 22:14:33 UTC (1,556 KB) [v3] Fri, 6 Dec 2024 17:31:39 UTC (1,143 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.11527'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.11527'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.11527'}]
2024-08-25	Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution	Computation and Language	https://www.arxiv.org/abs/2408.10548	Language Modeling on Tabular Data	https://x.com/omarsar0/status/1826094372179366023		2408.10548	['Yucheng Ruan', 'Xiang Lan', 'Jingying Ma', 'Yizhi Dong', 'Kai He', 'Mengling Feng']	ct:Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at:this https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.10548', 'html': 'https://arxiv.org/html/2408.10548v1', 'tex': '/src/2408.10548', 'doi': 'https://doi.org/10.48550/arXiv.2408.10548'}	Submission history From: Yucheng Ruan [ view email ] [v1] Tue, 20 Aug 2024 04:59:19 UTC (796 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.10548'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.10548'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.10548'}]
2024-08-25	Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information	Computation and Language	https://arxiv.org/abs/2408.10615	Enhancing Robustness in LLMs	https://x.com/omarsar0/status/1826451091774447983		2408.10615	['Ming Jiang', 'Tingting Huang', 'Biao Guo', 'Yao Lu', 'Feng Zhang']	ct:In recent years, Large language models (LLMs) have garnered significant attention due to their superior performance in complex reasoning tasks. However, recent studies may diminish their reasoning capabilities markedly when problem descriptions contain irrelevant information, even with the use of advanced prompting techniques. To further investigate this issue, a dataset of primary school mathematics problems containing irrelevant information, named GSMIR, was constructed. Testing prominent LLMs and prompting techniques on this dataset revealed that while LLMs can identify irrelevant information, they do not effectively mitigate the interference it causes once identified. A novel automatic construction method, ATF, which enhances the ability of LLMs to identify and self-mitigate the influence of irrelevant information, is proposed to address this shortcoming. This method operates in two steps: first, analysis of irrelevant information, followed by its filtering. The ATF method, as demonstrated by experimental results, significantly improves the reasoning performance of LLMs and prompting techniques, even in the presence of irrelevant information on the GSMIR dataset.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.10615', 'html': 'https://arxiv.org/html/2408.10615v2', 'tex': '/src/2408.10615', 'doi': 'https://doi.org/10.48550/arXiv.2408.10615'}	Submission history From: Tingting Huang [ view email ] [v1] Tue, 20 Aug 2024 07:49:38 UTC (1,063 KB) [v2] Fri, 23 May 2025 03:23:26 UTC (1,063 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.10615'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.10615'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.10615'}]
2024-08-25	Graph Retrieval-Augmented Generation: A Survey	Artificial Intelligence	https://arxiv.org/abs/2408.08921	A Comprehensive Overview of GraphRAG Methods	https://x.com/omarsar0/status/1825937537782698377		2408.08921	['Boci Peng', 'Yun Zhu', 'Yongchao Liu', 'Xiaohe Bo', 'Haizhou Shi', 'Chuntao Hong', 'Yan Zhang', 'Siliang Tang']	ct:Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at \url{this https URL}.	g work. Compared to the first version, several references have been added and a GitHub repository link has been provided	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2408.08921', 'html': 'https://arxiv.org/html/2408.08921v2', 'tex': '/src/2408.08921', 'doi': 'https://doi.org/10.48550/arXiv.2408.08921'}	Submission history From: Boci Peng [ view email ] [v1] Thu, 15 Aug 2024 12:20:24 UTC (1,089 KB) [v2] Tue, 10 Sep 2024 15:38:56 UTC (763 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.08921'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.08921'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.08921'}]
2024-08-25	MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding	Computation and Language	https://arxiv.org/abs/2408.11049	MagicDec	https://x.com/omarsar0/status/1826090969906778122		2408.11049	['Ranajoy Sadhukhan', 'Jian Chen', 'Zhuoming Chen', 'Vashisth Tiwari', 'Ruihang Lai', 'Jinyuan Shi', 'Ian En-Hsu Yen', 'Avner May', 'Tianqi Chen', 'Beidi Chen']	ct:Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency losslessly, but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy SD more effectively for high throughput inference. We leverage draft model with sparse KV cache to address the KV bottleneck, which scales with both sequence length and batch size. Additionally, we propose a theoretical model to select the optimal drafting strategy for maximum speedup. Our work highlights the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B when serving batch sizes ranging from 32 to 256 on various types of hardware and tasks.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.11049', 'html': 'https://arxiv.org/html/2408.11049v5', 'tex': '/src/2408.11049', 'doi': 'https://doi.org/10.48550/arXiv.2408.11049'}	Submission history From: Zhuoming Chen [ view email ] [v1] Tue, 20 Aug 2024 17:57:31 UTC (1,960 KB) [v2] Wed, 21 Aug 2024 17:55:29 UTC (2,094 KB) [v3] Fri, 23 Aug 2024 17:54:34 UTC (2,094 KB) [v4] Wed, 26 Mar 2025 17:42:17 UTC (2,367 KB) [v5] Wed, 2 Apr 2025 01:58:38 UTC (2,367 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.11049'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.11049'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.11049'}]
2024-08-25	Controllable Text Generation for Large Language Models: A Survey	Computation and Language	https://arxiv.org/abs/2408.12599	Controllable Text Generation for LLMs	https://x.com/omarsar0/status/1826824199010132429		2408.12599	['Xun Liang', 'Hanyu Wang', 'Yezhaohui Wang', 'Shichao Song', 'Jiawei Yang', 'Simin Niu', 'Jie Hu', 'Dan Liu', 'Shunyu Yao', 'Feiyu Xiong', 'Zhiyu Li']	ct:In Natural Language Processing (NLP), Large Language Models (LLMs) have demonstrated high text generation quality. However, in real-world applications, LLMs must meet increasingly complex requirements. Beyond avoiding misleading or inappropriate content, LLMs are also expected to cater to specific user needs, such as imitating particular writing styles or generating text with poetic richness. These varied demands have driven the development of Controllable Text Generation (CTG) techniques, which ensure that outputs adhere to predefined control conditions--such as safety, sentiment, thematic consistency, and linguistic style--while maintaining high standards of helpfulness, fluency, and diversity.This paper systematically reviews the latest advancements in CTG for LLMs, offering a comprehensive definition of its core concepts and clarifying the requirements for control conditions and text quality. We categorize CTG tasks into two primary types: content control and attribute control. The key methods are discussed, including model retraining, fine-tuning, reinforcement learning, prompt engineering, latent space manipulation, and decoding-time intervention. We analyze each method's characteristics, advantages, and limitations, providing nuanced insights for achieving generation control. Additionally, we review CTG evaluation methods, summarize its applications across domains, and address key challenges in current research, including reduced fluency and practicality. We also propose several appeals, such as placing greater emphasis on real-world applications in future research. This paper aims to offer valuable guidance to researchers and developers in the field. Our reference list and Chinese version are open-sourced atthis https URL.	es, 11 figures, 7 tables, 11 equations	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.12599', 'html': 'https://arxiv.org/html/2408.12599v1', 'tex': '/src/2408.12599', 'doi': 'https://doi.org/10.48550/arXiv.2408.12599'}	Submission history From: Zhiyu Li [ view email ] [v1] Thu, 22 Aug 2024 17:59:04 UTC (2,051 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.12599'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.12599'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.12599'}]
2024-08-25	PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars	Computation and Language	https://arxiv.org/abs/2408.08869	PEDAL	https://x.com/omarsar0/status/1825373675631071609		2408.08869	['Sumanth Prabhu']	ct:Self-ensembling techniques with diverse reasoning paths such as Self-Consistency have demonstrated remarkable performance gains in text generation with Large Language Models (LLMs). However, such techniques depend on the availability of an accurate answer extraction process to aggregate across multiple outputs. Moreover, they acquire higher inference cost, in comparison to Greedy Decoding, due to generation of relatively higher number of output tokens. Research has shown that the free form text outputs from Self-Consistency can be aggregated reliably using LLMs to produce the final output. Additionally, recent advancements in LLM inference have demonstrated that usage of diverse exemplars in prompts have the ability to induce diversity in the LLM outputs. Such proven techniques can be easily extended to self-ensembling based approaches to achieve enhanced results in text generation. In this paper, we introduce PEDAL (Prompts based on Exemplar Diversity Aggregated using LLMs), a hybrid self-ensembling approach, that combines the strengths of diverse exemplar based prompts and LLM based aggregation to achieve improvement in overall performance. On the publicly available SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve better accuracy than Greedy Decoding based strategies with lower inference cost compared to Self Consistency based approaches.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2408.08869', 'html': 'https://arxiv.org/html/2408.08869v2', 'tex': '/src/2408.08869', 'doi': 'https://doi.org/10.48550/arXiv.2408.08869'}	Submission history From: Sumanth Prabhu [ view email ] [v1] Fri, 16 Aug 2024 17:54:09 UTC (144 KB) [v2] Mon, 19 Aug 2024 04:29:34 UTC (144 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.08869'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.08869'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.08869'}]
2024-08-25	Challenges and Responses in the Practice of Large Language Models	Computation and Language	https://arxiv.org/abs/2408.09416	Challenges and Responses in the Practice of LLMs	https://x.com/omarsar0/status/1825932441980162374		2408.09416	['Hongyin Zhu']	ct:This paper carefully summarizes extensive and profound questions from all walks of life, focusing on the current high-profile AI field, covering multiple dimensions such as industry trends, academic research, technological innovation and business applications. This paper meticulously curates questions that are both thought-provoking and practically relevant, providing nuanced and insightful answers to each. To facilitate readers' understanding and reference, this paper specifically classifies and organizes these questions systematically and meticulously from the five core dimensions of computing power infrastructure, software architecture, data resources, application scenarios, and brain science. This work aims to provide readers with a comprehensive, in-depth and cutting-edge AI knowledge framework to help people from all walks of life grasp the pulse of AI development, stimulate innovative thinking, and promote industrial progress.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.09416', 'html': 'https://arxiv.org/html/2408.09416v2', 'tex': '/src/2408.09416', 'doi': 'https://doi.org/10.48550/arXiv.2408.09416'}	Submission history From: Hongyin Zhu [ view email ] [v1] Sun, 18 Aug 2024 09:15:11 UTC (93 KB) [v2] Wed, 21 Aug 2024 11:24:42 UTC (94 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.09416'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.09416'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.09416'}]
2024-08-18	The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery	Artificial Intelligence	https://arxiv.org/abs/2408.06292	The AI Scientist	https://x.com/omarsar0/status/1823189280883097788		2408.06292	['Chris Lu', 'Cong Lu', 'Robert Tjarko Lange', 'Jakob Foerster', 'Jeff Clune', 'David Ha']	ct:One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced atthis https URL		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2408.06292', 'html': None, 'tex': '/src/2408.06292', 'doi': 'https://doi.org/10.48550/arXiv.2408.06292'}	Submission history From: Christopher Lu [ view email ] [v1] Mon, 12 Aug 2024 16:58:11 UTC (11,109 KB) [v2] Thu, 15 Aug 2024 15:42:50 UTC (11,110 KB) [v3] Sun, 1 Sep 2024 00:41:18 UTC (11,112 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.06292'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.06292'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.06292'}]
2024-08-18	LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs	Computation and Language	https://arxiv.org/abs/2408.07055	LongWriter	https://x.com/omarsar0/status/1823551063946850712		2408.07055	['Yushi Bai', 'Jiajie Zhang', 'Xin Lv', 'Linzhi Zheng', 'Siqi Zhu', 'Lei Hou', 'Yuxiao Dong', 'Jie Tang', 'Juanzi Li']	ct:Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words. Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has seen during supervised fine-tuning (SFT). In other words, their output limitation is due to the scarcity of long-output examples in existing SFT datasets. To address this, we introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we construct LongWriter-6k, a dataset containing 6,000 SFT data with output lengths ranging from 2k to 32k words. By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10,000 words while maintaining output quality. We also develop LongBench-Write, a comprehensive benchmark for evaluating ultra-long generation capabilities. Our 9B parameter model, further improved through DPO, achieves state-of-the-art performance on this benchmark, surpassing even much larger proprietary models. In general, our work demonstrates that existing long context LLM already possesses the potential for a larger output window--all you need is data with extended output during model alignment to unlock this capability. Our code & models are at:this https URL.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2408.07055', 'html': None, 'tex': '/src/2408.07055', 'doi': 'https://doi.org/10.48550/arXiv.2408.07055'}	Submission history From: Yushi Bai [ view email ] [v1] Tue, 13 Aug 2024 17:46:12 UTC (521 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.07055'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.07055'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.07055'}]
2024-08-18	EfficientRAG: Efficient Retriever for Multi-Hop Question Answering	Computation and Language	https://arxiv.org/abs/2408.04259	EfficientRAG	https://x.com/omarsar0/status/1822744591810114044		2408.04259	['Ziyuan Zhuang', 'Zhiyang Zhang', 'Sitao Cheng', 'Fangkai Yang', 'Jia Liu', 'Shujian Huang', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang', 'Qi Zhang']	ct:Retrieval-augmented generation (RAG) methods encounter difficulties when addressing complex questions like multi-hop queries. While iterative retrieval methods improve performance by gathering additional information, current approaches often rely on multiple calls of large language models (LLMs). In this paper, we introduce EfficientRAG, an efficient retriever for multi-hop question answering. EfficientRAG iteratively generates new queries without the need for LLM calls at each iteration and filters out irrelevant information. Experimental results demonstrate that EfficientRAG surpasses existing RAG methods on three open-domain multi-hop question-answering datasets.	es, 4 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.04259', 'html': 'https://arxiv.org/html/2408.04259v2', 'tex': '/src/2408.04259', 'doi': 'https://doi.org/10.48550/arXiv.2408.04259'}	Submission history From: Ziyuan Zhuang [ view email ] [v1] Thu, 8 Aug 2024 06:57:49 UTC (7,001 KB) [v2] Thu, 26 Sep 2024 11:42:35 UTC (7,001 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.04259'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.04259'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.04259'}]
2024-08-18	RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation	Computation and Language	https://arxiv.org/abs/2408.08067	RAGChecker	https://x.com/omarsar0/status/1824460245051081216		2408.08067	['Dongyu Ru', 'Lin Qiu', 'Xiangkun Hu', 'Tianhang Zhang', 'Peng Shi', 'Shuaichen Chang', 'Cheng Jiayang', 'Cunxiang Wang', 'Shichao Sun', 'Huanyu Li', 'Zizhao Zhang', 'Binjie Wang', 'Jiarong Jiang', 'Tong He', 'Zhiguo Wang', 'Pengfei Liu', 'Yue Zhang', 'Zheng Zhang']	ct:Despite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGChecker, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGChecker has significantly better correlations with human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGChecker can guide researchers and practitioners in developing more effective RAG systems. This work has been open sourced atthis https URL.	Review. Github Repo:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.08067', 'html': 'https://arxiv.org/html/2408.08067v2', 'tex': '/src/2408.08067', 'doi': 'https://doi.org/10.48550/arXiv.2408.08067'}	Submission history From: Dongyu Ru [ view email ] [v1] Thu, 15 Aug 2024 10:20:54 UTC (6,007 KB) [v2] Sat, 17 Aug 2024 00:30:04 UTC (6,170 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.08067'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.08067'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.08067'}]
2024-08-18	HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction	Computation and Language	https://arxiv.org/abs/2408.04948	HybridRAG	https://x.com/omarsar0/status/1822832843455648000		2408.04948	['Bhaskarjit Sarmah', 'Benika Hall', 'Rohan Rao', 'Sunil Patel', 'Stefano Pasquali', 'Dhagash Mehta']	ct:Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain	s, 2 figures, 5 tables	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Statistical Finance (q-fin.ST)', 'Applications (stat.AP)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2408.04948', 'html': 'https://arxiv.org/html/2408.04948v1', 'tex': '/src/2408.04948', 'doi': 'https://doi.org/10.48550/arXiv.2408.04948'}	Submission history From: Dhagash Mehta [ view email ] [v1] Fri, 9 Aug 2024 09:07:48 UTC (58 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.04948'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.04948'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.04948'}]
2024-08-18	Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers	Computation and Language	https://arxiv.org/abs/2408.06195	rStar	https://x.com/AtakanTekparmak/status/1823776878747877572		2408.06195	['Zhenting Qi', 'Mingyuan Ma', 'Jiahang Xu', 'Li Lyna Zhang', 'Fan Yang', 'Mao Yang']	ct:This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be available atthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.06195', 'html': 'https://arxiv.org/html/2408.06195v1', 'tex': '/src/2408.06195', 'doi': 'https://doi.org/10.48550/arXiv.2408.06195'}	Submission history From: Li Lyna Zhang [ view email ] [v1] Mon, 12 Aug 2024 14:42:13 UTC (1,140 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.06195'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.06195'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.06195'}]
2024-08-18	A Survey of Text-to-SQL in the Era of LLMs: Where are we, and where are we going?	Databases	https://arxiv.org/abs/2408.05109	Scaling LLM Test-Time Compute Optimally	https://x.com/sea_snell/status/1821263798772363598		2408.05109	['Xinyu Liu', 'Shuyu Shen', 'Boyan Li', 'Peixian Ma', 'Runzhi Jiang', 'Yuxin Zhang', 'Ju Fan', 'Guoliang Li', 'Nan Tang', 'Yuyu Luo']	ct:Translating users' natural language queries (NL) into SQL queries (i.e., Text-to-SQL, a.k.a. NL2SQL) can significantly reduce barriers to accessing relational databases and support various commercial applications. The performance of Text-to-SQL has been greatly enhanced with the emergence of Large Language Models (LLMs). In this survey, we provide a comprehensive review of Text-to-SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: (1) Model: Text-to-SQL translation techniques that tackle not only NL ambiguity and under-specification, but also properly map NL with database schema and instances; (2) Data: From the collection of training data, data synthesis due to training data scarcity, to Text-to-SQL benchmarks; (3) Evaluation: Evaluating Text-to-SQL methods from multiple angles using different metrics and granularities; and (4) Error Analysis: analyzing Text-to-SQL errors to find the root cause and guiding Text-to-SQL models to evolve. Moreover, we offer a rule of thumb for developing Text-to-SQL solutions. Finally, we discuss the research challenges and open problems of Text-to-SQL in the LLMs era.	es, 11 figures, 3 tables	['Databases (cs.DB)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.05109', 'html': 'https://arxiv.org/html/2408.05109v5', 'tex': '/src/2408.05109', 'doi': 'https://doi.org/10.48550/arXiv.2408.05109'}	Submission history From: Xinyu Liu [ view email ] [v1] Fri, 9 Aug 2024 14:59:36 UTC (9,078 KB) [v2] Sun, 3 Nov 2024 15:24:11 UTC (8,229 KB) [v3] Wed, 4 Dec 2024 04:57:04 UTC (8,228 KB) [v4] Tue, 4 Mar 2025 06:51:36 UTC (8,652 KB) [v5] Sun, 15 Jun 2025 15:53:09 UTC (5,533 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.05109'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.05109'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.05109'}]
2024-08-18	Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2408.04187	MedGraphRAG	https://x.com/Marktechpost/status/1823069406924288110		2408.04187	['Junde Wu', 'Jiayuan Zhu', 'Yunli Qi', 'Jingkun Chen', 'Min Xu', 'Filippo Menolascina', 'Vicente Grau']	ct:We introduce a novel graph-based Retrieval-Augmented Generation (RAG) framework specifically designed for the medical domain, called \textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM) capabilities for generating evidence-based medical responses, thereby improving safety and reliability when handling private medical data. Graph-based RAG (GraphRAG) leverages LLMs to organize RAG data into graphs, showing strong potential for gaining holistic insights from long-form documents. However, its standard implementation is overly complex for general use and lacks the ability to generate evidence-based responses, limiting its effectiveness in the medical field. To extend the capabilities of GraphRAG to the medical domain, we propose unique Triple Graph Construction and U-Retrieval techniques over it. In our graph construction, we create a triple-linked structure that connects user documents to credible medical sources and controlled vocabularies. In the retrieval process, we propose U-Retrieval which combines Top-down Precise Retrieval with Bottom-up Response Refinement to balance global context awareness with precise indexing. These effort enable both source information retrieval and comprehensive response generation. Our approach is validated on 9 medical Q\&A benchmarks, 2 health fact-checking benchmarks, and one collected dataset testing long-form generation. The results show that MedGraphRAG consistently outperforms state-of-the-art models across all benchmarks, while also ensuring that responses include credible source documentation and definitions. Our code is released at:this https URL.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2408.04187', 'html': 'https://arxiv.org/html/2408.04187v2', 'tex': '/src/2408.04187', 'doi': 'https://doi.org/10.48550/arXiv.2408.04187'}	Submission history From: Junde Wu [ view email ] [v1] Thu, 8 Aug 2024 03:11:12 UTC (3,196 KB) [v2] Tue, 15 Oct 2024 17:37:42 UTC (3,722 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.04187'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.04187'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.04187'}]
2024-08-18	A Survey of Text-to-SQL in the Era of LLMs: Where are we, and where are we going?	Databases	https://arxiv.org/abs/2408.05109	Survey of NL2QL	https://x.com/_reachsumit/status/1822835969743347815		2408.05109	['Xinyu Liu', 'Shuyu Shen', 'Boyan Li', 'Peixian Ma', 'Runzhi Jiang', 'Yuxin Zhang', 'Ju Fan', 'Guoliang Li', 'Nan Tang', 'Yuyu Luo']	ct:Translating users' natural language queries (NL) into SQL queries (i.e., Text-to-SQL, a.k.a. NL2SQL) can significantly reduce barriers to accessing relational databases and support various commercial applications. The performance of Text-to-SQL has been greatly enhanced with the emergence of Large Language Models (LLMs). In this survey, we provide a comprehensive review of Text-to-SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: (1) Model: Text-to-SQL translation techniques that tackle not only NL ambiguity and under-specification, but also properly map NL with database schema and instances; (2) Data: From the collection of training data, data synthesis due to training data scarcity, to Text-to-SQL benchmarks; (3) Evaluation: Evaluating Text-to-SQL methods from multiple angles using different metrics and granularities; and (4) Error Analysis: analyzing Text-to-SQL errors to find the root cause and guiding Text-to-SQL models to evolve. Moreover, we offer a rule of thumb for developing Text-to-SQL solutions. Finally, we discuss the research challenges and open problems of Text-to-SQL in the LLMs era.	es, 11 figures, 3 tables	['Databases (cs.DB)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.05109', 'html': 'https://arxiv.org/html/2408.05109v5', 'tex': '/src/2408.05109', 'doi': 'https://doi.org/10.48550/arXiv.2408.05109'}	Submission history From: Xinyu Liu [ view email ] [v1] Fri, 9 Aug 2024 14:59:36 UTC (9,078 KB) [v2] Sun, 3 Nov 2024 15:24:11 UTC (8,229 KB) [v3] Wed, 4 Dec 2024 04:57:04 UTC (8,228 KB) [v4] Tue, 4 Mar 2025 06:51:36 UTC (8,652 KB) [v5] Sun, 15 Jun 2025 15:53:09 UTC (5,533 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.05109'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.05109'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.05109'}]
2024-08-11	Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models	Computation and Language	https://arxiv.org/abs/2408.02442	Structured Generation Limits Reasoning	https://x.com/omarsar0/status/1822357786820284555		2408.02442	['Zhi Rui Tam', 'Cheng-Kuang Wu', 'Yi-Lin Tsai', 'Chieh-Yen Lin', 'Hung-yi Lee', 'Yun-Nung Chen']	ct:Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs). This study investigates whether such constraints on generation space impact LLMs abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.	es	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.02442', 'html': 'https://arxiv.org/html/2408.02442v3', 'tex': '/src/2408.02442', 'doi': 'https://doi.org/10.48550/arXiv.2408.02442'}	Submission history From: Zhi Rui Tam [ view email ] [v1] Mon, 5 Aug 2024 13:08:24 UTC (252 KB) [v2] Sat, 21 Sep 2024 01:29:05 UTC (253 KB) [v3] Mon, 14 Oct 2024 13:57:29 UTC (254 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.02442'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.02442'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.02442'}]
2024-08-11	From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future	Software Engineering	https://arxiv.org/abs/2408.02479	From LLMs to LLM-based Agents for Sofware Engineering	https://x.com/omarsar0/status/1821549401866686604		2408.02479	['Haolin Jin', 'Linghan Huang', 'Haipeng Cai', 'Jun Yan', 'Bo Li', 'Huaming Chen']	ct:With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.		['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.02479', 'html': 'https://arxiv.org/html/2408.02479v2', 'tex': '/src/2408.02479', 'doi': 'https://doi.org/10.48550/arXiv.2408.02479'}	Submission history From: Haolin Jin [ view email ] [v1] Mon, 5 Aug 2024 14:01:15 UTC (2,141 KB) [v2] Sun, 13 Apr 2025 09:42:30 UTC (6,210 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.02479'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.02479'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.02479'}]
2024-08-11	Transformer Explainer: Interactive Learning of Text-Generative Models	Machine Learning	https://arxiv.org/abs/2408.04619	Transformer Explainer	https://x.com/omarsar0/status/1821986172215742716		2408.04619	['Aeree Cho', 'Grace C. Kim', 'Alexander Karpekov', 'Alec Helbling', 'Zijie J. Wang', 'Seongmin Lee', 'Benjamin Hoover', 'Duen Horng Chau']	ct:Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user's browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public's education access to modern generative AI techniques. Our open-sourced tool is available atthis https URL. A video demo is available atthis https URL.	presented at IEEE VIS 2024	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2408.04619', 'html': 'https://arxiv.org/html/2408.04619v1', 'tex': '/src/2408.04619', 'doi': 'https://doi.org/10.48550/arXiv.2408.04619'}	Submission history From: Grace C. Kim [ view email ] [v1] Thu, 8 Aug 2024 17:49:07 UTC (3,212 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.04619'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.04619'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.04619'}]
2024-08-11	RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation	Computation and Language	https://arxiv.org/abs/2408.02545	Enhancing LLMs for RAG	https://x.com/omarsar0/status/1820864003590995973		2408.02545	['Daniel Fleischer', 'Moshe Berchansky', 'Moshe Wasserblat', 'Peter Izsak']	ct:Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source inthis https URL.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2408.02545', 'html': None, 'tex': '/src/2408.02545', 'doi': 'https://doi.org/10.48550/arXiv.2408.02545'}	Submission history From: Daniel Fleischer [ view email ] [v1] Mon, 5 Aug 2024 15:16:24 UTC (97 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.02545'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.02545'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.02545'}]
2024-08-11	Synthesizing Text-to-SQL Data from Weak and Strong LLMs	Computation and Language	https://arxiv.org/abs/2408.03256	Synthesizing Text-to-SQL Data from Weak and Strong LLMs	https://x.com/omarsar0/status/1821227584920621061		2408.03256	['Jiaxi Yang', 'Binyuan Hui', 'Min Yang', 'Jian Yang', 'Junyang Lin', 'Chang Zhou']	ct:The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.	es, 7 figures, ACL 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.03256', 'html': 'https://arxiv.org/html/2408.03256v1', 'tex': '/src/2408.03256', 'doi': 'https://doi.org/10.48550/arXiv.2408.03256'}	Submission history From: Jiaxi Yang [ view email ] [v1] Tue, 6 Aug 2024 15:40:32 UTC (1,592 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.03256'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.03256'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.03256'}]
2024-08-11	Conversational Prompt Engineering	Computation and Language	https://arxiv.org/abs/2408.04560	Conversational Prompt Engineering	https://x.com/omarsar0/status/1821981401861718488		2408.04560	['Liat Ein-Dor', 'Orith Toledo-Ronen', 'Artem Spector', 'Shai Gretz', 'Lena Dankin', 'Alon Halfon', 'Yoav Katz', 'Noam Slonim']	ct:Prompts are how humans communicate with LLMs. Informative prompts are essential for guiding LLMs to produce the desired output. However, prompt engineering is often tedious and time-consuming, requiring significant expertise, limiting its widespread use. We propose Conversational Prompt Engineering (CPE), a user-friendly tool that helps users create personalized prompts for their specific tasks. CPE uses a chat model to briefly interact with users, helping them articulate their output preferences and integrating these into the prompt. The process includes two main stages: first, the model uses user-provided unlabeled data to generate data-driven questions and utilize user responses to shape the initial instruction. Then, the model shares the outputs generated by the instruction and uses user feedback to further refine the instruction and the outputs. The final result is a few-shot prompt, where the outputs approved by the user serve as few-shot examples. A user study on summarization tasks demonstrates the value of CPE in creating personalized, high-performing prompts. The results suggest that the zero-shot prompt obtained is comparable to its - much longer - few-shot counterpart, indicating significant savings in scenarios involving repetitive tasks with large text volumes.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2408.04560', 'html': 'https://arxiv.org/html/2408.04560v1', 'tex': '/src/2408.04560', 'doi': 'https://doi.org/10.48550/arXiv.2408.04560'}	Submission history From: Lena Dankin Mrs. [ view email ] [v1] Thu, 8 Aug 2024 16:18:39 UTC (9,406 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.04560'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.04560'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.04560'}]
2024-08-11	Self-Taught Evaluators	Computation and Language	https://arxiv.org/abs/2408.02666	Self-Taught Evaluators	https://x.com/omarsar0/status/1820849115607044401		2408.02666	['Tianlu Wang', 'Ilia Kulikov', 'Olga Golovneva', 'Ping Yu', 'Weizhe Yuan', 'Jane Dwivedi-Yu', 'Richard Yuanzhe Pang', 'Maryam Fazel-Zarandi', 'Jason Weston', 'Xian Li']	ct:Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.02666', 'html': 'https://arxiv.org/html/2408.02666v2', 'tex': '/src/2408.02666', 'doi': 'https://doi.org/10.48550/arXiv.2408.02666'}	Submission history From: Jason Weston [ view email ] [v1] Mon, 5 Aug 2024 17:57:02 UTC (1,171 KB) [v2] Thu, 8 Aug 2024 17:09:58 UTC (1,173 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.02666'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.02666'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.02666'}]
2024-08-11	RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework	Computation and Language	https://arxiv.org/abs/2408.01262	RAGEval	https://x.com/omarsar0/status/1820507831491239978		2408.01262	['Kunlun Zhu', 'Yifan Luo', 'Dingling Xu', 'Yukun Yan', 'Zhenghao Liu', 'Shi Yu', 'Ruobing Wang', 'Shuo Wang', 'Yishan Li', 'Nan Zhang', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun']	ct:Retrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics: Completeness, Hallucination, and Irrelevance to evaluate LLM generated responses rigorously. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications. The code and dataset are released atthis https URL.	ttps URL	['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2408.01262', 'html': None, 'tex': '/src/2408.01262', 'doi': 'https://doi.org/10.48550/arXiv.2408.01262'}	Submission history From: Kunlun Zhu [ view email ] [v1] Fri, 2 Aug 2024 13:35:11 UTC (295 KB) [v2] Sun, 18 Aug 2024 15:48:02 UTC (293 KB) [v3] Tue, 27 Aug 2024 03:13:50 UTC (301 KB) [v4] Thu, 17 Oct 2024 02:20:47 UTC (401 KB) [v5] Mon, 3 Mar 2025 22:45:57 UTC (531 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.01262'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.01262'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.01262'}]
2024-08-11	A Survey of Mamba	Machine Learning	https://arxiv.org/abs/2408.01129	Survey of Mamba	https://x.com/omarsar0/status/1821556218168549561		2408.01129	['Haohao Qu', 'Liangbo Ning', 'Rui An', 'Wenqi Fan', 'Tyler Derr', 'Hui Liu', 'Xin Xu', 'Qing Li']	ct:As one of the most representative DL techniques, Transformer architecture has empowered numerous advanced models, especially the large language models (LLMs) that comprise billions of parameters, becoming a cornerstone in deep learning. Despite the impressive achievements, Transformers still face inherent limitations, particularly the time-consuming inference resulting from the quadratic computation complexity of attention calculation. Recently, a novel architecture named Mamba, drawing inspiration from classical state space models (SSMs), has emerged as a promising alternative for building foundation models, delivering comparable modeling abilities to Transformers while preserving near-linear scalability concerning sequence length. This has sparked an increasing number of studies actively exploring Mamba's potential to achieve impressive performance across diverse domains. Given such rapid evolution, there is a critical need for a systematic review that consolidates existing Mamba-empowered models, offering a comprehensive understanding of this emerging model architecture. In this survey, we therefore conduct an in-depth investigation of recent Mamba-associated studies, covering three main aspects: the advancements of Mamba-based models, the techniques of adapting Mamba to diverse data, and the applications where Mamba can excel. Specifically, we first review the foundational knowledge of various representative deep learning models and the details of Mamba-1&2 as preliminaries. Then, to showcase the significance of Mamba for AI, we comprehensively review the related studies focusing on Mamba models' architecture design, data adaptability, and applications. Finally, we present a discussion of current limitations and explore various promising research directions to provide deeper insights for future investigations.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2408.01129', 'html': 'https://arxiv.org/html/2408.01129v6', 'tex': '/src/2408.01129', 'doi': 'https://doi.org/10.48550/arXiv.2408.01129'}	Submission history From: Haohao Qu [ view email ] [v1] Fri, 2 Aug 2024 09:18:41 UTC (7,051 KB) [v2] Sun, 18 Aug 2024 07:26:04 UTC (7,052 KB) [v3] Thu, 22 Aug 2024 07:18:01 UTC (7,052 KB) [v4] Fri, 18 Oct 2024 10:46:43 UTC (7,054 KB) [v5] Fri, 13 Dec 2024 06:16:06 UTC (7,054 KB) [v6] Tue, 17 Jun 2025 09:27:55 UTC (6,999 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2408.01129'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2408.01129'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2408.01129'}]
2024-08-04	Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge	Computation and Language	https://arxiv.org/abs/2407.19594	Meta-Rewarding LLMs	https://x.com/omarsar0/status/1818680848058585119		2407.19594	['Tianhao Wu', 'Weizhe Yuan', 'Olga Golovneva', 'Jing Xu', 'Yuandong Tian', 'Jiantao Jiao', 'Jason Weston', 'Sainbayar Sukhbaatar']	ct:Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, we introduce a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge {\em and} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.19594', 'html': 'https://arxiv.org/html/2407.19594v2', 'tex': '/src/2407.19594', 'doi': 'https://doi.org/10.48550/arXiv.2407.19594'}	Submission history From: Jason Weston [ view email ] [v1] Sun, 28 Jul 2024 21:58:28 UTC (987 KB) [v2] Tue, 30 Jul 2024 01:38:06 UTC (673 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.19594'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.19594'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.19594'}]
2024-08-04	MindSearch: Mimicking Human Minds Elicits Deep AI Searcher	Computation and Language	https://arxiv.org/abs/2407.20183	MindSearch	https://x.com/omarsar0/status/1818673381069226053		2407.20183	['Zehui Chen', 'Kuikun Liu', 'Qiuchen Wang', 'Jiangning Liu', 'Wenwei Zhang', 'Kai Chen', 'Feng Zhao']	ct:Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web andthis http URLapplications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.	cal Report. Project Page:this https URLCode:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.20183', 'html': 'https://arxiv.org/html/2407.20183v1', 'tex': '/src/2407.20183', 'doi': 'https://doi.org/10.48550/arXiv.2407.20183'}	Submission history From: Zehui Chen [ view email ] [v1] Mon, 29 Jul 2024 17:12:40 UTC (2,783 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.20183'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.20183'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.20183'}]
2024-08-04	Improving Retrieval Augmented Language Model with Self-Reasoning	Computation and Language	https://arxiv.org/abs/2407.19813	Improved RAG with Self-Reasoning	https://x.com/omarsar0/status/1818139150882664696		2407.19813	['Yuan Xia', 'Jingbo Zhou', 'Zhenhui Shi', 'Jun Chen', 'Haifeng Huang']	ct:The Retrieval-Augmented Language Model (RALM) has shown remarkable performance on knowledge-intensive tasks by incorporating external knowledge during inference, which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements, challenges persist in the implementation of RALMs, particularly concerning their reliability and traceability. To be specific, the irrelevant document retrieval may result in unhelpful response generation or even deteriorate the performance of LLMs, while the lack of proper citations in generated outputs complicates efforts to verify the trustworthiness of the models. To this end, we propose a novel self-reasoning framework aimed at improving the reliability and traceability of RALMs, whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self-reason trajectories with three processes: a relevance-aware process, an evidence-aware selective process, and a trajectory analysis process. We have evaluated our framework across four public datasets (two short-form QA datasets, one long-form QA dataset, and one fact verification dataset) to demonstrate the superiority of our method, which can outperform existing state-of-the-art models and can achieve comparable performance with GPT-4, while only using 2,000 training samples.	025 (main conference)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.19813', 'html': 'https://arxiv.org/html/2407.19813v3', 'tex': '/src/2407.19813', 'doi': 'https://doi.org/10.48550/arXiv.2407.19813'}	Submission history From: Yuan Xia [ view email ] [v1] Mon, 29 Jul 2024 09:05:10 UTC (12,240 KB) [v2] Fri, 2 Aug 2024 12:11:17 UTC (9,959 KB) [v3] Thu, 19 Dec 2024 06:27:44 UTC (9,760 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.19813'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.19813'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.19813'}]
2024-08-04	Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost	Computation and Language	https://arxiv.org/abs/2407.19825	Constrained-CoT	https://x.com/omarsar0/status/1818133220484898992		2407.19825	['Sania Nayab', 'Giulio Rossolini', 'Marco Simoni', 'Andrea Saracino', 'Giorgio Buttazzo', 'Nicolamaria Manes', 'Fabrizio Giacomelli']	ct:Today's large language models (LLMs) can solve challenging question-answering tasks, and prompt engineering techniques, such as chain-of-thought (CoT), have gained attention for enhancing the explanation and correctness of outputs. However, many models and techniques tend to produce excessively verbose and lengthy answers, leading to issues with both conciseness and generation time. To address this, this paper analyzes the impact of output lengths on LLM inference pipelines by introducing and proposing novel metrics to evaluate the \textit{correct conciseness} of a model and related prompting techniques. Then, we examine the impact of controlling output length through a refined prompt engineering strategy, Constrained-CoT (CCoT), which encourages the model to produce more concise outputs. To better understand the effects of such a prompt, we also introduce two additional scores for analyzing the conciseness, measured in terms of redundancy and information flow in generated answers. Experiments on pretrained LLMs and multiple datasets demonstrate the benefits of the proposed metrics and the effectiveness of CCoT across different models.	nt version, under review	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.19825', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2407.19825'}	Submission history From: Sania Nayab [ view email ] [v1] Mon, 29 Jul 2024 09:21:52 UTC (451 KB) [v2] Thu, 23 Jan 2025 08:45:52 UTC (664 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.19825'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.19825'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.19825'}]
2024-08-04	Adaptive Retrieval-Augmented Generation for Conversational Systems	Computation and Language	https://arxiv.org/abs/2407.21712	Adaptive RAG for Conversations Sytems	https://x.com/omarsar0/status/1818843407977959756		2407.21712	['Xi Wang', 'Procheta Sen', 'Ruizhe Li', 'Emine Yilmaz']	ct:Despite the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. Hence, many existing studies commonly assume the always need for Retrieval Augmented Generation (RAG) in a conversational system without explicit control. This raises a research question about such a necessity. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop RAGate, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying RAGate to conversational models and well-rounded analyses of different conversational scenarios. Our experimental results and analysis indicate the effective application of RAGate in RAG-based conversational systems in identifying system responses for appropriate RAG with high-quality responses and a high generation confidence. This study also identifies the correlation between the generation's confidence level and the relevance of the augmented knowledge.	es, under review	['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2407.21712', 'html': 'https://arxiv.org/html/2407.21712v1', 'tex': '/src/2407.21712', 'doi': 'https://doi.org/10.48550/arXiv.2407.21712'}	Submission history From: Xi Wang [ view email ] [v1] Wed, 31 Jul 2024 16:04:03 UTC (312 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.21712'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.21712'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.21712'}]
2024-08-04	ShieldGemma: Generative AI Content Moderation Based on Gemma	Computation and Language	https://arxiv.org/abs/2407.21772	ShieldGemma	https://x.com/omarsar0/status/1818837753292853349		2407.21772	['Wenjun Zeng', 'Yuchi Liu', 'Ryan Mullins', 'Ludovic Peran', 'Joe Fernandez', 'Hamza Harkous', 'Karthik Narasimhan', 'Drew Proud', 'Piyush Kumar', 'Bhaktipriya Radharapu', 'Olivia Sturman', 'Oscar Wahltinez']	ct:We present ShieldGemma, a comprehensive suite of LLM-based safety content moderation models built upon Gemma2. These models provide robust, state-of-the-art predictions of safety risks across key harm types (sexually explicit, dangerous content, harassment, hate speech) in both user input and LLM-generated output. By evaluating on both public and internal benchmarks, we demonstrate superior performance compared to existing models, such as Llama Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%). Additionally, we present a novel LLM-based data curation pipeline, adaptable to a variety of safety-related tasks and beyond. We have shown strong generalization performance for model trained mainly on synthetic data. By releasing ShieldGemma, we provide a valuable resource to the research community, advancing LLM safety and enabling the creation of more effective content moderation solutions for developers.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.21772', 'html': 'https://arxiv.org/html/2407.21772v2', 'tex': '/src/2407.21772', 'doi': 'https://doi.org/10.48550/arXiv.2407.21772'}	Submission history From: Wenjun Zeng [ view email ] [v1] Wed, 31 Jul 2024 17:48:14 UTC (316 KB) [v2] Sun, 4 Aug 2024 22:13:39 UTC (316 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.21772'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.21772'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.21772'}]
2024-08-04	PersonaGym: Evaluating Persona Agents and LLMs	Computation and Language	https://arxiv.org/abs/2407.18416	Evaluating Persona Agents	https://x.com/omarsar0/status/1817964944949739544		2407.18416	['Vinay Samuel', 'Henry Peng Zou', 'Yue Zhou', 'Shreyas Chaudhari', 'Ashwin Kalyan', 'Tanmay Rajpurohit', 'Ameet Deshpande', 'Karthik Narasimhan', 'Vishvak Murahari']	ct:Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.	es, 5 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.18416', 'html': 'https://arxiv.org/html/2407.18416v4', 'tex': '/src/2407.18416', 'doi': 'https://doi.org/10.48550/arXiv.2407.18416'}	Submission history From: Vinay Samuel [ view email ] [v1] Thu, 25 Jul 2024 22:24:45 UTC (2,216 KB) [v2] Mon, 29 Jul 2024 02:30:35 UTC (2,216 KB) [v3] Wed, 18 Dec 2024 14:25:08 UTC (2,221 KB) [v4] Mon, 19 May 2025 23:26:43 UTC (2,758 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.18416'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.18416'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.18416'}]
2024-08-04	Machine Unlearning in Generative AI: A Survey	Machine Learning	https://arxiv.org/abs/2407.20516	Machine Unlearning Survey	https://x.com/omarsar0/status/1818476462262906985		2407.20516	['Zheyuan Liu', 'Guangyao Dou', 'Zhaoxuan Tan', 'Yijun Tian', 'Meng Jiang']	ct:Generative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found:this https URL.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.20516', 'html': 'https://arxiv.org/html/2407.20516v1', 'tex': '/src/2407.20516', 'doi': 'https://doi.org/10.48550/arXiv.2407.20516'}	Submission history From: Zheyuan Liu [ view email ] [v1] Tue, 30 Jul 2024 03:26:09 UTC (675 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.20516'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.20516'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.20516'}]
2024-08-04	ThinK: Thinner Key Cache by Query-Driven Pruning	Computation and Language	https://arxiv.org/abs/2407.21018	ThinK	https://x.com/omarsar0/status/1818474655461621903		2407.21018	['Yuhui Xu', 'Zhanming Jie', 'Hanze Dong', 'Lei Wang', 'Xudong Lu', 'Aojun Zhou', 'Amrita Saha', 'Caiming Xiong', 'Doyen Sahoo']	ct:Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights. In response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly the same quality, enabling up to a 5x increase in batch size when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of ThinK, establishing a new baseline algorithm for efficient LLM deployment without compromising performance. Our code has been made available atthis https URL.	025 (Spotlight)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.21018', 'html': 'https://arxiv.org/html/2407.21018v3', 'tex': '/src/2407.21018', 'doi': 'https://doi.org/10.48550/arXiv.2407.21018'}	Submission history From: Yuhui Xu [ view email ] [v1] Tue, 30 Jul 2024 17:59:08 UTC (2,629 KB) [v2] Thu, 3 Oct 2024 03:03:29 UTC (3,052 KB) [v3] Thu, 27 Feb 2025 12:30:43 UTC (3,059 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.21018'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.21018'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.21018'}]
2024-08-04	Know Your Limits: A Survey of Abstention in Large Language Models	Computation and Language	https://arxiv.org/abs/2407.18418	The Art of Refusal	https://x.com/omarsar0/status/1817961056465035596		2407.18418	['Bingbing Wen', 'Jihan Yao', 'Shangbin Feng', 'Chenjun Xu', 'Yulia Tsvetkov', 'Bill Howe', 'Lucy Lu Wang']	ct:Abstention, the refusal of large language models (LLMs) to provide an answer, is increasingly recognized for its potential to mitigate hallucinations and enhance safety in LLM systems. In this survey, we introduce a framework to examine abstention from three perspectives: the query, the model, and human values. We organize the literature on abstention methods, benchmarks, and evaluation metrics using this framework, and discuss merits and limitations of prior work. We further identify and motivate areas for future research, such as whether abstention can be achieved as a meta-capability that transcends specific tasks or domains, and opportunities to optimize abstention abilities in specific contexts. In doing so, we aim to broaden the scope and impact of abstention methodologies in AI systems.	024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.18418', 'html': 'https://arxiv.org/html/2407.18418v3', 'tex': '/src/2407.18418', 'doi': 'https://doi.org/10.48550/arXiv.2407.18418'}	Submission history From: Bingbing Wen [ view email ] [v1] Thu, 25 Jul 2024 22:31:50 UTC (8,725 KB) [v2] Thu, 8 Aug 2024 17:39:47 UTC (8,699 KB) [v3] Wed, 12 Feb 2025 05:42:09 UTC (9,218 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.18418'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.18418'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.18418'}]
2024-07-28	Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach	Computation and Language	https://arxiv.org/abs/2407.16833	RAG vs. Long-Context LLMs	https://x.com/omarsar0/status/1816495687984709940		2407.16833	['Zhuowan Li', 'Cheng Li', 'Mingyang Zhang', 'Qiaozhu Mei', 'Michael Bendersky']	ct:Retrieval Augmented Generation (RAG) has been a powerful tool for Large Language Models (LLMs) to efficiently process overly lengthy contexts. However, recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to understand long contexts directly. We conduct a comprehensive comparison between RAG and long-context (LC) LLMs, aiming to leverage the strengths of both. We benchmark RAG and LC across various public datasets using three latest LLMs. Results reveal that when resourced sufficiently, LC consistently outperforms RAG in terms of average performance. However, RAG's significantly lower cost remains a distinct advantage. Based on this observation, we propose Self-Route, a simple yet effective method that routes queries to RAG or LC based on model self-reflection. Self-Route significantly reduces the computation cost while maintaining a comparable performance to LC. Our findings provide a guideline for long-context applications of LLMs using RAG and LC.	ed to EMNLP 2024 industry track	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.16833', 'html': 'https://arxiv.org/html/2407.16833v2', 'tex': '/src/2407.16833', 'doi': 'https://doi.org/10.48550/arXiv.2407.16833'}	Submission history From: Zhuowan Li [ view email ] [v1] Tue, 23 Jul 2024 20:51:52 UTC (425 KB) [v2] Thu, 17 Oct 2024 17:51:19 UTC (426 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.16833'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.16833'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.16833'}]
2024-07-28	OpenHands: An Open Platform for AI Software Developers as Generalist Agents	Software Engineering	https://arxiv.org/abs/2407.16741	OpenDevin	https://x.com/omarsar0/status/1816872317286281688		2407.16741	['Xingyao Wang', 'Boxuan Li', 'Yufan Song', 'Frank F. Xu', 'Xiangru Tang', 'Mingchen Zhuge', 'Jiayi Pan', 'Yueqi Song', 'Bowen Li', 'Jaskirat Singh', 'Hoang H. Tran', 'Fuqiang Li', 'Ren Ma', 'Mingzhang Zheng', 'Bill Qian', 'Yanjun Shao', 'Niklas Muennighoff', 'Yizhe Zhang', 'Binyuan Hui', 'Junyang Lin', 'Robert Brennan', 'Hao Peng', 'Heng Ji', 'Graham Neubig']	ct:Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2.1K contributions from over 188 contributors.	ed by ICLR 2025; Code:this https URL	['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.16741', 'html': None, 'tex': '/src/2407.16741', 'doi': 'https://doi.org/10.48550/arXiv.2407.16741'}	Submission history From: Xingyao Wang [ view email ] [v1] Tue, 23 Jul 2024 17:50:43 UTC (3,147 KB) [v2] Fri, 4 Oct 2024 14:54:08 UTC (4,594 KB) [v3] Fri, 18 Apr 2025 18:14:31 UTC (2,510 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.16741'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.16741'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.16741'}]
2024-07-28	LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference	Computation and Language	https://arxiv.org/abs/2407.14057	LazyLLM	https://x.com/omarsar0/status/1815225416409309264		2407.14057	['Qichen Fu', 'Minsik Cho', 'Thomas Merth', 'Sachin Mehta', 'Mohammad Rastegari', 'Mahyar Najibi']	ct:The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.14057', 'html': 'https://arxiv.org/html/2407.14057v1', 'tex': '/src/2407.14057', 'doi': 'https://doi.org/10.48550/arXiv.2407.14057'}	Submission history From: Qichen Fu [ view email ] [v1] Fri, 19 Jul 2024 06:34:45 UTC (1,663 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.14057'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.14057'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.14057'}]
2024-07-28	Recursive Introspection: Teaching Language Model Agents How to Self-Improve	Machine Learning	https://arxiv.org/abs/2407.18219	Teaching LLM Agents to Self-Improve	https://x.com/omarsar0/status/1816671382585114855		2407.18219	['Yuxiao Qu', 'Tianjun Zhang', 'Naman Garg', 'Aviral Kumar']	ct:A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.18219', 'html': 'https://arxiv.org/html/2407.18219v2', 'tex': '/src/2407.18219', 'doi': 'https://doi.org/10.48550/arXiv.2407.18219'}	Submission history From: Yuxiao Qu [ view email ] [v1] Thu, 25 Jul 2024 17:35:59 UTC (6,889 KB) [v2] Fri, 26 Jul 2024 17:50:27 UTC (6,894 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.18219'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.18219'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.18219'}]
2024-07-28	A Survey on Employing Large Language Models for Text-to-SQL Tasks	Computation and Language	https://arxiv.org/abs/2407.15186	Text-to-SQL Survey	https://x.com/omarsar0/status/1815599057974223015		2407.15186	['Liang Shi', 'Zhengju Tang', 'Nan Zhang', 'Xiaotong Zhang', 'Zhi Yang']	ct:With the development of the Large Language Models (LLMs), a large range of LLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a comprehensive review of LLM-based Text2SQL studies. We first enumerate classic benchmarks and evaluation metrics. For the two mainstream methods, prompt engineering and finetuning, we introduce a comprehensive taxonomy and offer practical insights into each subcategory. We present an overall analysis of the above methods and various models evaluated on well-known datasets and extract some characteristics. Finally, we discuss the challenges and future directions in this field.	ed by ACM Computing Surveys (CSUR)	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.15186', 'html': 'https://arxiv.org/html/2407.15186v5', 'tex': '/src/2407.15186', 'doi': 'https://doi.org/10.48550/arXiv.2407.15186'}	Submission history From: Liang Shi [ view email ] [v1] Sun, 21 Jul 2024 14:48:23 UTC (942 KB) [v2] Sun, 11 Aug 2024 13:54:21 UTC (1,062 KB) [v3] Mon, 9 Sep 2024 06:17:21 UTC (1,191 KB) [v4] Thu, 7 Nov 2024 03:26:58 UTC (1,654 KB) [v5] Tue, 3 Jun 2025 14:40:01 UTC (4,112 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.15186'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.15186'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.15186'}]
2024-07-28	MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2406.11271	MINT-1T	https://x.com/omarsar0/status/1816250935930142834		2406.11271	['Anas Awadalla', 'Le Xue', 'Oscar Lo', 'Manli Shu', 'Hannah Lee', 'Etash Kumar Guha', 'Matt Jordan', 'Sheng Shen', 'Mohamed Awadalla', 'Silvio Savarese', 'Caiming Xiong', 'Ran Xu', 'Yejin Choi', 'Ludwig Schmidt']	ct:Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs). Despite the rapid progression of open-source LMMs, there remains a pronounced scarcity of large-scale, diverse open-source multimodal interleaved datasets. In response, we introduce MINT-1T, the most extensive and diverse open-source Multimodal INTerleaved dataset to date. MINT-1T comprises one trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires substantial engineering effort, sharing the data curation process and releasing the dataset greatly benefits the community. Our experiments show that LMMs trained on MINT-1T rival the performance of models trained on the previous leading dataset, OBELICS. Our data and code will be released atthis https URL.		['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2406.11271', 'html': 'https://arxiv.org/html/2406.11271v5', 'tex': '/src/2406.11271', 'doi': 'https://doi.org/10.48550/arXiv.2406.11271'}	Submission history From: Anas Awadalla [ view email ] [v1] Mon, 17 Jun 2024 07:21:36 UTC (1,911 KB) [v2] Wed, 24 Jul 2024 02:59:40 UTC (2,681 KB) [v3] Wed, 31 Jul 2024 03:06:40 UTC (1,946 KB) [v4] Thu, 19 Sep 2024 20:54:33 UTC (1,925 KB) [v5] Thu, 31 Oct 2024 03:29:34 UTC (1,925 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.11271'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.11271'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.11271'}]
2024-07-28	Generation Constraint Scaling Can Mitigate Hallucination	Computation and Language	https://arxiv.org/abs/2407.16908	Mitigating Hallucination via Generation Constraint	https://x.com/omarsar0/status/1816491986209104104		2407.16908	['Georgios Kollias', 'Payel Das', 'Subhajit Chaudhury']	ct:Addressing the issue of hallucinations in large language models (LLMs) is a critical challenge. As the cognitive mechanisms of hallucination have been related to memory, here we explore hallucination for LLM that is enabled with explicit memory mechanisms. We empirically demonstrate that by simply scaling the readout vector that constrains generation in a memory-augmented LLM decoder, hallucination mitigation can be achieved in a training-free manner. Our method is geometry-inspired and outperforms a state-of-the-art LLM editing method on the task of generation of Wikipedia-like biography entries both in terms of generation quality and runtime complexity.	s; accepted at ICML 2024 Workshop on Large Language Models and Cognition	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.16908', 'html': 'https://arxiv.org/html/2407.16908v1', 'tex': '/src/2407.16908', 'doi': 'https://doi.org/10.48550/arXiv.2407.16908'}	Submission history From: Georgios Kollias [ view email ] [v1] Tue, 23 Jul 2024 23:58:19 UTC (1,027 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.16908'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.16908'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.16908'}]
2024-07-21	Prover-Verifier Games improve legibility of LLM outputs	Computation and Language	https://arxiv.org/abs/2407.13692	Improving Legibility of LLM Outputs	https://x.com/OpenAI/status/1813623470452064432		2407.13692	['Jan Hendrik Kirchner', 'Yining Chen', 'Harri Edwards', 'Jan Leike', 'Nat McAleese', 'Yuri Burda']	"ct:One way to increase confidence in the outputs of Large Language Models (LLMs) is to support them with reasoning that is clear and easy to check -- a property we call legibility. We study legibility in the context of solving grade-school math problems and show that optimizing chain-of-thought solutions only for answer correctness can make them less legible. To mitigate the loss in legibility, we propose a training algorithm inspired by Prover-Verifier Game from Anil et al. (2021). Our algorithm iteratively trains small verifiers to predict solution correctness, ""helpful"" provers to produce correct solutions that the verifier accepts, and ""sneaky"" provers to produce incorrect solutions that fool the verifier. We find that the helpful prover's accuracy and the verifier's robustness to adversarial attacks increase over the course of training. Furthermore, we show that legibility training transfers to time-constrained humans tasked with verifying solution correctness. Over course of LLM training human accuracy increases when checking the helpful prover's solutions, and decreases when checking the sneaky prover's solutions. Hence, training for checkability by small verifiers is a plausible technique for increasing output legibility. Our results suggest legibility training against small verifiers as a practical avenue for increasing legibility of large LLMs to humans, and thus could help with alignment of superhuman models."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.13692', 'html': 'https://arxiv.org/html/2407.13692v2', 'tex': '/src/2407.13692', 'doi': 'https://doi.org/10.48550/arXiv.2407.13692'}	Submission history From: Jan H. Kirchner [ view email ] [v1] Thu, 18 Jul 2024 16:58:18 UTC (1,482 KB) [v2] Thu, 1 Aug 2024 17:18:54 UTC (1,483 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.13692'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.13692'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.13692'}]
2024-07-21	SpreadsheetLLM: Encoding Spreadsheets for Large Language Models	Artificial Intelligence	https://arxiv.org/abs/2407.09025	SpreadsheetLLM	https://x.com/_akhaliq/status/1812674543963578794		2407.09025	['Haoyu Dong', 'Jianbo Zhao', 'Yuzhang Tian', 'Junyu Xiong', 'Shiyu Xia', 'Mengyu Zhou', 'Yun Lin', 'José Cambronero', 'Yeye He', 'Shi Han', 'Dongmei Zhang']	ct:Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in the spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, and achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate it in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.		['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.09025', 'html': 'https://arxiv.org/html/2407.09025v2', 'tex': '/src/2407.09025', 'doi': 'https://doi.org/10.48550/arXiv.2407.09025'}	Submission history From: Jianbo Zhao [ view email ] [v1] Fri, 12 Jul 2024 06:34:21 UTC (7,207 KB) [v2] Wed, 2 Apr 2025 14:33:38 UTC (12,847 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.09025'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.09025'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.09025'}]
2024-07-21	Context Embeddings for Efficient Answer Generation in RAG	Computation and Language	http://arxiv.org/abs/2407.09252	Context Embeddings for Efficient Answer Generation in RAG	https://x.com/omarsar0/status/1812937765769867561		2407.09252	['David Rau', 'Shuai Wang', 'Hervé Déjean', 'Stéphane Clinchant']	ct:Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge of LLMs by extending the input with external information. As a consequence, the contextual inputs to the model become much longer which slows down decoding time directly translating to the time a user has to wait for an answer. We address this challenge by presenting COCOM, an effective context compression method, reducing long contexts to only a handful of Context Embeddings speeding up the generation time by a large margin. Our method allows for different compression rates trading off decoding time for answer quality. Compared to earlier methods, COCOM allows for handling multiple contexts more effectively, significantly reducing decoding time for long inputs. Our method demonstrates a speed-up of up to 5.69 $\times$ while achieving higher performance compared to existing efficient context compression methods.	es	['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2407.09252', 'html': 'https://arxiv.org/html/2407.09252v3', 'tex': '/src/2407.09252', 'doi': 'https://doi.org/10.48550/arXiv.2407.09252'}	Submission history From: David Rau [ view email ] [v1] Fri, 12 Jul 2024 13:30:44 UTC (9,154 KB) [v2] Tue, 23 Jul 2024 12:28:31 UTC (9,156 KB) [v3] Tue, 29 Oct 2024 17:34:54 UTC (9,156 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.09252'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.09252'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.09252'}]
2024-07-21	Weak-to-Strong Reasoning	Computation and Language	https://arxiv.org/abs/2407.13647	Weak-to-Strong Reasoning	https://x.com/omarsar0/status/1814130275485704597		2407.13647	['Yuqing Yang', 'Yan Ma', 'Pengfei Liu']	ct:When large language models (LLMs) exceed human-level capabilities, it becomes increasingly challenging to provide full-scale and accurate supervision for these models. Weak-to-strong learning, which leverages a less capable model to unlock the latent abilities of a stronger model, proves valuable in this context. Yet, the efficacy of this approach for complex reasoning tasks is still untested. Furthermore, tackling reasoning tasks under the weak-to-strong setting currently lacks efficient methods to avoid blindly imitating the weak supervisor including its errors. In this paper, we introduce a progressive learning framework that enables the strong model to autonomously refine its training data, without requiring input from either a more advanced model or human-annotated data. This framework begins with supervised fine-tuning on a selective small but high-quality dataset, followed by preference optimization on contrastive samples identified by the strong model itself. Extensive experiments on the GSM8K and MATH datasets demonstrate that our method significantly enhances the reasoning capabilities of Llama2-70b using three separate weak models. This method is further validated in a forward-looking experimental setup, where Llama3-8b-instruct effectively supervises Llama3-70b on the highly challenging OlympicArena dataset. This work paves the way for a more scalable and sophisticated strategy to enhance AI reasoning powers. All relevant code and resources are available in \url{this https URL}.	Findings 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.13647', 'html': 'https://arxiv.org/html/2407.13647v2', 'tex': '/src/2407.13647', 'doi': 'https://doi.org/10.48550/arXiv.2407.13647'}	Submission history From: Yuqing Yang [ view email ] [v1] Thu, 18 Jul 2024 16:25:17 UTC (1,481 KB) [v2] Tue, 1 Oct 2024 05:28:54 UTC (1,482 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.13647'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.13647'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.13647'}]
2024-07-21	A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks	Computation and Language	https://arxiv.org/abs/2407.12994	A Survey of Prompt Engineering Methods in LLMs	https://x.com/omarsar0/status/1814135222562165104		2407.12994	['Shubham Vatsal', 'Harsh Dubey']	ct:Large language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract LLMs' knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper, we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.12994', 'html': 'https://arxiv.org/html/2407.12994v2', 'tex': '/src/2407.12994', 'doi': 'https://doi.org/10.48550/arXiv.2407.12994'}	Submission history From: Shubham Vatsal [ view email ] [v1] Wed, 17 Jul 2024 20:23:19 UTC (87 KB) [v2] Wed, 24 Jul 2024 03:53:41 UTC (88 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.12994'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.12994'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.12994'}]
2024-07-21	Does Refusal Training in LLMs Generalize to the Past Tense?	Computation and Language	https://arxiv.org/abs/2407.11969	Does Refusal Training in LLMs Generalize to the Past Tense?	https://x.com/maksym_andr/status/1813608842699079750		2407.11969	['Maksym Andriushchenko', 'Nicolas Flammarion']	"ct:Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or illegal outputs. We reveal a curious generalization gap in the current refusal training approaches: simply reformulating a harmful request in the past tense (e.g., ""How to make a Molotov cocktail?"" to ""How did people make a Molotov cocktail?"") is often sufficient to jailbreak many state-of-the-art LLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet, GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini, o1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For example, the success rate of this simple attack on GPT-4o increases from 1% using direct requests to 88% using 20 past tense reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find that reformulations in the future tense are less effective, suggesting that refusal guardrails tend to consider past historical questions more benign than hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending against past reformulations is feasible when past tense examples are explicitly included in the fine-tuning data. Overall, our findings highlight that the widely used alignment techniques -- such as SFT, RLHF, and adversarial training -- employed to align the studied models can be brittle and do not always generalize as intended. We provide code and jailbreak artifacts atthis https URL."	ed at ICLR 2025. Updates in v2 and v3: added GPT-4o, Claude 3.5 Sonnet, o1-mini, and o1-preview results. Code and jailbreak artifacts:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.11969', 'html': 'https://arxiv.org/html/2407.11969v4', 'tex': '/src/2407.11969', 'doi': 'https://doi.org/10.48550/arXiv.2407.11969'}	Submission history From: Maksym Andriushchenko [ view email ] [v1] Tue, 16 Jul 2024 17:59:55 UTC (864 KB) [v2] Fri, 19 Jul 2024 13:03:01 UTC (872 KB) [v3] Thu, 3 Oct 2024 16:46:09 UTC (3,098 KB) [v4] Thu, 17 Apr 2025 18:36:08 UTC (3,102 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.11969'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.11969'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.11969'}]
2024-07-21	NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context?	Computation and Language	https://arxiv.org/abs/2407.11963	Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?	https://x.com/omarsar0/status/1813581074624070109		2407.11963	['Mo Li', 'Songyang Zhang', 'Taolin Zhang', 'Haodong Duan', 'Yunxin Liu', 'Kai Chen']	ct:The capability of large language models to handle long-context information is crucial across various real-world applications. Existing evaluation methods often rely either on real-world long texts, making it difficult to exclude the influence of models' inherent knowledge, or introduce irrelevant filler content to artificially achieve target lengths, reducing assessment effectiveness. To address these limitations, we introduce NeedleBench, a synthetic framework for assessing retrieval and reasoning performance in bilingual long-context tasks with adaptive context lengths. NeedleBench systematically embeds key data points at varying depths to rigorously test model capabilities. Tasks are categorized into two scenarios: information-sparse, featuring minimal relevant details within extensive irrelevant text to simulate simple retrieval tasks; and information-dense (the Ancestral Trace Challenge), where relevant information is continuously distributed throughout the context to simulate complex reasoning tasks. Our experiments reveal that although recent reasoning models like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they struggle with continuous retrieval and reasoning in information-dense scenarios, even at shorter context lengths. We also characterize a phenomenon termed 'under-thinking', where models prematurely conclude reasoning despite available information. NeedleBench thus provides critical insights and targeted tools essential for evaluating and improving LLMs' long-context capabilities. All resources are available at OpenCompass:this https URL.	dated with tested models and Multi-Needle Reasoning implementation	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.11963', 'html': 'https://arxiv.org/html/2407.11963v2', 'tex': '/src/2407.11963', 'doi': 'https://doi.org/10.48550/arXiv.2407.11963'}	Submission history From: Mo Li [ view email ] [v1] Tue, 16 Jul 2024 17:59:06 UTC (1,092 KB) [v2] Fri, 9 May 2025 09:23:22 UTC (1,257 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.11963'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.11963'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.11963'}]
2024-07-21	Distilling System 2 into System 1	Computation and Language	https://arxiv.org/abs/2407.06023v1	Distilling System 2 into System 1	https://x.com/willccbb/status/1813012865454121179		2407.06023v1	['Ping Yu', 'Jing Xu', 'Jason Weston', 'Ilia Kulikov']	ct:Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to produce better final responses. Since Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have been proposed such as Rephrase and Respond (Deng et al., 2023a), System 2 Attention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al., 2023). In this work we investigate self-supervised methods to ``compile'' (distill) higher quality outputs from System 2 techniques back into LLM generations without intermediate reasoning token sequences, as this reasoning has been distilled into System 1. We show that several such techniques can be successfully distilled, resulting in improved results compared to the original System 1 performance, and with less inference cost than System 2. We posit that such System 2 distillation will be an important feature of future continually learning AI systems, enabling them to focus System 2 capabilities on the reasoning tasks that they cannot yet do well.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.06023v1', 'html': 'https://arxiv.org/html/2407.06023v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2407.06023'}	Submission history From: Jason Weston [ view email ] [v1] Mon, 8 Jul 2024 15:17:46 UTC (1,579 KB) [v2] Tue, 9 Jul 2024 16:29:11 UTC (1,579 KB) [v3] Wed, 24 Jul 2024 18:40:36 UTC (595 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.06023'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.06023'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.06023'}]
2024-07-21	Exploring Advanced Large Language Models with LLMsuite	Computation and Language	https://arxiv.org/abs/2407.12036	Exploring Advanced LLMs with LLMSuite	https://x.com/omarsar0/status/1813980712346763589		2407.12036	['Giorgio Roffo']	ct:This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategies, including instruction fine-tuning, parameter-efficient methods like LoRA, and Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced Self-Training (ReST). Additionally, it provides a comprehensive survey of transformer architectures and training techniques for LLMs. The source code can be accessed by contacting the author via email for a request.	ds: Language Model Benchmarking, Pre-Trained LLM Comparison, LLM Performance Analysis, NLP Model Evaluation Tools, Public Dataset Inference for LLMs, BLEU and ROUGE Metrics for LLM, Open Source LLM Testing Tools, Large Language Model Evaluation Software, NLP Benchmarking Suite, Comprehensive LLM Evaluation Toolkit	['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2407.12036', 'html': 'https://arxiv.org/html/2407.12036v2', 'tex': '/src/2407.12036', 'doi': 'https://doi.org/10.48550/arXiv.2407.12036'}	Submission history From: Giorgio Roffo [ view email ] [v1] Mon, 1 Jul 2024 05:37:17 UTC (3,606 KB) [v2] Tue, 12 Nov 2024 10:12:49 UTC (3,606 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.12036'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.12036'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.12036'}]
2024-07-21	Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures	Machine Learning	https://www.arxiv.org/abs/2407.09468	Beyond Euclid	https://x.com/omarsar0/status/1812927886766010653		2407.09468	['Mathilde Papillon', 'Sophia Sanborn', 'Johan Mathe', 'Louisa Cornelis', 'Abby Bertics', 'Domas Buracas', 'Hansen J Lillemark', 'Christian Shewmake', 'Fatih Dinc', 'Xavier Pennec', 'Nina Miolane']	ct:The enduring legacy of Euclidean geometry underpins classical machine learning, which, for decades, has been primarily developed for data lying in Euclidean space. Yet, modern machine learning increasingly encounters richly structured data that is inherently nonEuclidean. This data can exhibit intricate geometric, topological and algebraic structure: from the geometry of the curvature of space-time, to topologically complex interactions between neurons in the brain, to the algebraic transformations describing symmetries of physical systems. Extracting knowledge from such non-Euclidean data necessitates a broader mathematical perspective. Echoing the 19th-century revolutions that gave rise to non-Euclidean geometry, an emerging line of research is redefining modern machine learning with non-Euclidean structures. Its goal: generalizing classical methods to unconventional data types with geometry, topology, and algebra. In this review, we provide an accessible gateway to this fast-growing field and propose a graphical taxonomy that integrates recent advances into an intuitive unified framework. We subsequently extract insights into current challenges and highlight exciting opportunities for future development in this field.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.09468', 'html': 'https://arxiv.org/html/2407.09468v2', 'tex': '/src/2407.09468', 'doi': 'https://doi.org/10.48550/arXiv.2407.09468'}	Submission history From: Johan Mathe [ view email ] [v1] Fri, 12 Jul 2024 17:48:36 UTC (39,168 KB) [v2] Thu, 24 Jul 2025 17:41:43 UTC (44,110 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.09468'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.09468'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.09468'}]
2024-07-14	RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs	Computation and Language	https://arxiv.org/abs/2407.02485v1	RankRAG	https://x.com/_weiping/status/1808551184309104896		2407.02485v1	['Yue Yu', 'Wei Ping', 'Zihan Liu', 'Boxin Wang', 'Jiaxuan You', 'Chao Zhang', 'Mohammad Shoeybi', 'Bryan Catanzaro']	ct:Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.02485v1', 'html': 'https://arxiv.org/html/2407.02485v1', 'tex': '/src/2407.02485v1', 'doi': 'https://doi.org/10.48550/arXiv.2407.02485'}	Submission history From: Wei Ping [ view email ] [v1] Tue, 2 Jul 2024 17:59:17 UTC (614 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.02485'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.02485'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.02485'}]
2024-07-14	Mixture of A Million Experts	Machine Learning	https://arxiv.org/abs/2407.04153	Mixture of A Million Experts	https://x.com/omarsar0/status/1810389538340290724		2407.04153	['Xu Owen He']	ct:The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.04153', 'html': 'https://arxiv.org/html/2407.04153v1', 'tex': '/src/2407.04153', 'doi': 'https://doi.org/10.48550/arXiv.2407.04153'}	Submission history From: Xu Owen He [ view email ] [v1] Thu, 4 Jul 2024 20:59:20 UTC (536 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.04153'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.04153'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.04153'}]
2024-07-14	Reasoning in Large Language Models: A Geometric Perspective	Artificial Intelligence	https://arxiv.org/abs/2407.02678	Reasoning in LLMs: A Geometric Perspective	https://x.com/omarsar0/status/1810329294884741594		2407.02678	['Romain Cosentino', 'Sarath Shekkizhar']	ct:The advancement of large language models (LLMs) for real-world applications hinges critically on enhancing their reasoning capabilities. In this work, we explore the reasoning abilities of large language models (LLMs) through their geometrical understanding. We establish a connection between the expressive power of LLMs and the density of their self-attention graphs. Our analysis demonstrates that the density of these graphs defines the intrinsic dimension of the inputs to the MLP blocks. We demonstrate through theoretical analysis and toy examples that a higher intrinsic dimension implies a greater expressive capacity of the LLM. We further provide empirical evidence linking this geometric framework to recent advancements in methods aimed at enhancing the reasoning capabilities of LLMs.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.02678', 'html': 'https://arxiv.org/html/2407.02678v1', 'tex': '/src/2407.02678', 'doi': 'https://doi.org/10.48550/arXiv.2407.02678'}	Submission history From: Romain Cosentino Dr [ view email ] [v1] Tue, 2 Jul 2024 21:39:53 UTC (1,611 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.02678'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.02678'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.02678'}]
2024-07-14	Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps	Computation and Language	https://arxiv.org/abs/2407.07071	Contextual Hallucinations Mitigation in LLMs	https://x.com/omarsar0/status/1811072508637884750		2407.07071	['Yung-Sung Chuang', 'Linlu Qiu', 'Cheng-Yu Hsieh', 'Ranjay Krishna', 'Yoon Kim', 'James Glass']	ct:When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.	2024 main conference long paper. The source code is available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.07071', 'html': 'https://arxiv.org/html/2407.07071v2', 'tex': '/src/2407.07071', 'doi': 'https://doi.org/10.48550/arXiv.2407.07071'}	Submission history From: Yung-Sung Chuang [ view email ] [v1] Tue, 9 Jul 2024 17:44:34 UTC (733 KB) [v2] Thu, 3 Oct 2024 17:26:48 UTC (737 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.07071'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.07071'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.07071'}]
2024-07-14	RouteLLM: Learning to Route LLMs with Preference Data	Machine Learning	https://arxiv.org/abs/2406.18665v2	RouteLLM	https://x.com/lmsysorg/status/1807812671238258931		2406.18665v2	['Isaac Ong', 'Amjad Almahairi', 'Vincent Wu', 'Wei-Lin Chiang', 'Tianhao Wu', 'Joseph E. Gonzalez', 'M Waleed Kadous', 'Ion Stoica']	ct:Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.18665v2', 'html': 'https://arxiv.org/html/2406.18665v2', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2406.18665'}	Submission history From: Isaac Ong [ view email ] [v1] Wed, 26 Jun 2024 18:10:22 UTC (580 KB) [v2] Mon, 1 Jul 2024 05:38:08 UTC (623 KB) [v3] Sun, 21 Jul 2024 10:33:08 UTC (623 KB) [v4] Sun, 23 Feb 2025 08:50:33 UTC (782 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.18665'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.18665'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.18665'}]
2024-07-14	A Survey on Mixture of Experts in Large Language Models	Machine Learning	https://arxiv.org/abs/2407.06204	A Survey on Mixture of Experts	https://x.com/omarsar0/status/1811127876819026283		2407.06204	['Weilin Cai', 'Juyong Jiang', 'Fan Wang', 'Jing Tang', 'Sunghun Kim', 'Jiayi Huang']	ct:Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE research, we have established a resource repository atthis https URL.	rst three authors contributed equally to this work; Accepted by TKDE	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.06204', 'html': 'https://arxiv.org/html/2407.06204v3', 'tex': '/src/2407.06204', 'doi': 'https://doi.org/10.48550/arXiv.2407.06204'}	Submission history From: Weilin Cai [ view email ] [v1] Wed, 26 Jun 2024 16:34:33 UTC (3,620 KB) [v2] Thu, 8 Aug 2024 07:13:37 UTC (8,003 KB) [v3] Wed, 9 Apr 2025 13:54:59 UTC (865 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.06204'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.06204'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.06204'}]
2024-07-14	Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence	Computation and Language	https://arxiv.org/abs/2407.07061v2	Internet of Agents	https://x.com/_akhaliq/status/1810872693501157855		2407.07061v2	['Weize Chen', 'Ziming You', 'Ran Li', 'Yitong Guan', 'Chen Qian', 'Chenyang Zhao', 'Cheng Yang', 'Ruobing Xie', 'Zhiyuan Liu', 'Maosong Sun']	ct:The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. Our codebase has been released at \url{this https URL}.	n progress	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.07061v2', 'html': 'https://arxiv.org/html/2407.07061v2', 'tex': '/src/2407.07061v2', 'doi': 'https://doi.org/10.48550/arXiv.2407.07061'}	Submission history From: Weize Chen [ view email ] [v1] Tue, 9 Jul 2024 17:33:24 UTC (4,812 KB) [v2] Wed, 10 Jul 2024 15:57:21 UTC (4,812 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.07061'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.07061'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.07061'}]
2024-07-14	Learning to (Learn at Test Time): RNNs with Expressive Hidden States	Machine Learning	https://arxiv.org/abs/2407.04620	Learning at Test Time	https://x.com/arankomatsuzaki/status/1810148710258508046		2407.04620	['Yu Sun', 'Xinhao Li', 'Karan Dalal', 'Jiarui Xu', 'Arjun Vikram', 'Genghan Zhang', 'Yann Dubois', 'Xinlei Chen', 'Xiaolong Wang', 'Sanmi Koyejo', 'Tatsunori Hashimoto', 'Carlos Guestrin']	ct:Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden states. We present a practical framework for instantiating sequence modeling layers with linear complexity and expressive hidden states. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.	rrent version contains updates on related work and limitations. All experiments were completed in the first version	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.04620', 'html': 'https://arxiv.org/html/2407.04620v3', 'tex': '/src/2407.04620', 'doi': 'https://doi.org/10.48550/arXiv.2407.04620'}	Submission history From: Yu Sun [ view email ] [v1] Fri, 5 Jul 2024 16:23:20 UTC (897 KB) [v2] Sun, 11 Aug 2024 00:42:18 UTC (897 KB) [v3] Thu, 3 Apr 2025 18:30:11 UTC (924 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.04620'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.04620'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.04620'}]
2024-07-07	APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets	Computation and Language	https://arxiv.org/abs/2406.18518	APIGen	https://x.com/Benioff/status/1808365628551844186		2406.18518	['Zuxin Liu', 'Thai Hoang', 'Jianguo Zhang', 'Ming Zhu', 'Tian Lan', 'Shirley Kokane', 'Juntao Tan', 'Weiran Yao', 'Zhiwei Liu', 'Yihao Feng', 'Rithesh Murthy', 'Liangwei Yang', 'Silvio Savarese', 'Juan Carlos Niebles', 'Huan Wang', 'Shelby Heinecke', 'Caiming Xiong']	ct:The advancement of function-calling agent models requires diverse, reliable, and high-quality datasets. This paper presents APIGen, an automated data generation pipeline designed to synthesize verifiable high-quality datasets for function-calling applications. We leverage APIGen and collect 3,673 executable APIs across 21 different categories to generate diverse function-calling datasets in a scalable and structured manner. Each data in our dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, ensuring its reliability and correctness. We demonstrate that models trained with our curated datasets, even with only 7B parameters, can achieve state-of-the-art performance on the Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models. Moreover, our 1B model achieves exceptional performance, surpassing GPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000 high-quality entries, aiming to advance the field of function-calling agent domains. The dataset is available on Huggingface:this https URLand the project homepage:this https URL		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2406.18518', 'html': None, 'tex': '/src/2406.18518', 'doi': 'https://doi.org/10.48550/arXiv.2406.18518'}	Submission history From: Zuxin Liu [ view email ] [v1] Wed, 26 Jun 2024 17:49:11 UTC (1,586 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.18518'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.18518'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.18518'}]
2024-07-07	Searching for Best Practices in Retrieval-Augmented Generation	Computation and Language	https://arxiv.org/abs/2407.01219	Searching for Best Practices in RAG	https://x.com/omarsar0/status/1808177231342018748		2407.01219	['Xiaohua Wang', 'Zhenghua Wang', 'Xuan Gao', 'Feiran Zhang', 'Yixin Wu', 'Zhibo Xu', 'Tianyuan Shi', 'Zhengyuan Wang', 'Shizheng Li', 'Qi Qian', 'Ruicheng Yin', 'Changze Lv', 'Xiaoqing Zheng', 'Xuanjing Huang']	"ct:Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a ""retrieval as generation"" strategy."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.01219', 'html': 'https://arxiv.org/html/2407.01219v1', 'tex': '/src/2407.01219', 'doi': 'https://doi.org/10.48550/arXiv.2407.01219'}	Submission history From: Xiaohua Wang [ view email ] [v1] Mon, 1 Jul 2024 12:06:34 UTC (844 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.01219'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.01219'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.01219'}]
2024-07-07	Scaling Synthetic Data Creation with 1,000,000,000 Personas	Computation and Language	https://arxiv.org/abs/2406.20094	Scaling Synthetic Data Creation	https://x.com/omarsar0/status/1807827401122238628		2406.20094	['Tao Ge', 'Xin Chan', 'Xiaoyang Wang', 'Dian Yu', 'Haitao Mi', 'Dong Yu']	ct:We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.	n progress	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2406.20094', 'html': 'https://arxiv.org/html/2406.20094v3', 'tex': '/src/2406.20094', 'doi': 'https://doi.org/10.48550/arXiv.2406.20094'}	Submission history From: Tao Ge [ view email ] [v1] Fri, 28 Jun 2024 17:59:01 UTC (2,583 KB) [v2] Tue, 24 Sep 2024 00:38:10 UTC (2,583 KB) [v3] Thu, 8 May 2025 00:24:02 UTC (2,583 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.20094'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.20094'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.20094'}]
2024-07-07	Self-Evaluation as a Defense Against Adversarial Attacks on LLMs	Machine Learning	https://arxiv.org/abs/2407.03234	Self-Evaluation as a Defense Against Adversarial Attacks on LLMs	https://x.com/omarsar0/status/1809241930963853621		2407.03234	['Hannah Brown', 'Leon Lin', 'Kenji Kawaguchi', 'Michael Shieh']	ct:We introduce a defense against adversarial attacks on LLMs utilizing self-evaluation. Our method requires no model fine-tuning, instead using pre-trained models to evaluate the inputs and outputs of a generator model, significantly reducing the cost of implementation in comparison to other, finetuning-based methods. Our method can significantly reduce the attack success rate of attacks on both open and closed-source LLMs, beyond the reductions demonstrated by Llama-Guard2 and commonly used content moderation APIs. We present an analysis of the effectiveness of our method, including attempts to attack the evaluator in various settings, demonstrating that it is also more resilient to attacks than existing methods. Code and data will be made available atthis https URL.	s, 7 figures	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Cryptography and Security (cs.CR)']	{'pdf': '/pdf/2407.03234', 'html': None, 'tex': '/src/2407.03234', 'doi': 'https://doi.org/10.48550/arXiv.2407.03234'}	Submission history From: Hannah Brown [ view email ] [v1] Wed, 3 Jul 2024 16:03:42 UTC (865 KB) [v2] Mon, 15 Jul 2024 05:20:18 UTC (867 KB) [v3] Tue, 6 Aug 2024 11:15:00 UTC (867 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.03234'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.03234'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.03234'}]
2024-07-07	Agentless: Demystifying LLM-based Software Engineering Agents	Software Engineering	https://arxiv.org/abs/2407.01489	Agentless	https://x.com/LingmingZhang/status/1808501612056629569		2407.01489	['Chunqiu Steven Xia', 'Yinlin Deng', 'Soren Dunn', 'Lingming Zhang']	ct:Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless -- an agentless approach to automatically solve software development problems. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (32.00%, 96 correct fixes) and low cost ($0.70) compared with all existing open-source software agents! Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patch or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-S by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the current overlooked potential of a simple, interpretable technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction.		['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.01489', 'html': 'https://arxiv.org/html/2407.01489v2', 'tex': '/src/2407.01489', 'doi': 'https://doi.org/10.48550/arXiv.2407.01489'}	Submission history From: Chunqiu Steven Xia [ view email ] [v1] Mon, 1 Jul 2024 17:24:45 UTC (504 KB) [v2] Tue, 29 Oct 2024 17:29:27 UTC (653 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.01489'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.01489'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.01489'}]
2024-07-07	Adaptable Logical Control for Large Language Models	Computation and Language	https://arxiv.org/abs/2406.13892	Adaptable Logical Control for LLMs	https://x.com/HonghuaZhang2/status/1806727439823102325		2406.13892	['Honghua Zhang', 'Po-Nien Kung', 'Masahiro Yoshida', 'Guy Van den Broeck', 'Nanyun Peng']	ct:Despite the success of Large Language Models (LLMs) on various tasks following human instructions, controlling model generation at inference time poses a persistent challenge. In this paper, we introduce Ctrl-G, an adaptable framework that facilitates tractable and flexible control of LLM generation to reliably follow logical constraints. Ctrl-G combines any production-ready LLM with a Hidden Markov Model, enabling LLM outputs to adhere to logical constraints represented as deterministic finite automata. We show that Ctrl-G, when applied to a TULU2-7B model, outperforms GPT3.5 and GPT4 on the task of interactive text editing: specifically, for the task of generating text insertions/continuations following logical constraints, Ctrl-G achieves over 30% higher satisfaction rate in human evaluation compared to GPT4. When applied to medium-size language models (e.g., GPT2-large), Ctrl-G also beats its counterparts for constrained generation by large margins on standard benchmarks. Additionally, as a proof-of-concept study, we experiment Ctrl-G on the Grade School Math benchmark to assist LLM reasoning, foreshadowing the application of Ctrl-G, as well as other constrained generation approaches, beyond traditional language generation tasks.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.13892', 'html': 'https://arxiv.org/html/2406.13892v2', 'tex': '/src/2406.13892', 'doi': 'https://doi.org/10.48550/arXiv.2406.13892'}	Submission history From: Honghua Zhang [ view email ] [v1] Wed, 19 Jun 2024 23:47:59 UTC (1,601 KB) [v2] Fri, 16 Aug 2024 19:51:51 UTC (1,602 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.13892'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.13892'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.13892'}]
2024-07-07	LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives	Computation and Language	https://arxiv.org/abs/2407.01490	LLM See, LLM Do	https://x.com/lushimabucoro/status/1808083881632878843		2407.01490	['Luísa Shimabucoro', 'Sebastian Ruder', 'Julia Kreutzer', 'Marzieh Fadaee', 'Sara Hooker']	"ct:The widespread adoption of synthetic data raises new questions about how models generating the data can influence other large language models (LLMs) via distilled data. To start, our work exhaustively characterizes the impact of passive inheritance of model properties by systematically studying the consequences of synthetic data integration. We provide one of the most comprehensive studies to-date of how the source of synthetic data shapes models' internal biases, calibration and generations' textual attributes and preferences. We find that models are surprisingly sensitive towards certain attributes even when the synthetic data prompts appear ""neutral"". which invites the question whether this sensitivity can be exploited for good.Our findings invite the question can we explicitly steer the models towards the properties we want at test time by exploiting the data generation process? This would have historically been considered infeasible due to the cost of collecting data with a specific characteristic or objective in mind. However, improvement in the quality of synthetic data, as well as a shift towards general-purpose models designed to follow a diverse way of instructions, means this question is timely. We propose active inheritance as a term to describe intentionally constraining synthetic data according to a non-differentiable objective. We demonstrate how active inheritance can steer the generation profiles of models towards desirable non-differentiable attributes, e.g. high lexical diversity or low toxicity."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2407.01490', 'html': 'https://arxiv.org/html/2407.01490v2', 'tex': '/src/2407.01490', 'doi': 'https://doi.org/10.48550/arXiv.2407.01490'}	Submission history From: Luísa Shimabucoro [ view email ] [v1] Mon, 1 Jul 2024 17:26:21 UTC (426 KB) [v2] Fri, 19 Jul 2024 10:45:21 UTC (435 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.01490'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.01490'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.01490'}]
2024-07-07	Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems	Computation and Language	https://arxiv.org/abs/2407.01370	Summary of a Haystack	https://x.com/_philschmid/status/1808420168558649479		2407.01370	['Philippe Laban', 'Alexander R. Fabbri', 'Caiming Xiong', 'Chien-Sheng Wu']	"ct:LLMs and RAG systems are now capable of handling millions of input tokens or more. However, evaluating the output quality of such systems on long-context tasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity. In this work, we argue that summarization can play a central role in such evaluation. We design a procedure to synthesize Haystacks of documents, ensuring that specific \textit{insights} repeat across documents. The ""Summary of a Haystack"" (SummHay) task then requires a system to process the Haystack and generate, given a query, a summary that identifies the relevant insights and precisely cites the source documents. Since we have precise knowledge of what insights should appear in a haystack summary and what documents should be cited, we implement a highly reproducible automatic evaluation that can score summaries on two aspects - Coverage and Citation. We generate Haystacks in two domains (conversation, news), and perform a large-scale evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate that SummHay is an open challenge for current systems, as even systems provided with an Oracle signal of document relevance lag our estimate of human performance (56\%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also be used to study enterprise RAG systems and position bias in long-context models. We hope future systems can equal and surpass human performance on SummHay."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2407.01370', 'html': 'https://arxiv.org/html/2407.01370v1', 'tex': '/src/2407.01370', 'doi': 'https://doi.org/10.48550/arXiv.2407.01370'}	Submission history From: Philippe Laban [ view email ] [v1] Mon, 1 Jul 2024 15:23:42 UTC (8,575 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.01370'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.01370'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.01370'}]
2024-07-07	AI Agents That Matter	Machine Learning	https://arxiv.org/abs/2407.01502	AI Agents That Matter	https://x.com/random_walker/status/1808138818182434955		2407.01502	['Sayash Kapoor', 'Benedikt Stroebl', 'Zachary S. Siegel', 'Nitya Nadgir', 'Arvind Narayanan']	ct:AI agents are an exciting new research direction, and agent development is driven by benchmarks. Our analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. Our focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. We design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. We prescribe a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. We hope that the steps we introduce for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2407.01502', 'html': 'https://arxiv.org/html/2407.01502v1', 'tex': '/src/2407.01502', 'doi': 'https://doi.org/10.48550/arXiv.2407.01502'}	Submission history From: Sayash Kapoor [ view email ] [v1] Mon, 1 Jul 2024 17:48:14 UTC (1,112 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2407.01502'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2407.01502'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2407.01502'}]
2024-06-30	LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs	Computation and Language	https://arxiv.org/abs/2406.15319	Enhancing RAG with Long-Context LLMs	https://x.com/omarsar0/status/1805230323799560199		2406.15319	['Ziyan Jiang', 'Xueguang Ma', 'Wenhu Chen']	ct:In traditional RAG framework, the basic retrieval units are normally short. The common retrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design forces the retriever to search over a large corpus to find the `needle' unit. In contrast, the readers only need to generate answers from the short retrieved units. The imbalanced `heavy' retriever and `light' reader design can lead to sub-optimal performance. The loss of contextual information in the short, chunked units may increase the likelihood of introducing hard negatives during the retrieval stage. Additionally, the reader might not fully leverage the capabilities of recent advancements in LLMs. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a `long retriever' and a `long reader'. In the two Wikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire Wikipedia corpus into 4K-token units by grouping related documents. By increasing the unit size, we significantly reduce the total number of units. This greatly reduces the burden on the retriever, resulting in strong retrieval performance with only a few (less than 8) top units. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which are on par with the (fully-trained) SoTA model. Furthermore, we test on two non-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes each individual document as a single (long) unit rather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5% on MultiFieldQA-en. Our study offers insights into the future roadmap for combining RAG with long-context LLMs.	cal Report	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2406.15319', 'html': 'https://arxiv.org/html/2406.15319v3', 'tex': '/src/2406.15319', 'doi': 'https://doi.org/10.48550/arXiv.2406.15319'}	Submission history From: Ziyan Jiang [ view email ] [v1] Fri, 21 Jun 2024 17:23:21 UTC (465 KB) [v2] Sun, 30 Jun 2024 15:01:36 UTC (659 KB) [v3] Sun, 1 Sep 2024 17:21:18 UTC (1,834 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.15319'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.15319'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.15319'}]
2024-06-30	From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data	Machine Learning	https://arxiv.org/abs/2406.19292	Improving Retrieval in LLMs through Synthetic Data	https://x.com/omarsar0/status/1806738385039692033		2406.19292	['Zheyang Xiong', 'Vasilis Papageorgiou', 'Kangwook Lee', 'Dimitris Papailiopoulos']	ct:Recent studies have shown that Large Language Models (LLMs) struggle to accurately retrieve information and maintain reasoning capabilities when processing long-context inputs. To address these limitations, we propose a finetuning approach utilizing a carefully designed synthetic dataset comprising numerical key-value retrieval tasks. Our experiments on models like GPT-3.5 Turbo and Mistral 7B demonstrate that finetuning LLMs on this dataset significantly improves LLMs' information retrieval and reasoning capabilities in longer-context settings. We present an analysis of the finetuned models, illustrating the transfer of skills from synthetic to real task evaluations (e.g., $10.5\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5 Turbo). We also find that finetuned LLMs' performance on general benchmarks remains almost constant while LLMs finetuned on other baseline long-context augmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B finetuned on our synthetic data cause no performance drop while other baseline data can cause a drop that ranges from $2.33\%$ to $6.19\%$). Our study highlights the potential of finetuning on synthetic data for improving the performance of LLMs on longer-context tasks.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.19292', 'html': None, 'tex': '/src/2406.19292', 'doi': 'https://doi.org/10.48550/arXiv.2406.19292'}	Submission history From: Zheyang Xiong [ view email ] [v1] Thu, 27 Jun 2024 16:05:13 UTC (165 KB) [v2] Mon, 14 Oct 2024 02:58:42 UTC (165 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.19292'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.19292'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.19292'}]
2024-06-30	GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models	Computation and Language	https://arxiv.org/abs/2406.14550v1	GraphReader	https://x.com/omarsar0/status/1806802925517218078		2406.14550v1	['Shilong Li', 'Yancheng He', 'Hangyu Guo', 'Xingyuan Bu', 'Ge Bai', 'Jie Liu', 'Jiaheng Liu', 'Xingwei Qu', 'Yangguang Li', 'Wanli Ouyang', 'Wenbo Su', 'Bo Zheng']	ct:Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.	rst four authors contributed equally, 27 pages	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2406.14550v1', 'html': 'https://arxiv.org/html/2406.14550v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2406.14550'}	Submission history From: Xingyuan Bu [ view email ] [v1] Thu, 20 Jun 2024 17:57:51 UTC (544 KB) [v2] Tue, 5 Nov 2024 16:51:40 UTC (1,342 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.14550'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.14550'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.14550'}]
2024-06-30	EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees	Computation and Language	https://arxiv.org/abs/2406.16858	Faster LLM Inference with Dynamic Draft Trees	https://x.com/omarsar0/status/1805629496634294760		2406.16858	['Yuhui Li', 'Fangyun Wei', 'Chao Zhang', 'Hongyang Zhang']	ct:Inference with modern Large Language Models (LLMs) is expensive and time-consuming, and speculative sampling has proven to be an effective solution. Most speculative sampling methods such as EAGLE use a static draft tree, implicitly assuming that the acceptance rate of draft tokens depends only on their position. Interestingly, we found that the acceptance rate of draft tokens is also context-dependent. In this paper, building upon EAGLE, we propose EAGLE-2, which introduces a new technique of context-aware dynamic draft tree into drafting modeling. This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors. We conducted extensive evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving speedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a lossless acceleration algorithm.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2406.16858', 'html': 'https://arxiv.org/html/2406.16858v2', 'tex': '/src/2406.16858', 'doi': 'https://doi.org/10.48550/arXiv.2406.16858'}	Submission history From: Yuhui Li [ view email ] [v1] Mon, 24 Jun 2024 17:59:11 UTC (1,463 KB) [v2] Sun, 30 Jun 2024 15:03:25 UTC (1,464 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.16858'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.16858'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.16858'}]
2024-06-30	Following Length Constraints in Instructions	Computation and Language	https://arxiv.org/abs/2406.17744	Following Length Constraints in Instructions	https://x.com/jaseweston/status/1805771223747481690		2406.17744	['Weizhe Yuan', 'Ilia Kulikov', 'Ping Yu', 'Kyunghyun Cho', 'Sainbayar Sukhbaatar', 'Jason Weston', 'Jing Xu']	ct:Aligned instruction following models can better fulfill user requests than their unaligned counterparts. However, it has been shown that there is a length bias in evaluation of such models, and that training algorithms tend to exploit this bias by learning longer responses. In this work we show how to train models that can be controlled at inference time with instructions containing desired length constraints. Such models are superior in length instructed evaluations, outperforming standard instruction following models such as GPT4, Llama 3 and Mixtral.	es	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.17744', 'html': 'https://arxiv.org/html/2406.17744v1', 'tex': '/src/2406.17744', 'doi': 'https://doi.org/10.48550/arXiv.2406.17744'}	Submission history From: Weizhe Yuan [ view email ] [v1] Tue, 25 Jun 2024 17:29:52 UTC (1,318 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.17744'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.17744'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.17744'}]
2024-06-30	On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey	Computation and Language	https://arxiv.org/abs/2406.15126	On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation	https://x.com/omarsar0/status/1805652404404207919		2406.15126	['Lin Long', 'Rui Wang', 'Ruixuan Xiao', 'Junbo Zhao', 'Xiao Ding', 'Gang Chen', 'Haobo Wang']	ct:Within the evolving landscape of deep learning, the dilemma of data quantity and quality has been a long-standing problem. The recent advent of Large Language Models (LLMs) offers a data-centric solution to alleviate the limitations of real-world data with synthetic data generation. However, current investigations into this field lack a unified framework and mostly stay on the surface. Therefore, this paper provides an organization of relevant studies based on a generic workflow of synthetic data generation. By doing so, we highlight the gaps within existing research and outline prospective avenues for future study. This work aims to shepherd the academic and industrial communities towards deeper, more methodical inquiries into the capabilities and applications of LLMs-driven synthetic data generation.	ey on LLMs-driven synthetic data generation, curation and evaluation	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.15126', 'html': 'https://arxiv.org/html/2406.15126v1', 'tex': '/src/2406.15126', 'doi': 'https://doi.org/10.48550/arXiv.2406.15126'}	Submission history From: Lin Long [ view email ] [v1] Fri, 14 Jun 2024 07:47:09 UTC (19,282 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.15126'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.15126'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.15126'}]
2024-06-30	Adam-mini: Use Fewer Learning Rates To Gain More	Machine Learning	https://arxiv.org/abs/2406.16793	Adam-mini	https://x.com/arankomatsuzaki/status/1805439246318125299		2406.16793	['Yushun Zhang', 'Congliang Chen', 'Ziniu Li', 'Tian Ding', 'Chenwei Wu', 'Diederik P. Kingma', 'Yinyu Ye', 'Zhi-Quan Luo', 'Ruoyu Sun']	ct:We propose Adam-mini, an optimizer that achieves on par or better performance than AdamW with 50% less memory footprint. Adam-mini reduces memory by cutting down the learning rate resources in Adam (i.e., $1/\sqrt{v}$). By investigating the Hessian structure of neural nets, we find Adam's $v$ might not function at its full potential as effectively as we expected. We find that $\geq$ 99.9% of these learning rates in $v$ could be harmlessly removed if we (1) carefully partition the parameters into blocks following our new principle on Hessian structure; (2) assign a single but good learning rate to each parameter block. We then provide one simple way to find good learning rates and propose Adam-mini. Empirically, we verify that Adam-mini performs on par or better than AdamW on various language models sized from 39M to 13B for pre-training, supervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini also alleviates communication overheads among GPUs, thereby increasing throughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamW when pre-training Llama 2-7B on $2\times$ A800-80GB GPUs, which saves 33% wall-clock time for pre-training.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2406.16793', 'html': 'https://arxiv.org/html/2406.16793v7', 'tex': '/src/2406.16793', 'doi': 'https://doi.org/10.48550/arXiv.2406.16793'}	Submission history From: Yushun Zhang [ view email ] [v1] Mon, 24 Jun 2024 16:56:41 UTC (7,091 KB) [v2] Tue, 25 Jun 2024 17:45:06 UTC (7,450 KB) [v3] Wed, 26 Jun 2024 13:03:16 UTC (7,450 KB) [v4] Mon, 1 Jul 2024 17:46:19 UTC (7,449 KB) [v5] Wed, 3 Jul 2024 16:38:17 UTC (7,451 KB) [v6] Mon, 11 Nov 2024 16:59:58 UTC (11,191 KB) [v7] Mon, 24 Feb 2025 11:29:08 UTC (11,657 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.16793'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.16793'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.16793'}]
2024-06-23	"TextGrad: Automatic ""Differentiation"" via Text"	Computation and Language	https://arxiv.org/abs/2406.07496v1	TextGrad	https://x.com/james_y_zou/status/1800917174124740667		2406.07496v1	['Mert Yuksekgonul', 'Federico Bianchi', 'Joseph Boen', 'Sheng Liu', 'Zhi Huang', 'Carlos Guestrin', 'James Zou']	ct:AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components. As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges. Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key. Inspired by this, we introduce TextGrad, a powerful framework performing automatic ``differentiation'' via text. TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system. In our framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures. TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use. It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework. We showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning. Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\%$ to $55\%$, yields $20\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity. TextGrad lays a foundation to accelerate the development of the next-generation of AI systems.	es, 6 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2406.07496v1', 'html': 'https://arxiv.org/html/2406.07496v1', 'tex': '/src/2406.07496v1', 'doi': 'https://doi.org/10.48550/arXiv.2406.07496'}	Submission history From: Mert Yuksekgonul [ view email ] [v1] Tue, 11 Jun 2024 17:32:21 UTC (766 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.07496'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.07496'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.07496'}]
2024-06-23	Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?	Computation and Language	https://arxiv.org/abs/2406.13121	Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?	https://x.com/omarsar0/status/1804184820806766875		2406.13121	['Jinhyuk Lee', 'Anthony Chen', 'Zhuyun Dai', 'Dheeru Dua', 'Devendra Singh Sachan', 'Michael Boratko', 'Yi Luan', 'Sébastien M. R. Arnold', 'Vincent Perot', 'Siddharth Dalmia', 'Hexiang Hu', 'Xudong Lin', 'Panupong Pasupat', 'Aida Amini', 'Jeremy R. Cole', 'Sebastian Riedel', 'Iftekhar Naim', 'Ming-Wei Chang', 'Kelvin Guu']	ct:Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of information offers numerous advantages. It enhances user-friendliness by eliminating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. To assess this paradigm shift, we introduce LOFT, a benchmark of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs' performance on in-context retrieval and reasoning. Our findings reveal LCLMs' surprising ability to rival state-of-the-art retrieval and RAG systems, despite never having been explicitly trained for these tasks. However, LCLMs still face challenges in areas like compositional reasoning that are required in SQL-like tasks. Notably, prompting strategies significantly influence performance, emphasizing the need for continued research as context lengths grow. Overall, LOFT provides a rigorous testing ground for LCLMs, showcasing their potential to supplant existing paradigms and tackle novel tasks as model capabilities scale.	es. Dataset available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2406.13121', 'html': 'https://arxiv.org/html/2406.13121v1', 'tex': '/src/2406.13121', 'doi': 'https://doi.org/10.48550/arXiv.2406.13121'}	Submission history From: Jinhyuk Lee [ view email ] [v1] Wed, 19 Jun 2024 00:28:58 UTC (2,124 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.13121'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.13121'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.13121'}]
2024-06-23	PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers	Computation and Language	https://arxiv.org/abs/2406.12430	PlanRAG	https://x.com/omarsar0/status/1803262374574448757		2406.12430	['Myeonghwa Lee', 'Seonho An', 'Min-Soo Kim']	ct:In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define Decision QA as the task of answering the best decision, $d_{best}$, for a decision-making question $Q$, business rules $R$ and a database $D$. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, DQA. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA. To address Decision QA effectively, we also propose a new RAG technique called the iterative plan-then-retrieval augmented generation (PlanRAG). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark atthis https URL.	2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2406.12430', 'html': 'https://arxiv.org/html/2406.12430v1', 'tex': '/src/2406.12430', 'doi': 'https://doi.org/10.48550/arXiv.2406.12430'}	Submission history From: Seonho An [ view email ] [v1] Tue, 18 Jun 2024 09:25:35 UTC (9,674 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.12430'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.12430'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.12430'}]
2024-06-23	Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs	Computation and Language	https://arxiv.org/abs/2406.10209	Mitigating Memorization in LLMs	https://x.com/omarsar0/status/1802729440163647754		2406.10209	['Abhimanyu Hans', 'Yuxin Wen', 'Neel Jain', 'John Kirchenbauer', 'Hamid Kazemi', 'Prajwal Singhania', 'Siddharth Singh', 'Gowthami Somepalli', 'Jonas Geiping', 'Abhinav Bhatele', 'Tom Goldstein']	ct:Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale Llama-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.	es, 8 figures, and 1 table in the main body. Code available atthis https URLand checkpoints atthis https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.10209', 'html': 'https://arxiv.org/html/2406.10209v2', 'tex': '/src/2406.10209', 'doi': 'https://doi.org/10.48550/arXiv.2406.10209'}	Submission history From: Abhimanyu Hans [ view email ] [v1] Fri, 14 Jun 2024 17:44:22 UTC (660 KB) [v2] Sat, 2 Nov 2024 23:19:18 UTC (239 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.10209'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.10209'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.10209'}]
2024-06-23	Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B	Artificial Intelligence	https://arxiv.org/abs/2406.07394v2	Monte Carlos Tree Self-Refine	https://x.com/rohanpaul_ai/status/1801259208341373013		2406.07394v2	['Di Zhang', 'Xiaoshui Huang', 'Dongzhan Zhou', 'Yuqiang Li', 'Wanli Ouyang']	ct:This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.		['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2406.07394v2', 'html': 'https://arxiv.org/html/2406.07394v2', 'tex': '/src/2406.07394v2', 'doi': 'https://doi.org/10.48550/arXiv.2406.07394'}	Submission history From: Di Zhang [ view email ] [v1] Tue, 11 Jun 2024 16:01:07 UTC (106 KB) [v2] Thu, 13 Jun 2024 07:19:06 UTC (106 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.07394'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.07394'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.07394'}]
2024-06-23	From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries	Computation and Language	https://arxiv.org/abs/2406.12824	From RAG to Rich Parameters	https://x.com/omarsar0/status/1803254134289895555		2406.12824	['Hitesh Wadhwa', 'Rahul Seetharaman', 'Somyaa Aggarwal', 'Reshmi Ghosh', 'Samyadeep Basu', 'Soundararajan Srinivasan', 'Wenlong Zhao', 'Shreyas Chaudhari', 'Ehsan Aghazadeh']	ct:Retrieval Augmented Generation (RAG) enriches the ability of language models to reason using external context to augment responses for a given user prompt. This approach has risen in popularity due to practical applications in various applications of language models in search, question/answering, and chat-bots. However, the exact nature of how this approach works isn't clearly understood. In this paper, we mechanistically examine the RAG pipeline to highlight that language models take shortcut and have a strong bias towards utilizing only the context information to answer the question, while relying minimally on their parametric memory. We probe this mechanistic behavior in language models with: (i) Causal Mediation Analysis to show that the parametric memory is minimally utilized when answering a question and (ii) Attention Contributions and Knockouts to show that the last token residual stream do not get enriched from the subject token in the question, but gets enriched from other informative tokens in the context. We find this pronounced shortcut behaviour true across both LLaMa and Phi family of models.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2406.12824', 'html': 'https://arxiv.org/html/2406.12824v1', 'tex': '/src/2406.12824', 'doi': 'https://doi.org/10.48550/arXiv.2406.12824'}	Submission history From: Reshmi Ghosh [ view email ] [v1] Tue, 18 Jun 2024 17:46:08 UTC (4,502 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.12824'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.12824'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.12824'}]
2024-06-16	Discovering Preference Optimization Algorithms with and for Large Language Models	Machine Learning	https://arxiv.org/abs/2406.08414	Discovering Preference Optimization Algorithms with LLMs	https://x.com/SakanaAILabs/status/1801069076003082502		2406.08414	['Chris Lu', 'Samuel Holt', 'Claudio Fanconi', 'Alex J. Chan', 'Jakob Foerster', 'Mihaela van der Schaar', 'Robert Tjarko Lange']	ct:Offline preference optimization is a key method for enhancing and controlling the quality of Large Language Model (LLM) outputs. Typically, preference optimization is approached as an offline supervised learning task using manually-crafted convex loss functions. While these methods are based on theoretical insights, they are inherently constrained by human creativity, so the large search space of possible loss functions remains under explored. We address this by performing LLM-driven objective discovery to automatically discover new state-of-the-art preference optimization algorithms without (expert) human intervention. Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously-evaluated performance metrics. This process leads to the discovery of previously-unknown and performant preference optimization algorithms. The best performing of these we call Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses. Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2406.08414', 'html': 'https://arxiv.org/html/2406.08414v3', 'tex': '/src/2406.08414', 'doi': 'https://doi.org/10.48550/arXiv.2406.08414'}	Submission history From: Christopher Lu [ view email ] [v1] Wed, 12 Jun 2024 16:58:41 UTC (4,614 KB) [v2] Sun, 1 Sep 2024 22:58:51 UTC (2,216 KB) [v3] Sat, 2 Nov 2024 22:34:31 UTC (5,787 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.08414'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.08414'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.08414'}]
2024-06-16	SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals	Computation and Language	https://arxiv.org/abs/2406.04784	SelfGoal	https://x.com/omarsar0/status/1800183982404829457		2406.04784	['Ruihan Yang', 'Jiangjie Chen', 'Yikai Zhang', 'Siyu Yuan', 'Aili Chen', 'Kyle Richardson', 'Yanghua Xiao', 'Deqing Yang']	ct:Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming. However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed. In this paper, we present SelfGoal, a novel automatic approach designed to enhance agents' capabilities to achieve high-level goals with limited human prior and environmental feedback. The core concept of SelfGoal involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most useful subgoals and progressively updating this structure. Experimental results demonstrate that SelfGoal significantly enhances the performance of language agents across various tasks, including competitive, cooperative, and deferred feedback environments. Project page:this https URL.	nt	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2406.04784', 'html': None, 'tex': '/src/2406.04784', 'doi': 'https://doi.org/10.48550/arXiv.2406.04784'}	Submission history From: Jiangjie Chen [ view email ] [v1] Fri, 7 Jun 2024 09:32:03 UTC (321 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.04784'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.04784'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.04784'}]
2024-06-16	Mixture-of-Agents Enhances Large Language Model Capabilities	Computation and Language	https://arxiv.org/abs/2406.04692	Mixture-of-Agents	https://x.com/togethercompute/status/1800536106729157054		2406.04692	['Junlin Wang', 'Jue Wang', 'Ben Athiwaratkun', 'Ce Zhang', 'James Zou']	ct:Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.04692', 'html': 'https://arxiv.org/html/2406.04692v1', 'tex': '/src/2406.04692', 'doi': 'https://doi.org/10.48550/arXiv.2406.04692'}	Submission history From: Jue Wang [ view email ] [v1] Fri, 7 Jun 2024 07:04:10 UTC (861 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.04692'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.04692'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.04692'}]
2024-06-16	Transformers meet Neural Algorithmic Reasoners	Computation and Language	https://arxiv.org/abs/2406.09308	Transformers Meet Neural Algorithmic Reasoners	https://x.com/omarsar0/status/1801448036389843228		2406.09308	['Wilfried Bounsi', 'Borja Ibarz', 'Andrew Dudzik', 'Jessica B. Hamrick', 'Larisa Markeeva', 'Alex Vitvitskyi', 'Razvan Pascanu', 'Petar Veličković']	ct:Transformers have revolutionized machine learning with their simple yet effective architecture. Pre-training Transformers on massive text datasets from the Internet has led to unmatched generalization for natural language understanding (NLU) tasks. However, such language models remain fragile when tasked with algorithmic forms of reasoning, where computations must be precise and robust. To address this limitation, we propose a novel approach that combines the Transformer's language understanding with the robustness of graph neural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs proved effective as generic solvers for algorithmic tasks, when specified in graph form. To make their embeddings accessible to a Transformer, we propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the language model to cross-attend to the node embeddings from the NAR. We evaluate our resulting TransNAR model on CLRS-Text, the text-based version of the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only models for algorithmic reasoning, both in and out of distribution.	ear at CVPR 2024 Multimodal Algorithmic Reasoning (MAR) Workshop. 10 pages, 5 figures	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2406.09308', 'html': 'https://arxiv.org/html/2406.09308v1', 'tex': '/src/2406.09308', 'doi': 'https://doi.org/10.48550/arXiv.2406.09308'}	Submission history From: Wilfried Bounsi [ view email ] [v1] Thu, 13 Jun 2024 16:42:06 UTC (969 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.09308'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.09308'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.09308'}]
2024-06-16	Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching	Computation and Language	https://arxiv.org/abs/2406.06326	Self-Tuning with LLMs	https://x.com/omarsar0/status/1800552376513810463		2406.06326	['Xiaoying Zhang', 'Baolin Peng', 'Ye Tian', 'Jingyan Zhou', 'Yipeng Zhang', 'Haitao Mi', 'Helen Meng']	ct:Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from unseen raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on various models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.	25 Findings	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.06326', 'html': 'https://arxiv.org/html/2406.06326v5', 'tex': '/src/2406.06326', 'doi': 'https://doi.org/10.48550/arXiv.2406.06326'}	Submission history From: Xiaoying Zhang [ view email ] [v1] Mon, 10 Jun 2024 14:42:20 UTC (4,820 KB) [v2] Tue, 11 Jun 2024 15:03:43 UTC (4,821 KB) [v3] Sat, 15 Jun 2024 09:45:37 UTC (4,821 KB) [v4] Sat, 15 Feb 2025 03:22:07 UTC (4,846 KB) [v5] Fri, 16 May 2025 05:30:57 UTC (4,846 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.06326'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.06326'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.06326'}]
2024-06-16	Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2406.09403	Sketching as a Visual Chain of Thought	https://x.com/omarsar0/status/1801450829234188760		2406.09403	['Yushi Hu', 'Weijia Shi', 'Xingyu Fu', 'Dan Roth', 'Mari Ostendorf', 'Luke Zettlemoyer', 'Noah A Smith', 'Ranjay Krishna']	ct:Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions are missing in current multimodal language models (LMs). Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps. In this work, we introduce Sketchpad, a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts planning and reasoning according to the visual artifacts it has drawn. Different from prior work, which uses text-to-image models to enable LMs to draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is closer to human sketching and better facilitates reasoning. Sketchpad can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning. We experiment with a wide range of math tasks (including geometry, functions, graphs, and chess) and complex visual reasoning tasks. Sketchpad substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial reasoning (83.9%), and visual correspondence (80.8%). All codes and data are inthis https URL.	ed to NeurIPS 2024. Project and codes url:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.09403', 'html': 'https://arxiv.org/html/2406.09403v3', 'tex': '/src/2406.09403', 'doi': 'https://doi.org/10.48550/arXiv.2406.09403'}	Submission history From: Yushi Hu [ view email ] [v1] Thu, 13 Jun 2024 17:59:31 UTC (27,041 KB) [v2] Wed, 10 Jul 2024 18:09:56 UTC (27,041 KB) [v3] Mon, 11 Nov 2024 00:54:32 UTC (27,041 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.09403'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.09403'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.09403'}]
2024-06-16	Multimodal Table Understanding	Computation and Language	https://arxiv.org/abs/2406.08100	Multimodal Table Understanding	https://x.com/omarsar0/status/1801271773796716646		2406.08100	['Mingyu Zheng', 'Xinwei Feng', 'Qingyi Si', 'Qiaoqiao She', 'Zheng Lin', 'Wenbin Jiang', 'Weiping Wang']	ct:Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings. The code and data is available at thisthis https URL	es, 16 figures, ACL 2024 main conference, camera-ready version	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2406.08100', 'html': 'https://arxiv.org/html/2406.08100v1', 'tex': '/src/2406.08100', 'doi': 'https://doi.org/10.48550/arXiv.2406.08100'}	Submission history From: Mingyu Zheng [ view email ] [v1] Wed, 12 Jun 2024 11:27:03 UTC (10,797 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.08100'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.08100'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.08100'}]
2024-06-16	An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding	Computation and Language	https://arxiv.org/abs/2406.07138	Consistent Middle Enhancement in LLMs	https://x.com/omarsar0/status/1800903031736631473		2406.07138	['Tong Wu', 'Yanpeng Zhao', 'Zilong Zheng']	"ct:Recently, many methods have been developed to extend the context length of pre-trained large language models (LLMs), but they often require fine-tuning at the target length ($\gg4K$) and struggle to effectively utilize information from the middle part of the context. To address these issues, we propose $\textbf{C}$ontinuity-$\textbf{R}$elativity ind$\textbf{E}$xing with g$\textbf{A}$ussian $\textbf{M}$iddle ($\texttt{CREAM}$), which interpolates positional encodings by manipulating position indices. Apart from being simple, $\texttt{CREAM}$ is training-efficient: it only requires fine-tuning at the pre-trained context window (e.g., Llama 2-4K) and can extend LLMs to a much longer target context length (e.g., 256K). To ensure that the model focuses more on the information in the middle, we introduce a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning, thus alleviating the ""Lost-in-the-Middle"" problem faced by long-context LLMs. Experimental results show that $\texttt{CREAM}$ successfully extends LLMs to the target length for both Base and Chat versions of $\texttt{Llama2-7B}$ with ""Never Miss A Beat"". Our code is publicly available atthis https URL."	ed by NeurIPS 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.07138', 'html': 'https://arxiv.org/html/2406.07138v2', 'tex': '/src/2406.07138', 'doi': 'https://doi.org/10.48550/arXiv.2406.07138'}	Submission history From: Tong Wu [ view email ] [v1] Tue, 11 Jun 2024 10:35:49 UTC (1,319 KB) [v2] Thu, 10 Oct 2024 07:46:14 UTC (1,331 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.07138'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.07138'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.07138'}]
2024-06-09	Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality	Machine Learning	https://arxiv.org/abs/2405.21060	Mamba-2	https://x.com/_albertgu/status/1797651223035904355		2405.21060	['Tri Dao', 'Albert Gu']	ct:While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.	024	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2405.21060', 'html': None, 'tex': '/src/2405.21060', 'doi': 'https://doi.org/10.48550/arXiv.2405.21060'}	Submission history From: Albert Gu [ view email ] [v1] Fri, 31 May 2024 17:50:01 UTC (1,815 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.21060'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.21060'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.21060'}]
2024-06-09	Scalable MatMul-free Language Modeling	Computation and Language	https://arxiv.org/abs/2406.02528	MatMul-free LLMs	https://x.com/omarsar0/status/1798373841741185261		2406.02528	['Rui-Jie Zhu', 'Yu Zhang', 'Steven Abreu', 'Ethan Sifferman', 'Tyler Sheaves', 'Yiqiao Wang', 'Dustin Richmond', 'Sumit Bam Shrestha', 'Peng Zhou', 'Jason K. Eshraghian']	ct:Large Language Models (LLMs) have fundamentally altered how we approach scaling in machine learning. However, these models pose substantial computational and memory challenges, primarily due to the reliance on matrix multiplication (MatMul) within their attention and feed-forward (FFN) layers. We demonstrate that MatMul operations can be eliminated from LLMs while maintaining strong performance, even at billion-parameter scales. Our MatMul-free models, tested on models up to 2.7B parameters, are comparable to state-of-the-art pre-trained Transformers, and the performance gap narrows as model size increases. Our approach yields significant memory savings: a GPU-efficient implementation reduces memory consumption by up to 61% during training and over 10x during inference. When adapted for a multi-chip neuromorphic system, the model leverages asynchronous processing to achieve 4x higher throughput with 10x less energy than edge GPUs.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.02528', 'html': 'https://arxiv.org/html/2406.02528v7', 'tex': '/src/2406.02528', 'doi': 'https://doi.org/10.48550/arXiv.2406.02528'}	Submission history From: Rui-Jie Zhu [ view email ] [v1] Tue, 4 Jun 2024 17:50:34 UTC (1,050 KB) [v2] Mon, 10 Jun 2024 14:55:29 UTC (1,051 KB) [v3] Tue, 11 Jun 2024 06:18:28 UTC (1,051 KB) [v4] Fri, 14 Jun 2024 07:48:33 UTC (1,051 KB) [v5] Tue, 18 Jun 2024 17:30:06 UTC (1,051 KB) [v6] Mon, 14 Jul 2025 03:30:40 UTC (2,238 KB) [v7] Fri, 25 Jul 2025 22:38:22 UTC (2,238 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.02528'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.02528'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.02528'}]
2024-06-09	Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models	Computation and Language	https://arxiv.org/abs/2406.04271	Buffer of Thoughts	https://x.com/omarsar0/status/1799113545696567416		2406.04271	['Ling Yang', 'Zhaochen Yu', 'Tianjun Zhang', 'Shiyi Cao', 'Minkai Xu', 'Wentao Zhang', 'Joseph E. Gonzalez', 'Bin Cui']	ct:We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11% on Game of 24, 20% on Geometric Shapes and 51% on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Notably, we find that our Llama3-8B+BoT has the potential to surpass Llama3-70B model. Our project is available at:this https URL	S 2024 Spotlight. Project:this https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.04271', 'html': None, 'tex': '/src/2406.04271', 'doi': 'https://doi.org/10.48550/arXiv.2406.04271'}	Submission history From: Ling Yang [ view email ] [v1] Thu, 6 Jun 2024 17:22:08 UTC (1,852 KB) [v2] Mon, 14 Oct 2024 07:12:27 UTC (1,852 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.04271'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.04271'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.04271'}]
2024-06-09	SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales	Computation and Language	https://arxiv.org/abs/2405.20974	SaySelf	https://x.com/omarsar0/status/1797682549608833477		2405.20974	['Tianyang Xu', 'Shujin Wu', 'Shizhe Diao', 'Xiaoze Liu', 'Xingyao Wang', 'Yangyi Chen', 'Jing Gao']	ct:Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public atthis https URL.	2024 Main	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2405.20974', 'html': 'https://arxiv.org/html/2405.20974v3', 'tex': '/src/2405.20974', 'doi': 'https://doi.org/10.48550/arXiv.2405.20974'}	Submission history From: Yangyi Chen [ view email ] [v1] Fri, 31 May 2024 16:21:16 UTC (10,502 KB) [v2] Wed, 5 Jun 2024 17:04:01 UTC (11,287 KB) [v3] Fri, 4 Oct 2024 17:23:48 UTC (11,350 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.20974'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.20974'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.20974'}]
2024-06-09	The Geometry of Categorical and Hierarchical Concepts in Large Language Models	Computation and Language	https://arxiv.org/abs/2406.01506	The Geometry of Concepts in LLMs	https://x.com/omarsar0/status/1798010546522103898		2406.01506	['Kiho Park', 'Yo Joong Choe', 'Yibo Jiang', 'Victor Veitch']	ct:The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has shown how to make this notion precise for representing binary concepts that have natural contrasts (e.g., {male, female}) as directions in representation space. However, many natural concepts do not have natural contrasts (e.g., whether the output is about an animal). In this work, we show how to extend the formalization of the linear representation hypothesis to represent features (e.g., is_animal) as vectors. This allows us to immediately formalize the representation of categorical concepts as polytopes in the representation space. Further, we use the formalization to prove a relationship between the hierarchical structure of concepts and the geometry of their representations. We validate these theoretical results on the Gemma and LLaMA-3 large language models, estimating representations for 900+ hierarchically related concepts using data from WordNet.	ed for an oral presentation at ICLR 2025. Best Paper Award at the ICML 2024 Workshop on Mechanistic Interpretability. Code is available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2406.01506', 'html': 'https://arxiv.org/html/2406.01506v3', 'tex': '/src/2406.01506', 'doi': 'https://doi.org/10.48550/arXiv.2406.01506'}	Submission history From: Kiho Park [ view email ] [v1] Mon, 3 Jun 2024 16:34:01 UTC (6,202 KB) [v2] Wed, 9 Oct 2024 03:39:11 UTC (6,963 KB) [v3] Tue, 18 Feb 2025 02:23:45 UTC (6,969 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.01506'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.01506'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.01506'}]
2024-06-09	Aligning Language Models with Demonstrated Feedback	Computation and Language	https://arxiv.org/abs/2406.00888	Aligning LLMs with Demonstrated Feedback	https://x.com/arankomatsuzaki/status/1797833884463472653		2406.00888	['Omar Shaikh', 'Michelle S. Lam', 'Joey Hejna', 'Yijia Shao', 'Hyundong Cho', 'Michael S. Bernstein', 'Diyi Yang']	ct:Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (< 10) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. Concretely, DITTO operates by having an LLM generate examples that are presumed to be inferior to expert demonstrations. The method iteratively constructs pairwise preference relationships between these LLM-generated samples and expert demonstrations, potentially including comparisons between different training checkpoints. These constructed preference pairs are then used to train the model using a preference optimization algorithm (e.g. DPO). We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants (N = 16). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an avg. of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.	025; 28 pages, 8 figures	['Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2406.00888', 'html': 'https://arxiv.org/html/2406.00888v2', 'tex': '/src/2406.00888', 'doi': 'https://doi.org/10.48550/arXiv.2406.00888'}	Submission history From: Omar Shaikh [ view email ] [v1] Sun, 2 Jun 2024 23:13:56 UTC (844 KB) [v2] Fri, 18 Apr 2025 19:45:34 UTC (930 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.00888'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.00888'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.00888'}]
2024-06-09	Towards Scalable Automated Alignment of LLMs: A Survey	Computation and Language	https://arxiv.org/abs/2406.01252	Towards Scalable Automated Alignment of LLMs	https://x.com/omarsar0/status/1798014572663583165		2406.01252	['Boxi Cao', 'Keming Lu', 'Xinyu Lu', 'Jiawei Chen', 'Mengjie Ren', 'Hao Xiang', 'Peilin Liu', 'Yaojie Lu', 'Ben He', 'Xianpei Han', 'Le Sun', 'Hongyu Lin', 'Bowen Yu']	ct:Alignment is the most critical step in building large language models (LLMs) that meet human needs. With the rapid development of LLMs gradually surpassing human capabilities, traditional alignment methods based on human-annotation are increasingly unable to meet the scalability demands. Therefore, there is an urgent need to explore new sources of automated alignment signals and technical approaches. In this paper, we systematically review the recently emerging methods of automated alignment, attempting to explore how to achieve effective, scalable, automated alignment once the capabilities of LLMs exceed those of humans. Specifically, we categorize existing automated alignment methods into 4 major categories based on the sources of alignment signals and discuss the current status and potential development of each category. Additionally, we explore the underlying mechanisms that enable automated alignment and discuss the essential factors that make automated alignment technologies feasible and effective from the fundamental role of alignment.	List:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2406.01252', 'html': 'https://arxiv.org/html/2406.01252v3', 'tex': '/src/2406.01252', 'doi': 'https://doi.org/10.48550/arXiv.2406.01252'}	Submission history From: Boxi Cao [ view email ] [v1] Mon, 3 Jun 2024 12:10:26 UTC (2,088 KB) [v2] Wed, 17 Jul 2024 03:26:31 UTC (2,097 KB) [v3] Tue, 3 Sep 2024 07:07:59 UTC (2,099 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.01252'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.01252'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.01252'}]
2024-06-09	AgentGym: Evolving Large Language Model-based Agents across Diverse Environments	Artificial Intelligence	https://arxiv.org/abs/2406.04151	AgentGym	https://x.com/arankomatsuzaki/status/1798904095669121443		2406.04151	['Zhiheng Xi', 'Yiwen Ding', 'Wenxiang Chen', 'Boyang Hong', 'Honglin Guo', 'Junzhe Wang', 'Dingwen Yang', 'Chenyang Liao', 'Xin Guo', 'Wei He', 'Songyang Gao', 'Lu Chen', 'Rui Zheng', 'Yicheng Zou', 'Tao Gui', 'Qi Zhang', 'Xipeng Qiu', 'Xuanjing Huang', 'Zuxuan Wu', 'Yu-Gang Jiang']	ct:Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available onthis https URL.	t site:this https URL	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2406.04151', 'html': 'https://arxiv.org/html/2406.04151v1', 'tex': '/src/2406.04151', 'doi': 'https://doi.org/10.48550/arXiv.2406.04151'}	Submission history From: Zhiheng Xi [ view email ] [v1] Thu, 6 Jun 2024 15:15:41 UTC (2,776 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2406.04151'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2406.04151'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2406.04151'}]
2024-06-02	Contextual Position Encoding: Learning to Count What's Important	Computation and Language	https://arxiv.org/abs/2405.18719	Contextual Position Encoding	https://x.com/jaseweston/status/1795978611784089799		2405.18719	['Olga Golovneva', 'Tianlu Wang', 'Jason Weston', 'Sainbayar Sukhbaatar']	ct:The attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but is order-invariant. Incorporating position encoding (PE) makes it possible to address by position, such as attending to the i-th token. However, current PE methods use token counts to derive position, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence. In this paper, we propose a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the $i$-th particular word, noun, or sentence. We show that CoPE can solve the selective copy, counting and Flip-Flop tasks where popular position embeddings fail, and improves perplexity on language modeling and coding tasks.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2405.18719', 'html': 'https://arxiv.org/html/2405.18719v2', 'tex': '/src/2405.18719', 'doi': 'https://doi.org/10.48550/arXiv.2405.18719'}	Submission history From: Jason Weston [ view email ] [v1] Wed, 29 May 2024 02:57:15 UTC (2,991 KB) [v2] Thu, 30 May 2024 17:51:53 UTC (2,991 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.18719'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.18719'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.18719'}]
2024-06-02	Faithful Logical Reasoning via Symbolic Chain-of-Thought	Computation and Language	https://arxiv.org/abs/2405.18357	Symbolic Chain-of-Thought	https://x.com/omarsar0/status/1795925943543898157		2405.18357	['Jundong Xu', 'Hao Fei', 'Liangming Pan', 'Qian Liu', 'Mong-Li Lee', 'Wynne Hsu']	ct:While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open atthis https URL.	ed by ACL 2024 (main proceeding)	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.18357', 'html': None, 'tex': '/src/2405.18357', 'doi': 'https://doi.org/10.48550/arXiv.2405.18357'}	Submission history From: Jundong Xu [ view email ] [v1] Tue, 28 May 2024 16:55:33 UTC (517 KB) [v2] Tue, 11 Jun 2024 07:41:03 UTC (517 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.18357'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.18357'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.18357'}]
2024-06-02	Transformers Can Do Arithmetic with the Right Embeddings	Machine Learning	https://arxiv.org/abs/2405.17399	Abacus Embeddings	https://x.com/omarsar0/status/1795552696432202045		2405.17399	['Sean McLeish', 'Arpit Bansal', 'Alex Stein', 'Neel Jain', 'John Kirchenbauer', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Abhinav Bhatele', 'Jonas Geiping', 'Avi Schwarzschild', 'Tom Goldstein']	ct:The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further.With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2405.17399', 'html': 'https://arxiv.org/html/2405.17399v2', 'tex': '/src/2405.17399', 'doi': 'https://doi.org/10.48550/arXiv.2405.17399'}	Submission history From: Sean McLeish [ view email ] [v1] Mon, 27 May 2024 17:49:18 UTC (1,221 KB) [v2] Mon, 23 Dec 2024 12:46:06 UTC (1,334 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.17399'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.17399'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.17399'}]
2024-06-02	An Introduction to Vision-Language Modeling	Machine Learning	https://arxiv.org/abs/2405.17247	Introduction to Vision-Language Modeling	https://x.com/AIatMeta/status/1795499770519392499		2405.17247	['Florian Bordes', 'Richard Yuanzhe Pang', 'Anurag Ajay', 'Alexander C. Li', 'Adrien Bardes', 'Suzanne Petryk', 'Oscar Mañas', 'Zhiqiu Lin', 'Anas Mahmoud', 'Bargav Jayaraman', 'Mark Ibrahim', 'Melissa Hall', 'Yunyang Xiong', 'Jonathan Lebensold', 'Candace Ross', 'Srihari Jayakumar', 'Chuan Guo', 'Diane Bouchacourt', 'Haider Al-Tahan', 'Karthik Padthe', 'Vasu Sharma', 'Hu Xu', 'Xiaoqing Ellen Tan', 'Megan Richards', 'Samuel Lavoie', 'Pietro Astolfi', 'Reyhane Askari Hemmat', 'Jun Chen', 'Kushal Tirumala', 'Rim Assouel', 'Mazda Moayeri', 'Arjang Talattof', 'Kamalika Chaudhuri', 'Zechun Liu', 'Xilun Chen', 'Quentin Garrido', 'Karen Ullrich', 'Aishwarya Agrawal', 'Kate Saenko', 'Asli Celikyilmaz', 'Vikas Chandra']	ct:Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2405.17247', 'html': 'https://arxiv.org/html/2405.17247v1', 'tex': '/src/2405.17247', 'doi': 'https://doi.org/10.48550/arXiv.2405.17247'}	Submission history From: Florian Bordes [ view email ] [v1] Mon, 27 May 2024 15:01:23 UTC (9,026 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.17247'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.17247'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.17247'}]
2024-06-02	GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning	Computation and Language	https://arxiv.org/abs/2405.20139	GNN-RAG	https://x.com/omarsar0/status/1796578239105679585		2405.20139	['Costas Mavromatis', 'George Karypis']	ct:Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG. Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2405.20139', 'html': 'https://arxiv.org/html/2405.20139v1', 'tex': '/src/2405.20139', 'doi': 'https://doi.org/10.48550/arXiv.2405.20139'}	Submission history From: Costas Mavromatis [ view email ] [v1] Thu, 30 May 2024 15:14:24 UTC (218 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.20139'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.20139'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.20139'}]
2024-06-02	Attention as an RNN	Machine Learning	https://arxiv.org/abs/2405.13956	Attention as an RNN	https://x.com/iScienceLuvr/status/1793933723756286075		2405.13956	['Leo Feng', 'Frederick Tung', 'Hossein Hajimirsadeghi', 'Mohamed Osama Ahmed', 'Yoshua Bengio', 'Greg Mori']	ct:The advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its \textit{many-to-one} RNN output efficiently. We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, we (3) introduce a new efficient method of computing attention's \textit{many-to-many} RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we (4) introduce \textbf{Aaren}, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on $38$ datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2405.13956', 'html': 'https://arxiv.org/html/2405.13956v2', 'tex': '/src/2405.13956', 'doi': 'https://doi.org/10.48550/arXiv.2405.13956'}	Submission history From: Leo Feng [ view email ] [v1] Wed, 22 May 2024 19:45:01 UTC (1,074 KB) [v2] Tue, 28 May 2024 04:44:06 UTC (1,068 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.13956'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.13956'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.13956'}]
2024-06-02	Aya 23: Open Weight Releases to Further Multilingual Progress	Computation and Language	https://arxiv.org/abs/2405.15032	Aya23	https://x.com/CohereForAI/status/1794044201677574446		2405.15032	['Viraat Aryabumi', 'John Dang', 'Dwarak Talupuru', 'Saurabh Dash', 'David Cairuz', 'Hangyu Lin', 'Bharat Venkitesh', 'Madeline Smith', 'Jon Ander Campos', 'Yi Chern Tan', 'Kelly Marchisio', 'Max Bartolo', 'Sebastian Ruder', 'Acyr Locatelli', 'Julia Kreutzer', 'Nick Frosst', 'Aidan Gomez', 'Phil Blunsom', 'Marzieh Fadaee', 'Ahmet Üstün', 'Sara Hooker']	ct:This technical report introduces Aya 23, a family of multilingual language models. Aya 23 builds on the recent release of the Aya model (Üstün et al., 2024), focusing on pairing a highly performant pre-trained model with the recently released Aya collection (Singh et al., 2024). The result is a powerful multilingual large language model serving 23 languages, expanding state-of-art language modeling capabilities to approximately half of the world's population. The Aya model covered 101 languages whereas Aya 23 is an experiment in depth vs breadth, exploring the impact of allocating more capacity to fewer languages that are included during pre-training. Aya 23 outperforms both previous massively multilingual models like Aya 101 for the languages it covers, as well as widely used models like Gemma, Mistral and Mixtral on an extensive range of discriminative and generative tasks. We release the open weights for both the 8B and 35B models as part of our continued commitment for expanding access to multilingual progress.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.15032', 'html': 'https://arxiv.org/html/2405.15032v2', 'tex': '/src/2405.15032', 'doi': 'https://doi.org/10.48550/arXiv.2405.15032'}	Submission history From: Ahmet Üstün [ view email ] [v1] Thu, 23 May 2024 20:10:38 UTC (8,040 KB) [v2] Fri, 31 May 2024 14:47:55 UTC (8,040 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.15032'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.15032'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.15032'}]
2024-06-02	Are Long-LLMs A Necessity For Long-Context Tasks?	Computation and Language	https://arxiv.org/abs/2405.15318	Are Long-LLMs A Necessity For Long-Context Tasks?	https://x.com/omarsar0/status/1795188655243264299		2405.15318	['Hongjin Qian', 'Zheng Liu', 'Peitian Zhang', 'Kelong Mao', 'Yujia Zhou', 'Xu Chen', 'Zhicheng Dou']	ct:The learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we argue that the long-LLMs are not a necessity to solve long-context tasks, as common long-context tasks are short-context solvable, i.e. they can be solved by purely working with oracle short-contexts within the long-context tasks' inputs. On top of this argument, we propose a framework called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the long-context tasks in a bootstrapping manner. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, LC-Boost can serve as a general framework to handle diversified long-context processing problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where LC-Boost is able to achieve a substantially improved performance with a much smaller consumption of resource.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2405.15318', 'html': 'https://arxiv.org/html/2405.15318v1', 'tex': '/src/2405.15318', 'doi': 'https://doi.org/10.48550/arXiv.2405.15318'}	Submission history From: Hongjin Qian [ view email ] [v1] Fri, 24 May 2024 07:59:30 UTC (374 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.15318'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.15318'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.15318'}]
2024-06-02	SimPO: Simple Preference Optimization with a Reference-Free Reward	Computation and Language	https://arxiv.org/abs/2405.14734	SimPO	https://x.com/rasbt/status/1794711330085036061		2405.14734	['Yu Meng', 'Mengzhou Xia', 'Danqi Chen']	ct:Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further improving the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on extensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench, and Arena-Hard. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Gemma-2-9B-it, achieves a 72.4% length-controlled win rate on AlpacaEval 2, a 59.1% win rate on Arena-Hard, and ranks 1st on Chatbot Arena among <10B models with real user votes.	S 2024. Code & models:this https URL. v3 updates: Gemma 2 results (Appendix J); more discussions about length normalization (Section 2.2) and KL regularization (Section 2.3)	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2405.14734', 'html': 'https://arxiv.org/html/2405.14734v3', 'tex': '/src/2405.14734', 'doi': 'https://doi.org/10.48550/arXiv.2405.14734'}	Submission history From: Yu Meng [ view email ] [v1] Thu, 23 May 2024 16:01:46 UTC (4,221 KB) [v2] Mon, 8 Jul 2024 17:55:24 UTC (5,539 KB) [v3] Fri, 1 Nov 2024 20:05:19 UTC (4,728 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.14734'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.14734'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.14734'}]
2024-05-26	Agent Planning with World Knowledge Model	Computation and Language	https://arxiv.org/abs/2405.14205	Agent Planning with World Knowledge Model	https://x.com/omarsar0/status/1793851075411296761		2405.14205	['Shuofei Qiao', 'Runnan Fang', 'Ningyu Zhang', 'Yuqi Zhu', 'Xiang Chen', 'Shumin Deng', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen']	ct:Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ``real'' physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. The code is available atthis https URL.	S 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2405.14205', 'html': None, 'tex': '/src/2405.14205', 'doi': 'https://doi.org/10.48550/arXiv.2405.14205'}	Submission history From: Ningyu Zhang [ view email ] [v1] Thu, 23 May 2024 06:03:19 UTC (9,157 KB) [v2] Tue, 15 Oct 2024 13:58:17 UTC (4,562 KB) [v3] Thu, 19 Dec 2024 02:35:48 UTC (4,562 KB) [v4] Fri, 3 Jan 2025 16:44:55 UTC (4,562 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.14205'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.14205'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.14205'}]
2024-05-26	Risks and Opportunities of Open-Source Generative AI	Machine Learning	https://arxiv.org/abs/2405.08597	Risks and Opportunities of Open-Source Generative AI	https://x.com/fgirbal/status/1791454665764159794		2405.08597	['Francisco Eiras', 'Aleksandar Petrov', 'Bertie Vidgen', 'Christian Schroeder', 'Fabio Pizzati', 'Katherine Elkins', 'Supratik Mukhopadhyay', 'Adel Bibi', 'Aaron Purewal', 'Csaba Botos', 'Fabro Steibel', 'Fazel Keshtkar', 'Fazl Barez', 'Genevieve Smith', 'Gianluca Guadagni', 'Jon Chun', 'Jordi Cabot', 'Joseph Imperial', 'Juan Arturo Nolazco', 'Lori Landay', 'Matthew Jackson', 'Phillip H. S. Torr', 'Trevor Darrell', 'Yong Lee', 'Jakob Foerster']	ct:Applications of Generative AI (Gen AI) are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about the potential risks of the technology, and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. This regulation is likely to put at risk the budding field of open-source generative AI. Using a three-stage framework for Gen AI development (near, mid and long-term), we analyze the risks and opportunities of open-source generative AI models with similar capabilities to the ones currently available (near to mid-term) and with greater capabilities (long-term). We argue that, overall, the benefits of open-source Gen AI outweigh its risks. As such, we encourage the open sourcing of models, training and evaluation data, and provide a set of recommendations and best practices for managing risks associated with open-source generative AI.	ion ofarXiv:2404.17047	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2405.08597', 'html': 'https://arxiv.org/html/2405.08597v3', 'tex': '/src/2405.08597', 'doi': 'https://doi.org/10.48550/arXiv.2405.08597'}	Submission history From: Francisco Eiras [ view email ] [v1] Tue, 14 May 2024 13:37:36 UTC (303 KB) [v2] Fri, 24 May 2024 13:12:49 UTC (303 KB) [v3] Wed, 29 May 2024 10:05:40 UTC (303 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.08597'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.08597'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.08597'}]
2024-05-26	Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models	Computation and Language	https://arxiv.org/abs/2405.12939	Enhancing Answer Selection in LLMs	https://x.com/omarsar0/status/1793132875237163405		2405.12939	['Zhangyue Yin', 'Qiushi Sun', 'Qipeng Guo', 'Zhiyuan Zeng', 'Xiaonan Li', 'Tianxiang Sun', 'Cheng Chang', 'Qinyuan Cheng', 'Ding Wang', 'Xiaofeng Mou', 'Xipeng Qiu', 'XuanJing Huang']	ct:Recent advancements in Chain-of-Thought prompting have facilitated significant breakthroughs for Large Language Models (LLMs) in complex reasoning tasks. Current research enhances the reasoning performance of LLMs by sampling multiple reasoning chains and ensembling based on the answer frequency. However, this approach fails in scenarios where the correct answers are in the minority. We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers. To address this shortcoming, we introduce a hierarchical reasoning aggregation framework AoR (Aggregation of Reasoning), which selects answers based on the evaluation of reasoning chains. Additionally, AoR incorporates dynamic sampling, adjusting the number of reasoning chains in accordance with the complexity of the task. Experimental results on a series of complex reasoning tasks show that AoR outperforms prominent ensemble methods. Further analysis reveals that AoR not only adapts various LLMs but also achieves a superior performance ceiling when compared to current methods.	es, 14 figures, accepted by LREC-COLING 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.12939', 'html': 'https://arxiv.org/html/2405.12939v1', 'tex': '/src/2405.12939', 'doi': 'https://doi.org/10.48550/arXiv.2405.12939'}	Submission history From: Zhangyue Yin [ view email ] [v1] Tue, 21 May 2024 17:12:19 UTC (804 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.12939'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.12939'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.12939'}]
2024-05-26	How Far Are We From AGI	Artificial Intelligence	https://arxiv.org/abs/2405.10313v1	How Far Are We From AGI			2405.10313v1	['Tao Feng', 'Chuanyang Jin', 'Jingyu Liu', 'Kunlun Zhu', 'Haoqin Tu', 'Zirui Cheng', 'Guanyu Lin', 'Jiaxuan You']	ct:The evolution of artificial intelligence (AI) has profoundly impacted human society, driving significant advancements in multiple sectors. Yet, the escalating demands on AI have highlighted the limitations of AI's current offerings, catalyzing a movement towards Artificial General Intelligence (AGI). AGI, distinguished by its ability to execute diverse real-world tasks with efficiency and effectiveness comparable to human intelligence, reflects a paramount milestone in AI evolution. While existing works have summarized specific recent advancements of AI, they lack a comprehensive discussion of AGI's definitions, goals, and developmental trajectories. Different from existing survey papers, this paper delves into the pivotal questions of our proximity to AGI and the strategies necessary for its realization through extensive surveys, discussions, and original perspectives. We start by articulating the requisite capability frameworks for AGI, integrating the internal, interface, and system dimensions. As the realization of AGI requires more advanced capabilities and adherence to stringent constraints, we further discuss necessary AGI alignment technologies to harmonize these factors. Notably, we emphasize the importance of approaching AGI responsibly by first defining the key levels of AGI progression, followed by the evaluation framework that situates the status-quo, and finally giving our roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible insights into the ubiquitous impact of the integration of AI, we outline existing challenges and potential pathways toward AGI in multiple domains. In sum, serving as a pioneering exploration into the current state and future trajectory of AGI, this paper aims to foster a collective comprehension and catalyze broader public discussions among researchers and practitioners on AGI.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computers and Society (cs.CY)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2405.10313v1', 'html': 'https://arxiv.org/html/2405.10313v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2405.10313'}	Submission history From: Jiaxuan You [ view email ] [v1] Thu, 16 May 2024 17:59:02 UTC (22,195 KB) [v2] Sun, 24 Nov 2024 18:44:27 UTC (23,658 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.10313'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.10313'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.10313'}]
2024-05-26	Layer-Condensed KV Cache for Efficient Inference of Large Language Models	Computation and Language	https://arxiv.org/abs/2405.10637	Efficient Inference of LLMs	https://x.com/arankomatsuzaki/status/1792386318300749848		2405.10637	['Haoyi Wu', 'Kewei Tu']	ct:Huge memory consumption has been a major bottleneck for deploying high-throughput large language models in real-world applications. In addition to the large number of parameters, the key-value (KV) cache for the attention mechanism in the transformer architecture consumes a significant amount of memory, especially when the number of layers is large for deep language models. In this paper, we propose a novel method that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput. Our experiments on large language models show that our method achieves up to 26$\times$ higher throughput than standard transformers and competitive performance in language modeling and downstream tasks. In addition, our method is orthogonal to existing transformer memory-saving techniques, so it is straightforward to integrate them with our model, achieving further improvement in inference efficiency. Our code is available atthis https URL.	ed to ACL2024 main conference	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.10637', 'html': 'https://arxiv.org/html/2405.10637v2', 'tex': '/src/2405.10637', 'doi': 'https://doi.org/10.48550/arXiv.2405.10637'}	Submission history From: Haoyi Wu [ view email ] [v1] Fri, 17 May 2024 08:59:46 UTC (8,097 KB) [v2] Tue, 4 Jun 2024 00:08:10 UTC (8,097 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.10637'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.10637'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.10637'}]
2024-05-26	Lessons from the Trenches on Reproducible Evaluation of Language Models	Computation and Language	https://arxiv.org/abs/2405.14782	Guide for Evaluating LLMs	https://x.com/omarsar0/status/1793846120600474017		2405.14782	['Stella Biderman', 'Hailey Schoelkopf', 'Lintang Sutawika', 'Leo Gao', 'Jonathan Tow', 'Baber Abbasi', 'Alham Fikri Aji', 'Pawan Sasanka Ammanamanchi', 'Sidney Black', 'Jordan Clive', 'Anthony DiPofi', 'Julen Etxaniz', 'Benjamin Fattori', 'Jessica Zosa Forde', 'Charles Foster', 'Jeffrey Hsu', 'Mimansa Jaiswal', 'Wilson Y. Lee', 'Haonan Li', 'Charles Lovering', 'Niklas Muennighoff', 'Ellie Pavlick', 'Jason Phang', 'Aviya Skowron', 'Samson Tan', 'Xiangru Tang', 'Kevin A. Wang', 'Genta Indra Winata', 'François Yvon', 'Andy Zou']	ct:Effective evaluation of language models remains an open challenge in NLP. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the lack of reproducibility and transparency. In this paper we draw on three years of experience in evaluating large language models to provide guidance and lessons for researchers. First, we provide an overview of common challenges faced in language model evaluation. Second, we delineate best practices for addressing or lessening the impact of these challenges on research. Third, we present the Language Model Evaluation Harness (lm-eval): an open source library for independent, reproducible, and extensible evaluation of language models that seeks to address these issues. We describe the features of the library as well as case studies in which the library has been used to alleviate these methodological concerns.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.14782', 'html': 'https://arxiv.org/html/2405.14782v2', 'tex': '/src/2405.14782', 'doi': 'https://doi.org/10.48550/arXiv.2405.14782'}	Submission history From: Hailey Schoelkopf [ view email ] [v1] Thu, 23 May 2024 16:50:49 UTC (1,650 KB) [v2] Wed, 29 May 2024 17:15:53 UTC (1,650 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.14782'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.14782'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.14782'}]
2024-05-26	INDUS: Effective and Efficient Language Models for Scientific Applications	Computation and Language	https://arxiv.org/abs/2405.10725	Scientific Applications of LLMs	https://x.com/omarsar0/status/1792585422465335695		2405.10725	['Bishwaranjan Bhattacharjee', 'Aashka Trivedi', 'Masayasu Muraoka', 'Muthukumaran Ramasubramanian', 'Takuma Udagawa', 'Iksha Gurung', 'Nishan Pantha', 'Rong Zhang', 'Bharath Dandala', 'Rahul Ramachandran', 'Manil Maskey', 'Kaylin Bugbee', 'Mike Little', 'Elizabeth Fancher', 'Irina Gerasimov', 'Armin Mehrabian', 'Lauren Sanders', 'Sylvain Costes', 'Sergi Blanco-Cuaresma', 'Kelly Lockhart', 'Thomas Allen', 'Felix Grezes', 'Megan Ansdell', 'Alberto Accomazzi', 'Yousef El-Kurdi', 'Davis Wertheimer', 'Birgit Pfitzmann', 'Cesar Berrospi Ramis', 'Michele Dolfi', 'Rafael Teixeira de Lima', 'Panagiotis Vagenas', 'S. Karthik Mukkavilli', 'Peter Staar', 'Sanaz Vahidinia', 'Ryan McGranaghan', 'Tsendgar Lee']	ct:Large language models (LLMs) trained on general domain corpora showed remarkable results on natural language processing (NLP) tasks. However, previous research demonstrated LLMs trained using domain-focused corpora perform better on specialized tasks. Inspired by this insight, we developed INDUS, a comprehensive suite of LLMs tailored for the closely-related domains of Earth science, biology, physics, heliophysics, planetary sciences and astrophysics, and trained using curated scientific corpora drawn from diverse data sources. The suite of models include: (1) an encoder model trained using domain-specific vocabulary and corpora to address NLP tasks, (2) a contrastive-learning based text embedding model trained using a diverse set of datasets to address information retrieval tasks and (3) smaller versions of these models created using knowledge distillation for applications which have latency or resource constraints. We also created three new scientific benchmark datasets, CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA) and NASA-IR (IR) to accelerate research in these multi-disciplinary fields. We show that our models outperform both general-purpose (RoBERTa) and domain-specific (SCIBERT) encoders on these new tasks as well as existing tasks in the domains of interest. Furthermore, we demonstrate the use of these models in two industrial settings -- as a retrieval model for large-scale vector search applications and in automatic content tagging systems.	2024 (Industry Track)	['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2405.10725', 'html': 'https://arxiv.org/html/2405.10725v3', 'tex': '/src/2405.10725', 'doi': 'https://doi.org/10.48550/arXiv.2405.10725'}	Submission history From: Aashka Trivedi [ view email ] [v1] Fri, 17 May 2024 12:15:07 UTC (6,951 KB) [v2] Mon, 20 May 2024 23:49:12 UTC (6,951 KB) [v3] Wed, 30 Oct 2024 19:42:57 UTC (13,117 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.10725'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.10725'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.10725'}]
2024-05-26	DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data	Artificial Intelligence	https://arxiv.org/abs/2405.14333	DeepSeek-Prover	https://x.com/_akhaliq/status/1793864788579090917		2405.14333	['Huajian Xin', 'Daya Guo', 'Zhihong Shao', 'Zhizhou Ren', 'Qihao Zhu', 'Bo Liu', 'Chong Ruan', 'Wenda Li', 'Xiaodan Liang']	ct:Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability. Although large language models (LLMs) show promise in mathematical reasoning, their advancement in formal theorem proving is hindered by a lack of training data. To address this issue, we introduce an approach to generate extensive Lean 4 proof data derived from high-school and undergraduate-level mathematical competition problems. This approach involves translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data. After fine-tuning the DeepSeekMath 7B model on this synthetic dataset, which comprises 8 million formal statements with proofs, our model achieved whole-proof generation accuracies of 46.3% with 64 samples and 52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0% with 64 samples and a tree search reinforcement learning method at 41.0%. Additionally, our model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any. These results demonstrate the potential of leveraging large-scale synthetic data to enhance theorem-proving capabilities in LLMs. Both the synthetic dataset and the model will be made available to facilitate further research in this promising field.		['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2405.14333', 'html': 'https://arxiv.org/html/2405.14333v1', 'tex': '/src/2405.14333', 'doi': 'https://doi.org/10.48550/arXiv.2405.14333'}	Submission history From: Huajian Xin [ view email ] [v1] Thu, 23 May 2024 09:03:42 UTC (4,425 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.14333'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.14333'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.14333'}]
2024-05-26	Efficient Multimodal Large Language Models: A Survey	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2405.10739v1	Efficient Multimodal LLMs	https://x.com/omarsar0/status/1794072297260634244		2405.10739v1	['Yizhang Jin', 'Jian Li', 'Yexin Liu', 'Tianjun Gu', 'Kai Wu', 'Zhengkai Jiang', 'Muyang He', 'Bo Zhao', 'Xin Tan', 'Zhenye Gan', 'Yabiao Wang', 'Chengjie Wang', 'Lizhuang Ma']	ct:In the past year, Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance in tasks such as visual question answering, visual understanding and reasoning. However, the extensive model size and high training and inference costs have hindered the widespread application of MLLMs in academia and industry. Thus, studying efficient and lightweight MLLMs has enormous potential, especially in edge computing scenarios. In this survey, we provide a comprehensive and systematic review of the current state of efficient MLLMs. Specifically, we summarize the timeline of representative efficient MLLMs, research state of efficient structures and strategies, and the applications. Finally, we discuss the limitations of current efficient MLLM research and promising future directions. Please refer to our GitHub repository for more details:this https URL.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2405.10739v1', 'html': 'https://arxiv.org/html/2405.10739v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2405.10739'}	Submission history From: Jian Li [ view email ] [v1] Fri, 17 May 2024 12:37:10 UTC (4,264 KB) [v2] Fri, 9 Aug 2024 09:28:14 UTC (4,264 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.10739'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.10739'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.10739'}]
2024-05-19	Chameleon: Mixed-Modal Early-Fusion Foundation Models	Computation and Language	https://arxiv.org/abs/2405.09818	Chameleon	https://x.com/AIatMeta/status/1791263344714014733		2405.09818	['Chameleon Team']	ct:We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.09818', 'html': 'https://arxiv.org/html/2405.09818v2', 'tex': '/src/2405.09818', 'doi': 'https://doi.org/10.48550/arXiv.2405.09818'}	Submission history From: Armen Aghajanyan [ view email ] [v1] Thu, 16 May 2024 05:23:41 UTC (26,721 KB) [v2] Fri, 21 Mar 2025 05:54:00 UTC (26,722 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.09818'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.09818'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.09818'}]
2024-05-19	Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?	Computation and Language	https://arxiv.org/abs/2405.05904	Fine-tuning and Hallucinations	https://x.com/arankomatsuzaki/status/1788859706187882960		2405.05904	['Zorik Gekhman', 'Gal Yona', 'Roee Aharoni', 'Matan Eyal', 'Amir Feder', 'Roi Reichart', 'Jonathan Herzig']	ct:When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.	ed as a long paper at EMNLP 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.05904', 'html': 'https://arxiv.org/html/2405.05904v3', 'tex': '/src/2405.05904', 'doi': 'https://doi.org/10.48550/arXiv.2405.05904'}	Submission history From: Zorik Gekhman [ view email ] [v1] Thu, 9 May 2024 17:00:22 UTC (4,171 KB) [v2] Mon, 13 May 2024 07:29:58 UTC (4,171 KB) [v3] Tue, 1 Oct 2024 12:08:23 UTC (4,793 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.05904'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.05904'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.05904'}]
2024-05-19	Zero-Shot Tokenizer Transfer	Computation and Language	https://arxiv.org/abs/2405.07883	Zero-shot Tokenizer Transfer	https://x.com/bminixhofer/status/1790267652587258343		2405.07883	['Benjamin Minixhofer', 'Edoardo Maria Ponti', 'Ivan Vulić']	ct:Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.07883', 'html': 'https://arxiv.org/html/2405.07883v1', 'tex': '/src/2405.07883', 'doi': 'https://doi.org/10.48550/arXiv.2405.07883'}	Submission history From: Benjamin Minixhofer [ view email ] [v1] Mon, 13 May 2024 16:17:10 UTC (269 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.07883'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.07883'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.07883'}]
2024-05-19	WavCraft: Audio Editing and Generation with Large Language Models	Electrical Engineering and Systems Science > Audio and Speech Processing	https://arxiv.org/abs/2403.09527v3	WavCraft			2403.09527v3	['Jinhua Liang', 'Huan Zhang', 'Haohe Liu', 'Yin Cao', 'Qiuqiang Kong', 'Xubo Liu', 'Wenwu Wang', 'Mark D. Plumbley', 'Huy Phan', 'Emmanouil Benetos']	ct:We introduce WavCraft, a collective system that leverages large language models (LLMs) to connect diverse task-specific models for audio content creation and editing. Specifically, WavCraft describes the content of raw audio materials in natural language and prompts the LLM conditioned on audio descriptions and user requests. WavCraft leverages the in-context learning ability of the LLM to decomposes users' instructions into several tasks and tackle each task collaboratively with the particular module. Through task decomposition along with a set of task-specific models, WavCraft follows the input instruction to create or edit audio content with more details and rationales, facilitating user control. In addition, WavCraft is able to cooperate with users via dialogue interaction and even produce the audio content without explicit user commands. Experiments demonstrate that WavCraft yields a better performance than existing methods, especially when adjusting the local regions of audio clips. Moreover, WavCraft can follow complex instructions to edit and create audio content on the top of input recordings, facilitating audio producers in a broader range of applications. Our implementation and demos are available at thisthis https URL.		['Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2403.09527v3', 'html': 'https://arxiv.org/html/2403.09527v3', 'tex': '/src/2403.09527v3', 'doi': 'https://doi.org/10.48550/arXiv.2403.09527'}	Submission history From: Jinhua Liang [ view email ] [v1] Thu, 14 Mar 2024 16:10:34 UTC (1,068 KB) [v2] Fri, 15 Mar 2024 03:35:31 UTC (1,068 KB) [v3] Fri, 10 May 2024 07:54:17 UTC (1,215 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.09527'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.09527'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.09527'}]
2024-05-19	RLHF Workflow: From Reward Modeling to Online RLHF	Machine Learning	https://arxiv.org/abs/2405.07863v1	RLHF Workflow	https://x.com/CaimingXiong/status/1790379121719361776		2405.07863v1	['Hanze Dong', 'Wei Xiong', 'Bo Pang', 'Haoxiang Wang', 'Han Zhao', 'Yingbo Zhou', 'Nan Jiang', 'Doyen Sahoo', 'Caiming Xiong', 'Tong Zhang']	ct:We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback. Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. Our trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. Please refer tothis https URLandthis https URLfor more detailed information.	es, 8 figures	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2405.07863v1', 'html': 'https://arxiv.org/html/2405.07863v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2405.07863'}	Submission history From: Hanze Dong [ view email ] [v1] Mon, 13 May 2024 15:50:39 UTC (3,574 KB) [v2] Wed, 12 Jun 2024 04:40:53 UTC (3,613 KB) [v3] Tue, 12 Nov 2024 11:18:43 UTC (3,615 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.07863'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.07863'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.07863'}]
2024-05-19	You Only Cache Once: Decoder-Decoder Architectures for Language Models	Computation and Language	https://arxiv.org/abs/2405.05254	You Only Cache Once	https://x.com/arankomatsuzaki/status/1788435838474355098		2405.05254	['Yutao Sun', 'Li Dong', 'Yi Zhu', 'Shaohan Huang', 'Wenhui Wang', 'Shuming Ma', 'Quanlu Zhang', 'Jianyong Wang', 'Furu Wei']	ct:We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes. Code is available atthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.05254', 'html': 'https://arxiv.org/html/2405.05254v2', 'tex': '/src/2405.05254', 'doi': 'https://doi.org/10.48550/arXiv.2405.05254'}	Submission history From: Li Dong [ view email ] [v1] Wed, 8 May 2024 17:57:39 UTC (456 KB) [v2] Thu, 9 May 2024 14:12:45 UTC (455 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.05254'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.05254'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.05254'}]
2024-05-19	CAT3D: Create Anything in 3D with Multi-View Diffusion Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2405.10314	CAT3D	https://x.com/_akhaliq/status/1791294630614442009		2405.10314	['Ruiqi Gao', 'Aleksander Holynski', 'Philipp Henzler', 'Arthur Brussee', 'Ricardo Martin-Brualla', 'Pratul Srinivasan', 'Jonathan T. Barron', 'Ben Poole']	ct:Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene. We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model. Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene. These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time. CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation. See our project page for results and interactive demos atthis https URL.	t page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2405.10314', 'html': 'https://arxiv.org/html/2405.10314v1', 'tex': '/src/2405.10314', 'doi': 'https://doi.org/10.48550/arXiv.2405.10314'}	Submission history From: Aleksander Holynski [ view email ] [v1] Thu, 16 May 2024 17:59:05 UTC (30,235 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.10314'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.10314'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.10314'}]
2024-05-12	xLSTM: Extended Long Short-Term Memory	Machine Learning	https://arxiv.org/abs/2405.04517	xLSTM: Extended Long Short-Term Memory	https://x.com/omarsar0/status/1788236090265977224		2405.04517	['Maximilian Beck', 'Korbinian Pöppel', 'Markus Spanring', 'Andreas Auer', 'Oleksandra Prudnikova', 'Michael Kopp', 'Günter Klambauer', 'Johannes Brandstetter', 'Sepp Hochreiter']	ct:In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.	vailable atthis https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2405.04517', 'html': None, 'tex': '/src/2405.04517', 'doi': 'https://doi.org/10.48550/arXiv.2405.04517'}	Submission history From: Maximilian Beck [ view email ] [v1] Tue, 7 May 2024 17:50:21 UTC (1,455 KB) [v2] Fri, 6 Dec 2024 15:42:07 UTC (3,706 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.04517'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.04517'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.04517'}]
2024-05-12	DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model	Computation and Language	https://arxiv.org/abs/2405.04434v2	DeepSeek-V2	https://x.com/p_nawrot/status/1788479672067481664		2405.04434v2	['DeepSeek-AI']	ct:We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2405.04434v2', 'html': 'https://arxiv.org/html/2405.04434v2', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2405.04434'}	Submission history From: Wenfeng Liang [ view email ] [v1] Tue, 7 May 2024 15:56:43 UTC (431 KB) [v2] Wed, 8 May 2024 02:43:34 UTC (431 KB) [v3] Thu, 16 May 2024 17:25:01 UTC (432 KB) [v4] Fri, 24 May 2024 15:24:58 UTC (432 KB) [v5] Wed, 19 Jun 2024 06:04:17 UTC (432 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.04434'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.04434'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.04434'}]
2024-05-12	AlphaMath Almost Zero: Process Supervision without Process	Computation and Language	https://arxiv.org/abs/2405.03553	AlphaMath Almost Zero	https://x.com/omarsar0/status/1787678940158468283		2405.03553	['Guoxin Chen', 'Minpeng Liao', 'Chengxi Li', 'Kai Fan']	ct:Although recent advancements in large language models (LLMs) have significantly improved their performance on various tasks, they still face challenges with complex and symbolic multi-step reasoning, particularly in mathematical reasoning. To bolster the mathematical reasoning capabilities of LLMs, most existing efforts concentrate on seeking assistance from either domain experts or GPT-4 for high-quality process-supervised data, which is not only expensive but also labor-intensive. In our study, we propose an innovative framework, AlphaMath, that bypasses the need for process annotations (from humans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework focuses on unleashing the potential of a well-pretrained LLM to autonomously enhance its mathematical reasoning. Specifically, we integrate a value model with the LLM, automatically generating both process supervision and step-level evaluation signals in MCTS. Furthermore, we propose an efficient inference strategy, step-level beam search, where the value model is crafted to assist the policy model (i.e., LLM) in navigating more effective reasoning paths, rather than solely relying on prior probabilities. The experimental results on both in-domain and out-of-domain datasets demonstrate that even without GPT-4 or human-annotated process supervision, our AlphaMath framework achieves comparable or superior results to previous state-of-the-art methods.	 ready version for NeurIPS 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2405.03553', 'html': None, 'tex': '/src/2405.03553', 'doi': 'https://doi.org/10.48550/arXiv.2405.03553'}	Submission history From: Guoxin Chen [ view email ] [v1] Mon, 6 May 2024 15:20:30 UTC (519 KB) [v2] Thu, 23 May 2024 05:07:24 UTC (407 KB) [v3] Fri, 27 Sep 2024 08:16:28 UTC (1,333 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.03553'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.03553'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.03553'}]
2024-05-12	CLLMs: Consistency Large Language Models	Computation and Language	https://arxiv.org/abs/2403.00835	Consistency LLMs	https://x.com/omarsar0/status/1788594039865958762		2403.00835	['Siqi Kou', 'Lanxiang Hu', 'Zhezhi He', 'Zhijie Deng', 'Hao Zhang']	ct:Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.	 proceedings of the 41st International Conference on Machine Learning (ICML) 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2403.00835', 'html': 'https://arxiv.org/html/2403.00835v4', 'tex': '/src/2403.00835', 'doi': 'https://doi.org/10.48550/arXiv.2403.00835'}	Submission history From: Lanxiang Hu [ view email ] [v1] Wed, 28 Feb 2024 20:17:04 UTC (1,239 KB) [v2] Tue, 5 Mar 2024 08:01:01 UTC (1,238 KB) [v3] Fri, 8 Mar 2024 00:13:31 UTC (1,239 KB) [v4] Thu, 13 Jun 2024 08:41:28 UTC (1,240 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.00835'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.00835'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.00835'}]
2024-05-12	Is Flash Attention Stable?	Machine Learning	https://arxiv.org/abs/2405.02803	Is Flash Attention Stable?	https://x.com/arankomatsuzaki/status/1787674624647414168		2405.02803	['Alicia Golden', 'Samuel Hsia', 'Fei Sun', 'Bilge Acun', 'Basil Hosmer', 'Yejin Lee', 'Zachary DeVito', 'Jeff Johnson', 'Gu-Yeon Wei', 'David Brooks', 'Carole-Jean Wu']	ct:Training large-scale machine learning models poses distinct system challenges, given both the size and complexity of today's workloads. Recently, many organizations training state-of-the-art Generative AI models have reported cases of instability during training, often taking the form of loss spikes. Numeric deviation has emerged as a potential cause of this training instability, although quantifying this is especially challenging given the costly nature of training runs. In this work, we develop a principled approach to understanding the effects of numeric deviation, and construct proxies to put observations into context when downstream effects are difficult to quantify. As a case study, we apply this framework to analyze the widely-adopted Flash Attention optimization. We find that Flash Attention sees roughly an order of magnitude more numeric deviation as compared to Baseline Attention at BF16 when measured during an isolated forward pass. We then use a data-driven analysis based on the Wasserstein Distance to provide upper bounds on how this numeric deviation impacts model weights during training, finding that the numerical deviation present in Flash Attention is 2-5 times less significant than low-precision training.		['Machine Learning (cs.LG)', 'Distributed, Parallel, and Cluster Computing (cs.DC)']	{'pdf': '/pdf/2405.02803', 'html': 'https://arxiv.org/html/2405.02803v1', 'tex': '/src/2405.02803', 'doi': 'https://doi.org/10.48550/arXiv.2405.02803'}	Submission history From: Alicia Golden [ view email ] [v1] Sun, 5 May 2024 03:25:25 UTC (899 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.02803'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.02803'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.02803'}]
2024-05-12	Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2405.03520v1	Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond	https://x.com/dair_ai/status/1789640682082091442		2405.03520v1	['Zheng Zhu', 'Xiaofeng Wang', 'Wangbo Zhao', 'Chen Min', 'Nianchen Deng', 'Min Dou', 'Yuqi Wang', 'Botian Shi', 'Kai Wang', 'Chi Zhang', 'Yang You', 'Zhaoxiang Zhang', 'Dawei Zhao', 'Liang Xiao', 'Jian Zhao', 'Jiwen Lu', 'Guan Huang']	ct:General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. This survey will be regularly updated at:this https URL.	urvey will be regularly updated at:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2405.03520v1', 'html': 'https://arxiv.org/html/2405.03520v1', 'tex': '/src/2405.03520v1', 'doi': 'https://doi.org/10.48550/arXiv.2405.03520'}	Submission history From: Zheng Zhu [ view email ] [v1] Mon, 6 May 2024 14:37:07 UTC (2,549 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.03520'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.03520'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.03520'}]
2024-05-12	MAmmoTH2: Scaling Instructions from the Web	Computation and Language	https://arxiv.org/abs/2405.03548	MAmmoTH2	https://x.com/xiangyue96/status/1787684680336097645		2405.03548	['Xiang Yue', 'Tuney Zheng', 'Ge Zhang', 'Wenhu Chen']	ct:Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.03548', 'html': 'https://arxiv.org/html/2405.03548v4', 'tex': '/src/2405.03548', 'doi': 'https://doi.org/10.48550/arXiv.2405.03548'}	Submission history From: Xiang Yue [ view email ] [v1] Mon, 6 May 2024 15:11:38 UTC (882 KB) [v2] Tue, 14 May 2024 01:36:12 UTC (1,157 KB) [v3] Wed, 15 May 2024 15:37:55 UTC (1,208 KB) [v4] Thu, 23 May 2024 16:34:35 UTC (1,485 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.03548'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.03548'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.03548'}]
2024-05-12	Granite Code Models: A Family of Open Foundation Models for Code Intelligence	Artificial Intelligence	https://arxiv.org/abs/2405.04324v1	Granite Code Models	https://x.com/rohanpaul_ai/status/1788194161495052343	"{""Code"": ""https://github.com/ibm-granite/granite-code-models""}"	2405.04324v1	['Mayank Mishra', 'Matt Stallone', 'Gaoyuan Zhang', 'Yikang Shen', 'Aditya Prasad', 'Adriana Meza Soria', 'Michele Merler', 'Parameswaran Selvam', 'Saptha Surendran', 'Shivdeep Singh', 'Manish Sethi', 'Xuan-Hong Dang', 'Pengyuan Li', 'Kun-Lung Wu', 'Syed Zawad', 'Andrew Coleman', 'Matthew White', 'Mark Lewis', 'Raju Pavuluri', 'Yan Koyfman', 'Boris Lublinsky', 'Maximilien de Bayser', 'Ibrahim Abdelaziz', 'Kinjal Basu', 'Mayank Agarwal', 'Yi Zhou', 'Chris Johnson', 'Aanchal Goyal', 'Hima Patel', 'Yousaf Shah', 'Petros Zerfos', 'Heiko Ludwig', 'Asim Munawar', 'Maxwell Crouse', 'Pavan Kapanipathi', 'Shweta Salaria', 'Bob Calio', 'Sophia Wen', 'Seetharami Seelam', 'Brian Belgodere', 'Carlos Fonseca', 'Amith Singhee', 'Nirmit Desai', 'David D. Cox', 'Ruchir Puri', 'Rameswar Panda']	ct:Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.	ponding Authors: Rameswar Panda, Ruchir Puri; Equal Contributors: Mayank Mishra, Matt Stallone, Gaoyuan Zhang	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2405.04324v1', 'html': 'https://arxiv.org/html/2405.04324v1', 'tex': '/src/2405.04324v1', 'doi': 'https://doi.org/10.48550/arXiv.2405.04324'}	Submission history From: Rameswar Panda [ view email ] [v1] Tue, 7 May 2024 13:50:40 UTC (2,319 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.04324'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.04324'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.04324'}]
2024-05-05	KAN: Kolmogorov-Arnold Networks	Machine Learning	https://arxiv.org/abs/2404.19756	Kolmogorov-Arnold Networks	https://x.com/ZimingLiu11/status/1785483967719981538		2404.19756	['Ziming Liu', 'Yixuan Wang', 'Sachin Vaidya', 'Fabian Ruehle', 'James Halverson', 'Marin Soljačić', 'Thomas Y. Hou', 'Max Tegmark']	"ct:Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (""neurons""), KANs have learnable activation functions on edges (""weights""). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs."	ed by International Conference on Learning Representations (ICLR) 2025 (conference version:this https URL). Codes are available atthis https URL	['Machine Learning (cs.LG)', 'Disordered Systems and Neural Networks (cond-mat.dis-nn)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2404.19756', 'html': 'https://arxiv.org/html/2404.19756v5', 'tex': '/src/2404.19756', 'doi': 'https://doi.org/10.48550/arXiv.2404.19756'}	Submission history From: Ziming Liu [ view email ] [v1] Tue, 30 Apr 2024 17:58:29 UTC (15,986 KB) [v2] Thu, 2 May 2024 16:18:21 UTC (15,986 KB) [v3] Fri, 24 May 2024 22:30:07 UTC (15,991 KB) [v4] Sun, 16 Jun 2024 13:34:56 UTC (17,266 KB) [v5] Sun, 9 Feb 2025 21:09:09 UTC (17,267 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.19756'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.19756'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.19756'}]
2024-05-05	Better & Faster Large Language Models via Multi-token Prediction	Computation and Language	https://arxiv.org/abs/2404.19737	Better and Faster LLMs via Multi-token Prediction	https://x.com/arankomatsuzaki/status/1785486711646040440		2404.19737	['Fabian Gloeckle', 'Badr Youbi Idrissi', 'Baptiste Rozière', 'David Lopez-Paz', 'Gabriel Synnaeve']	ct:Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2404.19737', 'html': None, 'tex': '/src/2404.19737', 'doi': 'https://doi.org/10.48550/arXiv.2404.19737'}	Submission history From: Fabian Gloeckle [ view email ] [v1] Tue, 30 Apr 2024 17:33:57 UTC (1,300 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.19737'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.19737'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.19737'}]
2024-05-05	Capabilities of Gemini Models in Medicine	Artificial Intelligence	https://arxiv.org/abs/2404.18416	Med-Gemini	https://x.com/iScienceLuvr/status/1785247498744778886		2404.18416	['Khaled Saab', 'Tao Tu', 'Wei-Hung Weng', 'Ryutaro Tanno', 'David Stutz', 'Ellery Wulczyn', 'Fan Zhang', 'Tim Strother', 'Chunjong Park', 'Elahe Vedadi', 'Juanma Zambrano Chaves', 'Szu-Yeu Hu', 'Mike Schaekermann', 'Aishwarya Kamath', 'Yong Cheng', 'David G.T. Barrett', 'Cathy Cheung', 'Basil Mustafa', 'Anil Palepu', 'Daniel McDuff', 'Le Hou', 'Tomer Golany', 'Luyang Liu', 'Jean-baptiste Alayrac', 'Neil Houlsby', 'Nenad Tomasev', 'Jan Freyberg', 'Charles Lau', 'Jonas Kemp', 'Jeremy Lai', 'Shekoofeh Azizi', 'Kimberly Kanada', 'SiWai Man', 'Kavita Kulkarni', 'Ruoxi Sun', 'Siamak Shakeri', 'Luheng He', 'Ben Caine', 'Albert Webson', 'Natasha Latysheva', 'Melvin Johnson', 'Philip Mansfield', 'Jian Lu', 'Ehud Rivlin', 'Jesper Anderson', 'Bradley Green', 'Renee Wong', 'Jonathan Krause', 'Jonathon Shlens', 'Ewa Dominowska', 'S. M. Ali Eslami', 'Katherine Chou', 'Claire Cui', 'Oriol Vinyals', 'Koray Kavukcuoglu', 'James Manyika', 'Jeff Dean', 'Demis Hassabis', 'Yossi Matias', 'Dale Webster', 'Joelle Barral', 'Greg Corrado', 'Christopher Semturs', 'S. Sara Mahdavi', 'Juraj Gottweis', 'Alan Karthikesalingam', 'Vivek Natarajan']	ct:Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data. Gemini models, with strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine. Building on these core strengths of Gemini, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly use web search, and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin. On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1% accuracy, using a novel uncertainty-guided search strategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health & medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning. Finally, Med-Gemini's performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education. Taken together, our results offer compelling evidence for Med-Gemini's potential, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2404.18416', 'html': 'https://arxiv.org/html/2404.18416v2', 'tex': '/src/2404.18416', 'doi': 'https://doi.org/10.48550/arXiv.2404.18416'}	Submission history From: Khaled Saab [ view email ] [v1] Mon, 29 Apr 2024 04:11:28 UTC (4,986 KB) [v2] Wed, 1 May 2024 17:12:10 UTC (4,986 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.18416'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.18416'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.18416'}]
2024-05-05	When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively	Computation and Language	https://arxiv.org/abs/2404.19705	When to Retrieve?	https://x.com/omarsar0/status/1785498325913108556		2404.19705	['Tiziano Labruna', 'Jon Ander Campos', 'Gorka Azkune']	ct:In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question. Given the performance of IR systems, the optimal strategy for question answering does not always entail external information retrieval; rather, it often involves leveraging the parametric memory of the LLM itself. Prior research has identified this phenomenon in the PopQA dataset, wherein the most popular questions are effectively addressed using the LLM's parametric memory, while less popular ones require IR system usage. Following this, we propose a tailored training approach for LLMs, leveraging existing open-domain question answering datasets. Here, LLMs are trained to generate a special token, <RET>, when they do not know the answer to a question. Our evaluation of the Adaptive Retrieval LLM (Adapt-LLM) on the PopQA dataset showcases improvements over the same LLM under three configurations: (i) retrieving information for all the questions, (ii) using always the parametric memory of the LLM, and (iii) using a popularity threshold to decide when to use a retriever. Through our analysis, we demonstrate that Adapt-LLM is able to generate the <RET> token when it determines that it does not know how to answer a question, indicating the need for IR, while it achieves notably high accuracy levels when it chooses to rely only on its parametric memory.		['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2404.19705', 'html': 'https://arxiv.org/html/2404.19705v2', 'tex': '/src/2404.19705', 'doi': 'https://doi.org/10.48550/arXiv.2404.19705'}	Submission history From: Tiziano Labruna [ view email ] [v1] Tue, 30 Apr 2024 16:52:55 UTC (131 KB) [v2] Mon, 6 May 2024 19:08:10 UTC (132 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.19705'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.19705'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.19705'}]
2024-05-05	RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing	Computation and Language	https://arxiv.org/abs/2404.19543	A Survey on Retrieval-Augmented Language Models	https://x.com/omarsar0/status/1785666343062184422		2404.19543	['Yucheng Hu', 'Yuxing Lu']	ct:Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study:this https URL.	es, 7 figures. Draft version 1	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2404.19543', 'html': 'https://arxiv.org/html/2404.19543v2', 'tex': '/src/2404.19543', 'doi': 'https://doi.org/10.48550/arXiv.2404.19543'}	Submission history From: Yuxing Lu [ view email ] [v1] Tue, 30 Apr 2024 13:14:51 UTC (7,016 KB) [v2] Sun, 29 Jun 2025 14:57:19 UTC (7,019 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.19543'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.19543'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.19543'}]
2024-05-05	Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models	Computation and Language	https://arxiv.org/abs/2405.01535	An Open-source LM Specialized in Evaluating Other LMs	https://x.com/omarsar0/status/1786380398966014423		2405.01535	['Seungone Kim', 'Juyoung Suk', 'Shayne Longpre', 'Bill Yuchen Lin', 'Jamin Shin', 'Sean Welleck', 'Graham Neubig', 'Moontae Lee', 'Kyungjae Lee', 'Minjoon Seo']	ct:Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available atthis https URL.	2024 (Main Conference)	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.01535', 'html': 'https://arxiv.org/html/2405.01535v2', 'tex': '/src/2405.01535', 'doi': 'https://doi.org/10.48550/arXiv.2405.01535'}	Submission history From: Seungone Kim [ view email ] [v1] Thu, 2 May 2024 17:59:35 UTC (1,959 KB) [v2] Wed, 4 Dec 2024 19:23:17 UTC (1,951 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.01535'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.01535'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.01535'}]
2024-05-05	Self-Play Preference Optimization for Language Model Alignment	Machine Learning	https://arxiv.org/abs/2405.00675	Self-Play Preference Optimization	https://x.com/QuanquanGu/status/1785903241102049424		2405.00675	['Yue Wu', 'Zhiqing Sun', 'Huizhuo Yuan', 'Kaixuan Ji', 'Yiming Yang', 'Quanquan Gu']	ct:Standard reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed Self-Play Preference Optimization (SPPO), utilizes iterative policy updates to provably approximate the Nash equilibrium. Additionally, we propose a new SPPO objective which is both strongly motivated by theory and is simple and effective in practice. In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench, Arena-Hard, and the Open LLM Leaderboard. Starting from a stronger base model Llama-3-8B-Instruct, we are able to achieve a length-controlled win rate of 38.77%. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models. Codes are available atthis https URL.	es, 4 figures, 5 tables	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2405.00675', 'html': 'https://arxiv.org/html/2405.00675v5', 'tex': '/src/2405.00675', 'doi': 'https://doi.org/10.48550/arXiv.2405.00675'}	Submission history From: Yue Wu [ view email ] [v1] Wed, 1 May 2024 17:59:20 UTC (142 KB) [v2] Thu, 23 May 2024 17:58:39 UTC (137 KB) [v3] Sun, 26 May 2024 21:50:05 UTC (137 KB) [v4] Fri, 14 Jun 2024 05:57:01 UTC (134 KB) [v5] Fri, 4 Oct 2024 18:48:25 UTC (138 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.00675'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.00675'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.00675'}]
2024-05-05	A Primer on the Inner Workings of Transformer-based Language Models	Computation and Language	https://arxiv.org/abs/2405.00208	Inner Workings of Transformer Language Models	https://x.com/omarsar0/status/1786052338043466162		2405.00208	['Javier Ferrando', 'Gabriele Sarti', 'Arianna Bisazza', 'Marta R. Costa-jussà']	ct:The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.00208', 'html': 'https://arxiv.org/html/2405.00208v3', 'tex': '/src/2405.00208', 'doi': 'https://doi.org/10.48550/arXiv.2405.00208'}	Submission history From: Javier Ferrando [ view email ] [v1] Tue, 30 Apr 2024 21:20:17 UTC (3,012 KB) [v2] Thu, 2 May 2024 01:29:17 UTC (3,012 KB) [v3] Sun, 13 Oct 2024 21:05:31 UTC (3,017 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.00208'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.00208'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.00208'}]
2024-05-05	Hallucination of Multimodal Large Language Models: A Survey	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2404.18930	Multimodal LLM Hallucinations	https://x.com/DuaneJRich/status/1785220190411821111		2404.18930	['Zechen Bai', 'Pichao Wang', 'Tianjun Xiao', 'Tong He', 'Zongbo Han', 'Zheng Zhang', 'Mike Zheng Shou']	ct:This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at:this https URL.	ferences	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2404.18930', 'html': 'https://arxiv.org/html/2404.18930v2', 'tex': '/src/2404.18930', 'doi': 'https://doi.org/10.48550/arXiv.2404.18930'}	Submission history From: Pichao Wang [ view email ] [v1] Mon, 29 Apr 2024 17:59:41 UTC (330 KB) [v2] Tue, 1 Apr 2025 18:36:08 UTC (285 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.18930'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.18930'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.18930'}]
2024-05-05	In-Context Learning with Long-Context Models: An In-Depth Exploration	Computation and Language	https://arxiv.org/abs/2405.00200	In-Context Learning with Long-Context Models	https://x.com/abertsch72/status/1786392584765538350		2405.00200	['Amanda Bertsch', 'Maor Ivgi', 'Emily Xiao', 'Uri Alon', 'Jonathan Berant', 'Matthew R. Gormley', 'Graham Neubig']	ct:As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. We show that, for many datasets with large label spaces, performance continues to increase with thousands of demonstrations. We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can exceed long-context ICL performance with additional data. We use the ICL setting to study several properties of both in-context learning and long-context models. We show that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples negatively impacts performance, and that the performance boosts do not arise from cumulative gain from encoding many examples together. We conclude that long-context ICL can be an effective tool, and may not require long-context for encoding the demonstration set at all.	es; NAACL 2025 camera-ready	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2405.00200', 'html': None, 'tex': '/src/2405.00200', 'doi': 'https://doi.org/10.48550/arXiv.2405.00200'}	Submission history From: Amanda Bertsch [ view email ] [v1] Tue, 30 Apr 2024 21:06:52 UTC (233 KB) [v2] Mon, 3 Mar 2025 19:53:28 UTC (394 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2405.00200'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2405.00200'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2405.00200'}]
2024-04-28	Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone	Computation and Language	https://arxiv.org/abs/2404.14219	Phi-3	https://x.com/omarsar0/status/1782780923806699716		2404.14219	['Marah Abdin', 'Jyoti Aneja', 'Hany Awadalla', 'Ahmed Awadallah', 'Ammar Ahmad Awan', 'Nguyen Bach', 'Amit Bahree', 'Arash Bakhtiari', 'Jianmin Bao', 'Harkirat Behl', 'Alon Benhaim', 'Misha Bilenko', 'Johan Bjorck', 'Sébastien Bubeck', 'Martin Cai', 'Qin Cai', 'Vishrav Chaudhary', 'Dong Chen', 'Dongdong Chen', 'Weizhu Chen', 'Yen-Chun Chen', 'Yi-Ling Chen', 'Hao Cheng', 'Parul Chopra', 'Xiyang Dai', 'Matthew Dixon', 'Ronen Eldan', 'Victor Fragoso', 'Jianfeng Gao', 'Mei Gao', 'Min Gao', 'Amit Garg', 'Allie Del Giorno', 'Abhishek Goswami', 'Suriya Gunasekar', 'Emman Haider', 'Junheng Hao', 'Russell J. Hewett', 'Wenxiang Hu', 'Jamie Huynh', 'Dan Iter', 'Sam Ade Jacobs', 'Mojan Javaheripi', 'Xin Jin', 'Nikos Karampatziakis', 'Piero Kauffmann', 'Mahoud Khademi', 'Dongwoo Kim', 'Young Jin Kim', 'Lev Kurilenko', 'James R. Lee', 'Yin Tat Lee', 'Yuanzhi Li', 'Yunsheng Li', 'Chen Liang', 'Lars Liden', 'Xihui Lin', 'Zeqi Lin', 'Ce Liu', 'Liyuan Liu', 'Mengchen Liu', 'Weishung Liu', 'Xiaodong Liu', 'Chong Luo', 'Piyush Madan', 'Ali Mahmoudzadeh', 'David Majercak', 'Matt Mazzola', 'Caio César Teodoro Mendes', 'Arindam Mitra', 'Hardik Modi', 'Anh Nguyen', 'Brandon Norick', 'Barun Patra', 'Daniel Perez-Becker', 'Thomas Portet', 'Reid Pryzant', 'Heyang Qin', 'Marko Radmilac', 'Liliang Ren', 'Gustavo de Rosa', 'Corby Rosset', 'Sambudha Roy', 'Olatunji Ruwase', 'Olli Saarikivi', 'Amin Saied', 'Adil Salim', 'Michael Santacroce', 'Shital Shah', 'Ning Shang', 'Hiteshi Sharma', 'Yelong Shen', 'Swadheen Shukla', 'Xia Song', 'Masahiro Tanaka', 'Andrea Tupini', 'Praneetha Vaddamanu', 'Chunyu Wang', 'Guanhua Wang', 'Lijuan Wang', 'Shuohang Wang', 'Xin Wang', 'Yu Wang', 'Rachel Ward', 'Wen Wen', 'Philipp Witte', 'Haiping Wu', 'Xiaoxia Wu', 'Michael Wyatt', 'Bin Xiao', 'Can Xu', 'Jiahang Xu', 'Weijian Xu', 'Jilong Xue', 'Sonali Yadav', 'Fan Yang', 'Jianwei Yang', 'Yifan Yang', 'Ziyi Yang', 'Donghan Yu', 'Lu Yuan', 'Chenruidong Zhang', 'Cyril Zhang', 'Jianwen Zhang', 'Li Lyna Zhang', 'Yi Zhang', 'Yue Zhang', 'Yunan Zhang', 'Xiren Zhou', 'et al. (29 additional authors not shown)']	ct:We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2404.14219', 'html': 'https://arxiv.org/html/2404.14219v4', 'tex': '/src/2404.14219', 'doi': 'https://doi.org/10.48550/arXiv.2404.14219'}	Submission history From: Sebastien Bubeck [ view email ] [v1] Mon, 22 Apr 2024 14:32:33 UTC (3,072 KB) [v2] Tue, 23 Apr 2024 14:49:38 UTC (3,072 KB) [v3] Thu, 23 May 2024 22:42:40 UTC (12,248 KB) [v4] Fri, 30 Aug 2024 21:17:17 UTC (12,361 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.14219'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.14219'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.14219'}]
2024-04-28	OpenELM: An Efficient Language Model Family with Open Training and Inference Framework	Computation and Language	https://arxiv.org/abs/2404.14619	OpenELM	https://x.com/rasbt/status/1783480053847736713		2404.14619	['Sachin Mehta', 'Mohammad Hossein Sekhavat', 'Qingqing Cao', 'Maxwell Horton', 'Yanzi Jin', 'Chenfan Sun', 'Iman Mirzadeh', 'Mahyar Najibi', 'Dmitry Belenko', 'Peter Zatloukal', 'Mohammad Rastegari']	ct:The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring $2\times$ fewer pre-training tokens.Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.Our source code along with pre-trained model weights and training recipes is available at \url{this https URL}. Additionally, \model models can be found on HuggingFace at: \url{this https URL}.	corrections	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2404.14619', 'html': 'https://arxiv.org/html/2404.14619v2', 'tex': '/src/2404.14619', 'doi': 'https://doi.org/10.48550/arXiv.2404.14619'}	Submission history From: Sachin Mehta [ view email ] [v1] Mon, 22 Apr 2024 23:12:03 UTC (72 KB) [v2] Thu, 2 May 2024 00:30:57 UTC (72 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.14619'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.14619'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.14619'}]
2024-04-28	Make Your LLM Fully Utilize the Context	Computation and Language	https://arxiv.org/abs/2404.16811	Make Your LLM Fully Utilize the Context	https://x.com/omarsar0/status/1783905514578980949		2404.16811	['Shengnan An', 'Zexiong Ma', 'Zeqi Lin', 'Nanning Zheng', 'Jian-Guang Lou']	ct:While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU). Github Link:this https URL.	es, 7 figures, 3 tables, 9 examples	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2404.16811', 'html': 'https://arxiv.org/html/2404.16811v2', 'tex': '/src/2404.16811', 'doi': 'https://doi.org/10.48550/arXiv.2404.16811'}	Submission history From: Shengnan An [ view email ] [v1] Thu, 25 Apr 2024 17:55:14 UTC (1,419 KB) [v2] Fri, 26 Apr 2024 11:15:21 UTC (1,419 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.16811'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.16811'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.16811'}]
2024-04-28	AutoScraper: A Progressive Understanding Web Agent for Web Scraper Generation	Computation and Language	https://arxiv.org/abs/2404.12753	AutoCrawler	https://x.com/omarsar0/status/1782462314983071757		2404.12753	['Wenhao Huang', 'Zhouhong Gu', 'Chenghao Peng', 'Zhixu Li', 'Jiaqing Liang', 'Yanghua Xiao', 'Liqian Wen', 'Zulong Chen']	ct:Web scraping is a powerful technique that extracts data from websites, enabling automated data collection, enhancing data analysis capabilities, and minimizing manual data entry efforts. Existing methods, wrappers-based methods suffer from limited adaptability and scalability when faced with a new website, while language agents, empowered by large language models (LLMs), exhibit poor reusability in diverse web environments. In this work, we introduce the paradigm of generating web scrapers with LLMs and propose AutoScraper, a two-stage framework that can handle diverse and changing web environments more efficiently. AutoScraper leverages the hierarchical structure of HTML and similarity across different web pages for generating web scrapers. Besides, we propose a new executability metric for better measuring the performance of web scraper generation tasks. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Resources of this paper can be found at \url{this https URL}	es, 4 figures, 18 tables. Accepted to EMNLP 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2404.12753', 'html': 'https://arxiv.org/html/2404.12753v2', 'tex': '/src/2404.12753', 'doi': 'https://doi.org/10.48550/arXiv.2404.12753'}	Submission history From: Wenhao Huang [ view email ] [v1] Fri, 19 Apr 2024 09:59:44 UTC (1,116 KB) [v2] Thu, 26 Sep 2024 09:17:10 UTC (1,019 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.12753'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.12753'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.12753'}]
2024-04-28	Graph Machine Learning in the Era of Large Language Models (LLMs)	Machine Learning	https://arxiv.org/abs/2404.14928	Graph Machine Learning in the Era of LLMs	https://x.com/omarsar0/status/1783171591020392886		2404.14928	['Wenqi Fan', 'Shijie Wang', 'Jiani Huang', 'Zhikai Chen', 'Yu Song', 'Wenzhuo Tang', 'Haitao Mao', 'Hui Liu', 'Xiaorui Liu', 'Dawei Yin', 'Qing Li']	ct:Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Social and Information Networks (cs.SI)']	{'pdf': '/pdf/2404.14928', 'html': 'https://arxiv.org/html/2404.14928v2', 'tex': '/src/2404.14928', 'doi': 'https://doi.org/10.48550/arXiv.2404.14928'}	Submission history From: Wenqi Fan [ view email ] [v1] Tue, 23 Apr 2024 11:13:39 UTC (23,810 KB) [v2] Tue, 4 Jun 2024 01:31:30 UTC (23,848 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.14928'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.14928'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.14928'}]
2024-04-28	A Survey on Self-Evolution of Large Language Models	Computation and Language	https://arxiv.org/abs/2404.14387	Self-Evolution of LLMs	https://x.com/omarsar0/status/1782777977526231440		2404.14387	['Zhengwei Tao', 'Ting-En Lin', 'Xiancai Chen', 'Hangyu Li', 'Yuchuan Wu', 'Yongbin Li', 'Zhi Jin', 'Fei Huang', 'Dacheng Tao', 'Jingren Zhou']	ct:Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs. Our corresponding GitHub repository is available atthis https URL		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2404.14387', 'html': 'https://arxiv.org/html/2404.14387v2', 'tex': '/src/2404.14387', 'doi': 'https://doi.org/10.48550/arXiv.2404.14387'}	Submission history From: Ting-En Lin [ view email ] [v1] Mon, 22 Apr 2024 17:43:23 UTC (3,858 KB) [v2] Mon, 3 Jun 2024 17:47:30 UTC (1,545 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.14387'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.14387'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.14387'}]
2024-04-28	NExT: Teaching Large Language Models to Reason about Code Execution	Machine Learning	https://arxiv.org/abs/2404.14662	Naturalized Execution Tuning (NExT)	https://x.com/AnsongNi/status/1783311827390070941		2404.14662	['Ansong Ni', 'Miltiadis Allamanis', 'Arman Cohan', 'Yinlin Deng', 'Kensen Shi', 'Charles Sutton', 'Pengcheng Yin']	ct:A fundamental skill among human developers is the ability to understand and reason about program execution. As an example, a programmer can mentally simulate code execution in natural language to debug and repair code (aka. rubber duck debugging). However, large language models (LLMs) of code are typically trained on the surface textual form of programs, thus may lack a semantic understanding of how programs execute at run-time. To address this issue, we propose NExT, a method to teach LLMs to inspect the execution traces of programs (variable states of executed lines) and reason about their run-time behavior through chain-of-thought (CoT) rationales. Specifically, NExT uses self-training to bootstrap a synthetic training set of execution-aware rationales that lead to correct task solutions (e.g., fixed programs) without laborious manual annotation. Experiments on program repair tasks based on MBPP and HumanEval demonstrate that NExT improves the fix rate of a PaLM 2 model, by 26.1% and 14.3% absolute, respectively, with significantly improved rationale quality as verified by automated metrics and human raters. Our model can also generalize to scenarios where program traces are absent at test-time.	es	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Programming Languages (cs.PL)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2404.14662', 'html': None, 'tex': '/src/2404.14662', 'doi': 'https://doi.org/10.48550/arXiv.2404.14662'}	Submission history From: Ansong Ni [ view email ] [v1] Tue, 23 Apr 2024 01:46:32 UTC (935 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.14662'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.14662'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.14662'}]
2024-04-21	Chinchilla Scaling: A replication attempt	Artificial Intelligence	https://arxiv.org/abs/2404.10102	Chinchilla Scaling: A replication attempt	https://x.com/tamaybes/status/1780639257389904013		2404.10102	['Tamay Besiroglu', 'Ege Erdil', 'Matthew Barnett', 'Josh You']	ct:Hoffmann et al. (2022) propose three methods for estimating a compute-optimal scaling law. We attempt to replicate their third estimation procedure, which involves fitting a parametric loss function to a reconstruction of data from their plots. We find that the reported estimates are inconsistent with their first two estimation methods, fail at fitting the extracted data, and report implausibly narrow confidence intervals--intervals this narrow would require over 600,000 experiments, while they likely only ran fewer than 500. In contrast, our rederivation of the scaling law using the third approach yields results that are compatible with the findings from the first two estimation procedures described by Hoffmann et al.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2404.10102', 'html': 'https://arxiv.org/html/2404.10102v2', 'tex': '/src/2404.10102', 'doi': 'https://doi.org/10.48550/arXiv.2404.10102'}	Submission history From: Tamay Besiroglu [ view email ] [v1] Mon, 15 Apr 2024 19:19:56 UTC (947 KB) [v2] Wed, 15 May 2024 00:57:23 UTC (1,004 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.10102'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.10102'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.10102'}]
2024-04-21	ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence	Computation and Language	https://arxiv.org/abs/2404.10198	How Faithful are RAG Models?	https://x.com/omarsar0/status/1780613738585903182		2404.10198	['Kevin Wu', 'Eric Wu', 'James Zou']	ct:Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs -- namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect.	d June 9 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2404.10198', 'html': 'https://arxiv.org/html/2404.10198v3', 'tex': '/src/2404.10198', 'doi': 'https://doi.org/10.48550/arXiv.2404.10198'}	Submission history From: Eric Wu [ view email ] [v1] Tue, 16 Apr 2024 00:43:03 UTC (7,878 KB) [v2] Mon, 10 Jun 2024 04:44:57 UTC (7,528 KB) [v3] Fri, 7 Feb 2025 05:11:18 UTC (7,543 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.10198'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.10198'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.10198'}]
2024-04-21	A Survey on Retrieval-Augmented Text Generation for Large Language Models	Information Retrieval	https://arxiv.org/abs/2404.10981	A Survey on Retrieval-Augmented Text Generation for LLMs	https://x.com/omarsar0/status/1780961995178594324		2404.10981	['Yizheng Huang', 'Jimmy Huang']	ct:Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.	g Work	['Information Retrieval (cs.IR)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2404.10981', 'html': 'https://arxiv.org/html/2404.10981v2', 'tex': '/src/2404.10981', 'doi': 'https://doi.org/10.48550/arXiv.2404.10981'}	Submission history From: Yizheng Huang [ view email ] [v1] Wed, 17 Apr 2024 01:27:42 UTC (718 KB) [v2] Fri, 23 Aug 2024 00:17:02 UTC (3,467 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.10981'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.10981'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.10981'}]
2024-04-21	The Illusion of State in State-Space Models	Machine Learning	https://arxiv.org/abs/2404.08819	The Illusion of State in State-Space Models	https://x.com/lambdaviking/status/1780246351520887281		2404.08819	['William Merrill', 'Jackson Petty', 'Ashish Sabharwal']	"ct:State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill & Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). But do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the ""state"" in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems."	ear at ICML 2024. 9 pages + appendices	['Machine Learning (cs.LG)', 'Computational Complexity (cs.CC)', 'Computation and Language (cs.CL)', 'Formal Languages and Automata Theory (cs.FL)']	{'pdf': '/pdf/2404.08819', 'html': 'https://arxiv.org/html/2404.08819v3', 'tex': '/src/2404.08819', 'doi': 'https://doi.org/10.48550/arXiv.2404.08819'}	Submission history From: William Merrill [ view email ] [v1] Fri, 12 Apr 2024 21:30:06 UTC (69 KB) [v2] Tue, 4 Jun 2024 22:05:45 UTC (76 KB) [v3] Wed, 5 Mar 2025 23:00:57 UTC (86 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.08819'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.08819'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.08819'}]
2024-04-21	Reducing hallucination in structured outputs via Retrieval-Augmented Generation	Machine Learning	https://arxiv.org/abs/2404.08189	Reducing Hallucination in Structured Outputs via RAG	https://x.com/omarsar0/status/1779896289745846778		2404.08189	['Patrice Béchard', 'Orlando Marquez Ayala']	ct:A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.	presented at NAACL 2024. 11 pages and 4 figures	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2404.08189', 'html': 'https://arxiv.org/html/2404.08189v1', 'tex': '/src/2404.08189', 'doi': 'https://doi.org/10.48550/arXiv.2404.08189'}	Submission history From: Orlando Marquez Ayala [ view email ] [v1] Fri, 12 Apr 2024 01:42:09 UTC (975 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.08189'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.08189'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.08189'}]
2024-04-21	The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey	Artificial Intelligence	https://arxiv.org/abs/2404.11584	Emerging AI Agent Architectures	https://x.com/omarsar0/status/1780958785785200756		2404.11584	['Tula Masterman', 'Sandi Besen', 'Mason Sawtell', 'Alex Chao']	ct:This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.	es,6 figures,38 references	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2404.11584', 'html': 'https://arxiv.org/html/2404.11584v1', 'tex': '/src/2404.11584', 'doi': 'https://doi.org/10.48550/arXiv.2404.11584'}	Submission history From: Sandi Besen [ view email ] [v1] Wed, 17 Apr 2024 17:32:41 UTC (1,455 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.11584'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.11584'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.11584'}]
2024-04-21	LLM In-Context Recall is Prompt Dependent	Computation and Language	https://arxiv.org/abs/2404.08865	LM In-Context Recall is Prompt Dependent	https://x.com/omarsar0/status/1780244042007122129		2404.08865	['Daniel Machlab', 'Rick Battle']	"ct:The proliferation of Large Language Models (LLMs) highlights the critical importance of conducting thorough evaluations to discern their comparative advantages, limitations, and optimal use cases. Particularly important is assessing their capacity to accurately retrieve information included in a given prompt. A model's ability to do this significantly influences how effectively it can utilize contextual details, thus impacting its practical efficacy and dependability in real-world applications.Our research analyzes the in-context recall performance of various LLMs using the needle-in-a-haystack method. In this approach, a factoid (the ""needle"") is embedded within a block of filler text (the ""haystack""), which the model is asked to retrieve. We assess the recall performance of each model across various haystack lengths and with varying needle placements to identify performance patterns. This study demonstrates that an LLM's recall capability is not only contingent upon the prompt's content but also may be compromised by biases in its training data. Conversely, adjustments to model architecture, training strategy, or fine-tuning can improve performance. Our analysis provides insight into LLM behavior, offering direction for the development of more effective applications of LLMs."		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2404.08865', 'html': 'https://arxiv.org/html/2404.08865v1', 'tex': '/src/2404.08865', 'doi': 'https://doi.org/10.48550/arXiv.2404.08865'}	Submission history From: Rick Battle [ view email ] [v1] Sat, 13 Apr 2024 01:13:59 UTC (4,316 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.08865'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.08865'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.08865'}]
2024-04-21	State Space Model for New-Generation Network Alternative to Transformers: A Survey	Machine Learning	https://arxiv.org/abs/2404.09516	A Survey on State Space Models	https://x.com/omarsar0/status/1781430319926686190		2404.09516	['Xiao Wang', 'Shiao Wang', 'Yuhe Ding', 'Yuehang Li', 'Wentao Wu', 'Yao Rong', 'Weizhe Kong', 'Ju Huang', 'Shihao Li', 'Haoxiang Yang', 'Ziwen Wang', 'Bo Jiang', 'Chenglong Li', 'Yaowei Wang', 'Yonghong Tian', 'Jin Tang']	ct:In the post-deep learning era, the Transformer architecture has demonstrated its powerful performance across pre-trained big models and various downstream tasks. However, the enormous computational demands of this architecture have deterred many researchers. To further reduce the complexity of attention models, numerous efforts have been made to design more efficient methods. Among them, the State Space Model (SSM), as a possible replacement for the self-attention based Transformer model, has drawn more and more attention in recent years. In this paper, we give the first comprehensive review of these works and also provide experimental comparisons and analysis to better demonstrate the features and advantages of SSM. Specifically, we first give a detailed description of principles to help the readers quickly capture the key ideas of SSM. After that, we dive into the reviews of existing SSMs and their various applications, including natural language processing, computer vision, graph, multi-modal and multi-media, point cloud/event stream, time series data, and other domains. In addition, we give statistical comparisons and analysis of these models and hope it helps the readers to understand the effectiveness of different structures on various tasks. Then, we propose possible research points in this direction to better promote the development of the theoretical model and application of SSM. More related works will be continuously updated on the following GitHub:this https URL.	rst review of State Space Model (SSM)/Mamba and their applications in artificial intelligence, 33 pages	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Multimedia (cs.MM)']	{'pdf': '/pdf/2404.09516', 'html': 'https://arxiv.org/html/2404.09516v1', 'tex': '/src/2404.09516', 'doi': 'https://doi.org/10.48550/arXiv.2404.09516'}	Submission history From: Xiao Wang [ view email ] [v1] Mon, 15 Apr 2024 07:24:45 UTC (9,946 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.09516'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.09516'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.09516'}]
2024-04-14	Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention	Computation and Language	https://arxiv.org/abs/2404.07143	Leave No Context Behind	https://x.com/omarsar0/status/1778480897198612839		2404.07143	['Tsendsuren Munkhdalai', 'Manaal Faruqui', 'Siddharth Gopal']	ct:This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.	s, 4 figures, 4 tables (v2 adds: background, implementation details, recent citations and acknowledgments)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Neural and Evolutionary Computing (cs.NE)']	{'pdf': '/pdf/2404.07143', 'html': 'https://arxiv.org/html/2404.07143v2', 'tex': '/src/2404.07143', 'doi': 'https://doi.org/10.48550/arXiv.2404.07143'}	Submission history From: Tsendsuren Munkhdalai [ view email ] [v1] Wed, 10 Apr 2024 16:18:42 UTC (248 KB) [v2] Fri, 9 Aug 2024 22:37:25 UTC (250 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.07143'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.07143'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.07143'}]
2024-04-14	Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought	Computation and Language	https://arxiv.org/abs/2404.03414	LM-Guided Chain-of-Thought	https://x.com/omarsar0/status/1777755819150373121		2404.03414	['Jooyoung Lee', 'Fan Yang', 'Thanh Tran', 'Qian Hu', 'Emre Barut', 'Kai-Wei Chang', 'Chengwei Su']	ct:We introduce a novel framework, LM-Guided CoT, that leverages a lightweight (i.e., <1B) language model (LM) for guiding a black-box large (i.e., >10B) LM in reasoning tasks. Specifically, the lightweight LM first generates a rationale for each input instance. The Frozen large LM is then prompted to predict a task output based on the rationale generated by the lightweight LM. Our approach is resource-efficient in the sense that it only requires training the lightweight LM. We optimize the model through 1) knowledge distillation and 2) reinforcement learning from rationale-oriented and task-oriented reward signals. We assess our method with multi-hop extractive question answering (QA) benchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our approach outperforms all baselines regarding answer prediction accuracy. We also find that reinforcement learning helps the model to produce higher-quality rationales with improved QA performance.	aper is accepted to LREC-COLING 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2404.03414', 'html': 'https://arxiv.org/html/2404.03414v1', 'tex': '/src/2404.03414', 'doi': 'https://doi.org/10.48550/arXiv.2404.03414'}	Submission history From: Jooyoung Lee [ view email ] [v1] Thu, 4 Apr 2024 12:46:37 UTC (4,019 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.03414'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.03414'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.03414'}]
2024-04-14	Best Practices and Lessons Learned on Synthetic Data	Computation and Language	https://arxiv.org/abs/2404.07503	Best Practices and Lessons on Synthetic Data	https://x.com/omarsar0/status/1778804848038683066		2404.07503	['Ruibo Liu', 'Jerry Wei', 'Fangyu Liu', 'Chenglei Si', 'Yanzhe Zhang', 'Jinmeng Rao', 'Steven Zheng', 'Daiyi Peng', 'Diyi Yang', 'Denny Zhou', 'Andrew M. Dai']	ct:The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models.	M 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2404.07503', 'html': 'https://arxiv.org/html/2404.07503v2', 'tex': '/src/2404.07503', 'doi': 'https://doi.org/10.48550/arXiv.2404.07503'}	Submission history From: Ruibo Liu [ view email ] [v1] Thu, 11 Apr 2024 06:34:17 UTC (6,347 KB) [v2] Sat, 10 Aug 2024 20:46:47 UTC (1,758 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.07503'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.07503'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.07503'}]
2024-04-14	THOUGHTSCULPT: Reasoning with Intermediate Revision and Search	Computation and Language	https://arxiv.org/abs/2404.05966	Reasoning with Intermediate Revision and Search	https://x.com/omarsar0/status/1777896810805186757		2404.05966	['Yizhou Chi', 'Kevin Yang', 'Dan Klein']	ct:We present THOUGHTSCULPT, a general reasoning and search method for tasks with outputs that can be decomposed into components. THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an LLM evaluator. Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output. Empirically, THOUGHTSCULPT outperforms state-of-the-art reasoning methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage).	ed to NAACL 2025 Findings. Code and data available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2404.05966', 'html': 'https://arxiv.org/html/2404.05966v2', 'tex': '/src/2404.05966', 'doi': 'https://doi.org/10.48550/arXiv.2404.05966'}	Submission history From: Yizhou Chi [ view email ] [v1] Tue, 9 Apr 2024 02:53:14 UTC (4,433 KB) [v2] Sat, 15 Feb 2025 03:57:43 UTC (4,377 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.05966'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.05966'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.05966'}]
2024-04-14	Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers	Computation and Language	https://arxiv.org/abs/2404.04925	Overview of Multilingual LLMs	https://x.com/omarsar0/status/1778063103906771105		2404.04925	['Libo Qin', 'Qiguang Chen', 'Yuhang Zhou', 'Zhi Chen', 'Yinghui Li', 'Lizi Liao', 'Min Li', 'Wanxiang Che', 'Philip S. Yu']	ct:Multilingual Large Language Models are capable of using powerful Large Language Models to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks. Despite these breakthroughs, there still remains a lack of a comprehensive survey to summarize existing approaches and recent developments in this field. To this end, in this paper, we present a thorough review and provide a unified perspective to summarize the recent progress as well as emerging trends in multilingual large language models (MLLMs) literature. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) New taxonomy: we offer a new and unified perspective to summarize the current progress of MLLMs; (3) New frontiers: we highlight several emerging frontiers and discuss the corresponding challenges; (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community with quick access and spur breakthrough research in MLLMs.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2404.04925', 'html': 'https://arxiv.org/html/2404.04925v1', 'tex': '/src/2404.04925', 'doi': 'https://doi.org/10.48550/arXiv.2404.04925'}	Submission history From: Libo Qin [ view email ] [v1] Sun, 7 Apr 2024 11:52:44 UTC (2,521 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.04925'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.04925'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.04925'}]
2024-04-14	Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws	Computation and Language	https://arxiv.org/abs/2404.05405	The Physics of Language Models	https://x.com/omarsar0/status/1777709227319968034		2404.05405	['Zeyuan Allen-Zhu', 'Yuanzhi Li']	ct:Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications. Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation.More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Notable insights include:* The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train.* Prepending training data with domain names (e.g.,this http URL) significantly increases a model's knowledge capacity. Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2404.05405', 'html': None, 'tex': '/src/2404.05405', 'doi': 'https://doi.org/10.48550/arXiv.2404.05405'}	Submission history From: Zeyuan Allen-Zhu [ view email ] [v1] Mon, 8 Apr 2024 11:11:31 UTC (1,280 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.05405'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.05405'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.05405'}]
2024-04-14	Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data	Computation and Language	https://arxiv.org/abs/2404.03862	Aligning LLMs to Quote from Pre-Training Data	https://x.com/omarsar0/status/1777408054402646433		2404.03862	['Jingyu Zhang', 'Marc Marone', 'Tianjian Li', 'Benjamin Van Durme', 'Daniel Khashabi']	ct:To trust the fluent generations of large language models (LLMs), humans must be able to verify their correctness against trusted, external sources. Recent efforts, such as providing citations via retrieved documents or post-hoc provenance, enhance verifiability but provide no guarantees on their correctness. To address these limitations, we tackle the verifiability goal with a different philosophy: trivializing the verification process by developing models that quote verbatim statements from trusted sources in their pre-training data. We propose Quote-Tuning, which demonstrates the feasibility of aligning models to quote. The core of Quote-Tuning is a fast membership inference function that efficiently verifies text against trusted corpora. We leverage this tool to design a reward function to quantify quotes in model responses, and curate datasets for preference learning. Experiments show that Quote-Tuning significantly increases verbatim quotes from high-quality documents by up to 130% relative to base models while maintaining response quality. Quote-Tuning is applicable in different tasks, generalizes to out-of-domain data and diverse model families, and provides additional benefits to truthfulness. Our method not only serves as a hassle-free method to increase quoting but also opens up avenues for improving LLM trustworthiness through better verifiability.	2025 camera ready	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2404.03862', 'html': None, 'tex': '/src/2404.03862', 'doi': 'https://doi.org/10.48550/arXiv.2404.03862'}	Submission history From: Jingyu Zhang [ view email ] [v1] Fri, 5 Apr 2024 02:27:09 UTC (6,621 KB) [v2] Wed, 21 Aug 2024 15:23:28 UTC (6,623 KB) [v3] Thu, 14 Nov 2024 18:27:39 UTC (6,652 KB) [v4] Sat, 22 Feb 2025 01:13:38 UTC (6,728 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.03862'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.03862'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.03862'}]
2024-04-07	Mixture-of-Depths: Dynamically allocating compute in transformer-based language models	Machine Learning	https://arxiv.org/abs/2404.02258	Mixture-of-Depths	https://x.com/TheSeaMouse/status/1775782800362242157		2404.02258	['David Raposo', 'Sam Ritter', 'Blake Richards', 'Timothy Lillicrap', 'Peter Conway Humphreys', 'Adam Santoro']	ct:Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\% faster to step during post-training sampling.		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2404.02258', 'html': 'https://arxiv.org/html/2404.02258v1', 'tex': '/src/2404.02258', 'doi': 'https://doi.org/10.48550/arXiv.2404.02258'}	Submission history From: Adam Santoro [ view email ] [v1] Tue, 2 Apr 2024 19:28:11 UTC (1,763 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.02258'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.02258'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.02258'}]
2024-04-07	Long-context LLMs Struggle with Long In-context Learning	Computation and Language	https://arxiv.org/abs/2404.02060	Local Context LLMs Struggle with Long In-Context Learning	https://x.com/omarsar0/status/1775638933377786076		2404.02060	['Tianle Li', 'Ge Zhang', 'Quy Duc Do', 'Xiang Yue', 'Wenhu Chen']	ct:Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even to be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2404.02060', 'html': 'https://arxiv.org/html/2404.02060v3', 'tex': '/src/2404.02060', 'doi': 'https://doi.org/10.48550/arXiv.2404.02060'}	Submission history From: Tianle Li [ view email ] [v1] Tue, 2 Apr 2024 15:59:11 UTC (3,640 KB) [v2] Thu, 4 Apr 2024 00:01:25 UTC (3,625 KB) [v3] Wed, 12 Jun 2024 02:46:16 UTC (3,066 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.02060'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.02060'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.02060'}]
2024-04-07	Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models	Computation and Language	https://arxiv.org/abs/2404.03622	Visualization-of-Thought	https://x.com/omarsar0/status/1776082343813403063		2404.03622	['Wenshan Wu', 'Shaoguang Mao', 'Yadong Zhang', 'Yan Xia', 'Li Dong', 'Lei Cui', 'Furu Wei']	ct:Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs. Please find the dataset and codes atthis https URL	onference on Neural Information Processing Systems (NeurIPS 2024)	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2404.03622', 'html': None, 'tex': '/src/2404.03622', 'doi': 'https://doi.org/10.48550/arXiv.2404.03622'}	Submission history From: Wenshan Wu [ view email ] [v1] Thu, 4 Apr 2024 17:45:08 UTC (12,911 KB) [v2] Fri, 24 May 2024 04:07:44 UTC (12,914 KB) [v3] Wed, 23 Oct 2024 07:20:26 UTC (12,915 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.03622'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.03622'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.03622'}]
2024-04-07	The Unreasonable Ineffectiveness of the Deeper Layers	Computation and Language	https://arxiv.org/abs/2403.17887v1	The Unreasonable Ineffectiveness of the Deeper Layers	https://x.com/AlphaSignalAI/status/1774858806817906971		2403.17887v1	['Andrey Gromov', 'Kushal Tirumala', 'Hassan Shapourian', 'Paolo Glorioso', 'Daniel A. Roberts']	"ct:We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to ""heal"" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge."	0 pages, 5 + 4 figures	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2403.17887v1', 'html': 'https://arxiv.org/html/2403.17887v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2403.17887'}	Submission history From: Dan Roberts [ view email ] [v1] Tue, 26 Mar 2024 17:20:04 UTC (1,539 KB) [v2] Mon, 3 Mar 2025 17:02:05 UTC (1,679 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.17887'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.17887'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.17887'}]
2024-04-07	ReFT: Representation Finetuning for Language Models	Computation and Language	https://arxiv.org/abs/2404.03592	Representation Finetuning for LMs	https://x.com/arankomatsuzaki/status/1776057023697731913		2404.03592	['Zhengxuan Wu', 'Aryaman Arora', 'Zheng Wang', 'Atticus Geiger', 'Dan Jurafsky', 'Christopher D. Manning', 'Christopher Potts']	ct:Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15x--65x more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly atthis https URL.	nt	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2404.03592', 'html': None, 'tex': '/src/2404.03592', 'doi': 'https://doi.org/10.48550/arXiv.2404.03592'}	Submission history From: Zhengxuan Wu [ view email ] [v1] Thu, 4 Apr 2024 17:00:37 UTC (1,456 KB) [v2] Mon, 8 Apr 2024 03:06:10 UTC (1,457 KB) [v3] Wed, 22 May 2024 17:52:31 UTC (10,381 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.03592'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.03592'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.03592'}]
2024-04-07	Training LLMs over Neurally Compressed Text	Computation and Language	https://arxiv.org/abs/2404.03626	Training LLMs over Neurally Compressed Text	https://x.com/arankomatsuzaki/status/1776055420848631814		2404.03626	['Brian Lester', 'Jaehoon Lee', 'Alex Alemi', 'Jeffrey Pennington', 'Adam Roberts', 'Jascha Sohl-Dickstein', 'Noah Constant']	ct:In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text naïvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.	ed in TMLRthis https URL	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2404.03626', 'html': 'https://arxiv.org/html/2404.03626v3', 'tex': '/src/2404.03626', 'doi': 'https://doi.org/10.48550/arXiv.2404.03626'}	Submission history From: Brian Lester [ view email ] [v1] Thu, 4 Apr 2024 17:48:28 UTC (320 KB) [v2] Tue, 13 Aug 2024 22:01:42 UTC (371 KB) [v3] Thu, 12 Dec 2024 23:03:54 UTC (481 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2404.03626'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2404.03626'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2404.03626'}]
2024-03-31	A comparison of Human, GPT-3.5, and GPT-4 Performance in a University-Level Coding Course	Computation and Language	https://arxiv.org/abs/2403.16977	LLMs for University-Level Coding Course	https://x.com/omarsar0/status/1772647466820685895?s=20		2403.16977	['Will Yeadon', 'Alex Peach', 'Craig P. Testrow']	ct:This study evaluates the performance of ChatGPT variants, GPT-3.5 and GPT-4, both with and without prompt engineering, against solely student work and a mixed category containing both student and GPT-4 contributions in university-level physics coding assignments using the Python language. Comparing 50 student submissions to 50 AI-generated submissions across different categories, and marked blindly by three independent markers, we amassed $n = 300$ data points. Students averaged 91.9% (SE:0.4), surpassing the highest performing AI submission category, GPT-4 with prompt engineering, which scored 81.1% (SE:0.8) - a statistically significant difference (p = $2.482 \times 10^{-10}$). Prompt engineering significantly improved scores for both GPT-4 (p = $1.661 \times 10^{-4}$) and GPT-3.5 (p = $4.967 \times 10^{-9}$). Additionally, the blinded markers were tasked with guessing the authorship of the submissions on a four-point Likert scale from `Definitely AI' to `Definitely Human'. They accurately identified the authorship, with 92.1% of the work categorized as 'Definitely Human' being human-authored. Simplifying this to a binary `AI' or `Human' categorization resulted in an average accuracy rate of 85.3%. These findings suggest that while AI-generated work closely approaches the quality of university students' work, it often remains detectable by human evaluators.	s, 3 figures	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2403.16977', 'html': 'https://arxiv.org/html/2403.16977v1', 'tex': '/src/2403.16977', 'doi': 'https://doi.org/10.48550/arXiv.2403.16977'}	Submission history From: Will Yeadon [ view email ] [v1] Mon, 25 Mar 2024 17:41:02 UTC (594 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.16977'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.16977'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.16977'}]
2024-03-31	Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2403.18814v1	Mini-Gemini	https://x.com/_akhaliq/status/1773170068521713713?s=20		2403.18814v1	['Yanwei Li', 'Yuechen Zhang', 'Chengyao Wang', 'Zhisheng Zhong', 'Yixin Chen', 'Ruihang Chu', 'Shaoteng Liu', 'Jiaya Jia']	ct:In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. Code and models are available atthis https URL.	nd models are available atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2403.18814v1', 'html': 'https://arxiv.org/html/2403.18814v1', 'tex': '/src/2403.18814v1', 'doi': 'https://doi.org/10.48550/arXiv.2403.18814'}	Submission history From: Yanwei Li [ view email ] [v1] Wed, 27 Mar 2024 17:59:04 UTC (8,457 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.18814'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.18814'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.18814'}]
2024-03-31	Long-form factuality in large language models	Computation and Language	https://arxiv.org/abs/2403.18802v1	Long-form factuality in LLMs	https://x.com/JerryWeiAI/status/1773402343301877960?s=20		2403.18802v1	['Jerry Wei', 'Chengrun Yang', 'Xinying Song', 'Yifeng Lu', 'Nathan Hu', 'Dustin Tran', 'Daiyi Peng', 'Ruibo Liu', 'Da Huang', 'Cosmo Du', 'Quoc V. Le']	ct:Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall).Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2403.18802v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2403.18802'}	Submission history From: Jerry Wei [ view email ] [v1] Wed, 27 Mar 2024 17:48:55 UTC (236 KB) [v2] Mon, 1 Apr 2024 21:02:37 UTC (238 KB) [v3] Wed, 3 Apr 2024 20:54:11 UTC (238 KB) [v4] Thu, 7 Nov 2024 03:14:38 UTC (254 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.18802'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.18802'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.18802'}]
2024-03-31	Agent Lumos: Unified and Modular Training for Open-Source Language Agents	Artificial Intelligence	https://arxiv.org/abs/2311.05657	Agent Lumos	https://x.com/Wade_Yin9712/status/1773792306791055397?s=20		2311.05657	['Da Yin', 'Faeze Brahman', 'Abhilasha Ravichander', 'Khyathi Chandu', 'Kai-Wei Chang', 'Yejin Choi', 'Bill Yuchen Lin']	ct:Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce LUMOS, one of the first frameworks for training open-source LLM-based agents. LUMOS features a learnable, unified, and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, LUMOS exhibits several key advantages: (1) LUMOS excels multiple larger open-source agents on the held-out datasets (unused for training) for each task type. LUMOS even surpasses GPT agents on QA and web tasks; (2) LUMOS outperforms open-source agents produced by chain-of-thoughts and unmodularized integrated training; and (3) LUMOS effectively generalizes to unseen tasks, outperforming 33B-scale agents and domain-specific agents.	ed to ACL 2024 Main Conference; Camera Ready. Project website:this https URL	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.05657', 'html': 'https://arxiv.org/html/2311.05657v3', 'tex': '/src/2311.05657', 'doi': 'https://doi.org/10.48550/arXiv.2311.05657'}	Submission history From: Bill Yuchen Lin [ view email ] [v1] Thu, 9 Nov 2023 00:30:13 UTC (936 KB) [v2] Wed, 13 Mar 2024 10:54:21 UTC (2,558 KB) [v3] Wed, 10 Jul 2024 17:36:02 UTC (1,898 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.05657'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.05657'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.05657'}]
2024-03-31	AIOS: LLM Agent Operating System	Operating Systems	https://arxiv.org/abs/2403.16971v2	AIOS	https://x.com/arankomatsuzaki/status/1772460132745547976?s=20		2403.16971v2	['Kai Mei', 'Zelong Li', 'Shuyuan Xu', 'Ruosong Ye', 'Yingqiang Ge', 'Yongfeng Zhang']	"ct:The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS) as the brain of the OS, enabling an operating system ""with soul"" -- an important step towards AGI. Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, and maintain access control for agents. We present the architecture of such an operating system, outline the core challenges it aims to resolve, and provide the basic design and implementation of the AIOS. Our experiments on concurrent execution of multiple agents demonstrate the reliability and efficiency of our AIOS modules. Through this, we aim to not only improve the performance and efficiency of LLM agents but also to pioneer for better development and deployment of the AIOS ecosystem in the future. The project is open-source atthis https URL."	es, 5 figures, 5 tables; comments and suggestions are appreciated	['Operating Systems (cs.OS)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2403.16971v2', 'html': 'https://arxiv.org/html/2403.16971v2', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2403.16971'}	Submission history From: Yongfeng Zhang [ view email ] [v1] Mon, 25 Mar 2024 17:32:23 UTC (394 KB) [v2] Tue, 26 Mar 2024 02:35:07 UTC (394 KB) [v3] Thu, 7 Nov 2024 19:10:11 UTC (2,250 KB) [v4] Sun, 11 May 2025 20:23:45 UTC (2,860 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.16971'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.16971'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.16971'}]
2024-03-31	FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions	Information Retrieval	https://arxiv.org/abs/2403.15246	FollowIR	https://x.com/arankomatsuzaki/status/1772082608609833127?s=20		2403.15246	['Orion Weller', 'Benjamin Chang', 'Sean MacAvaney', 'Kyle Lo', 'Arman Cohan', 'Benjamin Van Durme', 'Dawn Lawrie', 'Luca Soldaini']	ct:Modern Language Models (LMs) are capable of following long and complex instructions that enable a large and diverse set of user requests. While Information Retrieval (IR) models use these LMs as the backbone of their architectures, virtually none of them allow users to provide detailed instructions alongside queries, thus limiting their ability to satisfy complex information needs. In this work, we study the use of instructions in IR systems. First, we introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR repurposes detailed instructions -- also known as narratives -- developed for professional assessors to evaluate retrieval systems. In particular, we build our benchmark from three collections curated for shared tasks at the Text REtrieval Conference (TREC). These collections contains hundreds to thousands of labeled documents per query, making them suitable for our exploration. Through this process, we can measure how well IR models follow instructions, through a new pairwise evaluation framework. Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form information. However, we show that it is possible for IR models to learn to follow complex instructions: our new FollowIR-7B model has significant improvements after fine-tuning on our training set.		['Information Retrieval (cs.IR)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2403.15246', 'html': 'https://arxiv.org/html/2403.15246v3', 'tex': '/src/2403.15246', 'doi': 'https://doi.org/10.48550/arXiv.2403.15246'}	Submission history From: Orion Weller [ view email ] [v1] Fri, 22 Mar 2024 14:42:29 UTC (168 KB) [v2] Mon, 6 May 2024 14:56:01 UTC (608 KB) [v3] Tue, 7 May 2024 14:25:15 UTC (604 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.15246'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.15246'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.15246'}]
2024-03-31	LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement	Computation and Language	https://arxiv.org/abs/2403.15042	LLM2LLM	https://x.com/arankomatsuzaki/status/1772078585903219007?s=20		2403.15042	['Nicholas Lee', 'Thanakul Wattanawong', 'Sehoon Kim', 'Karttikeya Mangalam', 'Sheng Shen', 'Gopala Anumanchipalli', 'Michael W. Mahoney', 'Kurt Keutzer', 'Amir Gholami']	ct:Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM. Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines. LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular fine-tuning in the low-data regime using a Llama-2-7B student model. Our code is available atthis https URL.	24	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2403.15042', 'html': 'https://arxiv.org/html/2403.15042v2', 'tex': '/src/2403.15042', 'doi': 'https://doi.org/10.48550/arXiv.2403.15042'}	Submission history From: Nicholas Lee [ view email ] [v1] Fri, 22 Mar 2024 08:57:07 UTC (209 KB) [v2] Sat, 13 Jul 2024 07:36:49 UTC (7,081 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.15042'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.15042'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.15042'}]
2024-03-25	Evolutionary Optimization of Model Merging Recipes	Neural and Evolutionary Computing	https://arxiv.org/abs/2403.13187	Evolutionary Model Merge	https://x.com/SakanaAILabs/status/1770613032198279663?s=20		2403.13187	['Takuya Akiba', 'Makoto Shing', 'Yujin Tang', 'Qi Sun', 'David Ha']	ct:Large language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. While model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.	s' submitted version before final edits. Published in Nature Machine Intelligence on January 27, 2025:this https URL	['Neural and Evolutionary Computing (cs.NE)']	{'pdf': '/pdf/2403.13187', 'html': 'https://arxiv.org/html/2403.13187v2', 'tex': '/src/2403.13187', 'doi': 'https://doi.org/10.48550/arXiv.2403.13187'}	Submission history From: Takuya Akiba [ view email ] [v1] Tue, 19 Mar 2024 22:56:53 UTC (1,162 KB) [v2] Mon, 27 Jan 2025 10:19:44 UTC (1,342 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.13187'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.13187'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.13187'}]
2024-03-25	RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners	Computation and Language	https://arxiv.org/abs/2403.12373	Step-by-Step Comparisons Make LLMs Better Reasoners	https://x.com/omarsar0/status/1770492690129359135?s=20		2403.12373	['Chi Hu', 'Yuan Ge', 'Xiangnan Ma', 'Hang Cao', 'Qiang Li', 'Yonghua Yang', 'Tong Xiao', 'Jingbo Zhu']	ct:Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, such as deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover, RankPrompt excels in LLM-based automatic evaluations for open-ended tasks, aligning with human judgments 74% of the time in the AlpacaEval dataset. It also exhibits robustness to variations in response order and consistency. Collectively, our results validate RankPrompt as an effective method for eliciting high-quality feedback from language models.	oling 2024 Long Paper	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2403.12373', 'html': 'https://arxiv.org/html/2403.12373v3', 'tex': '/src/2403.12373', 'doi': 'https://doi.org/10.48550/arXiv.2403.12373'}	Submission history From: Hu Chi [ view email ] [v1] Tue, 19 Mar 2024 02:34:18 UTC (132 KB) [v2] Thu, 21 Mar 2024 06:01:48 UTC (132 KB) [v3] Fri, 22 Mar 2024 06:18:54 UTC (132 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.12373'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.12373'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.12373'}]
2024-03-25	LLM4Decompile: Decompiling Binary Code with Large Language Models	Programming Languages	https://arxiv.org/abs/2403.05286v1	LLM4Decompile	https://x.com/omarsar0/status/1771218791399092351?s=20		2403.05286v1	['Hanzhuo Tan', 'Qi Luo', 'Jing Li', 'Yuqun Zhang']	ct:Decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. Large language models (LLMs) show promise for programming tasks, motivating their application to decompilation. However, there does not exist any open-source LLM for decompilation. Moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code. The open-source LLMs can serve as baselines for further development in the field. To ensure practical program evaluation, we introduce Decompile-Eval, the first dataset that considers re-compilability and re-executability for decompilation. The benchmark emphasizes the importance of evaluating the decompilation model from the perspective of program semantics. Experiments indicate that our LLM4Decompile has demonstrated the capability to accurately decompile 21% of the assembly code, which achieves a 50% improvement over GPT-4. Our code, dataset, and models are released atthis https URL	ng	['Programming Languages (cs.PL)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2403.05286v1', 'html': 'https://arxiv.org/html/2403.05286v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2403.05286'}	Submission history From: Hanzhuo Tan [ view email ] [v1] Fri, 8 Mar 2024 13:10:59 UTC (7,991 KB) [v2] Wed, 19 Jun 2024 02:45:03 UTC (810 KB) [v3] Tue, 22 Oct 2024 03:58:20 UTC (909 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.05286'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.05286'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.05286'}]
2024-03-25	Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models	Computation and Language	https://arxiv.org/abs/2403.12881v1	Agent-FLAN	https://x.com/_akhaliq/status/1770302813152690259?s=20		2403.12881v1	['Zehui Chen', 'Kuikun Liu', 'Qiuchen Wang', 'Wenwei Zhang', 'Jiangning Liu', 'Dahua Lin', 'Kai Chen', 'Feng Zhao']	ct:Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available atthis https URL.	cal Report	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2403.12881v1', 'html': 'https://arxiv.org/html/2403.12881v1', 'tex': '/src/2403.12881v1', 'doi': 'https://doi.org/10.48550/arXiv.2403.12881'}	Submission history From: Zehui Chen [ view email ] [v1] Tue, 19 Mar 2024 16:26:10 UTC (3,447 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.12881'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.12881'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.12881'}]
2024-03-25	Logits of API-Protected LLMs Leak Proprietary Information	Computation and Language	https://arxiv.org/abs/2403.09539	LLMs Leak Proprietary Information	https://x.com/DimitrisPapail/status/1768654579254579385?s=20		2403.09539	['Matthew Finlayson', 'Xiang Ren', 'Swabha Swayamdipta']	ct:Large language model (LLM) providers often hide the architectural details and parameters of their proprietary models by restricting public access to a limited API. In this work we show that, with only a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1000 USD for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We exploit this fact to unlock several capabilities, including (but not limited to) obtaining cheap full-vocabulary outputs, auditing for specific types of model updates, identifying the source LLM given a single full LLM output, and even efficiently discovering the LLM's hidden size. Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Cryptography and Security (cs.CR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2403.09539', 'html': 'https://arxiv.org/html/2403.09539v3', 'tex': '/src/2403.09539', 'doi': 'https://doi.org/10.48550/arXiv.2403.09539'}	Submission history From: Matthew Finlayson [ view email ] [v1] Thu, 14 Mar 2024 16:27:49 UTC (1,651 KB) [v2] Fri, 15 Mar 2024 02:07:30 UTC (1,651 KB) [v3] Fri, 8 Nov 2024 18:56:41 UTC (1,197 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.09539'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.09539'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.09539'}]
2024-03-25	DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset	Robotics	https://arxiv.org/abs/2403.12945	DROID	https://x.com/chelseabfinn/status/1770311755140575413?s=20		2403.12945	"['Alexander Khazatsky', 'Karl Pertsch', 'Suraj Nair', 'Ashwin Balakrishna', 'Sudeep Dasari', 'Siddharth Karamcheti', 'Soroush Nasiriany', 'Mohan Kumar Srirama', 'Lawrence Yunliang Chen', 'Kirsty Ellis', 'Peter David Fagan', 'Joey Hejna', 'Masha Itkina', 'Marion Lepert', 'Yecheng Jason Ma', 'Patrick Tree Miller', 'Jimmy Wu', 'Suneel Belkhale', 'Shivin Dass', 'Huy Ha', 'Arhan Jain', 'Abraham Lee', 'Youngwoon Lee', 'Marius Memmel', 'Sungjae Park', 'Ilija Radosavovic', 'Kaiyuan Wang', 'Albert Zhan', 'Kevin Black', 'Cheng Chi', 'Kyle Beltran Hatch', 'Shan Lin', 'Jingpei Lu', 'Jean Mercat', 'Abdul Rehman', 'Pannag R Sanketi', 'Archit Sharma', 'Cody Simpson', 'Quan Vuong', 'Homer Rich Walke', 'Blake Wulfe', 'Ted Xiao', 'Jonathan Heewon Yang', 'Arefeh Yavary', 'Tony Z. Zhao', 'Christopher Agia', 'Rohan Baijal', 'Mateo Guaman Castro', 'Daphne Chen', 'Qiuyu Chen', 'Trinity Chung', 'Jaimyn Drake', 'Ethan Paul Foster', 'Jensen Gao', 'Vitor Guizilini', 'David Antonio Herrera', 'Minho Heo', 'Kyle Hsu', 'Jiaheng Hu', 'Muhammad Zubair Irshad', 'Donovon Jackson', 'Charlotte Le', 'Yunshuang Li', 'Kevin Lin', 'Roy Lin', 'Zehan Ma', 'Abhiram Maddukuri', 'Suvir Mirchandani', 'Daniel Morton', 'Tony Nguyen', ""Abigail O'Neill"", 'Rosario Scalise', 'Derick Seale', 'Victor Son', 'Stephen Tian', 'Emi Tran', 'Andrew E. Wang', 'Yilin Wu', 'Annie Xie', 'Jingyun Yang', 'Patrick Yin', 'Yunchu Zhang', 'Osbert Bastani', 'Glen Berseth', 'Jeannette Bohg', 'Ken Goldberg', 'Abhinav Gupta', 'Abhishek Gupta', 'Dinesh Jayaraman', 'Joseph J Lim', 'Jitendra Malik', 'Roberto Martín-Martín', 'Subramanian Ramamoorthy', 'Dorsa Sadigh', 'Shuran Song', 'Jiajun Wu', 'Michael C. Yip', 'Yuke Zhu', 'Thomas Kollar', 'Sergey Levine', 'Chelsea Finn']"	ct:The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.	t website:this https URL	['Robotics (cs.RO)']	{'pdf': '/pdf/2403.12945', 'html': 'https://arxiv.org/html/2403.12945v2', 'tex': '/src/2403.12945', 'doi': 'https://doi.org/10.48550/arXiv.2403.12945'}	Submission history From: Karl Pertsch [ view email ] [v1] Tue, 19 Mar 2024 17:48:38 UTC (4,669 KB) [v2] Tue, 22 Apr 2025 17:57:51 UTC (8,641 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.12945'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.12945'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.12945'}]
2024-03-25	RAFT: Adapting Language Model to Domain Specific RAG	Computation and Language	https://arxiv.org/abs/2403.10131	Retrieval-Augmented Fine-Tuning	https://x.com/cwolferesearch/status/1770912695765660139?s=20		2403.10131	['Tianjun Zhang', 'Shishir G. Patil', 'Naman Jain', 'Sheng Shen', 'Matei Zaharia', 'Ion Stoica', 'Joseph E. Gonzalez']	"ct:Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a ""open-book"" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced atthis http URL."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2403.10131', 'html': 'https://arxiv.org/html/2403.10131v2', 'tex': '/src/2403.10131', 'doi': 'https://doi.org/10.48550/arXiv.2403.10131'}	Submission history From: Tianjun Zhang [ view email ] [v1] Fri, 15 Mar 2024 09:26:02 UTC (760 KB) [v2] Wed, 5 Jun 2024 17:27:51 UTC (1,068 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.10131'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.10131'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.10131'}]
2024-03-17	RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation	Computation and Language	https://arxiv.org/abs/2403.05313	Retrieval Augmented Thoughts	https://x.com/omarsar0/status/1767251740443746435?s=20		2403.05313	['Zihao Wang', 'Anji Liu', 'Haowei Lin', 'Jiaqi Li', 'Xiaojian Ma', 'Yitao Liang']	ct:We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found atthis https URL		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2403.05313', 'html': 'https://arxiv.org/html/2403.05313v1', 'tex': '/src/2403.05313', 'doi': 'https://doi.org/10.48550/arXiv.2403.05313'}	Submission history From: Zihao Wang [ view email ] [v1] Fri, 8 Mar 2024 13:42:19 UTC (9,482 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.05313'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.05313'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.05313'}]
2024-03-17	Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking	Computation and Language	https://arxiv.org/abs/2403.09629	LMs Can Teach Themselves to Think Before Speaking	https://x.com/omarsar0/status/1768681638009975088?s=20		2403.09629	['Eric Zelikman', 'Georges Harik', 'Yijia Shao', 'Varuna Jayasiri', 'Nick Haber', 'Noah D. Goodman']	ct:When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2403.09629', 'html': None, 'tex': '/src/2403.09629', 'doi': 'https://doi.org/10.48550/arXiv.2403.09629'}	Submission history From: Eric Zelikman [ view email ] [v1] Thu, 14 Mar 2024 17:58:16 UTC (510 KB) [v2] Mon, 18 Mar 2024 07:56:48 UTC (525 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.09629'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.09629'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.09629'}]
2024-03-17	Knowledge Conflicts for LLMs: A Survey	Computation and Language	https://arxiv.org/abs/2403.08319	Knowledge Conflicts for LLMs	https://x.com/omarsar0/status/1768288774532858003?s=20		2403.08319	['Rongwu Xu', 'Zehan Qi', 'Zhijiang Guo', 'Cunxiang Wang', 'Hongru Wang', 'Yue Zhang', 'Wei Xu']	ct:This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.	tHub repo is available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2403.08319', 'html': 'https://arxiv.org/html/2403.08319v2', 'tex': '/src/2403.08319', 'doi': 'https://doi.org/10.48550/arXiv.2403.08319'}	Submission history From: Rongwu Xu [ view email ] [v1] Wed, 13 Mar 2024 08:02:23 UTC (796 KB) [v2] Sat, 22 Jun 2024 08:31:40 UTC (747 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.08319'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.08319'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.08319'}]
2024-03-17	Stealing Part of a Production Language Model	Cryptography and Security	https://arxiv.org/abs/2403.06634	Stealing Part of a Production Language Model	https://x.com/omarsar0/status/1767641831079067694?s=20		2403.06634	['Nicholas Carlini', 'Daniel Paleka', 'Krishnamurthy Dj Dvijotham', 'Thomas Steinke', 'Jonathan Hayase', 'A. Feder Cooper', 'Katherine Lee', 'Matthew Jagielski', 'Milad Nasr', 'Arthur Conmy', 'Itay Yona', 'Eric Wallace', 'David Rolnick', 'Florian Tramèr']	ct:We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.		['Cryptography and Security (cs.CR)']	{'pdf': '/pdf/2403.06634', 'html': None, 'tex': '/src/2403.06634', 'doi': 'https://doi.org/10.48550/arXiv.2403.06634'}	Submission history From: Nicholas Carlini [ view email ] [v1] Mon, 11 Mar 2024 11:46:12 UTC (697 KB) [v2] Tue, 9 Jul 2024 17:44:00 UTC (724 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.06634'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.06634'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.06634'}]
2024-03-17	Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM	Computation and Language	https://arxiv.org/abs/2403.07816	Branch-Train-MiX	https://x.com/jaseweston/status/1767727740952682667?s=20		2403.07816	['Sainbayar Sukhbaatar', 'Olga Golovneva', 'Vasu Sharma', 'Hu Xu', 'Xi Victoria Lin', 'Baptiste Rozière', 'Jacob Kahn', 'Daniel Li', 'Wen-tau Yih', 'Jason Weston', 'Xian Li']	ct:We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2403.07816', 'html': 'https://arxiv.org/html/2403.07816v1', 'tex': '/src/2403.07816', 'doi': 'https://doi.org/10.48550/arXiv.2403.07816'}	Submission history From: Xian Li [ view email ] [v1] Tue, 12 Mar 2024 16:54:58 UTC (946 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.07816'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.07816'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.07816'}]
2024-03-17	Large language models surpass human experts in predicting neuroscience results	Quantitative Biology > Neurons and Cognition	https://arxiv.org/abs/2403.03230	LLMs Predict Neuroscience Results	https://x.com/ProfData/status/1765689739682754824?s=20		2403.03230	['Xiaoliang Luo', 'Akilles Rechardt', 'Guangzhi Sun', 'Kevin K. Nejad', 'Felipe Yáñez', 'Bati Yilmaz', 'Kangjoo Lee', 'Alexandra O. Cohen', 'Valentina Borghesani', 'Anton Pashkov', 'Daniele Marinazzo', 'Jonathan Nicholas', 'Alessandro Salatiello', 'Ilia Sucholutsky', 'Pasquale Minervini', 'Sepehr Razavi', 'Roberta Rocca', 'Elkhan Yusifov', 'Tereza Okalova', 'Nianlong Gu', 'Martin Ferianc', 'Mikail Khona', 'Kaustubh R. Patil', 'Pui-Shee Lee', 'Rui Mata', 'Nicholas E. Myers', 'Jennifer K Bizley', 'Sebastian Musslick', 'Isil Poyraz Bilgin', 'Guiomar Niso', 'Justin M. Ales', 'Michael Gaebler', 'N Apurva Ratan Murty', 'Leyla Loued-Khenissi', 'Anna Behler', 'Chloe M. Hall', 'Jessica Dafflon', 'Sherry Dongqi Bao', 'Bradley C. Love']	ct:Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.	test version of this paper has been published at Nature Human Behaviour, please seethis https URL	['Neurons and Cognition (q-bio.NC)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2403.03230', 'html': 'https://arxiv.org/html/2403.03230v4', 'tex': '/src/2403.03230', 'doi': 'https://doi.org/10.48550/arXiv.2403.03230'}	Submission history From: Xiaoliang Luo [ view email ] [v1] Mon, 4 Mar 2024 15:27:59 UTC (3,592 KB) [v2] Thu, 14 Mar 2024 23:32:15 UTC (3,575 KB) [v3] Fri, 21 Jun 2024 17:35:46 UTC (2,845 KB) [v4] Thu, 28 Nov 2024 08:49:00 UTC (2,845 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.03230'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.03230'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.03230'}]
2024-03-17	Is Cosine-Similarity of Embeddings Really About Similarity?	Information Retrieval	https://arxiv.org/abs/2403.05440	Is Cosine-Similarity Really About Simirity?	https://x.com/_reachsumit/status/1767045820384477575?s=20		2403.05440	['Harald Steck', 'Chaitanya Ekanadham', 'Nathan Kallus']	ct:Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.	s	['Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2403.05440', 'html': 'https://arxiv.org/html/2403.05440v1', 'tex': '/src/2403.05440', 'doi': 'https://doi.org/10.48550/arXiv.2403.05440'}	Submission history From: Harald Steck [ view email ] [v1] Fri, 8 Mar 2024 16:48:20 UTC (5,737 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.05440'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.05440'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.05440'}]
2024-03-17	MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2403.09611	Multimodal LLM Pre-training	https://x.com/DrJimFan/status/1769053019939967080?s=20		2403.09611	['Brandon McKinzie', 'Zhe Gan', 'Jean-Philippe Fauconnier', 'Sam Dodge', 'Bowen Zhang', 'Philipp Dufter', 'Dhruti Shah', 'Xianzhi Du', 'Futang Peng', 'Floris Weers', 'Anton Belyi', 'Haotian Zhang', 'Karanjeet Singh', 'Doug Kang', 'Ankur Jain', 'Hongyu Hè', 'Max Schwarzer', 'Tom Gunter', 'Xiang Kong', 'Aonan Zhang', 'Jianyu Wang', 'Chong Wang', 'Nan Du', 'Tao Lei', 'Sam Wiseman', 'Guoli Yin', 'Mark Lee', 'Zirui Wang', 'Ruoming Pang', 'Peter Grasch', 'Alexander Toshev', 'Yinfei Yang']	ct:In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.		['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2403.09611', 'html': None, 'tex': '/src/2403.09611', 'doi': 'https://doi.org/10.48550/arXiv.2403.09611'}	Submission history From: Zhe Gan [ view email ] [v1] Thu, 14 Mar 2024 17:51:32 UTC (14,464 KB) [v2] Tue, 19 Mar 2024 16:37:13 UTC (14,458 KB) [v3] Fri, 22 Mar 2024 17:03:16 UTC (14,458 KB) [v4] Thu, 18 Apr 2024 18:51:04 UTC (14,458 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.09611'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.09611'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.09611'}]
2024-03-10	Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap	Artificial Intelligence	https://arxiv.org/abs/2402.19450	Robust Evaluation of Reasoning	https://x.com/_saurabh/status/1763626711407816930?s=20		2402.19450	['Saurabh Srivastava', 'Annarose M B', 'Anto P V', 'Shashank Menon', 'Ajay Sukumar', 'Adwaith Samod T', 'Alan Philipose', 'Stevin Prince', 'Sooraj Thomas']	"ct:We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building ""gap 0"" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available atthis https URL."	es, 10 figures	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.19450', 'html': 'https://arxiv.org/html/2402.19450v1', 'tex': '/src/2402.19450', 'doi': 'https://doi.org/10.48550/arXiv.2402.19450'}	Submission history From: Saurabh Srivastava [ view email ] [v1] Thu, 29 Feb 2024 18:48:18 UTC (184 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.19450'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.19450'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.19450'}]
2024-03-10	GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection	Machine Learning	https://arxiv.org/abs/2403.03507	GaLore	https://x.com/AnimaAnandkumar/status/1765613815146893348?s=20		2403.03507	['Jiawei Zhao', 'Zhenyu Zhang', 'Beidi Chen', 'Zhangyang Wang', 'Anima Anandkumar', 'Yuandong Tian']	ct:Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.	024 (Oral)	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2403.03507', 'html': 'https://arxiv.org/html/2403.03507v2', 'tex': '/src/2403.03507', 'doi': 'https://doi.org/10.48550/arXiv.2403.03507'}	Submission history From: Jiawei Zhao [ view email ] [v1] Wed, 6 Mar 2024 07:29:57 UTC (325 KB) [v2] Sun, 2 Jun 2024 21:24:12 UTC (365 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.03507'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.03507'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.03507'}]
2024-03-10	Can Large Language Models Reason and Plan?	Artificial Intelligence	https://arxiv.org/abs/2403.04121	Can LLMs Reason and Plan?	https://x.com/omarsar0/status/1766123621326475285?s=20		2403.04121	['Subbarao Kambhampati']	ct:While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.	admin note: text overlap witharXiv:2402.01817(v2 add creative commons attribution to Figure 2 graphic)	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2403.04121', 'html': 'https://arxiv.org/html/2403.04121v2', 'tex': '/src/2403.04121', 'doi': 'https://doi.org/10.48550/arXiv.2403.04121'}	Submission history From: Subbarao Kambhampati [ view email ] [v1] Thu, 7 Mar 2024 00:36:32 UTC (5,418 KB) [v2] Fri, 8 Mar 2024 19:51:14 UTC (5,418 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.04121'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.04121'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.04121'}]
2024-03-10	Retrieval-Augmented Generation for AI-Generated Content: A Survey	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2402.19473v1	RAG for AI-Generated Content	https://x.com/omarsar0/status/1765414854397985175?s=20		2402.19473v1	['Penghao Zhao', 'Hailin Zhang', 'Qinhan Yu', 'Zhengren Wang', 'Yunteng Geng', 'Fangcheng Fu', 'Ling Yang', 'Wentao Zhang', 'Bin Cui']	ct:The development of Artificial Intelligence Generated Content (AIGC) has been facilitated by advancements in model algorithms, scalable foundation model architectures, and the availability of ample high-quality datasets. While AIGC has achieved remarkable performance, it still faces challenges, such as the difficulty of maintaining up-to-date and long-tail knowledge, the risk of data leakage, and the high costs associated with training and inference. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances AIGC results by retrieving relevant objects from available data stores, leading to greater accuracy and robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator. We distill the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Project:this https URL	 259 papers, 29 pages, 8 figures. Project:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2402.19473v1', 'html': 'https://arxiv.org/html/2402.19473v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2402.19473'}	Submission history From: Penghao Zhao [ view email ] [v1] Thu, 29 Feb 2024 18:59:01 UTC (7,407 KB) [v2] Wed, 27 Mar 2024 09:00:25 UTC (8,607 KB) [v3] Sun, 14 Apr 2024 07:01:41 UTC (6,525 KB) [v4] Thu, 2 May 2024 16:25:18 UTC (10,559 KB) [v5] Fri, 31 May 2024 13:56:39 UTC (10,559 KB) [v6] Fri, 21 Jun 2024 08:26:36 UTC (9,025 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.19473'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.19473'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.19473'}]
2024-03-10	KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents	Computation and Language	https://arxiv.org/abs/2403.03101	KnowAgent	https://x.com/omarsar0/status/1765408813467759037?s=20		2403.03101	['Yuqi Zhu', 'Shuofei Qiao', 'Yixin Ou', 'Shumin Deng', 'Shiwei Lyu', 'Yue Shen', 'Lei Liang', 'Jinjie Gu', 'Huajun Chen', 'Ningyu Zhang']	ct:Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation. Code is available inthis https URL.	2025 Findings. Project page:this https URLCode:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)', 'Machine Learning (cs.LG)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2403.03101', 'html': None, 'tex': '/src/2403.03101', 'doi': 'https://doi.org/10.48550/arXiv.2403.03101'}	Submission history From: Ningyu Zhang [ view email ] [v1] Tue, 5 Mar 2024 16:39:12 UTC (9,366 KB) [v2] Sun, 26 Jan 2025 14:02:12 UTC (16,052 KB) [v3] Fri, 21 Feb 2025 05:04:27 UTC (16,053 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.03101'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.03101'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.03101'}]
2024-03-10	Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2402.17177v2	Sora Overview	https://x.com/omarsar0/status/1765756669659603015?s=20		2402.17177v2	['Yixin Liu', 'Kai Zhang', 'Yuan Li', 'Zhiling Yan', 'Chujie Gao', 'Ruoxi Chen', 'Zhengqing Yuan', 'Yue Huang', 'Hanchi Sun', 'Jianfeng Gao', 'Lifang He', 'Lichao Sun']	"ct:Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this ""world simulator"". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation."	es, 18 figures; this is not an official report; GitHub:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.17177v2', 'html': 'https://arxiv.org/html/2402.17177v2', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2402.17177'}	Submission history From: Yixin Liu [ view email ] [v1] Tue, 27 Feb 2024 03:30:58 UTC (24,962 KB) [v2] Wed, 28 Feb 2024 18:20:20 UTC (24,962 KB) [v3] Wed, 17 Apr 2024 18:41:39 UTC (23,061 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.17177'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.17177'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.17177'}]
2024-03-10	SaulLM-7B: A pioneering Large Language Model for Law	Computation and Language	https://arxiv.org/abs/2403.03883	LLM for Law	https://x.com/_akhaliq/status/1765614083875738028?s=20		2403.03883	['Pierre Colombo', 'Telmo Pessoa Pires', 'Malik Boudiaf', 'Dominic Culver', 'Rui Melo', 'Caio Corro', 'Andre F. T. Martins', 'Fabrizio Esposito', 'Vera Lúcia Raposo', 'Sofia Morgado', 'Michael Desa']	ct:In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the MIT License.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2403.03883', 'html': 'https://arxiv.org/html/2403.03883v2', 'tex': '/src/2403.03883', 'doi': 'https://doi.org/10.48550/arXiv.2403.03883'}	Submission history From: Pierre Colombo [ view email ] [v1] Wed, 6 Mar 2024 17:42:16 UTC (8,531 KB) [v2] Thu, 7 Mar 2024 06:39:32 UTC (8,531 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.03883'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.03883'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.03883'}]
2024-03-10	Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End Engineering	Computation and Language	https://arxiv.org/abs/2403.03163	Design2Code	https://x.com/_akhaliq/status/1765199160653828385?s=20		2403.03163	['Chenglei Si', 'Yanzhe Zhang', 'Ryan Li', 'Zhengyuan Yang', 'Ruibo Liu', 'Diyi Yang']	ct:Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development in which multimodal large language models (MLLMs) directly convert visual designs into code implementations. In this work, we construct Design2Code - the first real-world benchmark for this task. Specifically, we manually curate 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations to validate the performance ranking. To rigorously benchmark MLLMs, we test various multimodal prompting methods on frontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained break-down metrics indicate that models mostly lag in recalling visual elements from the input webpages and generating correct layout designs.	2025; The first two authors contributed equally	['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2403.03163', 'html': None, 'tex': '/src/2403.03163', 'doi': 'https://doi.org/10.48550/arXiv.2403.03163'}	Submission history From: Chenglei Si [ view email ] [v1] Tue, 5 Mar 2024 17:56:27 UTC (3,151 KB) [v2] Thu, 21 Nov 2024 06:18:07 UTC (4,175 KB) [v3] Sun, 9 Feb 2025 04:22:26 UTC (4,077 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.03163'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.03163'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.03163'}]
2024-03-10	TripoSR: Fast 3D Object Reconstruction from a Single Image	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2403.02151v1	TripoSR	https://x.com/_akhaliq/status/1764841524431392794?s=20		2403.02151v1	['Dmitry Tochilkin', 'David Pankratz', 'Zexiang Liu', 'Zixuan Huang', 'Adam Letts', 'Yangguang Li', 'Ding Liang', 'Christian Laforte', 'Varun Jampani', 'Yan-Pei Cao']	ct:This technical report introduces TripoSR, a 3D reconstruction model leveraging transformer architecture for fast feed-forward 3D generation, producing 3D mesh from a single image in under 0.5 seconds. Building upon the LRM network architecture, TripoSR integrates substantial improvements in data processing, model design, and training techniques. Evaluations on public datasets show that TripoSR exhibits superior performance, both quantitatively and qualitatively, compared to other open-source alternatives. Released under the MIT license, TripoSR is intended to empower researchers, developers, and creatives with the latest advancements in 3D generative AI.	this https URLCode:this https URLDemo:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2403.02151v1', 'html': 'https://arxiv.org/html/2403.02151v1', 'tex': '/src/2403.02151v1', 'doi': 'https://doi.org/10.48550/arXiv.2403.02151'}	Submission history From: Zixuan Huang [ view email ] [v1] Mon, 4 Mar 2024 16:00:56 UTC (1,818 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2403.02151'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2403.02151'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2403.02151'}]
2024-03-03	Genie: Generative Interactive Environments	Machine Learning	https://arxiv.org/abs/2402.15391	Genie	https://x.com/_rockt/status/1762026090262872161?s=20		2402.15391	['Jake Bruce', 'Michael Dennis', 'Ashley Edwards', 'Jack Parker-Holder', 'Yuge Shi', 'Edward Hughes', 'Matthew Lai', 'Aditi Mavalankar', 'Richie Steigerwald', 'Chris Apps', 'Yusuf Aytar', 'Sarah Bechtle', 'Feryal Behbahani', 'Stephanie Chan', 'Nicolas Heess', 'Lucy Gonzalez', 'Simon Osindero', 'Sherjil Ozair', 'Scott Reed', 'Jingwei Zhang', 'Konrad Zolna', 'Jeff Clune', 'Nando de Freitas', 'Satinder Singh', 'Tim Rocktäschel']	ct:We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.	ttps URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2402.15391', 'html': 'https://arxiv.org/html/2402.15391v1', 'tex': '/src/2402.15391', 'doi': 'https://doi.org/10.48550/arXiv.2402.15391'}	Submission history From: Jack Parker-Holder [ view email ] [v1] Fri, 23 Feb 2024 15:47:26 UTC (35,178 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.15391'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.15391'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.15391'}]
2024-03-03	The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits	Computation and Language	https://arxiv.org/abs/2402.17764	The Era of 1-bit LLMs	https://x.com/_akhaliq/status/1762729757454618720?s=20		2402.17764	['Shuming Ma', 'Hongyu Wang', 'Lingxiao Ma', 'Lei Wang', 'Wenhui Wang', 'Shaohan Huang', 'Li Dong', 'Ruiping Wang', 'Jilong Xue', 'Furu Wei']	ct:Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.	n progress	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.17764', 'html': 'https://arxiv.org/html/2402.17764v1', 'tex': '/src/2402.17764', 'doi': 'https://doi.org/10.48550/arXiv.2402.17764'}	Submission history From: Shuming Ma [ view email ] [v1] Tue, 27 Feb 2024 18:56:19 UTC (201 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.17764'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.17764'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.17764'}]
2024-03-03	Datasets for Large Language Models: A Comprehensive Survey	Computation and Language	https://arxiv.org/abs/2402.18041	Dataset for LLMs	https://x.com/omarsar0/status/1763233452852134001?s=20		2402.18041	['Yang Liu', 'Jiahuan Cao', 'Chongyu Liu', 'Kai Ding', 'Lianwen Jin']	ct:This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at:this https URL.	ges, 21 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2402.18041', 'html': None, 'tex': '/src/2402.18041', 'doi': 'https://doi.org/10.48550/arXiv.2402.18041'}	Submission history From: Yang Liu [ view email ] [v1] Wed, 28 Feb 2024 04:35:51 UTC (5,605 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.18041'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.18041'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.18041'}]
2024-03-03	Empowering Large Language Model Agents through Action Learning	Artificial Intelligence	https://arxiv.org/abs/2402.15809	LearnAct	https://x.com/omarsar0/status/1762533498492010761?s=20		2402.15809	['Haiteng Zhao', 'Chang Ma', 'Guoyin Wang', 'Jing Su', 'Lingpeng Kong', 'Jingjing Xu', 'Zhi-Hong Deng', 'Hongxia Yang']	ct:Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents.	s	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.15809', 'html': None, 'tex': '/src/2402.15809', 'doi': 'https://doi.org/10.48550/arXiv.2402.15809'}	Submission history From: Haiteng Zhao [ view email ] [v1] Sat, 24 Feb 2024 13:13:04 UTC (1,618 KB) [v2] Thu, 8 Aug 2024 07:05:46 UTC (1,431 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.15809'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.15809'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.15809'}]
2024-03-03	EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2402.17485	EMO	https://x.com/_akhaliq/status/1762686465777999932?s=20		2402.17485	['Linrui Tian', 'Qi Wang', 'Bang Zhang', 'Liefeng Bo']	ct:In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2402.17485', 'html': 'https://arxiv.org/html/2402.17485v3', 'tex': '/src/2402.17485', 'doi': 'https://doi.org/10.48550/arXiv.2402.17485'}	Submission history From: Linrui Tian [ view email ] [v1] Tue, 27 Feb 2024 13:10:11 UTC (18,549 KB) [v2] Tue, 6 Aug 2024 12:33:30 UTC (12,396 KB) [v3] Thu, 8 Aug 2024 03:48:38 UTC (21,137 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.17485'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.17485'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.17485'}]
2024-03-03	Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding -- A Survey	Computation and Language	https://arxiv.org/abs/2402.17944	LLMs on Tabular Data	https://x.com/omarsar0/status/1763187964501254492?s=20		2402.17944	['Xi Fang', 'Weijie Xu', 'Fiona Anting Tan', 'Jiani Zhang', 'Ziqing Hu', 'Yanjun Qi', 'Scott Nickleach', 'Diego Socolinsky', 'Srinivasan Sengamedu', 'Christos Faloutsos']	ct:Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.	es, 4 figures, 8 tables	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.17944', 'html': 'https://arxiv.org/html/2402.17944v4', 'tex': '/src/2402.17944', 'doi': 'https://doi.org/10.48550/arXiv.2402.17944'}	Submission history From: Weijie Xu [ view email ] [v1] Tue, 27 Feb 2024 23:59:01 UTC (263 KB) [v2] Fri, 1 Mar 2024 00:14:42 UTC (552 KB) [v3] Mon, 10 Jun 2024 17:41:32 UTC (1,287 KB) [v4] Fri, 21 Jun 2024 19:59:54 UTC (1,293 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.17944'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.17944'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.17944'}]
2024-03-03	PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval	Computation and Language	https://arxiv.org/abs/2402.19273	PlanGPT	https://x.com/omarsar0/status/1763424166890377691?s=20		2402.19273	['He Zhu', 'Wenjia Zhang', 'Nuoxian Huang', 'Boyang Li', 'Luyao Niu', 'Zipei Fan', 'Tianle Lun', 'Yicheng Tao', 'Junyou Su', 'Zhaoya Gong', 'Chenyu Fang', 'Xing Liu']	ct:In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.19273', 'html': 'https://arxiv.org/html/2402.19273v1', 'tex': '/src/2402.19273', 'doi': 'https://doi.org/10.48550/arXiv.2402.19273'}	Submission history From: Junyou Su [ view email ] [v1] Thu, 29 Feb 2024 15:41:20 UTC (4,538 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.19273'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.19273'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.19273'}]
2024-02-25	Large Language Models for Data Annotation and Synthesis: A Survey	Computation and Language	https://arxiv.org/abs/2402.13446	LLMs for Data Annotation	https://x.com/omarsar0/status/1760664562779431367?s=20		2402.13446	['Zhen Tan', 'Dawei Li', 'Song Wang', 'Alimohammad Beigi', 'Bohan Jiang', 'Amrita Bhattacharjee', 'Mansooreh Karami', 'Jundong Li', 'Lu Cheng', 'Huan Liu']	ct:Data annotation and synthesis generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation and synthesis. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation and synthesis. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field.	ed to EMNLP 2024 Main	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.13446', 'html': 'https://arxiv.org/html/2402.13446v3', 'tex': '/src/2402.13446', 'doi': 'https://doi.org/10.48550/arXiv.2402.13446'}	Submission history From: Zhen Tan [ view email ] [v1] Wed, 21 Feb 2024 00:44:04 UTC (7,583 KB) [v2] Sun, 23 Jun 2024 21:51:45 UTC (1,347 KB) [v3] Mon, 2 Dec 2024 20:55:15 UTC (1,412 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.13446'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.13446'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.13446'}]
2024-02-25	Generative Representational Instruction Tuning	Computation and Language	https://arxiv.org/abs/2402.09906	GRIT	https://x.com/Muennighoff/status/1758307967802224770?s=20		2402.09906	['Niklas Muennighoff', 'Hongjin Su', 'Liang Wang', 'Nan Yang', 'Furu Wei', 'Tao Yu', 'Amanpreet Singh', 'Douwe Kiela']	ct:All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available atthis https URL.	es (16 main), 25 figures, 34 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.09906', 'html': 'https://arxiv.org/html/2402.09906v3', 'tex': '/src/2402.09906', 'doi': 'https://doi.org/10.48550/arXiv.2402.09906'}	Submission history From: Niklas Muennighoff [ view email ] [v1] Thu, 15 Feb 2024 12:12:19 UTC (573 KB) [v2] Wed, 17 Apr 2024 17:12:05 UTC (574 KB) [v3] Mon, 3 Mar 2025 04:28:49 UTC (653 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.09906'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.09906'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.09906'}]
2024-02-25	LoRA+: Efficient Low Rank Adaptation of Large Models	Machine Learning	https://arxiv.org/abs/2402.12354	LoRA+	https://x.com/omarsar0/status/1760063230406258892?s=20		2402.12354	['Soufiane Hayou', 'Nikhil Ghosh', 'Bin Yu']	ct:In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.	es	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2402.12354', 'html': 'https://arxiv.org/html/2402.12354v2', 'tex': '/src/2402.12354', 'doi': 'https://doi.org/10.48550/arXiv.2402.12354'}	Submission history From: Soufiane Hayou [ view email ] [v1] Mon, 19 Feb 2024 18:33:49 UTC (548 KB) [v2] Thu, 4 Jul 2024 18:33:00 UTC (780 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.12354'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.12354'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.12354'}]
2024-02-25	Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs	Machine Learning	https://arxiv.org/abs/2402.14740	Revisiting REINFORCE in RLHF	https://x.com/sarahookr/status/1761042445997945070?s=20		2402.14740	['Arash Ahmadian', 'Chris Cremer', 'Matthias Gallé', 'Marzieh Fadaee', 'Julia Kreutzer', 'Olivier Pietquin', 'Ahmet Üstün', 'Sara Hooker']	"ct:AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. Proximal Policy Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the formulation of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed ""RL-free"" methods such as DPO and RAFT. Our work suggests that careful adaptation to LLMs alignment characteristics enables benefiting from online RL optimization at low cost."	es, 7 figures, 2 tables	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.14740', 'html': 'https://arxiv.org/html/2402.14740v2', 'tex': '/src/2402.14740', 'doi': 'https://doi.org/10.48550/arXiv.2402.14740'}	Submission history From: Arash Ahmadian [ view email ] [v1] Thu, 22 Feb 2024 17:52:34 UTC (204 KB) [v2] Mon, 26 Feb 2024 18:26:25 UTC (205 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.14740'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.14740'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.14740'}]
2024-02-25	In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss	Computation and Language	https://arxiv.org/abs/2402.10790	Recurrent Memory Finds What LLMs Miss	https://x.com/omarsar0/status/1759591371126571028?s=20		2402.10790	['Yuri Kuratov', 'Aydar Bulatov', 'Petr Anokhin', 'Dmitry Sorokin', 'Artyom Sorokin', 'Mikhail Burtsev']	ct:This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $11\times 10^6$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.	kens, fix qa3 min facts per task in Table 1	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.10790', 'html': 'https://arxiv.org/html/2402.10790v2', 'tex': '/src/2402.10790', 'doi': 'https://doi.org/10.48550/arXiv.2402.10790'}	Submission history From: Yuri Kuratov [ view email ] [v1] Fri, 16 Feb 2024 16:15:01 UTC (6,329 KB) [v2] Wed, 21 Feb 2024 03:07:42 UTC (7,140 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.10790'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.10790'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.10790'}]
2024-02-25	When is Tree Search Useful for LLM Planning? It Depends on the Discriminator	Computation and Language	https://arxiv.org/abs/2402.10890	When is Tree Search Useful for LLM Planning	https://x.com/ysu_nlp/status/1759757711061704913?s=20		2402.10890	['Ziru Chen', 'Michael White', 'Raymond Mooney', 'Ali Payani', 'Yu Su', 'Huan Sun']	ct:In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data are available atthis https URL.	24 main	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.10890', 'html': 'https://arxiv.org/html/2402.10890v2', 'tex': '/src/2402.10890', 'doi': 'https://doi.org/10.48550/arXiv.2402.10890'}	Submission history From: Ziru Chen [ view email ] [v1] Fri, 16 Feb 2024 18:45:58 UTC (2,011 KB) [v2] Thu, 6 Jun 2024 14:55:40 UTC (1,630 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.10890'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.10890'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.10890'}]
2024-02-25	Chain-of-Thought Reasoning Without Prompting	Computation and Language	https://arxiv.org/abs/2402.10200	CoT Reasoning without Prompting	https://x.com/omarsar0/status/1758566808213234017?s=20		2402.10200	['Xuezhi Wang', 'Denny Zhou']	ct:In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding effectively elicits reasoning capabilities from language models, which were previously obscured by standard greedy decoding.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.10200', 'html': 'https://arxiv.org/html/2402.10200v2', 'tex': '/src/2402.10200', 'doi': 'https://doi.org/10.48550/arXiv.2402.10200'}	Submission history From: Xuezhi Wang [ view email ] [v1] Thu, 15 Feb 2024 18:55:41 UTC (752 KB) [v2] Thu, 23 May 2024 20:53:59 UTC (900 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.10200'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.10200'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.10200'}]
2024-02-25	OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement	Software Engineering	https://arxiv.org/abs/2402.14658	OpenCodeInterpreter	https://x.com/xiangyue96/status/1760891516107862104?s=20		2402.14658	['Tianyu Zheng', 'Ge Zhang', 'Tianhao Shen', 'Xueling Liu', 'Bill Yuchen Lin', 'Jie Fu', 'Wenhu Chen', 'Xiang Yue']	ct:The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.		['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.14658', 'html': 'https://arxiv.org/html/2402.14658v3', 'tex': '/src/2402.14658', 'doi': 'https://doi.org/10.48550/arXiv.2402.14658'}	Submission history From: Jie Fu [ view email ] [v1] Thu, 22 Feb 2024 16:06:23 UTC (2,930 KB) [v2] Wed, 28 Feb 2024 03:15:24 UTC (2,930 KB) [v3] Tue, 7 Jan 2025 05:37:04 UTC (2,932 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.14658'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.14658'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.14658'}]
2024-02-18	World Model on Million-Length Video And Language With Blockwise RingAttention	Machine Learning	https://arxiv.org/abs/2402.08268	Large World Model	https://x.com/haoliuhl/status/1757828392362389999?s=20		2402.08268	['Hao Liu', 'Wilson Yan', 'Matei Zaharia', 'Pieter Abbeel']	ct:Enabling long-context understanding remains a key challenge in scaling existing sequence models -- a crucial component in developing generally intelligent models that can process and operate over long temporal horizons that potentially consist of millions of tokens. In this paper, we aim to address these challenges by providing a comprehensive exploration of the full development process for producing 1M context language models and video-language models, setting new benchmarks in language retrieval and new capabilities in long video understanding. We detail our long context data curation process, progressive context extension from 4K to 1M tokens, and present an efficient open-source implementation for scalable training on long sequences. Additionally, we open-source a family of 7B parameter models capable of processing long text documents and videos exceeding 1M tokens.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.08268', 'html': 'https://arxiv.org/html/2402.08268v4', 'tex': '/src/2402.08268', 'doi': 'https://doi.org/10.48550/arXiv.2402.08268'}	Submission history From: Hao Liu [ view email ] [v1] Tue, 13 Feb 2024 07:47:36 UTC (7,336 KB) [v2] Thu, 14 Mar 2024 07:17:03 UTC (7,320 KB) [v3] Tue, 23 Jul 2024 16:57:26 UTC (7,321 KB) [v4] Mon, 3 Feb 2025 21:47:31 UTC (7,670 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.08268'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.08268'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.08268'}]
2024-02-18	The boundary of neural network trainability is fractal	Machine Learning	https://arxiv.org/abs/2402.06184	The boundary of neural network trainability is fractal	https://x.com/jaschasd/status/1756930242965606582?s=20		2402.06184	['Jascha Sohl-Dickstein']	ct:Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.	s, mesmerizing fractals	['Machine Learning (cs.LG)', 'Neural and Evolutionary Computing (cs.NE)', 'Chaotic Dynamics (nlin.CD)']	{'pdf': '/pdf/2402.06184', 'html': 'https://arxiv.org/html/2402.06184v1', 'tex': '/src/2402.06184', 'doi': 'https://doi.org/10.48550/arXiv.2402.06184'}	Submission history From: Jascha Sohl-Dickstein [ view email ] [v1] Fri, 9 Feb 2024 04:46:48 UTC (36,948 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.06184'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.06184'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.06184'}]
2024-02-18	OS-Copilot: Towards Generalist Computer Agents with Self-Improvement	Artificial Intelligence	https://arxiv.org/abs/2402.07456	OS-Copilot	https://x.com/omarsar0/status/1757443594976206885?s=20		2402.07456	['Zhiyong Wu', 'Chengcheng Han', 'Zichen Ding', 'Zhenmin Weng', 'Zhoumianze Liu', 'Shunyu Yao', 'Tao Yu', 'Lingpeng Kong']	ct:Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents.	t page:this https URL	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2402.07456', 'html': None, 'tex': '/src/2402.07456', 'doi': 'https://doi.org/10.48550/arXiv.2402.07456'}	Submission history From: Chengcheng Han [ view email ] [v1] Mon, 12 Feb 2024 07:29:22 UTC (2,761 KB) [v2] Thu, 15 Feb 2024 09:30:48 UTC (2,761 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.07456'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.07456'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.07456'}]
2024-02-18	Automated Unit Test Improvement using Large Language Models at Meta	Software Engineering	https://arxiv.org/abs/2402.09171	TestGen-LLM	https://x.com/nathanbenaich/status/1758036247115608317?s=20		2402.09171	['Nadia Alshahwan', 'Jubin Chheda', 'Anastasia Finegenova', 'Beliz Gokkaya', 'Mark Harman', 'Inna Harper', 'Alexandru Marginean', 'Shubho Sengupta', 'Eddy Wang']	ct:This paper describes Meta's TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM's test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta's Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.	es, 8 figures, 32nd ACM Symposium on the Foundations of Software Engineering (FSE 24)	['Software Engineering (cs.SE)']	{'pdf': '/pdf/2402.09171', 'html': None, 'tex': '/src/2402.09171', 'doi': 'https://doi.org/10.48550/arXiv.2402.09171'}	Submission history From: Alexandru Marginean [ view email ] [v1] Wed, 14 Feb 2024 13:43:14 UTC (1,490 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.09171'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.09171'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.09171'}]
2024-02-18	ChemLLM: A Chemical Large Language Model	Artificial Intelligence	https://arxiv.org/abs/2402.06852	ChemLLM	https://x.com/omarsar0/status/1757246740539773165?s=20		2402.06852	['Di Zhang', 'Wei Liu', 'Qian Tan', 'Jingdan Chen', 'Hang Yan', 'Yuliang Yan', 'Jiatong Li', 'Weiran Huang', 'Xiangyu Yue', 'Wanli Ouyang', 'Dongzhan Zhou', 'Shufei Zhang', 'Mao Su', 'Han-Sen Zhong', 'Yuqiang Li']	ct:Large language models (LLMs) have made impressive progress in chemistry applications. However, the community lacks an LLM specifically designed for chemistry. The main challenges are two-fold: firstly, most chemical data and scientific knowledge are stored in structured databases, which limits the model's ability to sustain coherent dialogue when used directly. Secondly, there is an absence of objective and fair benchmark that encompass most chemistry tasks. Here, we introduce ChemLLM, a comprehensive framework that features the first LLM dedicated to chemistry. It also includes ChemData, a dataset specifically designed for instruction tuning, and ChemBench, a robust benchmark covering nine essential chemistry tasks. ChemLLM is adept at performing various tasks across chemical disciplines with fluid dialogue interaction. Notably, ChemLLM achieves results comparable to GPT-4 on the core chemical tasks and demonstrates competitive performance with LLMs of similar size in general scenarios. ChemLLM paves a new path for exploration in chemical studies, and our method of incorporating structured chemical knowledge into dialogue systems sets a new standard for developing LLMs in various scientific fields. Codes, Datasets, and Model weights are publicly accessible atthis https URL	s, 5 figures	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.06852', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2402.06852'}	Submission history From: Di Zhang [ view email ] [v1] Sat, 10 Feb 2024 01:11:59 UTC (6,527 KB) [v2] Thu, 25 Apr 2024 14:34:28 UTC (1,405 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.06852'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.06852'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.06852'}]
2024-02-18	Large Language Models: A Survey	Computation and Language	https://arxiv.org/abs/2402.06196	Survey of LLMs	https://x.com/omarsar0/status/1757049645119799804?s=20		2402.06196	['Shervin Minaee', 'Tomas Mikolov', 'Narjes Nikzad', 'Meysam Chenaghlu', 'Richard Socher', 'Xavier Amatriain', 'Jianfeng Gao']	ct:Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2402.06196', 'html': 'https://arxiv.org/html/2402.06196v3', 'tex': '/src/2402.06196', 'doi': 'https://doi.org/10.48550/arXiv.2402.06196'}	Submission history From: Shervin Minaee [ view email ] [v1] Fri, 9 Feb 2024 05:37:09 UTC (7,242 KB) [v2] Tue, 20 Feb 2024 13:33:49 UTC (7,243 KB) [v3] Sun, 23 Mar 2025 14:51:01 UTC (7,243 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.06196'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.06196'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.06196'}]
2024-02-18	LLM Agents can Autonomously Hack Websites	Cryptography and Security	https://arxiv.org/abs/2402.06664v1	LLM Agents can Hack	https://x.com/emollick/status/1757937829340967240?s=20		2402.06664v1	['Richard Fang', 'Rohan Bindu', 'Akul Gupta', 'Qiusi Zhan', 'Daniel Kang']	ct:In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.		['Cryptography and Security (cs.CR)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2402.06664v1', 'html': 'https://arxiv.org/html/2402.06664v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2402.06664'}	Submission history From: Daniel Kang [ view email ] [v1] Tue, 6 Feb 2024 14:46:08 UTC (464 KB) [v2] Thu, 15 Feb 2024 01:20:07 UTC (464 KB) [v3] Fri, 16 Feb 2024 04:02:51 UTC (464 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.06664'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.06664'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.06664'}]
2024-02-11	Amortized Planning with Large-Scale Transformers: A Case Study on Chess	Machine Learning	https://arxiv.org/abs/2402.04494	Grandmaster-Level Chess Without Search	https://x.com/_akhaliq/status/1755466387798020229?s=20		2402.04494	['Anian Ruoss', 'Grégoire Delétang', 'Sourabh Medapati', 'Jordi Grau-Moya', 'Li Kevin Wenliang', 'Elliot Catt', 'John Reid', 'Cannada A. Lewis', 'Joel Veness', 'Tim Genewein']	ct:This paper uses chess, a landmark planning problem in AI, to assess transformers' performance on a planning task where memorization is futile $\unicode{x2013}$ even at a large scale. To this end, we release ChessBench, a large-scale benchmark dataset of 10 million chess games with legal move and value annotations (15 billion data points) provided by Stockfish 16, the state-of-the-art chess engine. We train transformers with up to 270 million parameters on ChessBench via supervised learning and perform extensive ablations to assess the impact of dataset size, model size, architecture type, and different prediction targets (state-values, action-values, and behavioral cloning). Our largest models learn to predict action-values for novel boards quite accurately, implying highly non-trivial generalization. Despite performing no explicit search, our resulting chess policy solves challenging chess puzzles and achieves a surprisingly strong Lichess blitz Elo of 2895 against humans (grandmaster level). We also compare to Leela Chess Zero and AlphaZero (trained without supervision via self-play) with and without search. We show that, although a remarkably good approximation of Stockfish's search-based algorithm can be distilled into large-scale transformers via supervised learning, perfect distillation is still beyond reach, thus making ChessBench well-suited for future research.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2402.04494', 'html': 'https://arxiv.org/html/2402.04494v2', 'tex': '/src/2402.04494', 'doi': 'https://doi.org/10.48550/arXiv.2402.04494'}	Submission history From: Anian Ruoss [ view email ] [v1] Wed, 7 Feb 2024 00:36:24 UTC (2,737 KB) [v2] Mon, 21 Oct 2024 09:37:12 UTC (2,708 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.04494'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.04494'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.04494'}]
2024-02-11	AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls	Computation and Language	https://arxiv.org/abs/2402.04253	AnyTool	https://x.com/omarsar0/status/1755065033791283601?s=20		2402.04253	['Yu Du', 'Fangyun Wei', 'Hongyang Zhang']	ct:We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available atthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.04253', 'html': None, 'tex': '/src/2402.04253', 'doi': 'https://doi.org/10.48550/arXiv.2402.04253'}	Submission history From: Fangyun Wei [ view email ] [v1] Tue, 6 Feb 2024 18:59:57 UTC (751 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.04253'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.04253'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.04253'}]
2024-02-11	A phase transition between positional and semantic learning in a solvable model of dot-product attention	Machine Learning	https://arxiv.org/abs/2402.03902	A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention	https://x.com/zdeborova/status/1755158457785704771?s=20		2402.03902	['Hugo Cui', 'Freya Behrens', 'Florent Krzakala', 'Lenka Zdeborová']	ct:Many empirical studies have provided evidence for the emergence of algorithmic mechanisms (abilities) in the learning of language models, that lead to qualitative improvements of the model capabilities. Yet, a theoretical characterization of how such mechanisms emerge remains elusive. In this paper, we take a step in this direction by providing a tight theoretical analysis of the emergence of semantic attention in a solvable model of dot-product attention. More precisely, we consider a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples we provide a tight closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional attention mechanism (with tokens attending to each other based on their respective positions) or a semantic attention mechanism (with tokens attending to each other based on their meaning), and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product attention layer to a linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.03902', 'html': 'https://arxiv.org/html/2402.03902v2', 'tex': '/src/2402.03902', 'doi': 'https://doi.org/10.48550/arXiv.2402.03902'}	Submission history From: Hugo Cui [ view email ] [v1] Tue, 6 Feb 2024 11:13:54 UTC (910 KB) [v2] Tue, 15 Oct 2024 19:54:06 UTC (1,038 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.03902'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.03902'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.03902'}]
2024-02-11	Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning	Computation and Language	https://arxiv.org/abs/2402.03667	Indirect Reasoning with LLMs	https://x.com/omarsar0/status/1755254627866419707?s=20		2402.03667	['Yanfang Zhang', 'Yiliu Sun', 'Yibing Zhan', 'Dapeng Tao', 'Dacheng Tao', 'Chen Gong']	ct:Recently, increasing attention has been focused on improving the ability of Large Language Models (LLMs) to perform complex reasoning. Advanced methods, such as Chain-of-Thought (CoT) and its variants, are found to enhance their reasoning skills by designing suitable prompts or breaking down complex problems into more manageable sub-problems. However, little concentration has been put on exploring the reasoning process, \textit{i.e.}, we discovered that most methods resort to Direct Reasoning (DR) and disregard Indirect Reasoning (IR). This can make LLMs difficult to solve IR tasks, which are often encountered in the real world. To address this issue, we propose a Direct-Indirect Reasoning (DIR) method, which considers DR and IR as multiple parallel reasoning paths that are merged to derive the final answer. We stimulate LLMs to implement IR by crafting prompt templates incorporating the principles of contrapositive and contradiction. These templates trigger LLMs to assume the negation of the conclusion as true, combine it with the premises to deduce a conclusion, and utilize the logical equivalence of the contrapositive to enhance their comprehension of the rules used in the reasoning process. Our DIR method is simple yet effective and can be straightforwardly integrated with existing variants of CoT methods. Experimental results on four datasets related to logical reasoning and mathematic proof demonstrate that our DIR method, when combined with various baseline methods, significantly outperforms all the original methods.	ed by COLING 2025 conference	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2402.03667', 'html': 'https://arxiv.org/html/2402.03667v2', 'tex': '/src/2402.03667', 'doi': 'https://doi.org/10.48550/arXiv.2402.03667'}	Submission history From: Yanfang Zhang [ view email ] [v1] Tue, 6 Feb 2024 03:41:12 UTC (1,842 KB) [v2] Mon, 27 Jan 2025 09:02:46 UTC (1,533 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.03667'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.03667'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.03667'}]
2024-02-11	More Agents Is All You Need	Computation and Language	https://arxiv.org/abs/2402.05120	More Agents is All You Need	https://x.com/omarsar0/status/1755794341069455376?s=20		2402.05120	['Junyou Li', 'Qin Zhang', 'Yangbin Yu', 'Qiang Fu', 'Deheng Ye']	ct:We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method, termed as Agent Forest, is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at:this https URL	hed at Transactions on Machine Learning Research (TMLR)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.05120', 'html': 'https://arxiv.org/html/2402.05120v2', 'tex': '/src/2402.05120', 'doi': 'https://doi.org/10.48550/arXiv.2402.05120'}	Submission history From: Deheng Ye [ view email ] [v1] Sat, 3 Feb 2024 05:55:24 UTC (2,521 KB) [v2] Fri, 11 Oct 2024 09:38:40 UTC (2,580 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.05120'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.05120'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.05120'}]
2024-02-11	Self-Discover: Large Language Models Self-Compose Reasoning Structures	Artificial Intelligence	https://arxiv.org/abs/2402.03620	Self-Discovered Reasoning Structures	https://x.com/peizNLP/status/1755265197953146997?s=20		2402.03620	['Pei Zhou', 'Jay Pujara', 'Xiang Ren', 'Xinyun Chen', 'Heng-Tze Cheng', 'Quoc V. Le', 'Ed H. Chi', 'Denny Zhou', 'Swaroop Mishra', 'Huaixiu Steven Zheng']	ct:We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.	es, 11 figures, 5 tables	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.03620', 'html': 'https://arxiv.org/html/2402.03620v1', 'tex': '/src/2402.03620', 'doi': 'https://doi.org/10.48550/arXiv.2402.03620'}	Submission history From: Pei Zhou [ view email ] [v1] Tue, 6 Feb 2024 01:13:53 UTC (1,783 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.03620'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.03620'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.03620'}]
2024-02-11	DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models	Computation and Language	https://arxiv.org/abs/2402.03300	DeepSeekMath	https://x.com/deepseek_ai/status/1754701472363958581?s=20		2402.03300	['Zhihong Shao', 'Peiyi Wang', 'Qihao Zhu', 'Runxin Xu', 'Junxiao Song', 'Xiao Bi', 'Haowei Zhang', 'Mingchuan Zhang', 'Y.K. Li', 'Y. Wu', 'Daya Guo']	ct:Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.03300', 'html': 'https://arxiv.org/html/2402.03300v3', 'tex': '/src/2402.03300', 'doi': 'https://doi.org/10.48550/arXiv.2402.03300'}	Submission history From: Zhihong Shao [ view email ] [v1] Mon, 5 Feb 2024 18:55:32 UTC (3,417 KB) [v2] Tue, 6 Feb 2024 18:39:38 UTC (3,417 KB) [v3] Sat, 27 Apr 2024 15:25:53 UTC (3,417 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.03300'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.03300'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.03300'}]
2024-02-11	Large Language Model for Table Processing: A Survey	Artificial Intelligence	https://arxiv.org/abs/2402.05121	LLMs for Table Processing	https://x.com/omarsar0/status/1755789530710339788?s=20		2402.05121	['Weizheng Lu', 'Jing Zhang', 'Ju Fan', 'Zihao Fu', 'Yueguo Chen', 'Xiaoyong Du']	ct:Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.05121', 'html': 'https://arxiv.org/html/2402.05121v3', 'tex': '/src/2402.05121', 'doi': 'https://doi.org/10.48550/arXiv.2402.05121'}	Submission history From: Weizheng Lu [ view email ] [v1] Sun, 4 Feb 2024 00:47:53 UTC (110 KB) [v2] Fri, 26 Jul 2024 14:12:33 UTC (1,454 KB) [v3] Thu, 24 Oct 2024 07:26:36 UTC (1,844 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.05121'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.05121'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.05121'}]
2024-02-11	Large Language Model based Multi-Agents: A Survey of Progress and Challenges	Computation and Language	https://arxiv.org/abs/2402.01680	LLM-based Multi-Agents	https://x.com/omarsar0/status/1754710117734375429?s=20		2402.01680	['Taicheng Guo', 'Xiuying Chen', 'Yaqi Wang', 'Ruidi Chang', 'Shichao Pei', 'Nitesh V. Chawla', 'Olaf Wiest', 'Xiangliang Zhang']	ct:Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.	ork is ongoing and we welcome your contribution!	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2402.01680', 'html': 'https://arxiv.org/html/2402.01680v2', 'tex': '/src/2402.01680', 'doi': 'https://doi.org/10.48550/arXiv.2402.01680'}	Submission history From: Taicheng Guo [ view email ] [v1] Sun, 21 Jan 2024 23:36:14 UTC (5,000 KB) [v2] Fri, 19 Apr 2024 01:15:16 UTC (5,001 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.01680'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.01680'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.01680'}]
2024-02-04	OLMo: Accelerating the Science of Language Models	Computation and Language	https://arxiv.org/abs/2402.00838	OLMo	https://x.com/omarsar0/status/1753080417530318872?s=20		2402.00838	['Dirk Groeneveld', 'Iz Beltagy', 'Pete Walsh', 'Akshita Bhagia', 'Rodney Kinney', 'Oyvind Tafjord', 'Ananya Harsh Jha', 'Hamish Ivison', 'Ian Magnusson', 'Yizhong Wang', 'Shane Arora', 'David Atkinson', 'Russell Authur', 'Khyathi Raghavi Chandu', 'Arman Cohan', 'Jennifer Dumas', 'Yanai Elazar', 'Yuling Gu', 'Jack Hessel', 'Tushar Khot', 'William Merrill', 'Jacob Morrison', 'Niklas Muennighoff', 'Aakanksha Naik', 'Crystal Nam', 'Matthew E. Peters', 'Valentina Pyatkin', 'Abhilasha Ravichander', 'Dustin Schwenk', 'Saurabh Shah', 'Will Smith', 'Emma Strubell', 'Nishant Subramani', 'Mitchell Wortsman', 'Pradeep Dasigi', 'Nathan Lambert', 'Kyle Richardson', 'Luke Zettlemoyer', 'Jesse Dodge', 'Kyle Lo', 'Luca Soldaini', 'Noah A. Smith', 'Hannaneh Hajishirzi']	ct:Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.00838', 'html': 'https://arxiv.org/html/2402.00838v4', 'tex': '/src/2402.00838', 'doi': 'https://doi.org/10.48550/arXiv.2402.00838'}	Submission history From: Dirk Groeneveld [ view email ] [v1] Thu, 1 Feb 2024 18:28:55 UTC (297 KB) [v2] Wed, 7 Feb 2024 18:53:02 UTC (323 KB) [v3] Wed, 28 Feb 2024 02:26:07 UTC (738 KB) [v4] Fri, 7 Jun 2024 21:59:52 UTC (754 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.00838'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.00838'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.00838'}]
2024-02-04	MM-LLMs: Recent Advances in MultiModal Large Language Models	Computation and Language	https://arxiv.org/abs/2401.13601	Advances in Multimodal LLMs	https://x.com/omarsar0/status/1751705689964089616?s=20		2401.13601	['Duzhen Zhang', 'Yahan Yu', 'Jiahua Dong', 'Chenxing Li', 'Dan Su', 'Chenhui Chu', 'Dong Yu']	ct:In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.	ed by ACL2024 (findings)	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.13601', 'html': 'https://arxiv.org/html/2401.13601v5', 'tex': '/src/2401.13601', 'doi': 'https://doi.org/10.48550/arXiv.2401.13601'}	Submission history From: Duzhen Zhang [ view email ] [v1] Wed, 24 Jan 2024 17:10:45 UTC (5,256 KB) [v2] Thu, 25 Jan 2024 03:46:15 UTC (5,256 KB) [v3] Sat, 17 Feb 2024 09:17:55 UTC (2,432 KB) [v4] Tue, 20 Feb 2024 09:51:37 UTC (2,432 KB) [v5] Tue, 28 May 2024 05:36:23 UTC (1,056 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.13601'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.13601'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.13601'}]
2024-02-04	Corrective Retrieval Augmented Generation	Computation and Language	https://arxiv.org/abs/2401.15884	Corrective RAG	https://x.com/omarsar0/status/1752173216942944556?s=20		2401.15884	['Shi-Qi Yan', 'Jia-Chen Gu', 'Yun Zhu', 'Zhen-Hua Ling']	ct:Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.	 results, add more analysis, and fix typos	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.15884', 'html': 'https://arxiv.org/html/2401.15884v3', 'tex': '/src/2401.15884', 'doi': 'https://doi.org/10.48550/arXiv.2401.15884'}	Submission history From: Jia-Chen Gu [ view email ] [v1] Mon, 29 Jan 2024 04:36:39 UTC (315 KB) [v2] Fri, 16 Feb 2024 19:10:36 UTC (319 KB) [v3] Mon, 7 Oct 2024 02:19:21 UTC (322 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.15884'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.15884'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.15884'}]
2024-02-04	Large Language Models for Mathematical Reasoning: Progresses and Challenges	Computation and Language	https://arxiv.org/abs/2402.00157	LLMs for Mathematical Reasoning	https://x.com/omarsar0/status/1753424518171738194?s=20		2402.00157	['Janice Ahn', 'Rishu Verma', 'Renze Lou', 'Di Liu', 'Rui Zhang', 'Wenpeng Yin']	ct:Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain. To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.	024 Student Research Workshop, 8 pages	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2402.00157', 'html': 'https://arxiv.org/html/2402.00157v4', 'tex': '/src/2402.00157', 'doi': 'https://doi.org/10.48550/arXiv.2402.00157'}	Submission history From: Jihyun Ahn [ view email ] [v1] Wed, 31 Jan 2024 20:26:32 UTC (6,944 KB) [v2] Sat, 23 Mar 2024 15:45:57 UTC (6,944 KB) [v3] Fri, 5 Apr 2024 04:06:51 UTC (6,945 KB) [v4] Mon, 16 Sep 2024 19:20:59 UTC (6,945 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.00157'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.00157'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.00157'}]
2024-02-04	A Comprehensive Survey of Compression Algorithms for Language Models	Computation and Language	https://arxiv.org/abs/2401.15347	Compression Algorithms for LLMs	https://x.com/omarsar0/status/1752746770377974072?s=20		2401.15347	['Seungcheol Park', 'Jaehyeon Choi', 'Sojin Lee', 'U Kang']	ct:How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2401.15347', 'html': 'https://arxiv.org/html/2401.15347v1', 'tex': '/src/2401.15347', 'doi': 'https://doi.org/10.48550/arXiv.2401.15347'}	Submission history From: Seungcheol Park [ view email ] [v1] Sat, 27 Jan 2024 08:38:56 UTC (1,084 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.15347'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.15347'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.15347'}]
2024-02-04	MoE-LLaVA: Mixture of Experts for Large Vision-Language Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2401.15947	MoE-LLaVA	https://x.com/LinBin46984/status/1753403875531375003?s=20		2401.15947	['Bin Lin', 'Zhenyu Tang', 'Yang Ye', 'Jinfa Huang', 'Junwu Zhang', 'Yatian Pang', 'Peng Jin', 'Munan Ning', 'Jiebo Luo', 'Li Yuan']	ct:Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs) effectively improves downstream task performances. However, existing scaling methods enable all model parameters to be active for each token in the calculation, which brings massive training and inferring costs. In this work, we propose a simple yet effective training strategy MoE-Tuning for LVLMs. This strategy innovatively addresses the common issue of performance degradation in multi-modal sparsity learning, consequently constructing a sparse model with an outrageous number of parameters but a constant computational cost. Furthermore, we present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments show the significant performance of MoE-LLaVA in a variety of visual understanding and object hallucination benchmarks. Remarkably, with only approximately 3B sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. Code is released atthis https URL.	 author	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2401.15947', 'html': 'https://arxiv.org/html/2401.15947v5', 'tex': '/src/2401.15947', 'doi': 'https://doi.org/10.48550/arXiv.2401.15947'}	Submission history From: Lin Bin [ view email ] [v1] Mon, 29 Jan 2024 08:13:40 UTC (3,357 KB) [v2] Sun, 4 Feb 2024 05:55:33 UTC (3,358 KB) [v3] Sat, 17 Feb 2024 02:12:29 UTC (3,358 KB) [v4] Sat, 6 Jul 2024 13:46:02 UTC (3,358 KB) [v5] Mon, 23 Dec 2024 08:05:14 UTC (3,366 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.15947'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.15947'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.15947'}]
2024-02-04	Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling	Computation and Language	https://arxiv.org/abs/2401.16380	Rephrasing the Web	https://x.com/pratyushmaini/status/1752337225097076809?s=20		2401.16380	['Pratyush Maini', 'Skyler Seto', 'He Bai', 'David Grangier', 'Yizhe Zhang', 'Navdeep Jaitly']	"ct:Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training ($\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as ""like Wikipedia"" or in ""question-answer format"" to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\sim3x$. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.16380', 'html': 'https://arxiv.org/html/2401.16380v1', 'tex': '/src/2401.16380', 'doi': 'https://doi.org/10.48550/arXiv.2401.16380'}	Submission history From: Pratyush Maini [ view email ] [v1] Mon, 29 Jan 2024 18:19:08 UTC (523 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.16380'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.16380'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.16380'}]
2024-02-04	The Power of Noise: Redefining Retrieval for RAG Systems	Information Retrieval	https://arxiv.org/abs/2401.14887	Redefining Retrieval in RAG	https://x.com/omarsar0/status/1751803310267314509?s=20		2401.14887	['Florin Cuconasu', 'Giovanni Trappolini', 'Federico Siciliano', 'Simone Filice', 'Cesare Campagnano', 'Yoelle Maarek', 'Nicola Tonellotto', 'Fabrizio Silvestri']	ct:Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.		['Information Retrieval (cs.IR)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.14887', 'html': 'https://arxiv.org/html/2401.14887v4', 'tex': '/src/2401.14887', 'doi': 'https://doi.org/10.48550/arXiv.2401.14887'}	Submission history From: Florin Cuconasu [ view email ] [v1] Fri, 26 Jan 2024 14:14:59 UTC (98 KB) [v2] Mon, 29 Jan 2024 18:52:52 UTC (106 KB) [v3] Mon, 12 Feb 2024 22:02:04 UTC (98 KB) [v4] Wed, 1 May 2024 08:15:07 UTC (1,145 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.14887'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.14887'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.14887'}]
2024-02-04	A Survey on Hallucination in Large Vision-Language Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2402.00253	Hallucination in LVLMs	https://x.com/omarsar0/status/1753449211931079101?s=20		2402.00253	['Hanchao Liu', 'Wenyuan Xue', 'Yifei Chen', 'Dapeng Chen', 'Xiutian Zhao', 'Ke Wang', 'Liping Hou', 'Rongjun Li', 'Wei Peng']	ct:Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.		['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2402.00253', 'html': 'https://arxiv.org/html/2402.00253v2', 'tex': '/src/2402.00253', 'doi': 'https://doi.org/10.48550/arXiv.2402.00253'}	Submission history From: Wenyuan Xue [ view email ] [v1] Thu, 1 Feb 2024 00:33:21 UTC (1,279 KB) [v2] Mon, 6 May 2024 01:10:01 UTC (1,275 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2402.00253'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2402.00253'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2402.00253'}]
2024-02-04	SliceGPT: Compress Large Language Models by Deleting Rows and Columns	Machine Learning	https://arxiv.org/abs/2401.15024v1	SliceGPT	https://x.com/_akhaliq/status/1751796334531592496?s=20		2401.15024v1	['Saleh Ashkboos', 'Maximilian L. Croci', 'Marcelo Gennari do Nascimento', 'Torsten Hoefler', 'James Hensman']	ct:Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at:this https URL	es, 8 figures, accepted at ICLR24	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.15024v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2401.15024'}	Submission history From: Saleh Ashkboos [ view email ] [v1] Fri, 26 Jan 2024 17:35:45 UTC (176 KB) [v2] Fri, 9 Feb 2024 17:59:40 UTC (176 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.15024'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.15024'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.15024'}]
2024-01-28	Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2401.10891v1	Depth Anything	https://x.com/_akhaliq/status/1749284669936275463?s=20		2401.10891v1	['Lihe Yang', 'Bingyi Kang', 'Zilong Huang', 'Xiaogang Xu', 'Jiashi Feng', 'Hengshuang Zhao']	ct:This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released atthis https URL.	t page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2401.10891v1', 'html': 'https://arxiv.org/html/2401.10891v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2401.10891'}	Submission history From: Lihe Yang [ view email ] [v1] Fri, 19 Jan 2024 18:59:52 UTC (9,208 KB) [v2] Sun, 7 Apr 2024 06:52:21 UTC (10,525 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.10891'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.10891'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.10891'}]
2024-01-28	Knowledge Fusion of Large Language Models	Computation and Language	https://arxiv.org/abs/2401.10491	Knowledge Fusion of LLMs	https://x.com/omarsar0/status/1749267663900057620?s=20		2401.10491	['Fanqi Wan', 'Xinting Huang', 'Deng Cai', 'Xiaojun Quan', 'Wei Bi', 'Shuming Shi']	ct:While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \url{this https URL}.	ed to ICLR 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.10491', 'html': None, 'tex': '/src/2401.10491', 'doi': 'https://doi.org/10.48550/arXiv.2401.10491'}	Submission history From: Fanqi Wan [ view email ] [v1] Fri, 19 Jan 2024 05:02:46 UTC (306 KB) [v2] Mon, 22 Jan 2024 17:16:37 UTC (306 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.10491'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.10491'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.10491'}]
2024-01-28	MambaByte: Token-free Selective State Space Model	Computation and Language	https://arxiv.org/abs/2401.13660	MambaByte	https://x.com/omarsar0/status/1750366964759859633?s=20		2401.13660	['Junxiong Wang', 'Tushaar Gangavarapu', 'Jing Nathan Yan', 'Alexander M. Rush']	ct:Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.	hed at COLM 2024	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2401.13660', 'html': 'https://arxiv.org/html/2401.13660v3', 'tex': '/src/2401.13660', 'doi': 'https://doi.org/10.48550/arXiv.2401.13660'}	Submission history From: Jing Nathan Yan [ view email ] [v1] Wed, 24 Jan 2024 18:53:53 UTC (767 KB) [v2] Wed, 3 Apr 2024 02:36:27 UTC (1,779 KB) [v3] Fri, 9 Aug 2024 20:18:57 UTC (1,773 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.13660'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.13660'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.13660'}]
2024-01-28	Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2401.13795	Diffuse to Choose	https://x.com/_akhaliq/status/1750737690553692570?s=20		2401.13795	['Mehmet Saygin Seyfioglu', 'Karim Bouyarmane', 'Suren Kumar', 'Amir Tavanaei', 'Ismail B. Tutar']	"ct:As online shopping is growing, the ability for buyers to virtually visualize products in their settings-a phenomenon we define as ""Virtual Try-All""-has become crucial. Recent diffusion models inherently contain a world model, rendering them suitable for this task within an inpainting context. However, traditional image-conditioned diffusion models often fail to capture the fine-grained details of products. In contrast, personalization-driven models such as DreamPaint are good at preserving the item's details but they are not optimized for real-time applications. We present ""Diffuse to Choose,"" a novel diffusion-based image-conditioned inpainting model that efficiently balances fast inference with the retention of high-fidelity details in a given reference item while ensuring accurate semantic manipulations in the given scene content. Our approach is based on incorporating fine-grained features from the reference image directly into the latent feature maps of the main diffusion model, alongside with a perceptual loss to further preserve the reference item's details. We conduct extensive testing on both in-house and publicly available datasets, and show that Diffuse to Choose is superior to existing zero-shot diffusion inpainting methods as well as few-shot diffusion personalization algorithms like DreamPaint."		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2401.13795', 'html': 'https://arxiv.org/html/2401.13795v1', 'tex': '/src/2401.13795', 'doi': 'https://doi.org/10.48550/arXiv.2401.13795'}	Submission history From: Mehmet Saygin Seyfioglu [ view email ] [v1] Wed, 24 Jan 2024 20:25:48 UTC (10,285 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.13795'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.13795'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.13795'}]
2024-01-28	WARM: On the Benefits of Weight Averaged Reward Models	Machine Learning	https://arxiv.org/abs/2401.12187	WARM	https://x.com/ramealexandre/status/1749719471806157304?s=20		2401.12187	['Alexandre Ramé', 'Nino Vieillard', 'Léonard Hussenot', 'Robert Dadashi', 'Geoffrey Cideron', 'Olivier Bachem', 'Johan Ferret']	ct:Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM.	es, 9 figures	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.12187', 'html': None, 'tex': '/src/2401.12187', 'doi': 'https://doi.org/10.48550/arXiv.2401.12187'}	Submission history From: Johan Ferret [ view email ] [v1] Mon, 22 Jan 2024 18:27:08 UTC (2,920 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.12187'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.12187'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.12187'}]
2024-01-28	A Survey of Resource-efficient LLM and Multimodal Foundation Models	Machine Learning	https://arxiv.org/abs/2401.08092v1	Resource-efficient LLMs & Multimodal Models	https://x.com/omarsar0/status/1749208653926654010?s=20		2401.08092v1	['Mengwei Xu', 'Wangsong Yin', 'Dongqi Cai', 'Rongjie Yi', 'Daliang Xu', 'Qipeng Wang', 'Bingyang Wu', 'Yihao Zhao', 'Chen Yang', 'Shihe Wang', 'Qiyang Zhang', 'Zhenyan Lu', 'Li Zhang', 'Shangguang Wang', 'Yuanchun Li', 'Yunxin Liu', 'Xin Jin', 'Xuanzhe Liu']	ct:Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Distributed, Parallel, and Cluster Computing (cs.DC)']	{'pdf': '/pdf/2401.08092v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2401.08092'}	Submission history From: Mengwei Xu [ view email ] [v1] Tue, 16 Jan 2024 03:35:26 UTC (22,912 KB) [v2] Mon, 23 Sep 2024 07:37:34 UTC (30,636 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.08092'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.08092'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.08092'}]
2024-01-28	Red Teaming Visual Language Models	Artificial Intelligence	https://arxiv.org/abs/2401.12915	Red Teaming Visual Language Models	https://x.com/omarsar0/status/1750170361843384790?s=20		2401.12915	['Mukai Li', 'Lei Li', 'Yuwei Yin', 'Masood Ahmed', 'Zhenguang Liu', 'Qi Liu']	ct:VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.	g in progress	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2401.12915', 'html': 'https://arxiv.org/html/2401.12915v1', 'tex': '/src/2401.12915', 'doi': 'https://doi.org/10.48550/arXiv.2401.12915'}	Submission history From: Mukai Li [ view email ] [v1] Tue, 23 Jan 2024 17:07:18 UTC (3,040 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.12915'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.12915'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.12915'}]
2024-01-28	Lumiere: A Space-Time Diffusion Model for Video Generation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2401.12945	Lumiere	https://x.com/GoogleAI/status/1751003814931689487?s=20		2401.12945	['Omer Bar-Tal', 'Hila Chefer', 'Omer Tov', 'Charles Herrmann', 'Roni Paiss', 'Shiran Zada', 'Ariel Ephrat', 'Junhwa Hur', 'Guanghui Liu', 'Amit Raj', 'Yuanzhen Li', 'Michael Rubinstein', 'Tomer Michaeli', 'Oliver Wang', 'Deqing Sun', 'Tali Dekel', 'Inbar Mosseri']	ct:We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.	e:this https URL| Video:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2401.12945', 'html': 'https://arxiv.org/html/2401.12945v2', 'tex': '/src/2401.12945', 'doi': 'https://doi.org/10.48550/arXiv.2401.12945'}	Submission history From: Omer Bar-Tal [ view email ] [v1] Tue, 23 Jan 2024 18:05:25 UTC (25,342 KB) [v2] Mon, 5 Feb 2024 16:36:30 UTC (43,003 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.12945'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.12945'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.12945'}]
2024-01-28	Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads	Machine Learning	https://arxiv.org/abs/2401.10774v1	Medusa	https://x.com/jiayq/status/1749461664393810350?s=20		2401.10774v1	['Tianle Cai', 'Yuhong Li', 'Zhengyang Geng', 'Hongwu Peng', 'Jason D. Lee', 'Deming Chen', 'Tri Dao']	ct:The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.	de for this implementation is available atthis https URL	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.10774v1', 'html': 'https://arxiv.org/html/2401.10774v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2401.10774'}	Submission history From: Tianle Cai [ view email ] [v1] Fri, 19 Jan 2024 15:48:40 UTC (1,632 KB) [v2] Wed, 5 Jun 2024 22:47:53 UTC (3,113 KB) [v3] Fri, 14 Jun 2024 23:32:32 UTC (3,113 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.10774'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.10774'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.10774'}]
2024-01-28	AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents	Computation and Language	https://arxiv.org/abs/2401.13178v1	AgentBoard	https://x.com/ma_chang_nlp/status/1750369056539218082?s=20		2401.13178v1	['Chang Ma', 'Junlei Zhang', 'Zhihao Zhu', 'Cheng Yang', 'Yujiu Yang', 'Yaohui Jin', 'Zhenzhong Lan', 'Lingpeng Kong', 'Junxian He']	ct:Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a significant step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.	nt	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2401.13178v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2401.13178'}	Submission history From: Junxian He [ view email ] [v1] Wed, 24 Jan 2024 01:51:00 UTC (2,581 KB) [v2] Mon, 23 Dec 2024 20:12:48 UTC (3,481 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.13178'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.13178'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.13178'}]
2024-01-21	Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering	Machine Learning	https://arxiv.org/abs/2401.08500	AlphaCodium	https://x.com/itamar_mar/status/1747957348293824676?s=20		2401.08500	['Tal Ridnik', 'Dedy Kredo', 'Itamar Friedman']	ct:Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at:this https URL		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2401.08500', 'html': None, 'tex': '/src/2401.08500', 'doi': 'https://doi.org/10.48550/arXiv.2401.08500'}	Submission history From: Tal Ridnik [ view email ] [v1] Tue, 16 Jan 2024 17:00:36 UTC (358 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.08500'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.08500'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.08500'}]
2024-01-21	RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture	Computation and Language	https://arxiv.org/abs/2401.08406	RAG vs. Finetuning	https://x.com/omarsar0/status/1747676541876596779?s=20		2401.08406	['Angels Balaguer', 'Vinamra Benara', 'Renato Luiz de Freitas Cunha', 'Roberto de M. Estevão Filho', 'Todd Hendry', 'Daniel Holstein', 'Jennifer Marsman', 'Nick Mecklenburg', 'Sara Malvar', 'Leonardo O. Nunes', 'Rafael Padilha', 'Morris Sharp', 'Bruno Silva', 'Swati Sharma', 'Vijay Aski', 'Ranveer Chandra']	ct:There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2401.08406', 'html': None, 'tex': '/src/2401.08406', 'doi': 'https://doi.org/10.48550/arXiv.2401.08406'}	Submission history From: Sara Malvar [ view email ] [v1] Tue, 16 Jan 2024 14:44:47 UTC (1,300 KB) [v2] Wed, 17 Jan 2024 20:03:15 UTC (1,300 KB) [v3] Tue, 30 Jan 2024 13:55:34 UTC (1,300 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.08406'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.08406'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.08406'}]
2024-01-21	Self-Rewarding Language Models	Computation and Language	https://arxiv.org/abs/2401.10020	Self-Rewarding Models	https://x.com/jaseweston/status/1748158323369611577?s=20		2401.10020	['Weizhe Yuan', 'Richard Yuanzhe Pang', 'Kyunghyun Cho', 'Xian Li', 'Sainbayar Sukhbaatar', 'Jing Xu', 'Jason Weston']	ct:We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.	024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2401.10020', 'html': 'https://arxiv.org/html/2401.10020v3', 'tex': '/src/2401.10020', 'doi': 'https://doi.org/10.48550/arXiv.2401.10020'}	Submission history From: Weizhe Yuan [ view email ] [v1] Thu, 18 Jan 2024 14:43:47 UTC (402 KB) [v2] Thu, 8 Feb 2024 10:19:53 UTC (1,048 KB) [v3] Fri, 28 Mar 2025 00:06:51 UTC (1,061 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.10020'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.10020'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.10020'}]
2024-01-21	Tuning Language Models by Proxy	Computation and Language	https://arxiv.org/abs/2401.08565	Tuning Language Models by Proxy	https://x.com/rasbt/status/1748021765790376385?s=20		2401.08565	['Alisa Liu', 'Xiaochuang Han', 'Yizhong Wang', 'Yulia Tsvetkov', 'Yejin Choi', 'Noah A. Smith']	ct:Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the same end as direct tuning, but by accessing only its predictions over the output vocabulary, not its parameters. Our method tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the larger untuned model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. We then demonstrate the generality of proxy-tuning by applying it to domain adaptation on code, and task-specific finetuning on question-answering and math problems. Finally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal adaptation, increasing its knowledge about recent events. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.	024 camera-ready, code available atthis https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.08565', 'html': 'https://arxiv.org/html/2401.08565v4', 'tex': '/src/2401.08565', 'doi': 'https://doi.org/10.48550/arXiv.2401.08565'}	Submission history From: Alisa Liu [ view email ] [v1] Tue, 16 Jan 2024 18:49:55 UTC (6,441 KB) [v2] Mon, 1 Apr 2024 17:33:49 UTC (6,533 KB) [v3] Mon, 15 Apr 2024 17:20:09 UTC (6,543 KB) [v4] Fri, 23 Aug 2024 05:21:44 UTC (6,544 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.08565'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.08565'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.08565'}]
2024-01-21	ReFT: Reasoning with Reinforced Fine-Tuning	Computation and Language	https://arxiv.org/abs/2401.08967	Reasoning with Reinforced Fine-Tuning	https://x.com/_akhaliq/status/1747820246268887199?s=20		2401.08967	['Trung Quoc Luong', 'Xinbo Zhang', 'Zhanming Jie', 'Peng Sun', 'Xiaoran Jin', 'Hang Li']	ct:One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.	24 main conference; adjust with reviewer comments; 13 pages	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.08967', 'html': None, 'tex': '/src/2401.08967', 'doi': 'https://doi.org/10.48550/arXiv.2401.08967'}	Submission history From: Zhanming Jie [ view email ] [v1] Wed, 17 Jan 2024 04:43:21 UTC (3,687 KB) [v2] Thu, 27 Jun 2024 15:29:15 UTC (5,080 KB) [v3] Fri, 13 Dec 2024 04:44:11 UTC (5,080 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.08967'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.08967'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.08967'}]
2024-01-21	Leveraging Large Language Models for NLG Evaluation: Advances and Challenges	Computation and Language	https://arxiv.org/abs/2401.07103	Overview of LLMs for Evaluation	https://x.com/omarsar0/status/1748016227090305167?s=20		2401.07103	['Zhen Li', 'Xiaohan Xu', 'Tao Shen', 'Can Xu', 'Jia-Chen Gu', 'Yuxuan Lai', 'Chongyang Tao', 'Shuai Ma']	ct:In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.	es, 5 figures	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.07103', 'html': 'https://arxiv.org/html/2401.07103v2', 'tex': '/src/2401.07103', 'doi': 'https://doi.org/10.48550/arXiv.2401.07103'}	Submission history From: Chongyang Tao [ view email ] [v1] Sat, 13 Jan 2024 15:59:09 UTC (7,300 KB) [v2] Wed, 12 Jun 2024 08:31:58 UTC (7,303 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.07103'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.07103'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.07103'}]
2024-01-21	Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models	Computation and Language	https://arxiv.org/abs/2401.06102	Patchscopes	https://x.com/ghandeharioun/status/1746946621215003041?s=20		2401.06102	['Asma Ghandeharioun', 'Avi Caciularu', 'Adam Pearce', 'Lucas Dixon', 'Mor Geva']	ct:Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.	024 (to appear)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2401.06102', 'html': None, 'tex': '/src/2401.06102', 'doi': 'https://doi.org/10.48550/arXiv.2401.06102'}	Submission history From: Asma Ghandeharioun [ view email ] [v1] Thu, 11 Jan 2024 18:33:48 UTC (760 KB) [v2] Fri, 12 Jan 2024 17:54:18 UTC (760 KB) [v3] Thu, 30 May 2024 02:52:08 UTC (825 KB) [v4] Thu, 6 Jun 2024 22:59:58 UTC (826 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.06102'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.06102'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.06102'}]
2024-01-21	The Unreasonable Effectiveness of Easy Training Data for Hard Tasks	Computation and Language	https://arxiv.org/abs/2401.06751	The Unreasonable Effectiveness of Easy Training Data for Hard Tasks	https://x.com/peterbhase/status/1747301128683839998?s=20		2401.06751	['Peter Hase', 'Mohit Bansal', 'Peter Clark', 'Sarah Wiegreffe']	ct:How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current pretrained language models often generalize relatively well from easy to hard data, even performing as well as oracle models finetuned on hard data. We demonstrate this kind of easy-to-hard generalization using simple finetuning methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect easy data rather than hard data for finetuning, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied. Our code is available at:this https URL	24. 23 pages, 20 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2401.06751', 'html': 'https://arxiv.org/html/2401.06751v2', 'tex': '/src/2401.06751', 'doi': 'https://doi.org/10.48550/arXiv.2401.06751'}	Submission history From: Peter Hase [ view email ] [v1] Fri, 12 Jan 2024 18:36:29 UTC (1,397 KB) [v2] Wed, 5 Jun 2024 14:10:11 UTC (1,315 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.06751'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.06751'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.06751'}]
2024-01-21	MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts	Machine Learning	https://arxiv.org/abs/2401.04081	MoE-Mamba	https://x.com/arankomatsuzaki/status/1744552215946100969?s=20		2401.04081	['Maciej Pióro', 'Kamil Ciebiera', 'Krystian Król', 'Jan Ludziejewski', 'Michał Krutul', 'Jakub Krajewski', 'Szymon Antoniak', 'Piotr Miłoś', 'Marek Cygan', 'Sebastian Jaszczur']	ct:State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based Large Language Models, including recent state-of-the-art open models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in $2.35\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.04081', 'html': 'https://arxiv.org/html/2401.04081v2', 'tex': '/src/2401.04081', 'doi': 'https://doi.org/10.48550/arXiv.2401.04081'}	Submission history From: Maciej Pióro [ view email ] [v1] Mon, 8 Jan 2024 18:35:07 UTC (3,545 KB) [v2] Mon, 26 Feb 2024 17:04:41 UTC (1,823 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.04081'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.04081'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.04081'}]
2024-01-14	InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2401.05335	InseRF	https://x.com/_akhaliq/status/1745293576794255757?s=20		2401.05335	['Mohamad Shahbazi', 'Liesbeth Claessens', 'Michael Niemeyer', 'Edo Collins', 'Alessio Tonioni', 'Luc Van Gool', 'Federico Tombari']	ct:We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scenes and provide an in-depth analysis of the proposed components. Our experiments with generative insertion of objects in several 3D scenes indicate the effectiveness of our method compared to the existing methods. InseRF is capable of controllable and 3D-consistent object insertion without requiring explicit 3D information as input. Please visit our project page atthis https URL.		['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2401.05335', 'html': 'https://arxiv.org/html/2401.05335v1', 'tex': '/src/2401.05335', 'doi': 'https://doi.org/10.48550/arXiv.2401.05335'}	Submission history From: Mohamad Shahbazi [ view email ] [v1] Wed, 10 Jan 2024 18:59:53 UTC (8,669 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.05335'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.05335'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.05335'}]
2024-01-14	Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training	Cryptography and Security	https://arxiv.org/abs/2401.05566	Sleeper Agents	https://x.com/AnthropicAI/status/1745854907968880970?s=20		2401.05566	['Evan Hubinger', 'Carson Denison', 'Jesse Mu', 'Mike Lambert', 'Meg Tong', 'Monte MacDiarmid', 'Tamera Lanham', 'Daniel M. Ziegler', 'Tim Maxwell', 'Newton Cheng', 'Adam Jermyn', 'Amanda Askell', 'Ansh Radhakrishnan', 'Cem Anil', 'David Duvenaud', 'Deep Ganguli', 'Fazl Barez', 'Jack Clark', 'Kamal Ndousse', 'Kshitij Sachan', 'Michael Sellitto', 'Mrinank Sharma', 'Nova DasSarma', 'Roger Grosse', 'Shauna Kravec', 'Yuntao Bai', 'Zachary Witten', 'Marina Favaro', 'Jan Brauner', 'Holden Karnofsky', 'Paul Christiano', 'Samuel R. Bowman', 'Logan Graham', 'Jared Kaplan', 'Sören Mindermann', 'Ryan Greenblatt', 'Buck Shlegeris', 'Nicholas Schiefer', 'Ethan Perez']	ct:Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.	d to add missing acknowledgements	['Cryptography and Security (cs.CR)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2401.05566', 'html': 'https://arxiv.org/html/2401.05566v3', 'tex': '/src/2401.05566', 'doi': 'https://doi.org/10.48550/arXiv.2401.05566'}	Submission history From: Evan Hubinger [ view email ] [v1] Wed, 10 Jan 2024 22:14:35 UTC (7,362 KB) [v2] Fri, 12 Jan 2024 02:34:39 UTC (7,440 KB) [v3] Wed, 17 Jan 2024 20:26:01 UTC (7,452 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.05566'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.05566'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.05566'}]
2024-01-14	Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM	Computation and Language	https://arxiv.org/abs/2401.02994	Blending Is All You Need	https://x.com/omarsar0/status/1744765981270950343?s=20		2401.02994	['Xiaoding Lu', 'Zongyi Liu', 'Adian Liusie', 'Vyas Raina', 'Vineet Mudupalli', 'Yuwen Zhang', 'William Beauchamp']	"ct:In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed ""blending"", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tested using A/B testing methodologies with a large user base on the Chai research platform over a span of thirty days. The findings underscore the potential of the ""blending"" strategy as a viable approach for enhancing chat AI efficacy without a corresponding surge in computational demands."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2401.02994', 'html': 'https://arxiv.org/html/2401.02994v3', 'tex': '/src/2401.02994', 'doi': 'https://doi.org/10.48550/arXiv.2401.02994'}	Submission history From: Xiaoding Lu [ view email ] [v1] Thu, 4 Jan 2024 07:45:49 UTC (8,622 KB) [v2] Tue, 9 Jan 2024 08:15:42 UTC (8,621 KB) [v3] Tue, 23 Jan 2024 04:43:56 UTC (8,621 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.02994'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.02994'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.02994'}]
2024-01-14	MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2401.04468	MagicVideo-V2	https://x.com/arankomatsuzaki/status/1744918551415443768?s=20		2401.04468	['Weimin Wang', 'Jiawei Liu', 'Zhijie Lin', 'Jiangqiao Yan', 'Shuo Chen', 'Chetwin Low', 'Tuyen Hoang', 'Jie Wu', 'Jun Hao Liew', 'Hanshu Yan', 'Daquan Zhou', 'Jiashi Feng']	ct:The growing demand for high-fidelity video generation from textual descriptions has catalyzed significant research in this field. In this work, we introduce MagicVideo-V2 that integrates the text-to-image model, video motion generator, reference image embedding module and frame interpolation module into an end-to-end video generation pipeline. Benefiting from these architecture designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution video with remarkable fidelity and smoothness. It demonstrates superior performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph, Moon Valley and Stable Video Diffusion model via user evaluation at large scale.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2401.04468', 'html': 'https://arxiv.org/html/2401.04468v1', 'tex': '/src/2401.04468', 'doi': 'https://doi.org/10.48550/arXiv.2401.04468'}	Submission history From: Weimin Wang [ view email ] [v1] Tue, 9 Jan 2024 10:12:52 UTC (41,574 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.04468'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.04468'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.04468'}]
2024-01-14	TrustLLM: Trustworthiness in Large Language Models	Computation and Language	https://arxiv.org/abs/2401.05561	Trustworthiness in LLMs	https://x.com/omarsar0/status/1745645273915736553?s=20		2401.05561	['Yue Huang', 'Lichao Sun', 'Haoran Wang', 'Siyuan Wu', 'Qihui Zhang', 'Yuan Li', 'Chujie Gao', 'Yixin Huang', 'Wenhan Lyu', 'Yixuan Zhang', 'Xiner Li', 'Zhengliang Liu', 'Yixin Liu', 'Yijue Wang', 'Zhikun Zhang', 'Bertie Vidgen', 'Bhavya Kailkhura', 'Caiming Xiong', 'Chaowei Xiao', 'Chunyuan Li', 'Eric Xing', 'Furong Huang', 'Hao Liu', 'Heng Ji', 'Hongyi Wang', 'Huan Zhang', 'Huaxiu Yao', 'Manolis Kellis', 'Marinka Zitnik', 'Meng Jiang', 'Mohit Bansal', 'James Zou', 'Jian Pei', 'Jian Liu', 'Jianfeng Gao', 'Jiawei Han', 'Jieyu Zhao', 'Jiliang Tang', 'Jindong Wang', 'Joaquin Vanschoren', 'John Mitchell', 'Kai Shu', 'Kaidi Xu', 'Kai-Wei Chang', 'Lifang He', 'Lifu Huang', 'Michael Backes', 'Neil Zhenqiang Gong', 'Philip S. Yu', 'Pin-Yu Chen', 'Quanquan Gu', 'Ran Xu', 'Rex Ying', 'Shuiwang Ji', 'Suman Jana', 'Tianlong Chen', 'Tianming Liu', 'Tianyi Zhou', 'William Wang', 'Xiang Li', 'Xiangliang Zhang', 'Xiao Wang', 'Xing Xie', 'Xun Chen', 'Xuyu Wang', 'Yan Liu', 'Yanfang Ye', 'Yinzhi Cao', 'Yong Chen', 'Yue Zhao']	ct:Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.	ork is still under work and we welcome your contribution	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.05561', 'html': 'https://arxiv.org/html/2401.05561v6', 'tex': '/src/2401.05561', 'doi': 'https://doi.org/10.48550/arXiv.2401.05561'}	Submission history From: Yue Huang [ view email ] [v1] Wed, 10 Jan 2024 22:07:21 UTC (1,498 KB) [v2] Sat, 13 Jan 2024 17:57:06 UTC (1,499 KB) [v3] Thu, 25 Jan 2024 17:49:03 UTC (1,500 KB) [v4] Mon, 18 Mar 2024 02:49:05 UTC (1,509 KB) [v5] Mon, 26 Aug 2024 05:31:38 UTC (1,510 KB) [v6] Mon, 30 Sep 2024 10:17:12 UTC (1,510 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.05561'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.05561'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.05561'}]
2024-01-14	Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding	Computation and Language	https://arxiv.org/abs/2401.04398	Prompting LLMs for Table Understanding	https://x.com/omarsar0/status/1745164182205452603?s=20		2401.04398	['Zilong Wang', 'Hao Zhang', 'Chun-Liang Li', 'Julian Martin Eisenschlos', 'Vincent Perot', 'Zifeng Wang', 'Lesly Miculicich', 'Yasuhisa Fujii', 'Jingbo Shang', 'Chen-Yu Lee', 'Tomas Pfister']	ct:Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.	ed to ICLR 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.04398', 'html': 'https://arxiv.org/html/2401.04398v2', 'tex': '/src/2401.04398', 'doi': 'https://doi.org/10.48550/arXiv.2401.04398'}	Submission history From: Zilong Wang [ view email ] [v1] Tue, 9 Jan 2024 07:46:26 UTC (1,035 KB) [v2] Fri, 19 Jan 2024 01:05:05 UTC (1,041 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.04398'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.04398'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.04398'}]
2024-01-14	From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models	Computation and Language	https://arxiv.org/abs/2401.02777	From LLM to Conversational Agents	https://x.com/omarsar0/status/1744400054624846269?s=20		2401.02777	['Na Liu', 'Liangyu Chen', 'Xiaoyu Tian', 'Wei Zou', 'Kaijiang Chen', 'Ming Cui']	ct:This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2401.02777', 'html': None, 'tex': '/src/2401.02777', 'doi': 'https://doi.org/10.48550/arXiv.2401.02777'}	Submission history From: Liangyu Chen [ view email ] [v1] Fri, 5 Jan 2024 12:26:46 UTC (891 KB) [v2] Tue, 30 Jan 2024 07:02:30 UTC (1,980 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.02777'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.02777'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.02777'}]
2024-01-14	Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting	Computation and Language	https://arxiv.org/abs/2310.11324	Quantifying LLM’s Sensitivity to Spurious Features in Prompt Design	https://x.com/melaniesclar/status/1745557109419458695?s=20		2310.11324	['Melanie Sclar', 'Yejin Choi', 'Yulia Tsvetkov', 'Alane Suhr']	ct:As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.	024 Camera Ready version. With respect to the original submission, we added text generation experiments, plots of entire accuracy distributions for each task + stdev computations, and prompt length correlation with spread analysis	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.11324', 'html': 'https://arxiv.org/html/2310.11324v2', 'tex': '/src/2310.11324', 'doi': 'https://doi.org/10.48550/arXiv.2310.11324'}	Submission history From: Melanie Sclar [ view email ] [v1] Tue, 17 Oct 2023 15:03:30 UTC (1,110 KB) [v2] Mon, 1 Jul 2024 22:28:01 UTC (1,221 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.11324'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.11324'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.11324'}]
2024-01-07	A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models	Computation and Language	https://arxiv.org/abs/2401.01313	Mitigating Hallucination in LLMs	https://x.com/omarsar0/status/1742633831234994189?s=20		2401.01313	['S.M Towhidul Islam Tonmoy', 'S M Mehedi Zaman', 'Vinija Jain', 'Anku Rani', 'Vipula Rawte', 'Aman Chadha', 'Amitava Das']	ct:As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.01313', 'html': None, 'tex': '/src/2401.01313', 'doi': 'https://doi.org/10.48550/arXiv.2401.01313'}	Submission history From: Anku Rani [ view email ] [v1] Tue, 2 Jan 2024 17:56:30 UTC (7,724 KB) [v2] Wed, 3 Jan 2024 17:13:00 UTC (7,722 KB) [v3] Mon, 8 Jan 2024 16:19:17 UTC (7,722 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.01313'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.01313'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.01313'}]
2024-01-07	Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models	Machine Learning	https://arxiv.org/abs/2401.01335	Self-Play Fine-tuning	https://x.com/_zxchen_/status/1742661587436216615?s=20		2401.01335	['Zixiang Chen', 'Yihe Deng', 'Huizhuo Yuan', 'Kaixuan Ji', 'Quanquan Gu']	ct:Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents. Codes are available atthis https URL.	es, 6 figures, 7 tables. In ICML 2024	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2401.01335', 'html': 'https://arxiv.org/html/2401.01335v3', 'tex': '/src/2401.01335', 'doi': 'https://doi.org/10.48550/arXiv.2401.01335'}	Submission history From: Zixiang Chen [ view email ] [v1] Tue, 2 Jan 2024 18:53:13 UTC (833 KB) [v2] Mon, 12 Feb 2024 22:22:37 UTC (833 KB) [v3] Fri, 14 Jun 2024 21:17:17 UTC (937 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.01335'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.01335'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.01335'}]
2024-01-07	LLaMA Pro: Progressive LLaMA with Block Expansion	Computation and Language	https://arxiv.org/abs/2401.02415	LLaMA Pro	https://x.com/_akhaliq/status/1743135851238805685?s=20		2401.02415	['Chengyue Wu', 'Yukang Gan', 'Yixiao Ge', 'Zeyu Lu', 'Jiahao Wang', 'Ye Feng', 'Ying Shan', 'Ping Luo']	ct:Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.	ed by ACL 2024, Main Conference	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.02415', 'html': 'https://arxiv.org/html/2401.02415v2', 'tex': '/src/2401.02415', 'doi': 'https://doi.org/10.48550/arXiv.2401.02415'}	Submission history From: Chengyue Wu [ view email ] [v1] Thu, 4 Jan 2024 18:59:12 UTC (3,730 KB) [v2] Thu, 30 May 2024 04:45:34 UTC (3,993 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.02415'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.02415'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.02415'}]
2024-01-07	LLM Augmented LLMs: Expanding Capabilities through Composition	Machine Learning	https://arxiv.org/abs/2401.02412	LLM Augmented LLMs	https://x.com/omarsar0/status/1743094632618106981?s=20		2401.02412	['Rachit Bansal', 'Bidisha Samanta', 'Siddharth Dalmia', 'Nitish Gupta', 'Shikhar Vashishth', 'Sriram Ganapathy', 'Abhishek Bapna', 'Prateek Jain', 'Partha Talukdar']	ct:Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.	es, 2 figures, 8 tables	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2401.02412', 'html': 'https://arxiv.org/html/2401.02412v1', 'tex': '/src/2401.02412', 'doi': 'https://doi.org/10.48550/arXiv.2401.02412'}	Submission history From: Rachit Bansal [ view email ] [v1] Thu, 4 Jan 2024 18:53:01 UTC (246 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.02412'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.02412'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.02412'}]
2024-01-07	Fast Inference of Mixture-of-Experts Language Models with Offloading	Machine Learning	https://arxiv.org/abs/2312.17238	Fast Inference of Mixture-of-Experts	https://x.com/rohanpaul_ai/status/1741044633495326861?s=20		2312.17238	['Artyom Eliseev', 'Denis Mazur']	ct:With the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) - a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their dense counterparts, but it also increases model size due to having multiple experts. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.	cal report	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Distributed, Parallel, and Cluster Computing (cs.DC)']	{'pdf': '/pdf/2312.17238', 'html': 'https://arxiv.org/html/2312.17238v1', 'tex': '/src/2312.17238', 'doi': 'https://doi.org/10.48550/arXiv.2312.17238'}	Submission history From: Denis Mazur [ view email ] [v1] Thu, 28 Dec 2023 18:58:13 UTC (267 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.17238'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.17238'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.17238'}]
2024-01-07	GPT-4V(ision) is a Generalist Web Agent, if Grounded	Information Retrieval	https://arxiv.org/abs/2401.01614	GPT-4V is a Generalist Web Agent	https://x.com/omarsar0/status/1742923330544706035?s=20		2401.01614	['Boyuan Zheng', 'Boyu Gou', 'Jihyung Kil', 'Huan Sun', 'Yu Su']	ct:The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents -- it can successfully complete 51.1 of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out to be not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML structure and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement. All code, data, and evaluation tools are available atthis https URL.		['Information Retrieval (cs.IR)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2401.01614', 'html': 'https://arxiv.org/html/2401.01614v2', 'tex': '/src/2401.01614', 'doi': 'https://doi.org/10.48550/arXiv.2401.01614'}	Submission history From: Boyuan Zheng [ view email ] [v1] Wed, 3 Jan 2024 08:33:09 UTC (26,735 KB) [v2] Tue, 12 Mar 2024 23:14:33 UTC (27,857 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.01614'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.01614'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.01614'}]
2024-01-07	DocLLM: A layout-aware generative language model for multimodal document understanding	Computation and Language	https://arxiv.org/abs/2401.00908	DocLLM	https://x.com/BrianRoemmele/status/1742572753251913742?s=20		2401.00908	['Dongsheng Wang', 'Natraj Raman', 'Mathieu Sibue', 'Zhiqiang Ma', 'Petr Babkin', 'Simerjot Kaur', 'Yulong Pei', 'Armineh Nourbakhsh', 'Xiaomo Liu']	ct:Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.	es, 4 figures	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.00908', 'html': 'https://arxiv.org/html/2401.00908v1', 'tex': '/src/2401.00908', 'doi': 'https://doi.org/10.48550/arXiv.2401.00908'}	Submission history From: Yulong Pei [ view email ] [v1] Sun, 31 Dec 2023 22:37:52 UTC (717 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.00908'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.00908'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.00908'}]
2024-01-07	If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents	Computation and Language	https://arxiv.org/abs/2401.00812	How Code Empowers LLMs	https://x.com/omarsar0/status/1742215295907811613?s=20		2401.00812	['Ke Yang', 'Jiateng Liu', 'John Wu', 'Chaoqi Yang', 'Yi R. Fung', 'Sha Li', 'Zixuan Huang', 'Xu Cao', 'Xingyao Wang', 'Yiquan Wang', 'Heng Ji', 'Chengxiang Zhai']	ct:The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.00812', 'html': None, 'tex': '/src/2401.00812', 'doi': 'https://doi.org/10.48550/arXiv.2401.00812'}	Submission history From: Ke Yang [ view email ] [v1] Mon, 1 Jan 2024 16:51:20 UTC (18,105 KB) [v2] Mon, 8 Jan 2024 16:22:42 UTC (17,745 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.00812'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.00812'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.00812'}]
2024-01-07	Instruct-Imagen: Image Generation with Multi-modal Instruction	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2401.01952	Instruct-Imagen	https://x.com/_akhaliq/status/1743108118630818039?s=20		2401.01952	['Hexiang Hu', 'Kelvin C.K. Chan', 'Yu-Chuan Su', 'Wenhu Chen', 'Yandong Li', 'Kihyuk Sohn', 'Yang Zhao', 'Xue Ben', 'Boqing Gong', 'William Cohen', 'Ming-Wei Chang', 'Xuhui Jia']	ct:This paper presents instruct-imagen, a model that tackles heterogeneous image generation tasks and generalizes across unseen tasks. We introduce *multi-modal instruction* for image generation, a task representation articulating a range of generation intents with precision. It uses natural language to amalgamate disparate modalities (e.g., text, edge, style, subject, etc.), such that abundant generation intents can be standardized in a uniform format.We then build instruct-imagen by fine-tuning a pre-trained text-to-image diffusion model with a two-stage framework. First, we adapt the model using the retrieval-augmented training, to enhance model's capabilities to ground its generation on external multimodal context. Subsequently, we fine-tune the adapted model on diverse image generation tasks that requires vision-language understanding (e.g., subject-driven generation, etc.), each paired with a multi-modal instruction encapsulating the task's essence. Human evaluation on various image generation datasets reveals that instruct-imagen matches or surpasses prior task-specific models in-domain and demonstrates promising generalization to unseen and more complex tasks.	es, 18 figures	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2401.01952', 'html': 'https://arxiv.org/html/2401.01952v1', 'tex': '/src/2401.01952', 'doi': 'https://doi.org/10.48550/arXiv.2401.01952'}	Submission history From: Hexiang Hu [ view email ] [v1] Wed, 3 Jan 2024 19:31:58 UTC (44,412 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2401.01952'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2401.01952'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2401.01952'}]
2023-12-31	CogAgent: A Visual Language Model for GUI Agents	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2312.08914	CogAgent	https://x.com/cenyk1230/status/1739916469272789222?s=20		2312.08914	['Wenyi Hong', 'Weihan Wang', 'Qingsong Lv', 'Jiazheng Xu', 'Wenmeng Yu', 'Junhui Ji', 'Yan Wang', 'Zihan Wang', 'Yuxuan Zhang', 'Juanzi Li', 'Bin Xu', 'Yuxiao Dong', 'Ming Ding', 'Jie Tang']	ct:People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available atthis https URL, with a new version of CogAgent-9B-20241220 available atthis https URL.	024 (Highlight), 27 pages, 19 figures	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2312.08914', 'html': 'https://arxiv.org/html/2312.08914v3', 'tex': '/src/2312.08914', 'doi': 'https://doi.org/10.48550/arXiv.2312.08914'}	Submission history From: Wenyi Hong [ view email ] [v1] Thu, 14 Dec 2023 13:20:57 UTC (11,917 KB) [v2] Thu, 21 Dec 2023 09:41:25 UTC (11,917 KB) [v3] Fri, 27 Dec 2024 06:56:18 UTC (12,587 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.08914'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.08914'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.08914'}]
2023-12-31	From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape	Artificial Intelligence	https://arxiv.org/abs/2312.10868	From Gemini to Q-Star	https://x.com/omarsar0/status/1740119485011390558?s=20		2312.10868	['Timothy R. McIntosh', 'Teo Susnjak', 'Tong Liu', 'Paul Watters', 'Malka N. Halgamuge']	ct:This comprehensive survey explored the evolving landscape of generative Artificial Intelligence (AI), with a specific focus on the transformative impacts of Mixture of Experts (MoE), multimodal learning, and the speculated advancements towards Artificial General Intelligence (AGI). It critically examined the current state and future trajectory of generative Artificial Intelligence (AI), exploring how innovations like Google's Gemini and the anticipated OpenAI Q* project are reshaping research priorities and applications across various domains, including an impact analysis on the generative AI research taxonomy. It assessed the computational challenges, scalability, and real-world implications of these technologies while highlighting their potential in driving significant progress in fields like healthcare, finance, and education. It also addressed the emerging academic challenges posed by the proliferation of both AI-themed and AI-generated preprints, examining their impact on the peer-review process and scholarly communication. The study highlighted the importance of incorporating ethical and human-centric methods in AI development, ensuring alignment with societal norms and welfare, and outlined a strategy for future AI research that focuses on a balanced and conscientious use of MoE, multimodality, and AGI in generative AI.	es	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computers and Society (cs.CY)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2312.10868', 'html': 'https://arxiv.org/html/2312.10868v1', 'tex': '/src/2312.10868', 'doi': 'https://doi.org/10.48550/arXiv.2312.10868'}	Submission history From: Timothy Raymond Mcintosh [ view email ] [v1] Mon, 18 Dec 2023 01:11:39 UTC (145 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.10868'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.10868'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.10868'}]
2023-12-31	PromptBench: A Unified Library for Evaluation of Large Language Models	Artificial Intelligence	https://arxiv.org/abs/2312.07910v1	PromptBench	https://x.com/omarsar0/status/1739360426134028631?s=20		2312.07910v1	['Kaijie Zhu', 'Qinlin Zhao', 'Hao Chen', 'Jindong Wang', 'Xing Xie']	ct:The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at:this https URLand will be continuously supported.	ension to PromptBench (arXiv:2306.04528) for unified evaluation of LLMs using the same name; code:this https URL	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2312.07910v1', 'html': 'https://arxiv.org/html/2312.07910v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2312.07910'}	Submission history From: Jindong Wang [ view email ] [v1] Wed, 13 Dec 2023 05:58:34 UTC (288 KB) [v2] Fri, 5 Jan 2024 14:45:00 UTC (292 KB) [v3] Tue, 20 Aug 2024 00:28:45 UTC (299 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.07910'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.07910'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.07910'}]
2023-12-31	Exploiting Novel GPT-4 APIs	Cryptography and Security	https://arxiv.org/abs/2312.14302	Exploiting Novel GPT-4 APIs	https://x.com/omarsar0/status/1739677995747450964?s=20		2312.14302	['Kellin Pelrine', 'Mohammad Taufeeque', 'Michał Zając', 'Euan McLean', 'Adam Gleave']	"ct:Language model attacks typically assume one of two extreme threat models: full white-box access to model weights, or black-box access limited to a text generation API. However, real-world APIs are often more flexible than just text generation: these APIs expose ""gray-box"" access leading to new threat vectors. To explore this, we red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. These vulnerabilities highlight that any additions to the functionality exposed by an API can create new vulnerabilities."	es, 1 figure, 4 tables	['Cryptography and Security (cs.CR)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2312.14302', 'html': 'https://arxiv.org/html/2312.14302v2', 'tex': '/src/2312.14302', 'doi': 'https://doi.org/10.48550/arXiv.2312.14302'}	Submission history From: Kellin Pelrine [ view email ] [v1] Thu, 21 Dec 2023 21:22:41 UTC (542 KB) [v2] Sun, 4 Aug 2024 17:48:33 UTC (684 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.14302'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.14302'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.14302'}]
2023-12-31	MathPile: A Billion-Token-Scale Pretraining Corpus for Math	Computation and Language	https://arxiv.org/abs/2312.17120	Generative AI for Math	https://x.com/arankomatsuzaki/status/1740564961032556942?s=20		2312.17120	['Zengzhi Wang', 'Xuefeng Li', 'Rui Xia', 'Pengfei Liu']	"ct:High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MathPile, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of ""less is more"", firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MathPile to boost language models' mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field."	es. Accepted by NeurIPS 2024.this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2312.17120', 'html': 'https://arxiv.org/html/2312.17120v2', 'tex': '/src/2312.17120', 'doi': 'https://doi.org/10.48550/arXiv.2312.17120'}	Submission history From: Zengzhi Wang [ view email ] [v1] Thu, 28 Dec 2023 16:55:40 UTC (7,174 KB) [v2] Tue, 29 Oct 2024 17:02:45 UTC (650 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.17120'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.17120'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.17120'}]
2023-12-31	Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4	Computation and Language	https://arxiv.org/abs/2312.16171v1	Pricipled Instructions Are All You Need	https://x.com/_akhaliq/status/1739857456161759455?s=20		2312.16171v1	['Sondos Mahmoud Bsharat', 'Aidar Myrzakhan', 'Zhiqiang Shen']	ct:This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work provides a better guide for researchers working on the prompting of large language models. Project page is available atthis https URL.	 at:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2312.16171v1', 'html': 'https://arxiv.org/html/2312.16171v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2312.16171'}	Submission history From: Zhiqiang Shen [ view email ] [v1] Tue, 26 Dec 2023 18:59:33 UTC (1,127 KB) [v2] Thu, 18 Jan 2024 18:41:09 UTC (1,183 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.16171'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.16171'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.16171'}]
2023-12-31	A Survey of Reasoning with Foundation Models	Artificial Intelligence	https://arxiv.org/abs/2312.11562v4	A Survey of Reasoning with Foundation Models	https://x.com/omarsar0/status/1740729489661874632?s=20		2312.11562v4	['Jiankai Sun', 'Chuanyang Zheng', 'Enze Xie', 'Zhengying Liu', 'Ruihang Chu', 'Jianing Qiu', 'Jiaqi Xu', 'Mingyu Ding', 'Hongyang Li', 'Mengzhe Geng', 'Yue Wu', 'Wenhai Wang', 'Junsong Chen', 'Zhangyue Yin', 'Xiaozhe Ren', 'Jie Fu', 'Junxian He', 'Wu Yuan', 'Qi Liu', 'Xihui Liu', 'Yu Li', 'Hao Dong', 'Yu Cheng', 'Ming Zhang', 'Pheng Ann Heng', 'Jifeng Dai', 'Ping Luo', 'Jingdong Wang', 'Ji-Rong Wen', 'Xipeng Qiu', 'Yike Guo', 'Hui Xiong', 'Qun Liu', 'Zhenguo Li']	ct:Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, and contribute to the development of AGI.	ures, 160 Pages, 750+ References, Project Pagethis https URL	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2312.11562v4', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2312.11562'}	Submission history From: Ruihang Chu [ view email ] [v1] Sun, 17 Dec 2023 15:16:13 UTC (3,868 KB) [v2] Wed, 20 Dec 2023 07:25:58 UTC (3,867 KB) [v3] Thu, 21 Dec 2023 13:21:59 UTC (3,870 KB) [v4] Tue, 26 Dec 2023 11:31:54 UTC (3,872 KB) [v5] Thu, 25 Jan 2024 11:20:16 UTC (3,873 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.11562'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.11562'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.11562'}]
2023-12-31	Making Large Language Models A Better Foundation For Dense Retrieval	Computation and Language	https://arxiv.org/abs/2312.15503v1	Making LLMs Better at Dense Retrieval			2312.15503v1	['Chaofan Li', 'Zheng Liu', 'Shitao Xiao', 'Yingxia Shao']	ct:Dense retrieval needs to learn discriminative text embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs' strong capability on semantic understanding. However, the LLMs are pre-trained by text generation tasks, whose working pattern is completely different from representing texts as embeddings. As a result, it is imperative to study how to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval.In this paper, we propose a novel approach, called LLaRA (LLM adapted for dense RetrievAl), which works as a post-hoc adaptation of LLM for the dense retrieval application. LLaRA consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the text embeddings from LLM are used to reconstruct the tokens for the input sentence and predict the tokens for the next sentence, respectively. LLaRA turns out to be simple, lightweight, and highly effective. It is applied to adapt LLaMA-2-7B (base) on the Wikipedia corpus, where it substantially improves the model's fine-tuned performances on a variety of dense retrieval benchmarks, like MSMARCO and BEIR. Our model and code will be made publicly available at BGE repository.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2312.15503v1', 'html': 'https://arxiv.org/html/2312.15503v1', 'tex': '/src/2312.15503v1', 'doi': 'https://doi.org/10.48550/arXiv.2312.15503'}	Submission history From: Zheng Liu [ view email ] [v1] Sun, 24 Dec 2023 15:10:35 UTC (7,081 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.15503'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.15503'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.15503'}]
2023-12-31	Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2312.15011v1	Gemini vs GPT-4V	https://x.com/omarsar0/status/1741177994377330895?s=20		2312.15011v1	['Zhangyang Qi', 'Ye Fang', 'Mengchen Zhang', 'Zeyi Sun', 'Tong Wu', 'Ziwei Liu', 'Dahua Lin', 'Jiaqi Wang', 'Hengshuang Zhao']	ct:The rapidly evolving sector of Multi-modal Large Language Models (MLLMs) is at the forefront of integrating linguistic and visual processing in artificial intelligence. This paper presents an in-depth comparative study of two pioneering models: Google's Gemini and OpenAI's GPT-4V(ision). Our study involves a multi-faceted evaluation of both models across key dimensions such as Vision-Language Capability, Interaction with Humans, Temporal Understanding, and assessments in both Intelligence and Emotional Quotients. The core of our analysis delves into the distinct visual comprehension abilities of each model. We conducted a series of structured experiments to evaluate their performance in various industrial application scenarios, offering a comprehensive perspective on their practical utility. We not only involve direct performance comparisons but also include adjustments in prompts and scenarios to ensure a balanced and fair analysis. Our findings illuminate the unique strengths and niches of both models. GPT-4V distinguishes itself with its precision and succinctness in responses, while Gemini excels in providing detailed, expansive answers accompanied by relevant imagery and links. These understandings not only shed light on the comparative merits of Gemini and GPT-4V but also underscore the evolving landscape of multimodal foundation models, paving the way for future advancements in this area. After the comparison, we attempted to achieve better results by combining the two models. Finally, We would like to express our profound gratitude to the teams behind GPT-4V and Gemini for their pioneering contributions to the field. Our acknowledgments are also extended to the comprehensive qualitative analysis presented in 'Dawn' by Yang et al. This work, with its extensive collection of image samples, prompts, and GPT-4V-related results, provided a foundational basis for our analysis.	t Page:this https URL. arXiv admin note: substantial text overlap witharXiv:2309.17421	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2312.15011v1', 'html': None, 'tex': '/src/2312.15011v1', 'doi': 'https://doi.org/10.48550/arXiv.2312.15011'}	Submission history From: Zhangyang Qi [ view email ] [v1] Fri, 22 Dec 2023 18:59:58 UTC (32,107 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.15011'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.15011'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.15011'}]
2023-12-24	An In-depth Look at Gemini's Language Abilities	Computation and Language	https://arxiv.org/abs/2312.11444	Gemini’s Language Abilities	https://x.com/gneubig/status/1737108966931673191?s=20		2312.11444	['Syeda Nahida Akter', 'Zichun Yu', 'Aashiq Muhamed', 'Tianyue Ou', 'Alex Bäuerle', 'Ángel Alexander Cabrera', 'Krish Dholakia', 'Chenyan Xiong', 'Graham Neubig']	ct:The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found atthis https URL		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2312.11444', 'html': 'https://arxiv.org/html/2312.11444v2', 'tex': '/src/2312.11444', 'doi': 'https://doi.org/10.48550/arXiv.2312.11444'}	Submission history From: Graham Neubig [ view email ] [v1] Mon, 18 Dec 2023 18:47:42 UTC (3,372 KB) [v2] Sun, 24 Dec 2023 12:25:10 UTC (7,734 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.11444'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.11444'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.11444'}]
2023-12-24	AppAgent: Multimodal Agents as Smartphone Users	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2312.13771	Multimodal Agents as Smartphone Users	https://x.com/omarsar0/status/1738265651188253051?s=20		2312.13771	['Chi Zhang', 'Zhao Yang', 'Jiaxuan Liu', 'Yucheng Han', 'Xin Chen', 'Zebiao Huang', 'Bin Fu', 'Gang Yu']	ct:Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent's functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent's proficiency in handling a diverse array of high-level tasks.	t Page isthis https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2312.13771', 'html': 'https://arxiv.org/html/2312.13771v2', 'tex': '/src/2312.13771', 'doi': 'https://doi.org/10.48550/arXiv.2312.13771'}	Submission history From: Yucheng Han [ view email ] [v1] Thu, 21 Dec 2023 11:52:45 UTC (10,766 KB) [v2] Fri, 22 Dec 2023 02:29:17 UTC (10,766 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.13771'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.13771'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.13771'}]
2023-12-24	LLM in a flash: Efficient Large Language Model Inference with Limited Memory	Computation and Language	https://arxiv.org/abs/2312.11514	LLM in a Flash	https://x.com/gabrielnocode/status/1737307286887133552?s=20		2312.11514	['Keivan Alizadeh', 'Iman Mirzadeh', 'Dmitry Belenko', 'Karen Khatamifard', 'Minsik Cho', 'Carlo C Del Mundo', 'Mohammad Rastegari', 'Mehrdad Farajtabar']	"ct:Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, ""windowing"" strategically reduces data transfer by reusing previously activated neurons, and second, ""row-column bundling"", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory."	24	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2312.11514', 'html': 'https://arxiv.org/html/2312.11514v3', 'tex': '/src/2312.11514', 'doi': 'https://doi.org/10.48550/arXiv.2312.11514'}	Submission history From: Keivan Alizadeh-Vahid [ view email ] [v1] Tue, 12 Dec 2023 18:57:08 UTC (951 KB) [v2] Thu, 4 Jan 2024 22:28:37 UTC (954 KB) [v3] Tue, 30 Jul 2024 23:37:20 UTC (1,780 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.11514'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.11514'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.11514'}]
2023-12-24	ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent	Computation and Language	https://arxiv.org/abs/2312.10003	ReST Meets ReAct	https://x.com/omarsar0/status/1736587397830176910?s=20		2312.10003	['Renat Aksitov', 'Sobhan Miryoosefi', 'Zonglin Li', 'Daliang Li', 'Sheila Babayan', 'Kavya Kopparapu', 'Zachary Fisher', 'Ruiqi Guo', 'Sushant Prakash', 'Pranesh Srinivasan', 'Manzil Zaheer', 'Felix Yu', 'Sanjiv Kumar']	ct:Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.	es, 4 figures, 4 tables, 8 listings	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2312.10003', 'html': 'https://arxiv.org/html/2312.10003v1', 'tex': '/src/2312.10003', 'doi': 'https://doi.org/10.48550/arXiv.2312.10003'}	Submission history From: Renat Aksitov [ view email ] [v1] Fri, 15 Dec 2023 18:20:15 UTC (427 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.10003'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.10003'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.10003'}]
2023-12-24	Retrieval-Augmented Generation for Large Language Models: A Survey	Computation and Language	https://arxiv.org/abs/2312.10997v1	RAG for LLMs	https://x.com/omarsar0/status/1738354427759612222?s=20		2312.10997v1	['Yunfan Gao', 'Yun Xiong', 'Xinyu Gao', 'Kangxiang Jia', 'Jinliu Pan', 'Yuxi Bi', 'Yi Dai', 'Jiawei Sun', 'Haofen Wang']	ct:Large language models (LLMs) demonstrate powerful capabilities, but they still face challenges in practical applications, such as hallucinations, slow knowledge updates, and lack of transparency in answers. Retrieval-Augmented Generation (RAG) refers to the retrieval of relevant information from external knowledge bases before answering questions with LLMs. RAG has been demonstrated to significantly enhance answer accuracy, reduce model hallucination, particularly for knowledge-intensive tasks. By citing sources, users can verify the accuracy of answers and increase trust in model outputs. It also facilitates knowledge updates and the introduction of domain-specific knowledge. RAG effectively combines the parameterized knowledge of LLMs with non-parameterized external knowledge bases, making it one of the most important methods for implementing large language models. This paper outlines the development paradigms of RAG in the era of LLMs, summarizing three paradigms: Naive RAG, Advanced RAG, and Modular RAG. It then provides a summary and organization of the three main components of RAG: retriever, generator, and augmentation methods, along with key technologies in each component. Furthermore, it discusses how to evaluate the effectiveness of RAG models, introducing two evaluation methods for RAG, emphasizing key metrics and abilities for evaluation, and presenting the latest automatic evaluation framework. Finally, potential future research directions are introduced from three aspects: vertical optimization, horizontal scalability, and the technical stack and ecosystem of RAG.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2312.10997v1', 'html': 'https://arxiv.org/html/2312.10997v1', 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2312.10997'}	Submission history From: Yunfan Gao [ view email ] [v1] Mon, 18 Dec 2023 07:47:33 UTC (7,541 KB) [v2] Fri, 29 Dec 2023 18:25:00 UTC (6,421 KB) [v3] Wed, 3 Jan 2024 17:04:40 UTC (7,508 KB) [v4] Fri, 5 Jan 2024 01:18:27 UTC (7,508 KB) [v5] Wed, 27 Mar 2024 09:16:57 UTC (1,955 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.10997'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.10997'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.10997'}]
2023-12-17	Mathematical Language Models: A Survey	Computation and Language	https://arxiv.org/abs/2312.07622	Mathematical LLMs	https://x.com/omarsar0/status/1735323577392542084?s=20		2312.07622	['Wentao Liu', 'Hanglei Hu', 'Jie Zhou', 'Yuyang Ding', 'Junsong Li', 'Jiayi Zeng', 'Mengliang He', 'Qin Chen', 'Bo Jiang', 'Aimin Zhou', 'Liang He']	ct:In recent years, there has been remarkable progress in leveraging Language Models (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale Language Models (LLMs), within the domain of mathematics. This paper conducts a comprehensive survey of mathematical LMs, systematically categorizing pivotal research endeavors from two distinct perspectives: tasks and methodologies. The landscape reveals a large number of proposed mathematical LLMs, which are further delineated into instruction learning, tool-based methods, fundamental CoT techniques, advanced CoT methodologies and multi-modal methods. To comprehend the benefits of mathematical LMs more thoroughly, we carry out an in-depth contrast of their characteristics and performance. In addition, our survey entails the compilation of over 60 mathematical datasets, including training datasets, benchmark datasets, and augmented datasets. Addressing the primary challenges and delineating future trajectories within the field of mathematical LMs, this survey is poised to facilitate and inspire future innovation among researchers invested in advancing this domain.	admin note: text overlap witharXiv:1705.04146,arXiv:2304.10977,arXiv:2112.00114,arXiv:1905.13319,arXiv:2304.12244,arXiv:2206.01347,arXiv:2006.09265by other authors	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2312.07622', 'html': 'https://arxiv.org/html/2312.07622v4', 'tex': '/src/2312.07622', 'doi': 'https://doi.org/10.48550/arXiv.2312.07622'}	Submission history From: Jie Zhou [ view email ] [v1] Tue, 12 Dec 2023 01:39:16 UTC (5,001 KB) [v2] Thu, 14 Dec 2023 04:01:17 UTC (5,001 KB) [v3] Fri, 23 Feb 2024 14:00:04 UTC (5,155 KB) [v4] Wed, 1 Jan 2025 08:16:41 UTC (458 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.07622'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.07622'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.07622'}]
2023-12-17	LLM360: Towards Fully Transparent Open-Source LLMs	Computation and Language	https://arxiv.org/abs/2312.06550	Towards Fully Transparent Open-Source LLMs	https://x.com/omarsar0/status/1734591071575744820?s=20		2312.06550	['Zhengzhong Liu', 'Aurick Qiao', 'Willie Neiswanger', 'Hongyi Wang', 'Bowen Tan', 'Tianhua Tao', 'Junbo Li', 'Yuqi Wang', 'Suqi Sun', 'Omkar Pangarkar', 'Richard Fan', 'Yi Gu', 'Victor Miller', 'Yonghao Zhuang', 'Guowei He', 'Haonan Li', 'Fajri Koto', 'Liping Tang', 'Nikhil Ranjan', 'Zhiqiang Shen', 'Xuguang Ren', 'Roberto Iriondo', 'Cun Mu', 'Zhiting Hu', 'Mark Schulze', 'Preslav Nakov', 'Tim Baldwin', 'Eric P. Xing']	ct:The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (atthis https URL). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2312.06550', 'html': 'https://arxiv.org/html/2312.06550v1', 'tex': '/src/2312.06550', 'doi': 'https://doi.org/10.48550/arXiv.2312.06550'}	Submission history From: Willie Neiswanger [ view email ] [v1] Mon, 11 Dec 2023 17:39:00 UTC (9,845 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.06550'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.06550'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.06550'}]
2023-12-17	A Survey of Large Language Models in Medicine: Progress, Application, and Challenge	Computation and Language	https://arxiv.org/abs/2311.05112	LLMs in Medicine	https://x.com/omarsar0/status/1734599425568231513?s=20		2311.05112	['Hongjian Zhou', 'Fenglin Liu', 'Boyang Gu', 'Xinyu Zou', 'Jinfa Huang', 'Jinge Wu', 'Yiru Li', 'Sam S. Chen', 'Peilin Zhou', 'Junling Liu', 'Yining Hua', 'Chengfeng Mao', 'Chenyu You', 'Xian Wu', 'Yefeng Zheng', 'Lei Clifton', 'Zheng Li', 'Jiebo Luo', 'David A. Clifton']	ct:Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tasks (e.g., enhancing clinical diagnostics and providing medical education), a review of these efforts, particularly their development, practical applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face. In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioners in developing medical LLMs tailored to their specific needs. In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide an understanding of the advantages and limitations of LLMs in medicine. Overall, in this review, we address the following questions: 1) What are the practices for developing medical LLMs 2) How to measure the medical task performance of LLMs in a medical setting? 3) How have medical LLMs been employed in real-world practice? 4) What challenges arise from the use of medical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insights into the opportunities for LLMs in medicine and serve as a practical resource. We also maintain a regularly updated list of practical guides on medical LLMs atthis https URL	nt. Version 6. Update Figures 1-5; Tables 2-3; 31 pages	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2311.05112', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2311.05112'}	Submission history From: Fenglin Liu [ view email ] [v1] Thu, 9 Nov 2023 02:55:58 UTC (48 KB) [v2] Mon, 11 Dec 2023 18:10:20 UTC (199 KB) [v3] Fri, 2 Feb 2024 06:48:24 UTC (417 KB) [v4] Sun, 3 Mar 2024 01:15:36 UTC (652 KB) [v5] Wed, 15 May 2024 13:38:45 UTC (1,049 KB) [v6] Wed, 10 Jul 2024 22:10:32 UTC (1,168 KB) [v7] Mon, 22 Jul 2024 12:16:30 UTC (1,168 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.05112'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.05112'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.05112'}]
2023-12-17	Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models	Machine Learning	https://arxiv.org/abs/2312.06585	Beyond Human Data for LLMs	https://x.com/omarsar0/status/1734953578274386002?s=20		2312.06585	['Avi Singh', 'John D. Co-Reyes', 'Rishabh Agarwal', 'Ankesh Anand', 'Piyush Patil', 'Xavier Garcia', 'Peter J. Liu', 'James Harrison', 'Jaehoon Lee', 'Kelvin Xu', 'Aaron Parisi', 'Abhishek Kumar', 'Alex Alemi', 'Alex Rizkowsky', 'Azade Nova', 'Ben Adlam', 'Bernd Bohnet', 'Gamaleldin Elsayed', 'Hanie Sedghi', 'Igor Mordatch', 'Isabelle Simpson', 'Izzeddin Gur', 'Jasper Snoek', 'Jeffrey Pennington', 'Jiri Hron', 'Kathleen Kenealy', 'Kevin Swersky', 'Kshiteej Mahajan', 'Laura Culp', 'Lechao Xiao', 'Maxwell L. Bileschi', 'Noah Constant', 'Roman Novak', 'Rosanne Liu', 'Tris Warkentin', 'Yundi Qian', 'Yamini Bansal', 'Ethan Dyer', 'Behnam Neyshabur', 'Jascha Sohl-Dickstein', 'Noah Fiedel']	ct:Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST$^{EM}$, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST$^{EM}$ scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can substantially reduce dependence on human-generated data.	ed to TMLR. Camera-ready version. First three authors contributed equally	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2312.06585', 'html': 'https://arxiv.org/html/2312.06585v4', 'tex': '/src/2312.06585', 'doi': 'https://doi.org/10.48550/arXiv.2312.06585'}	Submission history From: Avi Singh [ view email ] [v1] Mon, 11 Dec 2023 18:17:43 UTC (153 KB) [v2] Tue, 12 Dec 2023 23:16:16 UTC (153 KB) [v3] Fri, 22 Dec 2023 18:33:50 UTC (159 KB) [v4] Thu, 18 Apr 2024 03:12:09 UTC (173 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.06585'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.06585'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.06585'}]
2023-12-17	Pearl: A Production-ready Reinforcement Learning Agent	Machine Learning	https://arxiv.org/abs/2312.03814	Pearl	https://x.com/ZheqingZhu/status/1732880717263352149?s=20		2312.03814	['Zheqing Zhu', 'Rodrigo de Salvo Braz', 'Jalaj Bhandari', 'Daniel Jiang', 'Yi Wan', 'Yonathan Efroni', 'Liyuan Wang', 'Ruiyang Xu', 'Hongbo Guo', 'Alex Nikulkov', 'Dmytro Korenkevych', 'Urun Dogan', 'Frank Cheng', 'Zheng Wu', 'Wanqiao Xu']	ct:Reinforcement learning (RL) is a versatile framework for optimizing long-term goals. Although many real-world problems can be formalized with RL, learning and deploying a performant RL policy requires a system designed to address several important challenges, including the exploration-exploitation dilemma, partial observability, dynamic action spaces, and safety concerns. While the importance of these challenges has been well recognized, existing open-source RL libraries do not explicitly address them. This paper introduces Pearl, a Production-Ready RL software package designed to embrace these challenges in a modular way. In addition to presenting benchmarking results, we also highlight examples of Pearl's ongoing industry adoption to demonstrate its advantages for production use cases. Pearl is open sourced on GitHub atthis http URLand its official website isthis http URL.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2312.03814', 'html': 'https://arxiv.org/html/2312.03814v2', 'tex': '/src/2312.03814', 'doi': 'https://doi.org/10.48550/arXiv.2312.03814'}	Submission history From: Zheqing Zhu [ view email ] [v1] Wed, 6 Dec 2023 18:29:23 UTC (4,342 KB) [v2] Mon, 2 Sep 2024 05:18:49 UTC (7,023 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.03814'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.03814'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.03814'}]
2023-12-10	EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2312.00863	EfficientSAM	https://x.com/fiandola/status/1732171016783180132?s=20		2312.00863	['Yunyang Xiong', 'Bala Varadarajan', 'Lemeng Wu', 'Xiaoyu Xiang', 'Fanyi Xiao', 'Chenchen Zhu', 'Xiaoliang Dai', 'Dilin Wang', 'Fei Sun', 'Forrest Iandola', 'Raghuraman Krishnamoorthi', 'Vikas Chandra']	ct:Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot transfer and high versatility is a super large Transformer model trained on the extensive high-quality SA-1B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, light-weight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build EfficientSAMs, and finetune the models on SA-1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, instance segmentation, and semantic object detection, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2312.00863', 'html': 'https://arxiv.org/html/2312.00863v1', 'tex': '/src/2312.00863', 'doi': 'https://doi.org/10.48550/arXiv.2312.00863'}	Submission history From: Yunyang Xiong [ view email ] [v1] Fri, 1 Dec 2023 18:31:00 UTC (15,247 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.00863'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.00863'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.00863'}]
2023-12-10	Magicoder: Empowering Code Generation with OSS-Instruct	Computation and Language	https://arxiv.org/abs/2312.02120	Magicoder	https://x.com/omarsar0/status/1732063926613946863?s=20		2312.02120	['Yuxiang Wei', 'Zhe Wang', 'Jiawei Liu', 'Yifeng Ding', 'Lingming Zhang']	ct:We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.	hed at ICML 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2312.02120', 'html': 'https://arxiv.org/html/2312.02120v2', 'tex': '/src/2312.02120', 'doi': 'https://doi.org/10.48550/arXiv.2312.02120'}	Submission history From: Yuxiang Wei [ view email ] [v1] Mon, 4 Dec 2023 18:50:35 UTC (1,322 KB) [v2] Fri, 7 Jun 2024 02:50:56 UTC (1,485 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.02120'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.02120'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.02120'}]
2023-12-10	Large Language Models on Graphs: A Comprehensive Survey	Computation and Language	https://arxiv.org/abs/2312.02783	LLMs on Graphs	https://x.com/omarsar0/status/1732404393037762588?s=20		2312.02783	['Bowen Jin', 'Gang Liu', 'Chi Han', 'Meng Jiang', 'Heng Ji', 'Jiawei Han']	ct:Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found atthis https URL.	es	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2312.02783', 'html': 'https://arxiv.org/html/2312.02783v4', 'tex': '/src/2312.02783', 'doi': 'https://doi.org/10.48550/arXiv.2312.02783'}	Submission history From: Bowen Jin [ view email ] [v1] Tue, 5 Dec 2023 14:14:27 UTC (2,485 KB) [v2] Thu, 1 Feb 2024 22:51:24 UTC (6,984 KB) [v3] Thu, 3 Oct 2024 13:47:02 UTC (7,090 KB) [v4] Wed, 20 Nov 2024 21:24:10 UTC (7,091 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.02783'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.02783'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.02783'}]
2023-12-10	Chain of Code: Reasoning with a Language Model-Augmented Code Emulator	Computation and Language	https://arxiv.org/abs/2312.04474	Chain of Code	https://x.com/ChengshuEricLi/status/1733169631949701425?s=20		2312.04474	['Chengshu Li', 'Jacky Liang', 'Andy Zeng', 'Xinyun Chen', 'Karol Hausman', 'Dorsa Sadigh', 'Sergey Levine', 'Li Fei-Fei', 'Fei Xia', 'Brian Ichter']	"ct:Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter - we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for ""detect_sarcasm(string)"" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively ""emulate"" the interpreter by generating the expected output of ""detect_sarcasm(string)"". In this work, we propose Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an ""LMulator""). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by ""thinking in code""."	024 Oral; Project webpage:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Robotics (cs.RO)']	{'pdf': '/pdf/2312.04474', 'html': 'https://arxiv.org/html/2312.04474v4', 'tex': '/src/2312.04474', 'doi': 'https://doi.org/10.48550/arXiv.2312.04474'}	Submission history From: Chengshu Li [ view email ] [v1] Thu, 7 Dec 2023 17:51:43 UTC (2,938 KB) [v2] Fri, 8 Dec 2023 03:04:46 UTC (2,938 KB) [v3] Mon, 22 Jul 2024 14:27:56 UTC (2,663 KB) [v4] Mon, 29 Jul 2024 20:21:37 UTC (2,663 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.04474'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.04474'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.04474'}]
2023-12-10	Data Management For Training Large Language Models: A Survey	Computation and Language	https://arxiv.org/abs/2312.01700	Data Management For LLMs	https://x.com/omarsar0/status/1731877232493166969?s=20		2312.01700	['Zige Wang', 'Wanjun Zhong', 'Yufei Wang', 'Qi Zhu', 'Fei Mi', 'Baojun Wang', 'Lifeng Shang', 'Xin Jiang', 'Qun Liu']	ct:Data plays a fundamental role in training Large Language Models (LLMs). Efficient data management, particularly in formulating a well-suited training dataset, is significant for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning stages. Despite the considerable importance of data management, the underlying mechanism of current prominent practices are still unknown. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey aims to provide a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various aspects of data management strategy design. Looking into the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through efficient data management practices. The collection of the latest papers is available atthis https URL.	n progress	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2312.01700', 'html': 'https://arxiv.org/html/2312.01700v3', 'tex': '/src/2312.01700', 'doi': 'https://doi.org/10.48550/arXiv.2312.01700'}	Submission history From: Zige Wang [ view email ] [v1] Mon, 4 Dec 2023 07:42:16 UTC (68 KB) [v2] Tue, 26 Dec 2023 01:35:38 UTC (72 KB) [v3] Fri, 2 Aug 2024 03:56:35 UTC (98 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.01700'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.01700'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.01700'}]
2023-12-10	RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!	Information Retrieval	https://arxiv.org/abs/2312.02724	*8RankZephyr**	https://x.com/lintool/status/1732430269485867114?s=20		2312.02724	['Ronak Pradeep', 'Sahel Sharifymoghaddam', 'Jimmy Lin']	ct:In information retrieval, proprietary large language models (LLMs) such as GPT-4 and open-source counterparts such as LLaMA and Vicuna have played a vital role in reranking. However, the gap between open-source and closed models persists, with reliance on proprietary, non-transparent models constraining reproducibility. Addressing this gap, we introduce RankZephyr, a state-of-the-art, open-source LLM for listwise zero-shot reranking. RankZephyr not only bridges the effectiveness gap with GPT-4 but in some cases surpasses the proprietary model. Our comprehensive evaluations across several datasets (TREC Deep Learning Tracks; NEWS and COVID from BEIR) showcase this ability. RankZephyr benefits from strategic training choices and is resilient against variations in initial document ordering and the number of documents reranked. Additionally, our model outperforms GPT-4 on the NovelEval test set, comprising queries and passages past its training period, which addresses concerns about data contamination. To foster further research in this rapidly evolving field, we provide all code necessary to reproduce our results atthis https URL.		['Information Retrieval (cs.IR)']	{'pdf': '/pdf/2312.02724', 'html': 'https://arxiv.org/html/2312.02724v1', 'tex': '/src/2312.02724', 'doi': 'https://doi.org/10.48550/arXiv.2312.02724'}	Submission history From: Jimmy Lin [ view email ] [v1] Tue, 5 Dec 2023 12:39:00 UTC (39 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.02724'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.02724'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.02724'}]
2023-12-10	The Efficiency Spectrum of Large Language Models: An Algorithmic Survey	Computation and Language	https://arxiv.org/abs/2312.00678	The Efficiency Spectrum of LLMs	https://x.com/omarsar0/status/1731696419457606048?s=20		2312.00678	['Tianyu Ding', 'Tianyi Chen', 'Haidong Zhu', 'Jiachen Jiang', 'Yiqi Zhong', 'Jinxin Zhou', 'Guangzhi Wang', 'Zhihui Zhu', 'Ilya Zharkov', 'Luming Liang']	ct:The rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained at url{this https URL}.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2312.00678', 'html': 'https://arxiv.org/html/2312.00678v2', 'tex': '/src/2312.00678', 'doi': 'https://doi.org/10.48550/arXiv.2312.00678'}	Submission history From: Tianyi Chen [ view email ] [v1] Fri, 1 Dec 2023 16:00:25 UTC (615 KB) [v2] Thu, 18 Apr 2024 18:10:28 UTC (619 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2312.00678'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2312.00678'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2312.00678'}]
2023-12-03	ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?	Computation and Language	https://arxiv.org/abs/2311.16989	Open-Source LLMs vs. ChatGPT	https://x.com/sophiamyang/status/1730108858889097710?s=20		2311.16989	['Hailin Chen', 'Fangkai Jiao', 'Xingxuan Li', 'Chengwei Qin', 'Mathieu Ravaut', 'Ruochen Zhao', 'Caiming Xiong', 'Shafiq Joty']	ct:Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.	n v4, included latest top-performing open-sourced LLMs	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2311.16989', 'html': 'https://arxiv.org/html/2311.16989v4', 'tex': '/src/2311.16989', 'doi': 'https://doi.org/10.48550/arXiv.2311.16989'}	Submission history From: Hailin Chen [ view email ] [v1] Tue, 28 Nov 2023 17:44:51 UTC (660 KB) [v2] Wed, 29 Nov 2023 16:00:05 UTC (733 KB) [v3] Tue, 5 Dec 2023 16:58:46 UTC (736 KB) [v4] Mon, 15 Jan 2024 09:55:05 UTC (817 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.16989'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.16989'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.16989'}]
2023-12-03	MEDITRON-70B: Scaling Medical Pretraining for Large Language Models	Computation and Language	https://arxiv.org/abs/2311.16079v1	MEDITRON-70B	https://x.com/eric_zemingchen/status/1729563855213175010?s=20		2311.16079v1	['Zeming Chen', 'Alejandro Hernández Cano', 'Angelika Romanou', 'Antoine Bonnet', 'Kyle Matoba', 'Francesco Salvi', 'Matteo Pagliardini', 'Simin Fan', 'Andreas Köpf', 'Amirkeivan Mohtashami', 'Alexandre Sallinen', 'Alireza Sakhaeirad', 'Vinitra Swamy', 'Igor Krawczuk', 'Deniz Bayazit', 'Axel Marmet', 'Syrielle Montariol', 'Mary-Anne Hartley', 'Martin Jaggi', 'Antoine Bosselut']	ct:Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.16079v1', 'html': None, 'tex': '/src/2311.16079v1', 'doi': 'https://doi.org/10.48550/arXiv.2311.16079'}	Submission history From: Zeming Chen [ view email ] [v1] Mon, 27 Nov 2023 18:49:43 UTC (1,425 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.16079'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.16079'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.16079'}]
2023-12-03	Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine	Computation and Language	https://arxiv.org/abs/2311.16452	Foundation Models Outcompeting Special-Purpose Tuning	https://x.com/erichorvitz/status/1729854235443884385?s=20		2311.16452	['Harsha Nori', 'Yin Tat Lee', 'Sheng Zhang', 'Dean Carignan', 'Richard Edgar', 'Nicolo Fusi', 'Nicholas King', 'Jonathan Larson', 'Yuanzhi Li', 'Weishung Liu', 'Renqian Luo', 'Scott Mayer McKinney', 'Robert Osazuwa Ness', 'Hoifung Poon', 'Tao Qin', 'Naoto Usuyama', 'Chris White', 'Eric Horvitz']	ct:Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities of fine-tuned models. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. We build on a prior study of GPT-4's capabilities on medical challenge benchmarks in the absence of special training. Rather than using simple prompting to highlight the model's out-of-the-box capabilities, we perform a systematic exploration of prompt engineering. We find that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical benchmarks. The prompting methods we explore are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Our experimental design carefully controls for overfitting during the prompt engineering process. We introduce Medprompt, based on a composition of several prompting strategies. With Medprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms leading specialist models such as Med-PaLM 2 by a significant margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27% reduction in error rate on the MedQA dataset over the best methods to date achieved with specialist models and surpasses a score of 90% for the first time. Beyond medical problems, we show the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology.	es, 7 figures	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2311.16452', 'html': None, 'tex': '/src/2311.16452', 'doi': 'https://doi.org/10.48550/arXiv.2311.16452'}	Submission history From: Eric Horvitz [ view email ] [v1] Tue, 28 Nov 2023 03:16:12 UTC (2,654 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.16452'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.16452'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.16452'}]
2023-12-03	UniIR: Training and Benchmarking Universal Multimodal Information Retrievers	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2311.17136	UniIR	https://x.com/CongWei1230/status/1730307767469068476?s=20		2311.17136	['Cong Wei', 'Yang Chen', 'Haonan Chen', 'Hexiang Hu', 'Ge Zhang', 'Jie Fu', 'Alan Ritter', 'Wenhu Chen']	ct:Existing information retrieval (IR) models often assume a homogeneous format, limiting their applicability to diverse user needs, such as searching for images with text descriptions, searching for a news article with a headline image, or finding a similar photo with a query image. To approach such different information-seeking demands, we introduce UniIR, a unified instruction-guided multimodal retriever capable of handling eight distinct retrieval tasks across modalities. UniIR, a single retrieval system jointly trained on ten diverse multimodal-IR datasets, interprets user instructions to execute various retrieval tasks, demonstrating robust performance across existing datasets and zero-shot generalization to new tasks. Our experiments highlight that multi-task training and instruction tuning are keys to UniIR's generalization ability. Additionally, we construct the M-BEIR, a multimodal retrieval benchmark with comprehensive results, to standardize the evaluation of universal multimodal information retrieval.	de and dataset are available on this project page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2311.17136', 'html': None, 'tex': '/src/2311.17136', 'doi': 'https://doi.org/10.48550/arXiv.2311.17136'}	Submission history From: Cong Wei [ view email ] [v1] Tue, 28 Nov 2023 18:55:52 UTC (9,515 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.17136'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.17136'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.17136'}]
2023-12-03	On Bringing Robots Home	Robotics	https://arxiv.org/abs/2311.16098v1	On Bringing Robots Home	https://x.com/LerrelPinto/status/1729515379892826211?s=20		2311.16098v1	['Nur Muhammad Mahi Shafiullah', 'Anant Rai', 'Haritheja Etukuru', 'Yiqian Liu', 'Ishan Misra', 'Soumith Chintala', 'Lerrel Pinto']	"ct:Throughout history, we have successfully integrated various machines into our homes. Dishwashers, laundry machines, stand mixers, and robot vacuums are a few recent examples. However, these machines excel at performing only a single task effectively. The concept of a ""generalist machine"" in homes - a domestic assistant that can adapt and learn from our needs, all while remaining cost-effective - has long been a goal in robotics that has been steadily pursued for decades. In this work, we initiate a large-scale effort towards this goal by introducing Dobb-E, an affordable yet versatile general-purpose system for learning robotic manipulation within household settings. Dobb-E can learn a new task with only five minutes of a user showing it how to do it, thanks to a demonstration collection tool (""The Stick"") we built out of cheap parts and iPhones. We use the Stick to collect 13 hours of data in 22 homes of New York City, and train Home Pretrained Representations (HPR). Then, in a novel home environment, with five minutes of demonstrations and fifteen minutes of adapting the HPR model, we show that Dobb-E can reliably solve the task on the Stretch, a mobile robot readily available on the market. Across roughly 30 days of experimentation in homes of New York City and surrounding areas, we test our system in 10 homes, with a total of 109 tasks in different environments, and finally achieve a success rate of 81%. Beyond success percentages, our experiments reveal a plethora of unique challenges absent or ignored in lab robotics. These range from effects of strong shadows, to variable demonstration quality by non-expert users. With the hope of accelerating research on home robots, and eventually seeing robot butlers in every home, we open-source Dobb-E software stack and models, our data, and our hardware designs atthis https URL"	t website and videos are available atthis https URL, technical documentation for getting started is available atthis https URL, and code is released atthis https URL	['Robotics (cs.RO)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.16098v1', 'html': None, 'tex': '/src/2311.16098v1', 'doi': 'https://doi.org/10.48550/arXiv.2311.16098'}	Submission history From: Nur Muhammad (Mahi) Shafiullah [ view email ] [v1] Mon, 27 Nov 2023 18:59:25 UTC (16,844 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.16098'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.16098'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.16098'}]
2023-12-03	Translatotron 3: Speech to Speech Translation with Monolingual Data	Computation and Language	https://arxiv.org/abs/2305.17547	Translatotron 3	https://x.com/GoogleAI/status/1730654297350959413?s=20		2305.17547	['Eliya Nachmani', 'Alon Levkovitch', 'Yifan Ding', 'Chulayuth Asawaroengchai', 'Heiga Zen', 'Michelle Tadmor Ramanovich']	ct:This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting $18.14$ BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it. Audio samples can be found atthis http URL	ear in ICASSP 2024	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Sound (cs.SD)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2305.17547', 'html': None, 'tex': '/src/2305.17547', 'doi': 'https://doi.org/10.48550/arXiv.2305.17547'}	Submission history From: Eliya Nachmani [ view email ] [v1] Sat, 27 May 2023 18:30:54 UTC (311 KB) [v2] Thu, 1 Jun 2023 08:01:16 UTC (311 KB) [v3] Tue, 16 Jan 2024 08:27:38 UTC (310 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.17547'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.17547'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.17547'}]
2023-11-26	System 2 Attention (is something you might need too)	Computation and Language	https://arxiv.org/abs/2311.11829	System 2 Attention	https://x.com/jaseweston/status/1726784511357157618?s=20		2311.11829	['Jason Weston', 'Sainbayar Sukhbaatar']	ct:Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information from the context into its latent representations, which adversely affects next token generations. To help rectify these issues, we introduce System 2 Attention (S2A), which leverages the ability of LLMs to reason in natural language and follow instructions in order to decide what to attend to. S2A regenerates the input context to only include the relevant portions, before attending to the regenerated context to elicit the final response. In experiments, S2A outperforms standard attention-based LLMs on three tasks containing opinion or irrelevant information, QA, math word problems and longform generation, where S2A increases factuality and objectivity, and decreases sycophancy.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.11829', 'html': None, 'tex': '/src/2311.11829', 'doi': 'https://doi.org/10.48550/arXiv.2311.11829'}	Submission history From: Jason  Weston [ view email ] [v1] Mon, 20 Nov 2023 15:04:50 UTC (97 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.11829'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.11829'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.11829'}]
2023-11-26	Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey	Computation and Language	https://arxiv.org/abs/2311.12351	Advancing Long-Context LLMs	https://x.com/omarsar0/status/1727358484360945750?s=20		2311.12351	['Yunpeng Huang', 'Jingwei Xu', 'Junyu Lai', 'Zixu Jiang', 'Taolue Chen', 'Zenan Li', 'Yuan Yao', 'Xiaoxing Ma', 'Lijuan Yang', 'Hao Chen', 'Shupeng Li', 'Penghao Zhao']	ct:Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available atthis https URL.	es, 3 figures, 4 tables	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.12351', 'html': 'https://arxiv.org/html/2311.12351v2', 'tex': '/src/2311.12351', 'doi': 'https://doi.org/10.48550/arXiv.2311.12351'}	Submission history From: Yunpeng Huang [ view email ] [v1] Tue, 21 Nov 2023 04:59:17 UTC (25,462 KB) [v2] Fri, 23 Feb 2024 19:22:58 UTC (42,120 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.12351'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.12351'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.12351'}]
2023-11-26	PaSS: Parallel Speculative Sampling	Computation and Language	https://arxiv.org/abs/2311.13581	Parallel Speculative Sampling	https://x.com/omarsar0/status/1728066181796418009?s=20		2311.13581	['Giovanni Monea', 'Armand Joulin', 'Edouard Grave']	ct:Scaling the size of language models to tens of billions of parameters has led to impressive performance on a wide range of tasks. At generation, these models are used auto-regressively, requiring a forward pass for each generated token, and thus reading the full set of parameters from memory. This memory access forms the primary bottleneck for generation and it worsens as the model size increases. Moreover, executing a forward pass for multiple tokens in parallel often takes nearly the same time as it does for just one token. These two observations lead to the development of speculative sampling, where a second smaller model is used to draft a few tokens, that are then validated or rejected using a single forward pass of the large model. Unfortunately, this method requires two models that share the same tokenizer and thus limits its adoption. As an alternative, we propose to use parallel decoding as a way to draft multiple tokens from a single model with no computational cost, nor the need for a second model. Our approach only requires an additional input token that marks the words that will be generated simultaneously. We show promising performance (up to $30\%$ speed-up) while requiring only as few as $O(d_{emb})$ additional parameters.	ed at the 3rd workshop on Efficient Natural Language and Speech Processing (ENLSP, NeurIPS 2023)	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2311.13581', 'html': None, 'tex': '/src/2311.13581', 'doi': 'https://doi.org/10.48550/arXiv.2311.13581'}	Submission history From: Giovanni Monea [ view email ] [v1] Wed, 22 Nov 2023 18:37:27 UTC (23 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.13581'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.13581'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.13581'}]
2023-11-26	Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2311.05698	Mirasol3B	https://x.com/GoogleAI/status/1724553024088191211?s=20		2311.05698	['AJ Piergiovanni', 'Isaac Noble', 'Dahun Kim', 'Michael S. Ryoo', 'Victor Gomes', 'Anelia Angelova']	ct:One of the main challenges of multimodal learning is the need to combine heterogeneous modalities (e.g., video, audio, text). For example, video and audio are obtained at much higher rates than text and are roughly aligned in time. They are often not synchronized with text, which comes as a global context, e.g., a title, or a description. Furthermore, video and audio inputs are of much larger volumes, and grow as the video length increases, which naturally requires more compute dedicated to these modalities and makes modeling of long-range dependencies harder.We here decouple the multimodal modeling, dividing it into separate, focused autoregressive models, processing the inputs according to the characteristics of the modalities. We propose a multimodal model, called Mirasol3B, consisting of an autoregressive component for the time-synchronized modalities (audio and video), and an autoregressive component for the context modalities which are not necessarily aligned in time but are still sequential. To address the long-sequences of the video-audio inputs, we propose to further partition the video and audio sequences in consecutive snippets and autoregressively process their representations. To that end, we propose a Combiner mechanism, which models the audio-video information jointly within a timeframe. The Combiner learns to extract audio and video features from raw spatio-temporal signals, and then learns to fuse these features producing compact but expressive representations per snippet.Our approach achieves the state-of-the-art on well established multimodal benchmarks, outperforming much larger models. It effectively addresses the high computational demand of media inputs by both learning compact representations, controlling the sequence length of the audio-video feature representations, and modeling their dependencies in time.	024	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2311.05698', 'html': 'https://arxiv.org/html/2311.05698v3', 'tex': '/src/2311.05698', 'doi': 'https://doi.org/10.48550/arXiv.2311.05698'}	Submission history From: Aj Piergiovanni [ view email ] [v1] Thu, 9 Nov 2023 19:15:12 UTC (949 KB) [v2] Mon, 13 Nov 2023 14:53:10 UTC (949 KB) [v3] Wed, 3 Apr 2024 20:04:49 UTC (2,234 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.05698'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.05698'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.05698'}]
2023-11-26	Orca 2: Teaching Small Language Models How to Reason	Artificial Intelligence	https://arxiv.org/abs/2311.11045	Teaching Small LMs To Reason	https://x.com/omarsar0/status/1726990087399915995?s=20		2311.11045	['Arindam Mitra', 'Luciano Del Corro', 'Shweti Mahajan', 'Andres Codas', 'Clarisse Simoes', 'Sahaj Agarwal', 'Xuxi Chen', 'Anastasia Razdaibiedina', 'Erik Jones', 'Kriti Aggarwal', 'Hamid Palangi', 'Guoqing Zheng', 'Corby Rosset', 'Hamed Khanpour', 'Ahmed Awadallah']	ct:Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available atthis http URLto support research on the development, evaluation, and alignment of smaller LMs	url to model weights fixed typo in Author name	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2311.11045', 'html': None, 'tex': '/src/2311.11045', 'doi': 'https://doi.org/10.48550/arXiv.2311.11045'}	Submission history From: Arindam Mitra [ view email ] [v1] Sat, 18 Nov 2023 11:44:52 UTC (446 KB) [v2] Tue, 21 Nov 2023 19:43:31 UTC (446 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.11045'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.11045'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.11045'}]
2023-11-26	GPQA: A Graduate-Level Google-Proof Q&A Benchmark	Artificial Intelligence	https://arxiv.org/abs/2311.12022	GPQA	https://x.com/idavidrein/status/1727033002234909060?s=20		2311.12022	['David Rein', 'Betty Li Hou', 'Asa Cooper Stickland', 'Jackson Petty', 'Richard Yuanzhe Pang', 'Julien Dirani', 'Julian Michael', 'Samuel R. Bowman']	"ct:We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are ""Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities."	es, 5 figures, 7 tables	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2311.12022', 'html': None, 'tex': '/src/2311.12022', 'doi': 'https://doi.org/10.48550/arXiv.2311.12022'}	Submission history From: David Rein [ view email ] [v1] Mon, 20 Nov 2023 18:57:34 UTC (7,790 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.12022'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.12022'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.12022'}]
2023-11-26	Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents	Computation and Language	https://arxiv.org/abs/2311.11797	The Hitchhiker’s Guide From Chain-of-Thought Reasoning to Language Agents	https://x.com/omarsar0/status/1726803725220487277?s=20		2311.11797	['Zhuosheng Zhang', 'Yao Yao', 'Aston Zhang', 'Xiangru Tang', 'Xinbei Ma', 'Zhiwei He', 'Yiming Wang', 'Mark Gerstein', 'Rui Wang', 'Gongshen Liu', 'Hai Zhao']	ct:Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Human-Computer Interaction (cs.HC)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2311.11797', 'html': None, 'tex': '/src/2311.11797', 'doi': 'https://doi.org/10.48550/arXiv.2311.11797'}	Submission history From: Zhuosheng Zhang [ view email ] [v1] Mon, 20 Nov 2023 14:30:55 UTC (3,629 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.11797'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.11797'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.11797'}]
2023-11-26	GAIA: a benchmark for General AI Assistants	Computation and Language	https://arxiv.org/abs/2311.12983	GAIA	https://x.com/ThomasScialom/status/1727683993045201339?s=20		2311.12983	['Grégoire Mialon', 'Clémentine Fourrier', 'Craig Swift', 'Thomas Wolf', 'Yann LeCun', 'Thomas Scialom']	ct:We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92\% vs. 15\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2311.12983', 'html': None, 'tex': '/src/2311.12983', 'doi': 'https://doi.org/10.48550/arXiv.2311.12983'}	Submission history From: Grégoire Mialon [ view email ] [v1] Tue, 21 Nov 2023 20:34:47 UTC (3,688 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.12983'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.12983'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.12983'}]
2023-11-26	MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning	Computation and Language	https://arxiv.org/abs/2311.10537	LLMs as Collaborators for Medical Reasoning	https://x.com/omarsar0/status/1726627951582511135?s=20		2311.10537	['Xiangru Tang', 'Anni Zou', 'Zhuosheng Zhang', 'Ziming Li', 'Yilun Zhao', 'Xingyao Zhang', 'Arman Cohan', 'Mark Gerstein']	ct:Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2311.10537', 'html': 'https://arxiv.org/html/2311.10537v4', 'tex': '/src/2311.10537', 'doi': 'https://doi.org/10.48550/arXiv.2311.10537'}	Submission history From: Xiangru Tang [ view email ] [v1] Thu, 16 Nov 2023 11:47:58 UTC (1,904 KB) [v2] Mon, 19 Feb 2024 18:26:46 UTC (2,347 KB) [v3] Tue, 20 Feb 2024 06:12:14 UTC (2,347 KB) [v4] Tue, 4 Jun 2024 23:47:43 UTC (2,348 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.10537'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.10537'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.10537'}]
2023-11-26	Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2	Computation and Language	https://arxiv.org/abs/2311.10702	TÜLU 2	https://x.com/natolambert/status/1727350301131518454?s=20		2311.10702	['Hamish Ivison', 'Yizhong Wang', 'Valentina Pyatkin', 'Nathan Lambert', 'Matthew Peters', 'Pradeep Dasigi', 'Joel Jang', 'David Wadden', 'Noah A. Smith', 'Iz Beltagy', 'Hannaneh Hajishirzi']	ct:Since the release of TÜLU [Wang et al., 2023b], open resources for instruction tuning have developed quickly, from better base models to new finetuning techniques. We test and incorporate a number of these advances into TÜLU, resulting in TÜLU 2, a suite of improved TÜLU models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences. Concretely, we release: (1) TÜLU-V2-mix, an improved collection of high-quality instruction datasets; (2) TÜLU 2, LLAMA-2 models finetuned on the V2 mixture; (3) TÜLU 2+DPO, TÜLU 2 models trained with direct preference optimization (DPO), including the largest DPO-trained model to date (TÜLU 2+DPO 70B); (4) CODE TÜLU 2, CODE LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple perspectives shows that the TÜLU 2 suite achieves state-of-the-art performance among open models and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data, training and evaluation code to facilitate future open efforts on adapting large language models.	cal report; fixed zephyr numbers	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2311.10702', 'html': None, 'tex': '/src/2311.10702', 'doi': 'https://doi.org/10.48550/arXiv.2311.10702'}	Submission history From: Hamish Ivison [ view email ] [v1] Fri, 17 Nov 2023 18:45:45 UTC (8,125 KB) [v2] Mon, 20 Nov 2023 02:01:33 UTC (8,125 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.10702'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.10702'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.10702'}]
2023-11-19	Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models	Computation and Language	https://arxiv.org/abs/2311.09210	Chain-of-Note	https://x.com/omarsar0/status/1725181141693472959?s=20		2311.09210	['Wenhao Yu', 'Hongming Zhang', 'Xiaoman Pan', 'Kaixin Ma', 'Hongwei Wang', 'Dong Yu']	"ct:Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with ""unknown"" when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope."	2024 (main conference)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2311.09210', 'html': 'https://arxiv.org/html/2311.09210v2', 'tex': '/src/2311.09210', 'doi': 'https://doi.org/10.48550/arXiv.2311.09210'}	Submission history From: Wenhao Yu [ view email ] [v1] Wed, 15 Nov 2023 18:54:53 UTC (1,043 KB) [v2] Thu, 3 Oct 2024 04:35:39 UTC (356 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.09210'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.09210'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.09210'}]
2023-11-19	The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4	Computation and Language	https://arxiv.org/abs/2311.07361	LLMs for Scientific Discovery	https://x.com/omarsar0/status/1724465107046940893?s=20		2311.07361	['Microsoft Research AI4Science', 'Microsoft Azure Quantum']	ct:In recent years, groundbreaking advancements in natural language processing have culminated in the emergence of powerful large language models (LLMs), which have showcased remarkable capabilities across a vast array of domains, including the understanding, generation, and translation of natural language, and even tasks that extend beyond language processing. In this report, we delve into the performance of LLMs within the context of scientific discovery, focusing on GPT-4, the state-of-the-art language model. Our investigation spans a diverse range of scientific areas encompassing drug discovery, biology, computational chemistry (density functional theory (DFT) and molecular dynamics (MD)), materials design, and partial differential equations (PDE). Evaluating GPT-4 on scientific tasks is crucial for uncovering its potential across various research domains, validating its domain-specific expertise, accelerating scientific progress, optimizing resource allocation, guiding future model development, and fostering interdisciplinary research. Our exploration methodology primarily consists of expert-driven case assessments, which offer qualitative insights into the model's comprehension of intricate scientific concepts and relationships, and occasionally benchmark testing, which quantitatively evaluates the model's capacity to solve well-defined domain-specific problems. Our preliminary exploration indicates that GPT-4 exhibits promising potential for a variety of scientific applications, demonstrating its aptitude for handling complex problem-solving and knowledge integration tasks. Broadly speaking, we evaluate GPT-4's knowledge base, scientific understanding, scientific numerical calculation abilities, and various scientific prediction capabilities.	ges report; 181 pages for main contents	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2311.07361', 'html': None, 'tex': '/src/2311.07361', 'doi': 'https://doi.org/10.48550/arXiv.2311.07361'}	Submission history From: Lijun Wu [ view email ] [v1] Mon, 13 Nov 2023 14:26:12 UTC (9,573 KB) [v2] Fri, 8 Dec 2023 06:30:12 UTC (13,435 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.07361'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.07361'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.07361'}]
2023-11-19	Fine-tuning Language Models for Factuality	Computation and Language	https://arxiv.org/abs/2311.08401	Fine-Tuning LLMs for Factuality	https://x.com/arankomatsuzaki/status/1724613041155608951?s=20		2311.08401	['Katherine Tian', 'Eric Mitchell', 'Huaxiu Yao', 'Christopher D. Manning', 'Chelsea Finn']	ct:The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.08401', 'html': None, 'tex': '/src/2311.08401', 'doi': 'https://doi.org/10.48550/arXiv.2311.08401'}	Submission history From: Eric A Mitchell [ view email ] [v1] Tue, 14 Nov 2023 18:59:15 UTC (970 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.08401'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.08401'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.08401'}]
2023-11-19	Contrastive Chain-of-Thought Prompting	Computation and Language	https://arxiv.org/abs/2311.09277	Contrastive CoT Prompting	https://x.com/arankomatsuzaki/status/1725340150819905723?s=20		2311.09277	['Yew Ken Chia', 'Guizhen Chen', 'Luu Anh Tuan', 'Soujanya Poria', 'Lidong Bing']	ct:Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2311.09277', 'html': None, 'tex': '/src/2311.09277', 'doi': 'https://doi.org/10.48550/arXiv.2311.09277'}	Submission history From: Yew Ken Chia [ view email ] [v1] Wed, 15 Nov 2023 18:54:01 UTC (2,817 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.09277'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.09277'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.09277'}]
2023-11-19	A Survey on Language Models for Code	Computation and Language	https://arxiv.org/abs/2311.07989v1	A Survey on Language Models for Code	https://x.com/omarsar0/status/1725637165256761553?s=20		2311.07989v1	['Ziyin Zhang', 'Chaoyu Chen', 'Bingchang Liu', 'Cong Liao', 'Zi Gong', 'Hang Yu', 'Jianguo Li', 'Rui Wang']	ct:In this work we systematically review the recent advancements in code processing with language models, covering 50+ models, 30+ evaluation tasks, and 500 related works. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also discuss code-specific features such as AST, CFG, and unit tests, along with their application in training code language models, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on github repository atthis https URL.	s available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2311.07989v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2311.07989'}	Submission history From: Ziyin Zhang [ view email ] [v1] Tue, 14 Nov 2023 08:34:26 UTC (458 KB) [v2] Sun, 19 Nov 2023 08:37:31 UTC (473 KB) [v3] Tue, 5 Dec 2023 13:09:06 UTC (542 KB) [v4] Mon, 22 Jan 2024 12:27:47 UTC (574 KB) [v5] Tue, 16 Apr 2024 06:19:46 UTC (580 KB) [v6] Tue, 4 Jun 2024 12:39:47 UTC (617 KB) [v7] Wed, 26 Jun 2024 07:11:00 UTC (621 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.07989'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.07989'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.07989'}]
2023-11-19	JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models	Artificial Intelligence	https://arxiv.org/abs/2311.05997	JARVIS-1	https://x.com/arankomatsuzaki/status/1723882043514470629?s=20		2311.05997	['Zihao Wang', 'Shaofei Cai', 'Anji Liu', 'Yonggang Jin', 'Jinbing Hou', 'Bowei Zhang', 'Haowei Lin', 'Zhaofeng He', 'Zilong Zheng', 'Yaodong Yang', 'Xiaojian Ma', 'Yitao Liang']	"ct:Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable of completing over 200 different tasks using control and observation space similar to humans. These tasks range from short-horizon tasks, e.g., ""chopping trees"" to long-horizon tasks, e.g., ""obtaining a diamond pickaxe"". JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly perfect performance. In the classic long-term task of $\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon and more challenging tasks. The project page is available atthis https URL"	 project page	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2311.05997', 'html': None, 'tex': '/src/2311.05997', 'doi': 'https://doi.org/10.48550/arXiv.2311.05997'}	Submission history From: Zihao Wang [ view email ] [v1] Fri, 10 Nov 2023 11:17:58 UTC (8,264 KB) [v2] Wed, 22 Nov 2023 08:04:07 UTC (8,855 KB) [v3] Thu, 30 Nov 2023 07:39:48 UTC (8,854 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.05997'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.05997'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.05997'}]
2023-11-19	Learning to Filter Context for Retrieval-Augmented Generation	Computation and Language	https://arxiv.org/abs/2311.08377v1	Learning to Filter Context for RAG	https://x.com/ZhiruoW/status/1724792850079252886?s=20		2311.08377v1	['Zhiruo Wang', 'Jun Araki', 'Zhengbao Jiang', 'Md Rizwan Parvez', 'Graham Neubig']	ct:On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2311.08377v1', 'html': None, 'tex': '/src/2311.08377v1', 'doi': 'https://doi.org/10.48550/arXiv.2311.08377'}	Submission history From: Zhiruo Wang [ view email ] [v1] Tue, 14 Nov 2023 18:41:54 UTC (8,210 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.08377'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.08377'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.08377'}]
2023-11-19	MART: Improving LLM Safety with Multi-round Automatic Red-Teaming	Computation and Language	https://arxiv.org/abs/2311.07689	MART	https://x.com/AIatMeta/status/1724887918685425829?s=20		2311.07689	['Suyu Ge', 'Chunting Zhou', 'Rui Hou', 'Madian Khabsa', 'Yi-Chia Wang', 'Qifan Wang', 'Jiawei Han', 'Yuning Mao']	ct:Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses. While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them. In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM. Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning. On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2311.07689', 'html': None, 'tex': '/src/2311.07689', 'doi': 'https://doi.org/10.48550/arXiv.2311.07689'}	Submission history From: Suyu Ge [ view email ] [v1] Mon, 13 Nov 2023 19:13:29 UTC (242 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.07689'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.07689'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.07689'}]
2023-11-19	Large Language Models can Strategically Deceive their Users when Put Under Pressure	Computation and Language	https://arxiv.org/abs/2311.07590	LLMs can Deceive Users	https://x.com/ESYudkowsky/status/1725226563992715521?s=20		2311.07590	['Jérémy Scheurer', 'Mikita Balesni', 'Marius Hobbhahn']	ct:We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.07590', 'html': None, 'tex': '/src/2311.07590', 'doi': 'https://doi.org/10.48550/arXiv.2311.07590'}	Submission history From: Jérémy Scheurer [ view email ] [v1] Thu, 9 Nov 2023 17:12:44 UTC (812 KB) [v2] Mon, 27 Nov 2023 15:17:49 UTC (805 KB) [v3] Thu, 9 May 2024 07:18:43 UTC (834 KB) [v4] Mon, 15 Jul 2024 08:51:52 UTC (834 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.07590'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.07590'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.07590'}]
2023-11-12	A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions	Computation and Language	https://arxiv.org/abs/2311.05232	Hallucination in LLMs	https://x.com/omarsar0/status/1722985251129966705?s=20		2311.05232	['Lei Huang', 'Weijiang Yu', 'Weitao Ma', 'Weihong Zhong', 'Zhangyin Feng', 'Haotian Wang', 'Qianglong Chen', 'Weihua Peng', 'Xiaocheng Feng', 'Bing Qin', 'Ting Liu']	ct:The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.	ed by ACM Transactions on Information Systems (TOIS)	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2311.05232', 'html': 'https://arxiv.org/html/2311.05232v2', 'tex': '/src/2311.05232', 'doi': 'https://doi.org/10.48550/arXiv.2311.05232'}	Submission history From: Lei Huang [ view email ] [v1] Thu, 9 Nov 2023 09:25:37 UTC (983 KB) [v2] Tue, 19 Nov 2024 12:42:45 UTC (567 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.05232'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.05232'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.05232'}]
2023-11-12	Simplifying Transformer Blocks	Machine Learning	https://arxiv.org/abs/2311.01906	Simplifying Transformer Blocks	https://x.com/maksym_andr/status/1722235666724192688?s=20		2311.01906	['Bobby He', 'Thomas Hofmann']	ct:A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections & normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable.In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput, and using 15% fewer parameters.	024	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.01906', 'html': 'https://arxiv.org/html/2311.01906v2', 'tex': '/src/2311.01906', 'doi': 'https://doi.org/10.48550/arXiv.2311.01906'}	Submission history From: Bobby He [ view email ] [v1] Fri, 3 Nov 2023 13:30:52 UTC (7,026 KB) [v2] Fri, 31 May 2024 11:14:16 UTC (7,144 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.01906'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.01906'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.01906'}]
2023-11-12	Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models	Machine Learning	https://arxiv.org/abs/2311.00871	Understanding In-Context Learning Abilities in Transformers	https://x.com/abacaj/status/1721223737729581437?s=20		2311.00871	['Steve Yadlowsky', 'Lyric Doshi', 'Nilesh Tripuraneni']	ct:Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of $(x, f(x))$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2311.00871', 'html': None, 'tex': '/src/2311.00871', 'doi': 'https://doi.org/10.48550/arXiv.2311.00871'}	Submission history From: Steve Yadlowsky [ view email ] [v1] Wed, 1 Nov 2023 21:41:08 UTC (236 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.00871'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.00871'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.00871'}]
2023-11-12	Simple and Controllable Music Generation	Sound	https://arxiv.org/abs/2306.05284	MusicGen	https://x.com/AIatMeta/status/1723043913638810025?s=20		2306.05284	['Jade Copet', 'Felix Kreuk', 'Itai Gat', 'Tal Remez', 'David Kant', 'Gabriel Synnaeve', 'Yossi Adi', 'Alexandre Défossez']	ct:We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available atthis https URL	hed at Neurips 2023	['Sound (cs.SD)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2306.05284', 'html': None, 'tex': '/src/2306.05284', 'doi': 'https://doi.org/10.48550/arXiv.2306.05284'}	Submission history From: Yossi Adi [ view email ] [v1] Thu, 8 Jun 2023 15:31:05 UTC (269 KB) [v2] Tue, 7 Nov 2023 10:43:23 UTC (362 KB) [v3] Tue, 30 Jan 2024 04:49:16 UTC (1,304 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.05284'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.05284'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.05284'}]
2023-11-12	Alternating Updates for Efficient Transformers	Machine Learning	https://arxiv.org/abs/2301.13310	AltUp	https://x.com/GoogleAI/status/1722004366201418132?s=20		2301.13310	['Cenk Baykal', 'Dylan Cutler', 'Nishanth Dikkala', 'Nikhil Ghosh', 'Rina Panigrahy', 'Xin Wang']	ct:It has been well established that increasing scale in deep transformer networks leads to improved quality and performance. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. We introduce Alternating Updates (AltUp), a simple-to-implement method to increase a model's capacity without the computational burden. AltUp enables the widening of the learned representation, i.e., the token embedding, while only incurring a negligible increase in latency. AltUp achieves this by working on a subblock of the widened representation at each layer and using a predict-and-correct mechanism to update the inactivated blocks. We present extensions of AltUp, such as its applicability to the sequence dimension, and demonstrate how AltUp can be synergistically combined with existing approaches, such as Sparse Mixture-of-Experts models, to obtain efficient models with even higher capacity. Our experiments on benchmark transformer models and language tasks demonstrate the consistent effectiveness of AltUp on a diverse set of scenarios. Notably, on SuperGLUE and SQuAD benchmarks, AltUp enables up to $87\%$ speedup relative to the dense baselines at the same accuracy.		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2301.13310', 'html': None, 'tex': '/src/2301.13310', 'doi': 'https://doi.org/10.48550/arXiv.2301.13310'}	Submission history From: Xin Wang [ view email ] [v1] Mon, 30 Jan 2023 22:06:05 UTC (2,284 KB) [v2] Tue, 3 Oct 2023 21:40:41 UTC (3,091 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.13310'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.13310'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.13310'}]
2023-11-12	Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves	Computation and Language	https://arxiv.org/abs/2311.04205	Rephrase and Respond	https://x.com/QuanquanGu/status/1722364144379396513?s=20		2311.04205	['Yihe Deng', 'Weitong Zhang', 'Zixiang Chen', 'Quanquan Gu']	ct:Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models (LLMs). Such discrepancies can make LLMs interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped. In this paper, we present a method named `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of RaR, where a rephrasing LLM first rephrases the question and then passes the original and rephrased questions together to a different responding LLM. This facilitates the effective utilization of rephrased questions generated by one LLM with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive comparison between RaR and the popular Chain-of-Thought (CoT) methods, both theoretically and empirically. We show that RaR is complementary to CoT and can be combined with CoT to achieve even better performance. Our work not only contributes to enhancing LLM performance efficiently and effectively but also sheds light on a fair evaluation of LLM capabilities. Data and codes are available atthis https URL.	es, 7 figures, 22 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.04205', 'html': 'https://arxiv.org/html/2311.04205v2', 'tex': '/src/2311.04205', 'doi': 'https://doi.org/10.48550/arXiv.2311.04205'}	Submission history From: Yihe Deng [ view email ] [v1] Tue, 7 Nov 2023 18:43:34 UTC (6,444 KB) [v2] Thu, 18 Apr 2024 23:49:56 UTC (7,470 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.04205'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.04205'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.04205'}]
2023-11-12	On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2311.05332	On the Road with GPT-4V(ision)	https://x.com/arankomatsuzaki/status/1722795897359139057?s=20		2311.05332	['Licheng Wen', 'Xuemeng Yang', 'Daocheng Fu', 'Xiaofeng Wang', 'Pinlong Cai', 'Xin Li', 'Tao Ma', 'Yingxuan Li', 'Linran Xu', 'Dengke Shang', 'Zheng Zhu', 'Shaoyan Sun', 'Yeqi Bai', 'Xinyu Cai', 'Min Dou', 'Shuanglu Hu', 'Botian Shi', 'Yu Qiao']	ct:The pursuit of autonomous driving technology hinges on the sophisticated integration of perception, decision-making, and control systems. Traditional approaches, both data-driven and rule-based, have been hindered by their inability to grasp the nuance of complex driving environments and the intentions of other road users. This has been a significant bottleneck, particularly in the development of common sense reasoning and nuanced scene understanding necessary for safe and reliable autonomous driving. The advent of Visual Language Models (VLM) represents a novel frontier in realizing fully autonomous vehicle driving. This report provides an exhaustive evaluation of the latest state-of-the-art VLM, GPT-4V(ision), and its application in autonomous driving scenarios. We explore the model's abilities to understand and reason about driving scenes, make decisions, and ultimately act in the capacity of a driver. Our comprehensive tests span from basic scene recognition to complex causal reasoning and real-time decision-making under varying conditions. Our findings reveal that GPT-4V demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems. It showcases the potential to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts. However, challenges remain, particularly in direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks. These limitations underscore the need for further research and development. Project is now available on GitHub for interested parties to access and utilize: \url{this https URL}		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Robotics (cs.RO)']	{'pdf': '/pdf/2311.05332', 'html': None, 'tex': '/src/2311.05332', 'doi': 'https://doi.org/10.48550/arXiv.2311.05332'}	Submission history From: Licheng Wen [ view email ] [v1] Thu, 9 Nov 2023 12:58:37 UTC (22,074 KB) [v2] Tue, 28 Nov 2023 09:47:57 UTC (22,074 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.05332'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.05332'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.05332'}]
2023-11-12	GPT4All: An Ecosystem of Open Source Compressed Language Models	Computation and Language	https://arxiv.org/abs/2311.04931	GPT4All	https://x.com/_akhaliq/status/1722833378590793915?s=20		2311.04931	['Yuvanesh Anand', 'Zach Nussbaum', 'Adam Treat', 'Aaron Miller', 'Richard Guo', 'Ben Schmidt', 'GPT4All Community', 'Brandon Duderstadt', 'Andriy Mulyar']	ct:Large language models (LLMs) have recently achieved human-level performance on a range of professional and academic benchmarks. The accessibility of these models has lagged behind their performance. State-of-the-art LLMs require costly infrastructure; are only accessible via rate-limited, geo-locked, and censored web interfaces; and lack publicly available code and technical reports. In this paper, we tell the story of GPT4All, a popular open source repository that aims to democratize access to LLMs. We outline the technical details of the original GPT4All model family, as well as the evolution of the GPT4All project from a single model into a fully fledged open source ecosystem. It is our hope that this paper acts as both a technical overview of the original GPT4All models as well as a case study on the subsequent growth of the GPT4All open source ecosystem.	ed at NLP-OSS at EMNLP 2023	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2311.04931', 'html': None, 'tex': '/src/2311.04931', 'doi': 'https://doi.org/10.48550/arXiv.2311.04931'}	Submission history From: Andriy Mulyar [ view email ] [v1] Mon, 6 Nov 2023 23:50:20 UTC (13,092 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.04931'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.04931'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.04931'}]
2023-11-12	S-LoRA: Serving Thousands of Concurrent LoRA Adapters	Machine Learning	https://arxiv.org/abs/2311.03285v2	S-LoRA	https://x.com/ai_database/status/1722190708797592013?s=20		2311.03285v2	['Ying Sheng', 'Shiyi Cao', 'Dacheng Li', 'Coleman Hooper', 'Nicholas Lee', 'Shuo Yang', 'Christopher Chou', 'Banghua Zhu', 'Lianmin Zheng', 'Kurt Keutzer', 'Joseph E. Gonzalez', 'Ion Stoica']	"ct:The ""pretrain-then-finetune"" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. We observe that this paradigm presents significant opportunities for batched inference during serving. To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters. S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead. Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support of LoRA serving), S-LoRA can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude. As a result, S-LoRA enables scalable serving of many task-specific fine-tuned models and offers the potential for large-scale customized fine-tuning services. The code is available atthis https URL"		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Distributed, Parallel, and Cluster Computing (cs.DC)']	{'pdf': '/pdf/2311.03285v2', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2311.03285'}	Submission history From: Ying Sheng [ view email ] [v1] Mon, 6 Nov 2023 17:26:17 UTC (259 KB) [v2] Tue, 7 Nov 2023 06:59:33 UTC (274 KB) [v3] Wed, 5 Jun 2024 06:06:43 UTC (260 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.03285'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.03285'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.03285'}]
2023-11-12	FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation	Computation and Language	https://arxiv.org/abs/2310.03214	FreshLLMs	https://x.com/_akhaliq/status/1710108355157487635?s=20		2310.03214	['Tu Vu', 'Mohit Iyyer', 'Xuezhi Wang', 'Noah Constant', 'Jerry Wei', 'Jason Wei', 'Chris Tar', 'Yun-Hsuan Sung', 'Denny Zhou', 'Quoc Le', 'Thang Luong']	ct:Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such asthis http URL. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA atthis http URLand commit to updating it at regular intervals.	nt, 26 pages, 10 figures, 5 tables; Added FreshEval	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.03214', 'html': None, 'tex': '/src/2310.03214', 'doi': 'https://doi.org/10.48550/arXiv.2310.03214'}	Submission history From: Tu Vu [ view email ] [v1] Thu, 5 Oct 2023 00:04:12 UTC (2,014 KB) [v2] Wed, 22 Nov 2023 07:28:19 UTC (1,963 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.03214'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.03214'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.03214'}]
2023-11-05	Deep Learning for Day Forecasts from Sparse Observations	Physics > Atmospheric and Oceanic Physics	https://arxiv.org/abs/2306.06079	MetNet-3	https://x.com/GoogleAI/status/1719774923294687636?s=20		2306.06079	['Marcin Andrychowicz', 'Lasse Espeholt', 'Di Li', 'Samier Merchant', 'Alexander Merose', 'Fred Zyda', 'Shreya Agrawal', 'Nal Kalchbrenner']	ct:Deep neural networks offer an alternative paradigm for modeling weather conditions. The ability of neural models to make a prediction in less than a second once the data is available and to do so with very high temporal and spatial resolution, and the ability to learn directly from atmospheric observations, are just some of these models' unique advantages. Neural models trained using atmospheric observations, the highest fidelity and lowest latency data, have to date achieved good performance only up to twelve hours of lead time when compared with state-of-the-art probabilistic Numerical Weather Prediction models and only for the sole variable of precipitation. In this paper, we present MetNet-3 that extends significantly both the lead time range and the variables that an observation based neural model can predict well. MetNet-3 learns from both dense and sparse data sensors and makes predictions up to 24 hours ahead for precipitation, wind, temperature and dew point. MetNet-3 introduces a key densification technique that implicitly captures data assimilation and produces spatially dense forecasts in spite of the network training on extremely sparse targets. MetNet-3 has a high temporal and spatial resolution of, respectively, up to 2 minutes and 1 km as well as a low operational latency. We find that MetNet-3 is able to outperform the best single- and multi-member NWPs such as HRRR and ENS over the CONUS region for up to 24 hours ahead setting a new performance milestone for observation based neural models. MetNet-3 is operational and its forecasts are served in Google Search in conjunction with other models.		['Atmospheric and Oceanic Physics (physics.ao-ph)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.06079', 'html': None, 'tex': '/src/2306.06079', 'doi': 'https://doi.org/10.48550/arXiv.2306.06079'}	Submission history From: Lasse Espeholt [ view email ] [v1] Tue, 6 Jun 2023 07:07:54 UTC (7,414 KB) [v2] Mon, 12 Jun 2023 10:48:25 UTC (7,414 KB) [v3] Thu, 6 Jul 2023 07:07:49 UTC (7,414 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.06079'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.06079'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.06079'}]
2023-11-05	Evaluating Large Language Models: A Comprehensive Survey	Computation and Language	https://arxiv.org/abs/2310.19736	Evaluating LLMs	https://x.com/omarsar0/status/1719351676828602502?s=20		2310.19736	['Zishan Guo', 'Renren Jin', 'Chuang Liu', 'Yufei Huang', 'Dan Shi', 'Supryadi', 'Linhao Yu', 'Yan Liu', 'Jiaxuan Li', 'Bojian Xiong', 'Deyi Xiong']	ct:Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability.We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available atthis https URL.	ges	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2310.19736', 'html': None, 'tex': '/src/2310.19736', 'doi': 'https://doi.org/10.48550/arXiv.2310.19736'}	Submission history From: Zishan Guo [ view email ] [v1] Mon, 30 Oct 2023 17:00:52 UTC (301 KB) [v2] Tue, 31 Oct 2023 05:00:28 UTC (301 KB) [v3] Sat, 25 Nov 2023 17:35:12 UTC (301 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.19736'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.19736'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.19736'}]
2023-11-05	Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2310.19909	Battle of the Backbones	https://x.com/micahgoldblum/status/1719719308882801045?s=20		2310.19909	['Micah Goldblum', 'Hossein Souri', 'Renkun Ni', 'Manli Shu', 'Viraj Prabhu', 'Gowthami Somepalli', 'Prithvijit Chattopadhyay', 'Mark Ibrahim', 'Adrien Bardes', 'Judy Hoffman', 'Rama Chellappa', 'Andrew Gordon Wilson', 'Tom Goldstein']	ct:Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets. We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gauntlet here:this https URL	ed to NeurIPS 2023	['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.19909', 'html': None, 'tex': '/src/2310.19909', 'doi': 'https://doi.org/10.48550/arXiv.2310.19909'}	Submission history From: Micah Goldblum [ view email ] [v1] Mon, 30 Oct 2023 18:23:58 UTC (2,680 KB) [v2] Mon, 20 Nov 2023 03:05:50 UTC (2,681 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.19909'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.19909'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.19909'}]
2023-11-05	ChipNeMo: Domain-Adapted LLMs for Chip Design	Computation and Language	https://arxiv.org/abs/2311.00176	LLMs for Chip Design	https://x.com/omarsar0/status/1720066328961159387?s=20		2311.00176	['Mingjie Liu', 'Teodor-Dumitru Ene', 'Robert Kirby', 'Chris Cheng', 'Nathaniel Pinckney', 'Rongjian Liang', 'Jonah Alben', 'Himyanshu Anand', 'Sanmitra Banerjee', 'Ismet Bayraktaroglu', 'Bonita Bhaskaran', 'Bryan Catanzaro', 'Arjun Chaudhuri', 'Sharon Clay', 'Bill Dally', 'Laura Dang', 'Parikshit Deshpande', 'Siddhanth Dhodhi', 'Sameer Halepete', 'Eric Hill', 'Jiashang Hu', 'Sumit Jain', 'Ankit Jindal', 'Brucek Khailany', 'George Kokai', 'Kishor Kunal', 'Xiaowei Li', 'Charley Lind', 'Hao Liu', 'Stuart Oberman', 'Sujeet Omar', 'Ghasem Pasandi', 'Sreedhar Pratty', 'Jonathan Raiman', 'Ambar Sarkar', 'Zhengjiang Shao', 'Hanfei Sun', 'Pratik P Suthar', 'Varun Tej', 'Walker Turner', 'Kaizhe Xu', 'Haoxing Ren']	ct:ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: domain-adaptive tokenization, domain-adaptive continued pretraining, model alignment with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our evaluations demonstrate that domain-adaptive pretraining of language models, can lead to superior performance in domain related downstream tasks compared to their base LLaMA2 counterparts, without degradations in generic capabilities. In particular, our largest model, ChipNeMo-70B, outperforms the highly capable GPT-4 on two of our use cases, namely engineering assistant chatbot and EDA scripts generation, while exhibiting competitive performance on bug summarization and analysis. These results underscore the potential of domain-specific customization for enhancing the effectiveness of large language models in specialized applications.	d results for ChipNeMo-70B model	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2311.00176', 'html': 'https://arxiv.org/html/2311.00176v5', 'tex': '/src/2311.00176', 'doi': 'https://doi.org/10.48550/arXiv.2311.00176'}	Submission history From: Mingjie Liu [ view email ] [v1] Tue, 31 Oct 2023 22:35:58 UTC (1,083 KB) [v2] Mon, 13 Nov 2023 23:07:55 UTC (1,083 KB) [v3] Sun, 3 Dec 2023 00:56:36 UTC (1,083 KB) [v4] Thu, 7 Mar 2024 01:10:43 UTC (1,650 KB) [v5] Thu, 4 Apr 2024 20:18:57 UTC (1,650 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.00176'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.00176'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.00176'}]
2023-11-05	YaRN: Efficient Context Window Extension of Large Language Models	Computation and Language	https://arxiv.org/abs/2309.00071	Efficient Context Window Extension of LLMs	https://x.com/theemozilla/status/1720107186850877662?s=20		2309.00071	['Bowen Peng', 'Jeffrey Quesnelle', 'Honglu Fan', 'Enrico Shippole']	ct:Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length atthis https URL		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2309.00071', 'html': None, 'tex': '/src/2309.00071', 'doi': 'https://doi.org/10.48550/arXiv.2309.00071'}	Submission history From: Jeffrey Quesnelle [ view email ] [v1] Thu, 31 Aug 2023 18:18:07 UTC (42 KB) [v2] Wed, 1 Nov 2023 17:28:26 UTC (354 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.00071'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.00071'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.00071'}]
2023-11-05	The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct Air Capture	Condensed Matter > Materials Science	https://arxiv.org/abs/2311.00341	Open DAC 2023	https://x.com/AIatMeta/status/1720143486505341128?s=20		2311.00341	['Anuroop Sriram', 'Sihoon Choi', 'Xiaohan Yu', 'Logan M. Brabson', 'Abhishek Das', 'Zachary Ulissi', 'Matt Uyttendaele', 'Andrew J. Medford', 'David S. Sholl']	ct:New methods for carbon dioxide removal are urgently needed to combat global climate change. Direct air capture (DAC) is an emerging technology to capture carbon dioxide directly from ambient air. Metal-organic frameworks (MOFs) have been widely studied as potentially customizable adsorbents for DAC. However, discovering promising MOF sorbents for DAC is challenging because of the vast chemical space to explore and the need to understand materials as functions of humidity and temperature. We explore a computational approach benefiting from recent innovations in machine learning (ML) and present a dataset named Open DAC 2023 (ODAC23) consisting of more than 38M density functional theory (DFT) calculations on more than 8,400 MOF materials containing adsorbed $CO_2$ and/or $H_2O$. ODAC23 is by far the largest dataset of MOF adsorption calculations at the DFT level of accuracy currently available. In addition to probing properties of adsorbed molecules, the dataset is a rich source of information on structural relaxation of MOFs, which will be useful in many contexts beyond specific applications for DAC. A large number of MOFs with promising properties for DAC are identified directly in ODAC23. We also trained state-of-the-art ML models on this dataset to approximate calculations at the DFT level. This open-source dataset and our initial ML models will provide an important baseline for future efforts to identify MOFs for a wide range of applications, including DAC.		['Materials Science (cond-mat.mtrl-sci)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2311.00341', 'html': None, 'tex': '/src/2311.00341', 'doi': 'https://doi.org/10.48550/arXiv.2311.00341'}	Submission history From: Anuroop Sriram [ view email ] [v1] Wed, 1 Nov 2023 07:21:08 UTC (8,336 KB) [v2] Mon, 27 Nov 2023 05:51:13 UTC (8,962 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.00341'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.00341'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.00341'}]
2023-11-05	A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning	Machine Learning	https://arxiv.org/abs/2311.00212	Symmetry in Machine Learning	https://x.com/eigensteve/status/1720115655050227911?s=20		2311.00212	['Samuel E. Otto', 'Nicholas Zolman', 'J. Nathan Kutz', 'Steven L. Brunton']	ct:Symmetry is present throughout nature and continues to play an increasingly central role in physics and machine learning. Fundamental symmetries, such as Poincaré invariance, allow physical laws discovered in laboratories on Earth to be extrapolated to the farthest reaches of the universe. Symmetry is essential to achieving this extrapolatory power in machine learning applications. For example, translation invariance in image classification allows models with fewer parameters, such as convolutional neural networks, to be trained on smaller data sets and achieve state-of-the-art performance. In this paper, we provide a unifying theoretical and methodological framework for incorporating symmetry into machine learning models in three ways: 1. enforcing known symmetry when training a model; 2. discovering unknown symmetries of a given model or data set; and 3. promoting symmetry during training by learning a model that breaks symmetries within a user-specified group of candidates when there is sufficient evidence in the data. We show that these tasks can be cast within a common mathematical framework whose central object is the Lie derivative associated with fiber-linear Lie group actions on vector bundles. We extend and unify several existing results by showing that enforcing and discovering symmetry are linear-algebraic tasks that are dual with respect to the bilinear structure of the Lie derivative. We also propose a novel way to promote symmetry by introducing a class of convex regularization functions based on the Lie derivative and nuclear norm relaxation to penalize symmetry breaking during training of machine learning models. We explain how these ideas can be applied to a wide range of machine learning models including basis function regression, dynamical systems discovery, neural networks, and neural operators acting on fields.		['Machine Learning (cs.LG)', 'Differential Geometry (math.DG)', 'Numerical Analysis (math.NA)']	{'pdf': '/pdf/2311.00212', 'html': 'https://arxiv.org/html/2311.00212v3', 'tex': '/src/2311.00212', 'doi': 'https://doi.org/10.48550/arXiv.2311.00212'}	Submission history From: Samuel Otto [ view email ] [v1] Wed, 1 Nov 2023 01:19:54 UTC (1,939 KB) [v2] Mon, 19 Aug 2024 17:48:03 UTC (7,122 KB) [v3] Wed, 11 Jun 2025 18:48:38 UTC (10,596 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2311.00212'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2311.00212'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2311.00212'}]
2023-11-05	Large Language Models Understand and Can be Enhanced by Emotional Stimuli	Computation and Language	https://arxiv.org/abs/2307.11760	Enhancing LLMs by Emotion Stimuli	https://x.com/emollick/status/1720135672764285176?s=20		2307.11760	['Cheng Li', 'Jindong Wang', 'Yixuan Zhang', 'Kaijie Zhu', 'Wenxin Hou', 'Jianxun Lian', 'Fang Luo', 'Qiang Yang', 'Xing Xie']	"ct:Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call ""EmotionPrompt"" that combines the original prompt with emotional stimuli), e.g., 8.00% relative performance improvement in Instruction Induction and 115% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction."	cal report; updated the std error for human study; short version (v1) was accepted by LLM@IJCAI'23; 32 pages; more work:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2307.11760', 'html': None, 'tex': '/src/2307.11760', 'doi': 'https://doi.org/10.48550/arXiv.2307.11760'}	Submission history From: Jindong Wang [ view email ] [v1] Fri, 14 Jul 2023 00:57:12 UTC (817 KB) [v2] Tue, 25 Jul 2023 03:30:32 UTC (817 KB) [v3] Tue, 1 Aug 2023 01:58:11 UTC (817 KB) [v4] Wed, 20 Sep 2023 15:46:15 UTC (2,852 KB) [v5] Fri, 20 Oct 2023 09:01:19 UTC (2,825 KB) [v6] Mon, 6 Nov 2023 02:50:08 UTC (2,825 KB) [v7] Sun, 12 Nov 2023 12:35:41 UTC (2,835 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.11760'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.11760'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.11760'}]
2023-11-05	FP8-LM: Training FP8 Large Language Models	Machine Learning	https://arxiv.org/abs/2310.18313	FP8-LM	https://x.com/arankomatsuzaki/status/1718813303223222765?s=20		2310.18313	['Houwen Peng', 'Kan Wu', 'Yixuan Wei', 'Guoshuai Zhao', 'Yuxiang Yang', 'Ze Liu', 'Yifan Xiong', 'Ziyue Yang', 'Bolin Ni', 'Jingcheng Hu', 'Ruihang Li', 'Miaosen Zhang', 'Chen Li', 'Jia Ning', 'Ruizhe Wang', 'Zheng Zhang', 'Shuguang Liu', 'Joe Chau', 'Han Hu', 'Peng Cheng']	ct:In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 39% reduction in real memory usage but also ran 75% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 37%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at {this https URL}{this http URL}.		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.18313', 'html': 'https://arxiv.org/html/2310.18313v2', 'tex': '/src/2310.18313', 'doi': 'https://doi.org/10.48550/arXiv.2310.18313'}	Submission history From: Kan Wu [ view email ] [v1] Fri, 27 Oct 2023 17:59:51 UTC (400 KB) [v2] Tue, 19 Dec 2023 12:27:58 UTC (423 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.18313'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.18313'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.18313'}]
2023-10-29	Zephyr: Direct Distillation of LM Alignment	Machine Learning	https://arxiv.org/abs/2310.16944	Zephyr LLM	https://x.com/nazneenrajani/status/1717747969842417723?s=20		2310.16944	['Lewis Tunstall', 'Edward Beeching', 'Nathan Lambert', 'Nazneen Rajani', 'Kashif Rasul', 'Younes Belkada', 'Shengyi Huang', 'Leandro von Werra', 'Clémentine Fourrier', 'Nathan Habib', 'Nathan Sarrazin', 'Omar Sanseviero', 'Alexander M. Rush', 'Thomas Wolf']	ct:We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available atthis https URL.		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.16944', 'html': None, 'tex': '/src/2310.16944', 'doi': 'https://doi.org/10.48550/arXiv.2310.16944'}	Submission history From: Alexander M. Rush [ view email ] [v1] Wed, 25 Oct 2023 19:25:16 UTC (3,722 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.16944'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.16944'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.16944'}]
2023-10-29	The Perils & Promises of Fact-checking with Large Language Models	Computation and Language	https://arxiv.org/abs/2310.13549	Fact-checking with LLMs	https://x.com/omarsar0/status/1717550929145119212?s=20		2310.13549	['Dorian Quelle', 'Alexandre Bovet']	ct:Automated fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Understanding the capacities and limitations of LLMs in fact-checking tasks is therefore essential for ensuring the health of our information ecosystem. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.		['Computation and Language (cs.CL)', 'Computers and Society (cs.CY)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2310.13549', 'html': None, 'tex': '/src/2310.13549', 'doi': 'https://doi.org/10.48550/arXiv.2310.13549'}	Submission history From: Dorian Christoph Quelle [ view email ] [v1] Fri, 20 Oct 2023 14:49:47 UTC (551 KB) [v2] Wed, 7 Feb 2024 12:01:49 UTC (9,093 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.13549'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.13549'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.13549'}]
2023-10-29	Matryoshka Diffusion Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2310.15111	Matryoshka Diffusion Models	https://x.com/thoma_gu/status/1716923384846856691?s=20		2310.15111	['Jiatao Gu', 'Shuangfei Zhai', 'Yizhe Zhang', 'Josh Susskind', 'Navdeep Jaitly']	ct:Diffusion models are the de facto approach for generating high-quality images and videos, but learning high-dimensional models remains a formidable task due to computational and optimization challenges. Existing methods often resort to training cascaded models in pixel space or using a downsampled latent space of a separately trained auto-encoder. In this paper, we introduce Matryoshka Diffusion Models(MDM), an end-to-end framework for high-resolution image and video synthesis. We propose a diffusion process that denoises inputs at multiple resolutions jointly and uses a NestedUNet architecture where features and parameters for small-scale inputs are nested within those of large scales. In addition, MDM enables a progressive training schedule from lower to higher resolutions, which leads to significant improvements in optimization for high-resolution generation. We demonstrate the effectiveness of our approach on various benchmarks, including class-conditioned image generation, high-resolution text-to-image, and text-to-video applications. Remarkably, we can train a single pixel-space model at resolutions of up to 1024x1024 pixels, demonstrating strong zero-shot generalization using the CC12M dataset, which contains only 12 million images. Our code is released atthis https URL	ed by ICLR2024	['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.15111', 'html': 'https://arxiv.org/html/2310.15111v2', 'tex': '/src/2310.15111', 'doi': 'https://doi.org/10.48550/arXiv.2310.15111'}	Submission history From: Jiatao Gu [ view email ] [v1] Mon, 23 Oct 2023 17:20:01 UTC (42,769 KB) [v2] Fri, 30 Aug 2024 19:21:36 UTC (42,798 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.15111'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.15111'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.15111'}]
2023-10-29	Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM	Computation and Language	https://arxiv.org/abs/2305.15255	Spectron	https://x.com/GoogleAI/status/1717584836834001066?s=20		2305.15255	['Eliya Nachmani', 'Alon Levkovitch', 'Roy Hirsch', 'Julian Salazar', 'Chulayuth Asawaroengchai', 'Soroosh Mariooryad', 'Ehud Rivlin', 'RJ Skerry-Ryan', 'Michelle Tadmor Ramanovich']	ct:We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (this https URL) and spoken QA dataset (this https URL).	024 camera-ready	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Sound (cs.SD)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2305.15255', 'html': 'https://arxiv.org/html/2305.15255v4', 'tex': '/src/2305.15255', 'doi': 'https://doi.org/10.48550/arXiv.2305.15255'}	Submission history From: Julian Salazar [ view email ] [v1] Wed, 24 May 2023 15:39:43 UTC (184 KB) [v2] Thu, 1 Jun 2023 08:04:19 UTC (184 KB) [v3] Fri, 20 Oct 2023 05:55:39 UTC (225 KB) [v4] Fri, 31 May 2024 01:29:27 UTC (228 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.15255'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.15255'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.15255'}]
2023-10-29	ALCUNA: Large Language Models Meet New Knowledge	Computation and Language	https://arxiv.org/abs/2310.14820	LLMs Meet New Knowledge	https://x.com/omarsar0/status/1716817266195796186?s=20		2310.14820	['Xunjian Yin', 'Baizhou Huang', 'Xiaojun Wan']	ct:With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models' capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs' abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model's understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.	2023	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.14820', 'html': None, 'tex': '/src/2310.14820', 'doi': 'https://doi.org/10.48550/arXiv.2310.14820'}	Submission history From: Xunjian Yin [ view email ] [v1] Mon, 23 Oct 2023 11:40:05 UTC (587 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.14820'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.14820'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.14820'}]
2023-10-29	Detecting Pretraining Data from Large Language Models	Computation and Language	https://arxiv.org/abs/2310.16789	Detecting Pretraining Data from LLMs	https://x.com/WeijiaShi2/status/1717612387174687150?s=20		2310.16789	['Weijia Shi', 'Anirudh Ajith', 'Mengzhou Xia', 'Yangsibo Huang', 'Daogao Liu', 'Terra Blevins', 'Danqi Chen', 'Luke Zettlemoyer']	ct:Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to three real-world scenarios, copyrighted book detection, contaminated downstream example detection and privacy auditing of machine unlearning, and find it a consistently effective solution.		['Computation and Language (cs.CL)', 'Cryptography and Security (cs.CR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.16789', 'html': 'https://arxiv.org/html/2310.16789v3', 'tex': '/src/2310.16789', 'doi': 'https://doi.org/10.48550/arXiv.2310.16789'}	Submission history From: Weijia Shi [ view email ] [v1] Wed, 25 Oct 2023 17:21:23 UTC (2,405 KB) [v2] Fri, 3 Nov 2023 05:27:37 UTC (2,404 KB) [v3] Sat, 9 Mar 2024 22:26:06 UTC (2,404 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.16789'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.16789'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.16789'}]
2023-10-29	ConvNets Match Vision Transformers at Scale	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2310.16764	ConvNets Match Vision Transformers	https://x.com/_akhaliq/status/1717385905214759421?s=20		2310.16764	['Samuel L. Smith', 'Andrew Brock', 'Leonard Berrada', 'Soham De']	ct:Many researchers believe that ConvNets perform well on small or moderately sized datasets, but are not competitive with Vision Transformers when given access to datasets on the web-scale. We challenge this belief by evaluating a performant ConvNet architecture pre-trained on JFT-4B, a large labelled dataset of images often used for training foundation models. We consider pre-training compute budgets between 0.4k and 110k TPU-v4 core compute hours, and train a series of networks of increasing depth and width from the NFNet model family. We observe a log-log scaling law between held out loss and compute budget. After fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers with comparable compute budgets. Our strongest fine-tuned model achieves a Top-1 accuracy of 90.4%.		['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)', 'Neural and Evolutionary Computing (cs.NE)']	{'pdf': '/pdf/2310.16764', 'html': None, 'tex': '/src/2310.16764', 'doi': 'https://doi.org/10.48550/arXiv.2310.16764'}	Submission history From: Samuel L. Smith [ view email ] [v1] Wed, 25 Oct 2023 16:52:13 UTC (75 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.16764'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.16764'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.16764'}]
2023-10-29	CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2310.16825	CommonCanvas	https://x.com/iScienceLuvr/status/1717359916422496596?s=20		2310.16825	['Aaron Gokaslan', 'A. Feder Cooper', 'Jasmine Collins', 'Landan Seguin', 'Austin Jacobson', 'Mihir Patel', 'Jonathan Frankle', 'Cory Stephenson', 'Volodymyr Kuleshov']	ct:We assemble a dataset of Creative-Commons-licensed (CC) images, which we use to train a set of open diffusion models that are qualitatively competitive with Stable Diffusion 2 (SD2). This task presents two challenges: (1) high-resolution CC images lack the captions necessary to train text-to-image generative models; (2) CC images are relatively scarce. In turn, to address these challenges, we use an intuitive transfer learning technique to produce a set of high-quality synthetic captions paired with curated CC images. We then develop a data- and compute-efficient training recipe that requires as little as 3% of the LAION-2B data needed to train existing SD2 models, but obtains comparable quality. These results indicate that we have a sufficient number of CC images (~70 million) for training high-quality models. Our training recipe also implements a variety of optimizations that achieve ~3X training speed-ups, enabling rapid model iteration. We leverage this recipe to train several high-quality text-to-image models, which we dub the CommonCanvas family. Our largest model achieves comparable performance to SD2 on a human evaluation, despite being trained on our CC dataset that is significantly smaller than LAION and using synthetic captions for training. We release our models, data, and code atthis https URL		['Computer Vision and Pattern Recognition (cs.CV)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2310.16825', 'html': None, 'tex': '/src/2310.16825', 'doi': 'https://doi.org/10.48550/arXiv.2310.16825'}	Submission history From: Aaron Gokaslan [ view email ] [v1] Wed, 25 Oct 2023 17:56:07 UTC (36,607 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.16825'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.16825'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.16825'}]
2023-10-29	Branch-Solve-Merge Improves Large Language Model Evaluation and Generation	Computation and Language	https://arxiv.org/abs/2310.15123	Branch-Solve-Merge Reasoning in LLMs	https://x.com/jaseweston/status/1716635331393380619?s=20		2310.15123	['Swarnadeep Saha', 'Omer Levy', 'Asli Celikyilmaz', 'Mohit Bansal', 'Jason Weston', 'Xian Li']	ct:Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model's lack of coherence and inability to plan and decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%, and allowing LLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story generation task, BSM improves the coherence of stories while also improving constraint satisfaction by 12%.	2024 (19 pages, 7 figures, 11 tables)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.15123', 'html': 'https://arxiv.org/html/2310.15123v2', 'tex': '/src/2310.15123', 'doi': 'https://doi.org/10.48550/arXiv.2310.15123'}	Submission history From: Swarnadeep Saha [ view email ] [v1] Mon, 23 Oct 2023 17:29:48 UTC (583 KB) [v2] Fri, 7 Jun 2024 16:08:49 UTC (8,235 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.15123'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.15123'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.15123'}]
2023-10-22	Llemma: An Open Language Model For Mathematics	Computation and Language	https://arxiv.org/abs/2310.10631	Llemma	https://x.com/zhangir_azerbay/status/1714098025956864031?s=20		2310.10631	['Zhangir Azerbayev', 'Hailey Schoelkopf', 'Keiran Paster', 'Marco Dos Santos', 'Stephen McAleer', 'Albert Q. Jiang', 'Jia Deng', 'Stella Biderman', 'Sean Welleck']	ct:We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.	d references; corrected description of COPRA search budget	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Logic in Computer Science (cs.LO)']	{'pdf': '/pdf/2310.10631', 'html': 'https://arxiv.org/html/2310.10631v3', 'tex': '/src/2310.10631', 'doi': 'https://doi.org/10.48550/arXiv.2310.10631'}	Submission history From: Zhangir Azerbayev Mr [ view email ] [v1] Mon, 16 Oct 2023 17:54:07 UTC (386 KB) [v2] Fri, 1 Dec 2023 03:51:33 UTC (116 KB) [v3] Fri, 15 Mar 2024 19:14:39 UTC (116 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.10631'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.10631'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.10631'}]
2023-10-22	Large Language Models for Software Engineering: Survey and Open Problems	Software Engineering	https://arxiv.org/abs/2310.03533	LLMs for Software Engineering	https://x.com/omarsar0/status/1713940983199506910?s=20		2310.03533	['Angela Fan', 'Beliz Gokkaya', 'Mark Harman', 'Mitya Lyubarskiy', 'Shubho Sengupta', 'Shin Yoo', 'Jie M. Zhang']	ct:This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.		['Software Engineering (cs.SE)']	{'pdf': '/pdf/2310.03533', 'html': None, 'tex': '/src/2310.03533', 'doi': 'https://doi.org/10.48550/arXiv.2310.03533'}	Submission history From: Jie Zhang [ view email ] [v1] Thu, 5 Oct 2023 13:33:26 UTC (1,952 KB) [v2] Sun, 8 Oct 2023 20:22:31 UTC (1,953 KB) [v3] Wed, 11 Oct 2023 14:47:22 UTC (1,953 KB) [v4] Sat, 11 Nov 2023 21:15:19 UTC (1,955 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.03533'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.03533'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.03533'}]
2023-10-22	Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection	Computation and Language	https://arxiv.org/abs/2310.11511	Self-RAG	https://x.com/AkariAsai/status/1715110277077962937?s=20		2310.11511	['Akari Asai', 'Zeqiu Wu', 'Yizhong Wang', 'Avirup Sil', 'Hannaneh Hajishirzi']	ct:Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.	es, 2 figures, 12 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.11511', 'html': None, 'tex': '/src/2310.11511', 'doi': 'https://doi.org/10.48550/arXiv.2310.11511'}	Submission history From: Akari Asai [ view email ] [v1] Tue, 17 Oct 2023 18:18:32 UTC (896 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.11511'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.11511'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.11511'}]
2023-10-22	Understanding Retrieval Augmentation for Long-Form Question Answering	Computation and Language	https://arxiv.org/abs/2310.12150	Retrieval-Augmentation for Long-form Question Answering	https://x.com/omarsar0/status/1714986431859282144?s=20		2310.12150	['Hung-Ting Chen', 'Fangyuan Xu', 'Shane Arora', 'Eunsol Choi']	ct:We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.12150', 'html': None, 'tex': '/src/2310.12150', 'doi': 'https://doi.org/10.48550/arXiv.2310.12150'}	Submission history From: Hung-Ting Chen [ view email ] [v1] Wed, 18 Oct 2023 17:59:10 UTC (8,466 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.12150'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.12150'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.12150'}]
2023-10-22	Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations	Computation and Language	https://arxiv.org/abs/2310.11207	A Study of LLM-Generated Self-Explanations	https://x.com/omarsar0/status/1714665747752923620?s=20		2310.11207	['Shiyuan Huang', 'Siddarth Mamidanna', 'Shreedhar Jangam', 'Yilun Zhou', 'Leilani H. Gilpin']	"ct:Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce ""helpful"" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as ""fantastic"" and ""memorable"" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs."		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.11207', 'html': None, 'tex': '/src/2310.11207', 'doi': 'https://doi.org/10.48550/arXiv.2310.11207'}	Submission history From: Yilun Zhou [ view email ] [v1] Tue, 17 Oct 2023 12:34:32 UTC (460 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.11207'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.11207'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.11207'}]
2023-10-22	OpenAgents: An Open Platform for Language Agents in the Wild	Computation and Language	https://arxiv.org/abs/2310.10634v1	OpenAgents	https://x.com/ChengZhoujun/status/1714343204148113860?s=20		2310.10634v1	['Tianbao Xie', 'Fan Zhou', 'Zhoujun Cheng', 'Peng Shi', 'Luoxuan Weng', 'Yitao Liu', 'Toh Jing Hua', 'Junning Zhao', 'Qian Liu', 'Che Liu', 'Leo Z. Liu', 'Yiheng Xu', 'Hongjin Su', 'Dongchan Shin', 'Caiming Xiong', 'Tao Yu']	ct:Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.	es, 8 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2310.10634v1', 'html': None, 'tex': '/src/2310.10634v1', 'doi': 'https://doi.org/10.48550/arXiv.2310.10634'}	Submission history From: Fan Zhou [ view email ] [v1] Mon, 16 Oct 2023 17:54:53 UTC (7,589 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.10634'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.10634'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.10634'}]
2023-10-22	Eliciting Human Preferences with Language Models	Computation and Language	https://arxiv.org/abs/2310.11589	Eliciting Human Preferences with LLMs	https://x.com/AlexTamkin/status/1715040019520569395?s=20		2310.11589	['Belinda Z. Li', 'Alex Tamkin', 'Noah Goodman', 'Jacob Andreas']	ct:Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitation requires less effort than prompting or example labeling and surfaces novel considerations not initially anticipated by users. Our findings suggest that LM-driven elicitation can be a powerful tool for aligning models to complex human preferences and values.	es, 15 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.11589', 'html': None, 'tex': '/src/2310.11589', 'doi': 'https://doi.org/10.48550/arXiv.2310.11589'}	Submission history From: Belinda Z. Li [ view email ] [v1] Tue, 17 Oct 2023 21:11:21 UTC (3,025 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.11589'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.11589'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.11589'}]
2023-10-22	AutoMix: Automatically Mixing Language Models	Computation and Language	https://arxiv.org/abs/2310.12963	AutoMix	https://x.com/omarsar0/status/1715385477627334718?s=20		2310.12963	['Pranjal Aggarwal', 'Aman Madaan', 'Ankit Anand', 'Srividya Pranavi Potharaju', 'Swaroop Mishra', 'Pei Zhou', 'Aditya Gupta', 'Dheeraj Rajagopal', 'Karthik Kappaganthu', 'Yiming Yang', 'Shyam Upadhyay', 'Manaal Faruqui', 'Mausam']	ct:Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present Automix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to Automix are two key technical contributions. First, it has a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring extensive training. Second, given that self-verification can be noisy, it employs a POMDP based router that can effectively select an appropriately sized model, based on answer confidence. Experiments across five language models and five challenging datasets show that Automix consistently surpasses strong baselines, reducing computational cost by over 50% for comparable performance.	onference on Neural Information Processing Systems (NeurIPS 2024). The first two authors contributed equally. Work started and partly done during Aman's internship at Google. This version adds results on additional models and datasets	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2310.12963', 'html': None, 'tex': '/src/2310.12963', 'doi': 'https://doi.org/10.48550/arXiv.2310.12963'}	Submission history From: Pranjal Aggarwal [ view email ] [v1] Thu, 19 Oct 2023 17:57:39 UTC (7,208 KB) [v2] Wed, 15 Nov 2023 18:23:40 UTC (621 KB) [v3] Wed, 20 Mar 2024 16:36:06 UTC (469 KB) [v4] Fri, 28 Jun 2024 17:57:05 UTC (417 KB) [v5] Sun, 19 Jan 2025 15:59:56 UTC (592 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.12963'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.12963'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.12963'}]
2023-10-22	Video Language Planning	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2310.10625	Video Language Planning	https://x.com/du_yilun/status/1714297584842318157?s=20		2310.10625	['Yilun Du', 'Mengjiao Yang', 'Pete Florence', 'Fei Xia', 'Ayzaan Wahid', 'Brian Ichter', 'Pierre Sermanet', 'Tianhe Yu', 'Pieter Abbeel', 'Joshua B. Tenenbaum', 'Leslie Kaelbling', 'Andy Zeng', 'Jonathan Tompson']	ct:We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data. To this end, we present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP scales with increasing computation budget where more computation time results in improved video plans, and is able to synthesize long-horizon video plans across different robotics domains: from multi-object rearrangement, to multi-camera bi-arm dexterous manipulation. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video. Experiments show that VLP substantially improves long-horizon task success rates compared to prior methods on both simulated and real robots (across 3 hardware platforms).	ttps URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Robotics (cs.RO)']	{'pdf': '/pdf/2310.10625', 'html': None, 'tex': '/src/2310.10625', 'doi': 'https://doi.org/10.48550/arXiv.2310.10625'}	Submission history From: Yilun Du [ view email ] [v1] Mon, 16 Oct 2023 17:48:45 UTC (5,465 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.10625'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.10625'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.10625'}]
2023-10-15	Ring Attention with Blockwise Transformers for Near-Infinite Context	Computation and Language	https://arxiv.org/abs/2310.01889	Ring Attention	https://x.com/haoliuhl/status/1709630382457733596?s=20		2310.01889	['Hao Liu', 'Matei Zaharia', 'Pieter Abbeel']	ct:Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.	his https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.01889', 'html': None, 'tex': '/src/2310.01889', 'doi': 'https://doi.org/10.48550/arXiv.2310.01889'}	Submission history From: Hao Liu [ view email ] [v1] Tue, 3 Oct 2023 08:44:50 UTC (1,656 KB) [v2] Thu, 5 Oct 2023 06:25:34 UTC (1,664 KB) [v3] Thu, 12 Oct 2023 01:00:09 UTC (1,654 KB) [v4] Mon, 27 Nov 2023 06:38:47 UTC (1,662 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.01889'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.01889'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.01889'}]
2023-10-15	Learning Interactive Real-World Simulators	Artificial Intelligence	https://arxiv.org/abs/2310.06114	Universal Simulator	https://x.com/mengjiao_yang/status/1712153304757915925?s=20		2310.06114	['Sherry Yang', 'Yilun Du', 'Kamyar Ghasemipour', 'Jonathan Tompson', 'Leslie Kaelbling', 'Dale Schuurmans', 'Pieter Abbeel']	"ct:Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different dimensions (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, we can simulate the visual outcome of both high-level instructions such as ""open the drawer"" and low-level controls from otherwise static scenes and objects. We use the simulator to train both high-level vision-language policies and low-level reinforcement learning policies, each of which can be deployed in the real world in zero shot after training purely in simulation. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience, opening up even wider applications. Video demos can be found atthis https URL."	ttps URL	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2310.06114', 'html': 'https://arxiv.org/html/2310.06114v3', 'tex': '/src/2310.06114', 'doi': 'https://doi.org/10.48550/arXiv.2310.06114'}	Submission history From: Mengjiao Yang [ view email ] [v1] Mon, 9 Oct 2023 19:42:22 UTC (44,196 KB) [v2] Sat, 13 Jan 2024 00:42:24 UTC (36,104 KB) [v3] Thu, 26 Sep 2024 17:14:09 UTC (36,134 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.06114'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.06114'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.06114'}]
2023-10-15	Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity	Computation and Language	https://arxiv.org/abs/2310.07521	Overview of Factuality in LLMs	https://x.com/omarsar0/status/1712469661118517740?s=20		2310.07521	['Cunxiang Wang', 'Xiaoze Liu', 'Yuanhao Yue', 'Xiangru Tang', 'Tianhang Zhang', 'Cheng Jiayang', 'Yunzhi Yao', 'Wenyang Gao', 'Xuming Hu', 'Zehan Qi', 'Yidong Wang', 'Linyi Yang', 'Jindong Wang', 'Xing Xie', 'Zheng Zhang', 'Yue Zhang']	ct:This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs.	es; 300+ references	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.07521', 'html': 'https://arxiv.org/html/2310.07521v3', 'tex': '/src/2310.07521', 'doi': 'https://doi.org/10.48550/arXiv.2310.07521'}	Submission history From: Cunxiang Wang [ view email ] [v1] Wed, 11 Oct 2023 14:18:03 UTC (182 KB) [v2] Wed, 18 Oct 2023 14:09:19 UTC (187 KB) [v3] Sat, 16 Dec 2023 12:47:19 UTC (808 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.07521'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.07521'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.07521'}]
2023-10-15	Large Language Models can Learn Rules	Artificial Intelligence	https://arxiv.org/abs/2310.07064	LLMs can Learn Rules	https://x.com/zhu_zhaocheng/status/1712582734550647091?s=20		2310.07064	['Zhaocheng Zhu', 'Yuan Xue', 'Xinyun Chen', 'Denny Zhou', 'Jian Tang', 'Dale Schuurmans', 'Hanjun Dai']	ct:When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often generate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on relational reasoning, numerical reasoning and concept learning problems show that HtT improves existing prompting methods, with an absolute gain of 10-30% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.07064', 'html': None, 'tex': '/src/2310.07064', 'doi': 'https://doi.org/10.48550/arXiv.2310.07064'}	Submission history From: Zhaocheng Zhu [ view email ] [v1] Tue, 10 Oct 2023 23:07:01 UTC (253 KB) [v2] Wed, 24 Apr 2024 19:01:59 UTC (268 KB) [v3] Thu, 19 Dec 2024 19:16:27 UTC (268 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.07064'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.07064'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.07064'}]
2023-10-15	Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models	Computation and Language	https://arxiv.org/abs/2310.06692	Meta Chain-of-Thought Prompting	https://x.com/omarsar0/status/1712835499256090972?s=20		2310.06692	['Anni Zou', 'Zhuosheng Zhang', 'Hai Zhao', 'Xiangru Tang']	ct:Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on pre-defined task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose GeM-CoT, a Generalizable CoT prompting mechanism in Mixed-task scenarios where the type of input questions is unknown. GeM-CoT first categorizes the question type and subsequently samples or constructs demonstrations from the corresponding data pool in an automatic pattern. With this technical design, GeM-CoT simultaneously enjoys superior generalization capabilities and remarkable performances on 10 public reasoning tasks and 23 BBH tasks.	es, 12 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2310.06692', 'html': None, 'tex': '/src/2310.06692', 'doi': 'https://doi.org/10.48550/arXiv.2310.06692'}	Submission history From: Anni Zou [ view email ] [v1] Tue, 10 Oct 2023 15:10:03 UTC (2,864 KB) [v2] Wed, 11 Oct 2023 10:05:29 UTC (2,859 KB) [v3] Tue, 20 Feb 2024 15:27:20 UTC (6,788 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.06692'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.06692'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.06692'}]
2023-10-15	A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics	Computation and Language	https://arxiv.org/abs/2310.05694	A Survey of LLMs for Healthcare	https://x.com/omarsar0/status/1711755055777415485?s=20		2310.05694	['Kai He', 'Rui Mao', 'Qika Lin', 'Yucheng Ruan', 'Xiang Lan', 'Mengling Feng', 'Erik Cambria']	ct:The utilization of large language models (LLMs) in the Healthcare domain has generated both excitement and concern due to their ability to effectively respond to freetext queries with certain professional knowledge. This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with the aim of providing an overview of the development roadmap from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, as well as comparing various LLMs with each other. Then we summarize related Healthcare training data, training methods, optimization strategies, and usage. Finally, the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics. Our survey provide a comprehensive investigation from perspectives of both computer science and Healthcare specialty. Besides the discussion about Healthcare concerns, we supports the computer science community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations, and evaluation benchmarks in the Github. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a shift from model-centered methodologies to data-centered methodologies. Also, we determine that the biggest obstacle of using LLMs in Healthcare are fairness, accountability, transparency and ethics.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.05694', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2310.05694'}	Submission history From: Kai He [ view email ] [v1] Mon, 9 Oct 2023 13:15:23 UTC (16,955 KB) [v2] Tue, 11 Jun 2024 13:13:59 UTC (19,220 KB) [v3] Mon, 27 Jan 2025 03:50:31 UTC (3,287 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.05694'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.05694'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.05694'}]
2023-10-15	RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation	Computation and Language	https://arxiv.org/abs/2310.04408	Improving Retrieval-Augmented LMs with Compressors	https://x.com/omarsar0/status/1711384213092479130?s=20		2310.04408	['Fangyuan Xu', 'Weijia Shi', 'Eunsol Choi']	ct:Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summarythis http URLthe retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selectivethis http URLevaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide summaries largely faithful to the retrieved documents.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.04408', 'html': None, 'tex': '/src/2310.04408', 'doi': 'https://doi.org/10.48550/arXiv.2310.04408'}	Submission history From: Fangyuan Xu [ view email ] [v1] Fri, 6 Oct 2023 17:55:36 UTC (411 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.04408'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.04408'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.04408'}]
2023-10-15	InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining	Computation and Language	https://arxiv.org/abs/2310.07713	Instruct-Retro	https://x.com/omarsar0/status/1712466049428521433?s=20		2310.07713	['Boxin Wang', 'Wei Ping', 'Lawrence McAfee', 'Peng Xu', 'Bo Li', 'Mohammad Shoeybi', 'Bryan Catanzaro']	ct:Pretraining auto-regressive large language models~(LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA and reading comprehension tasks, 10% over GPT across 4 challenging long-form QA tasks, and 16% over GPT across 3 summarization tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. Our results highlight the promising direction to obtain a better GPT decoder through continued pretraining with retrieval before instruction tuning. Our code and checkpoints are publicly available at:this https URL.	024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.07713', 'html': 'https://arxiv.org/html/2310.07713v3', 'tex': '/src/2310.07713', 'doi': 'https://doi.org/10.48550/arXiv.2310.07713'}	Submission history From: Wei Ping [ view email ] [v1] Wed, 11 Oct 2023 17:59:05 UTC (1,795 KB) [v2] Wed, 31 Jan 2024 23:27:26 UTC (2,097 KB) [v3] Wed, 29 May 2024 04:15:39 UTC (2,100 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.07713'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.07713'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.07713'}]
2023-10-15	Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading	Computation and Language	https://arxiv.org/abs/2310.05029	MemWalker	https://x.com/__howardchen/status/1711584916708938042?s=20		2310.05029	['Howard Chen', 'Ramakanth Pasunuru', 'Jason Weston', 'Asli Celikyilmaz']	ct:Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.05029', 'html': None, 'tex': '/src/2310.05029', 'doi': 'https://doi.org/10.48550/arXiv.2310.05029'}	Submission history From: Howard Chen [ view email ] [v1] Sun, 8 Oct 2023 06:18:14 UTC (126 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.05029'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.05029'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.05029'}]
2023-10-15	FireAct: Toward Language Agent Fine-tuning	Computation and Language	https://arxiv.org/abs/2310.05915	Toward Language Agent Fine-tuning	https://x.com/omarsar0/status/1711757242905534479?s=20		2310.05915	['Baian Chen', 'Chang Shu', 'Ehsan Shareghi', 'Nigel Collier', 'Karthik Narasimhan', 'Shunyu Yao']	ct:Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.	data, and models are available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.05915', 'html': None, 'tex': '/src/2310.05915', 'doi': 'https://doi.org/10.48550/arXiv.2310.05915'}	Submission history From: Shunyu Yao [ view email ] [v1] Mon, 9 Oct 2023 17:58:38 UTC (1,032 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.05915'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.05915'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.05915'}]
2023-10-08	Language Models Represent Space and Time	Machine Learning	https://arxiv.org/abs/2310.02207	LLMs Represent Space and Time	https://x.com/wesg52/status/1709551516577902782?s=20		2310.02207	['Wes Gurnee', 'Max Tegmark']	"ct:The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ""space neurons"" and ""time neurons"" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model."		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.02207', 'html': 'https://arxiv.org/html/2310.02207v3', 'tex': '/src/2310.02207', 'doi': 'https://doi.org/10.48550/arXiv.2310.02207'}	Submission history From: Wes Gurnee [ view email ] [v1] Tue, 3 Oct 2023 17:06:52 UTC (6,602 KB) [v2] Thu, 14 Dec 2023 02:45:45 UTC (6,793 KB) [v3] Mon, 4 Mar 2024 18:25:29 UTC (6,793 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.02207'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.02207'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.02207'}]
2023-10-08	Retrieval meets Long Context Large Language Models	Computation and Language	https://arxiv.org/abs/2310.03025	Retrieval meets Long Context LLMs	https://x.com/omarsar0/status/1709749178199318545?s=20		2310.03025	['Peng Xu', 'Wei Ping', 'Xianchao Wu', 'Lawrence McAfee', 'Chen Zhu', 'Zihan Liu', 'Sandeep Subramanian', 'Evelina Bakhturina', 'Mohammad Shoeybi', 'Bryan Catanzaro']	ct:Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.	hed at ICLR 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Information Retrieval (cs.IR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.03025', 'html': 'https://arxiv.org/html/2310.03025v2', 'tex': '/src/2310.03025', 'doi': 'https://doi.org/10.48550/arXiv.2310.03025'}	Submission history From: Wei Ping [ view email ] [v1] Wed, 4 Oct 2023 17:59:41 UTC (388 KB) [v2] Tue, 23 Jan 2024 07:49:13 UTC (404 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.03025'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.03025'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.03025'}]
2023-10-08	Efficient Streaming Language Models with Attention Sinks	Computation and Language	https://arxiv.org/abs/2309.17453	StreamingLLM	https://x.com/Guangxuan_Xiao/status/1708943505731801325?s=20		2309.17453	['Guangxuan Xiao', 'Yuandong Tian', 'Beidi Chen', 'Song Han', 'Mike Lewis']	"ct:Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ""sink"" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided atthis https URL."	024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2309.17453', 'html': 'https://arxiv.org/html/2309.17453v4', 'tex': '/src/2309.17453', 'doi': 'https://doi.org/10.48550/arXiv.2309.17453'}	Submission history From: Guangxuan Xiao [ view email ] [v1] Fri, 29 Sep 2023 17:59:56 UTC (9,548 KB) [v2] Tue, 21 Nov 2023 05:04:49 UTC (13,230 KB) [v3] Tue, 12 Dec 2023 00:33:07 UTC (13,230 KB) [v4] Sun, 7 Apr 2024 00:56:53 UTC (13,231 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.17453'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.17453'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.17453'}]
2023-10-08	Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs	Neural and Evolutionary Computing	https://arxiv.org/abs/2307.08197	Neural Developmental Programs	https://x.com/risi1979/status/1708888992224362742?s=20		2307.08197	['Elias Najarro', 'Shyam Sudhakaran', 'Sebastian Risi']	ct:Biological nervous systems are created in a fundamentally different way than current artificial neural networks. Despite its impressive results in a variety of different domains, deep learning often requires considerable engineering effort to design high-performing neural architectures. By contrast, biological nervous systems are grown through a dynamic self-organizing process. In this paper, we take initial steps toward neural networks that grow through a developmental process that mirrors key properties of embryonic development in biological organisms. The growth process is guided by another neural network, which we call a Neural Developmental Program (NDP) and which operates through local communication alone. We investigate the role of neural growth on different machine learning benchmarks and different optimization methods (evolutionary training, online RL, offline RL, and supervised learning). Additionally, we highlight future research directions and opportunities enabled by having self-organization driving the growth of neural networks.		['Neural and Evolutionary Computing (cs.NE)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.08197', 'html': None, 'tex': '/src/2307.08197', 'doi': 'https://doi.org/10.48550/arXiv.2307.08197'}	Submission history From: Sebastian Risi [ view email ] [v1] Mon, 17 Jul 2023 01:58:52 UTC (14,847 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.08197'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.08197'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.08197'}]
2023-10-08	The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2309.17421	The Dawn of LMMs	https://x.com/omarsar0/status/1708860551110041871?s=20		2309.17421	['Zhengyuan Yang', 'Linjie Li', 'Kevin Lin', 'Jianfeng Wang', 'Chung-Ching Lin', 'Zicheng Liu', 'Lijuan Wang']	ct:Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution:this https URL		['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.17421', 'html': None, 'tex': '/src/2309.17421', 'doi': 'https://doi.org/10.48550/arXiv.2309.17421'}	Submission history From: Zhengyuan Yang [ view email ] [v1] Fri, 29 Sep 2023 17:34:51 UTC (41,898 KB) [v2] Wed, 11 Oct 2023 05:07:37 UTC (41,899 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.17421'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.17421'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.17421'}]
2023-10-08	Think before you speak: Training Language Models With Pause Tokens	Computation and Language	https://arxiv.org/abs/2310.02226	Training LLMs with Pause Tokens	https://x.com/omarsar0/status/1709573238123122959?s=20		2310.02226	['Sachin Goyal', 'Ziwei Ji', 'Ankit Singh Rawat', 'Aditya Krishna Menon', 'Sanjiv Kumar', 'Vaishnavh Nagarajan']	ct:Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\%$ EM score on the QA task of SQuAD, $8\%$ on CommonSenseQA and $1\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.	hed at ICLR 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.02226', 'html': 'https://arxiv.org/html/2310.02226v3', 'tex': '/src/2310.02226', 'doi': 'https://doi.org/10.48550/arXiv.2310.02226'}	Submission history From: Sachin Goyal [ view email ] [v1] Tue, 3 Oct 2023 17:32:41 UTC (610 KB) [v2] Wed, 13 Mar 2024 22:33:41 UTC (709 KB) [v3] Sun, 21 Apr 2024 03:39:21 UTC (628 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.02226'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.02226'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.02226'}]
2023-10-08	Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation	Computation and Language	https://arxiv.org/abs/2310.02304	Recursively Self-Improving Code Generation	https://x.com/ericzelikman/status/1709721771937587541?s=20		2310.02304	['Eric Zelikman', 'Eliana Lorch', 'Lester Mackey', 'Adam Tauman Kalai']	"ct:Several recent advances in AI systems solve problems by providing a ""scaffolding"" program that structures multiple calls to language models (LMs) to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed ""improver"" that improves an input program according to a given utility function by querying an LM several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. A variety of self-improvement strategies are proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in our experiments, is capable of writing code that can call itself to improve itself. We consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox."	hed as a conference paper at COLM 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2310.02304', 'html': None, 'tex': '/src/2310.02304', 'doi': 'https://doi.org/10.48550/arXiv.2310.02304'}	Submission history From: Lester Mackey [ view email ] [v1] Tue, 3 Oct 2023 17:59:32 UTC (198 KB) [v2] Fri, 1 Mar 2024 17:11:21 UTC (217 KB) [v3] Fri, 16 Aug 2024 17:28:08 UTC (214 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.02304'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.02304'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.02304'}]
2023-10-08	RA-DIT: Retrieval-Augmented Dual Instruction Tuning	Computation and Language	https://arxiv.org/abs/2310.01352	Retrieval-Augmented Dual Instruction Tuning	https://x.com/omarsar0/status/1709204756013490494?s=20		2310.01352	['Xi Victoria Lin', 'Xilun Chen', 'Mingda Chen', 'Weijia Shi', 'Maria Lomeli', 'Rich James', 'Pedro Rodriguez', 'Jacob Kahn', 'Gergely Szilvasy', 'Mike Lewis', 'Luke Zettlemoyer', 'Scott Yih']	ct:Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.	LR 2024 camera-ready version	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2310.01352', 'html': 'https://arxiv.org/html/2310.01352v4', 'tex': '/src/2310.01352', 'doi': 'https://doi.org/10.48550/arXiv.2310.01352'}	Submission history From: Xi Victoria Lin [ view email ] [v1] Mon, 2 Oct 2023 17:16:26 UTC (9,966 KB) [v2] Sun, 8 Oct 2023 22:05:20 UTC (10,240 KB) [v3] Sun, 5 Nov 2023 06:25:55 UTC (10,241 KB) [v4] Mon, 6 May 2024 07:50:35 UTC (10,266 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.01352'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.01352'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.01352'}]
2023-10-08	Kosmos-G: Generating Images in Context with Multimodal Large Language Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2310.02992	KOSMOG-G	https://x.com/omarsar0/status/1709934741158510625?s=20		2310.02992	['Xichen Pan', 'Li Dong', 'Shaohan Huang', 'Zhiliang Peng', 'Wenhu Chen', 'Furu Wei']	"ct:Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of ""image as a foreign language in image generation."" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of ""image as a foreign language in image generation."" The code can be found atthis https URL"	his https URLProject Page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2310.02992', 'html': None, 'tex': '/src/2310.02992', 'doi': 'https://doi.org/10.48550/arXiv.2310.02992'}	Submission history From: Xichen Pan [ view email ] [v1] Wed, 4 Oct 2023 17:28:44 UTC (9,334 KB) [v2] Fri, 15 Mar 2024 04:38:21 UTC (12,549 KB) [v3] Fri, 26 Apr 2024 01:24:57 UTC (12,550 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.02992'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.02992'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.02992'}]
2023-10-08	Large Language Models as Analogical Reasoners	Machine Learning	https://arxiv.org/abs/2310.01714	Analogical Prompting	https://x.com/michiyasunaga/status/1709582150025240854?s=20		2310.01714	['Michihiro Yasunaga', 'Xinyun Chen', 'Yujia Li', 'Panupong Pasupat', 'Jure Leskovec', 'Percy Liang', 'Ed H. Chi', 'Denny Zhou']	ct:Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.	hed at ICLR 2024	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2310.01714', 'html': 'https://arxiv.org/html/2310.01714v3', 'tex': '/src/2310.01714', 'doi': 'https://doi.org/10.48550/arXiv.2310.01714'}	Submission history From: Michihiro Yasunaga [ view email ] [v1] Tue, 3 Oct 2023 00:57:26 UTC (300 KB) [v2] Sat, 7 Oct 2023 06:05:46 UTC (300 KB) [v3] Sat, 9 Mar 2024 05:54:39 UTC (303 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2310.01714'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2310.01714'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2310.01714'}]
2023-10-01	Effective Long-Context Scaling of Foundation Models	Computation and Language	https://arxiv.org/abs/2309.16039	Effective Long-Context Scaling with LLMs	https://x.com/omarsar0/status/1707780482178400261?s=20		2309.16039	['Wenhan Xiong', 'Jingyu Liu', 'Igor Molybog', 'Hejia Zhang', 'Prajjwal Bhargava', 'Rui Hou', 'Louis Martin', 'Rashi Rungta', 'Karthik Abinav Sankararaman', 'Barlas Oguz', 'Madian Khabsa', 'Han Fang', 'Yashar Mehdad', 'Sharan Narang', 'Kshitiz Malik', 'Angela Fan', 'Shruti Bhosale', 'Sergey Edunov', 'Mike Lewis', 'Sinong Wang', 'Hao Ma']	ct:We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths -- our ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.16039', 'html': None, 'tex': '/src/2309.16039', 'doi': 'https://doi.org/10.48550/arXiv.2309.16039'}	Submission history From: Wenhan Xiong [ view email ] [v1] Wed, 27 Sep 2023 21:41:49 UTC (2,078 KB) [v2] Tue, 17 Oct 2023 17:32:17 UTC (2,078 KB) [v3] Tue, 14 Nov 2023 01:40:13 UTC (2,078 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.16039'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.16039'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.16039'}]
2023-10-01	Graph Neural Prompting with Large Language Models	Computation and Language	https://arxiv.org/abs/2309.15427	Graph Neural Prompting with LLMs	https://x.com/omarsar0/status/1707211751354212382?s=20		2309.15427	['Yijun Tian', 'Huan Song', 'Zichen Wang', 'Haozhu Wang', 'Ziqing Hu', 'Fang Wang', 'Nitesh V. Chawla', 'Panpan Xu']	ct:Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available atthis https URL.	ed by AAAI 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2309.15427', 'html': 'https://arxiv.org/html/2309.15427v2', 'tex': '/src/2309.15427', 'doi': 'https://doi.org/10.48550/arXiv.2309.15427'}	Submission history From: Yijun Tian [ view email ] [v1] Wed, 27 Sep 2023 06:33:29 UTC (1,717 KB) [v2] Thu, 28 Dec 2023 21:14:01 UTC (1,718 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.15427'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.15427'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.15427'}]
2023-10-01	Vision Transformers Need Registers	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2309.16588	Vision Transformers Need Registers	https://x.com/TimDarcet/status/1707769575981424866?s=20		2309.16588	['Timothée Darcet', 'Maxime Oquab', 'Julien Mairal', 'Piotr Bojanowski']	ct:Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2309.16588', 'html': 'https://arxiv.org/html/2309.16588v2', 'tex': '/src/2309.16588', 'doi': 'https://doi.org/10.48550/arXiv.2309.16588'}	Submission history From: Timothée Darcet [ view email ] [v1] Thu, 28 Sep 2023 16:45:46 UTC (5,243 KB) [v2] Fri, 12 Apr 2024 09:38:33 UTC (6,803 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.16588'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.16588'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.16588'}]
2023-10-01	Boolformer: Symbolic Regression of Logic Functions with Transformers	Machine Learning	https://arxiv.org/abs/2309.12207	Boolformer	https://x.com/stephanedascoli/status/1706235856778834015?s=20		2309.12207	"[""Stéphane d'Ascoli"", 'Arthur Renard', 'Vassilis Papadopoulos', 'Samy Bengio', 'Josh Susskind', 'Emmanuel Abbé']"	ct:We introduce Boolformer, a Transformer-based model trained to perform end-to-end symbolic regression of Boolean functions. First, we show that it can predict compact formulas for complex functions not seen during training, given their full truth table. Then, we demonstrate that even with incomplete or noisy observations, Boolformer is still able to find good approximate expressions. We evaluate Boolformer on a broad set of real-world binary classification datasets, demonstrating its potential as an interpretable alternative to classic machine learning methods. Finally, we apply it to the widespread task of modeling the dynamics of gene regulatory networks and show through a benchmark that Boolformer is competitive with state-of-the-art genetic algorithms, with a speedup of several orders of magnitude. Our code and models are available publicly.	d with new ESPRESSO experiments, reworked manuscript. Added 2 authors that participated in last submission	['Machine Learning (cs.LG)', 'Logic in Computer Science (cs.LO)']	{'pdf': '/pdf/2309.12207', 'html': None, 'tex': '/src/2309.12207', 'doi': 'https://doi.org/10.48550/arXiv.2309.12207'}	Submission history From: Stéphane d'Ascoli [ view email ] [v1] Thu, 21 Sep 2023 16:11:38 UTC (4,422 KB) [v2] Wed, 16 Jul 2025 20:21:16 UTC (2,607 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.12207'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.12207'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.12207'}]
2023-10-01	Aligning Large Multimodal Models with Factually Augmented RLHF	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2309.14525	LlaVA-RLHF	https://x.com/arankomatsuzaki/status/1706839311306621182?s=20		2309.14525	['Zhiqing Sun', 'Sheng Shen', 'Shengcao Cao', 'Haotian Liu', 'Chunyuan Li', 'Yikang Shen', 'Chuang Gan', 'Liang-Yan Gui', 'Yu-Xiong Wang', 'Yiming Yang', 'Kurt Keutzer', 'Trevor Darrell']	"ct:Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in ""hallucination"", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data atthis https URL."	nt	['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.14525', 'html': None, 'tex': '/src/2309.14525', 'doi': 'https://doi.org/10.48550/arXiv.2309.14525'}	Submission history From: Sheng Shen [ view email ] [v1] Mon, 25 Sep 2023 20:59:33 UTC (9,382 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.14525'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.14525'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.14525'}]
2023-10-01	Large Language Model Alignment: A Survey	Computation and Language	https://arxiv.org/abs/2309.15025	LLM Alignment Survey	https://x.com/omarsar0/status/1706845285064818905?s=20		2309.15025	['Tianhao Shen', 'Renren Jin', 'Yufei Huang', 'Chuang Liu', 'Weilong Dong', 'Zishan Guo', 'Xinwei Wu', 'Yan Liu', 'Deyi Xiong']	ct:Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead.Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2309.15025', 'html': None, 'tex': '/src/2309.15025', 'doi': 'https://doi.org/10.48550/arXiv.2309.15025'}	Submission history From: Tianhao Shen [ view email ] [v1] Tue, 26 Sep 2023 15:49:23 UTC (170 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.15025'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.15025'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.15025'}]
2023-10-01	Qwen Technical Report	Computation and Language	https://arxiv.org/abs/2309.16609	Qwen LLM	https://x.com/omarsar0/status/1707776749042364729?s=20		2309.16609	['Jinze Bai', 'Shuai Bai', 'Yunfei Chu', 'Zeyu Cui', 'Kai Dang', 'Xiaodong Deng', 'Yang Fan', 'Wenbin Ge', 'Yu Han', 'Fei Huang', 'Binyuan Hui', 'Luo Ji', 'Mei Li', 'Junyang Lin', 'Runji Lin', 'Dayiheng Liu', 'Gao Liu', 'Chengqiang Lu', 'Keming Lu', 'Jianxin Ma', 'Rui Men', 'Xingzhang Ren', 'Xuancheng Ren', 'Chuanqi Tan', 'Sinan Tan', 'Jianhong Tu', 'Peng Wang', 'Shijie Wang', 'Wei Wang', 'Shengguang Wu', 'Benfeng Xu', 'Jin Xu', 'An Yang', 'Hao Yang', 'Jian Yang', 'Shusheng Yang', 'Yang Yao', 'Bowen Yu', 'Hongyi Yuan', 'Zheng Yuan', 'Jianwei Zhang', 'Xingxuan Zhang', 'Yichang Zhang', 'Zhenru Zhang', 'Chang Zhou', 'Jingren Zhou', 'Xiaohuan Zhou', 'Tianhang Zhu']	ct:Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.	es, 5 figures	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.16609', 'html': None, 'tex': '/src/2309.16609', 'doi': 'https://doi.org/10.48550/arXiv.2309.16609'}	Submission history From: An Yang [ view email ] [v1] Thu, 28 Sep 2023 17:07:49 UTC (995 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.16609'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.16609'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.16609'}]
2023-10-01	MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models	Computation and Language	https://arxiv.org/abs/2309.13567	MentalLlaMa	https://x.com/SAnaniadou/status/1707668936634794442?s=20		2309.13567	['Kailai Yang', 'Tianlin Zhang', 'Ziyan Kuang', 'Qianqian Xie', 'Jimin Huang', 'Sophia Ananiadou']	ct:With the development of web technology, social media texts are becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear the problem of low interpretability, the recent large language models have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions. The results show that ChatGPT can generate approaching-human explanations for its correct classifications. However, LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner. Domain-specific finetuning is an effective solution, but faces 2 challenges: 1) lack of high-quality training data. 2) no open-source LLMs for interpretable mental health analysis were released to lower the finetuning cost. To alleviate these problems, we build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset on social media, with 105K data samples. The raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks. We use expert-written few-shot prompts and collected labels to prompt ChatGPT and obtain explanations from its responses. To ensure the reliability of the explanations, we perform strict automatic and human evaluations on the correctness, consistency, and quality of generated data. Based on the IMHI dataset and LLaMA2 foundation models, we train MentalLLaMA, the first open-source LLM series for interpretable mental health analysis with instruction-following capability. We also evaluate the performance of MentalLLaMA on the IMHI evaluation benchmark with 10 test sets, where their correctness for making predictions and the quality of explanations are examined. The results show that MentalLLaMA approaches state-of-the-art discriminative methods in correctness and generates high-quality explanations.	ed by WWW 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.13567', 'html': None, 'tex': '/src/2309.13567', 'doi': 'https://doi.org/10.48550/arXiv.2309.13567'}	Submission history From: Kailai Yang [ view email ] [v1] Sun, 24 Sep 2023 06:46:08 UTC (2,612 KB) [v2] Sun, 15 Oct 2023 07:44:56 UTC (3,259 KB) [v3] Sun, 4 Feb 2024 02:47:49 UTC (3,656 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.13567'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.13567'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.13567'}]
2023-10-01	Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic	Computation and Language	https://arxiv.org/abs/2309.13339	Logical Chain-of-Thought in LLMs	https://x.com/omarsar0/status/1706711389803287019?s=20		2309.13339	['Xufeng Zhao', 'Mengdi Li', 'Wenhao Lu', 'Cornelius Weber', 'Jae Hee Lee', 'Kun Chu', 'Stefan Wermter']	ct:Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of enhanced reasoning by logic. The implementation code for LoT can be accessed at:this https URL.	ed in COLING 2024. Code seethis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Symbolic Computation (cs.SC)']	{'pdf': '/pdf/2309.13339', 'html': 'https://arxiv.org/html/2309.13339v4', 'tex': '/src/2309.13339', 'doi': 'https://doi.org/10.48550/arXiv.2309.13339'}	Submission history From: Xufeng Zhao [ view email ] [v1] Sat, 23 Sep 2023 11:21:12 UTC (357 KB) [v2] Thu, 29 Feb 2024 07:26:00 UTC (312 KB) [v3] Sun, 24 Mar 2024 04:17:28 UTC (312 KB) [v4] Tue, 26 Mar 2024 01:53:30 UTC (312 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.13339'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.13339'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.13339'}]
2023-09-24	Chain-of-Verification Reduces Hallucination in Large Language Models	Computation and Language	https://arxiv.org/abs/2309.11495	Chain-of-Verification reduces Hallucination in LLMs	https://x.com/omarsar0/status/1704901425824772275?s=20		2309.11495	['Shehzaad Dhuliawala', 'Mojtaba Komeili', 'Jing Xu', 'Roberta Raileanu', 'Xian Li', 'Asli Celikyilmaz', 'Jason Weston']	ct:Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2309.11495', 'html': None, 'tex': '/src/2309.11495', 'doi': 'https://doi.org/10.48550/arXiv.2309.11495'}	Submission history From: Jason  Weston [ view email ] [v1] Wed, 20 Sep 2023 17:50:55 UTC (7,663 KB) [v2] Mon, 25 Sep 2023 15:25:49 UTC (7,665 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.11495'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.11495'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.11495'}]
2023-09-24	Contrastive Decoding Improves Reasoning in Large Language Models	Computation and Language	https://arxiv.org/abs/2309.09117	Contrastive Decoding Improves Reasoning in Large Language Models	https://x.com/_akhaliq/status/1703966776990597567?s=20		2309.09117	"[""Sean O'Brien"", 'Mike Lewis']"	ct:We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models.	res, 11 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2309.09117', 'html': None, 'tex': '/src/2309.09117', 'doi': 'https://doi.org/10.48550/arXiv.2309.09117'}	Submission history From: Sean O'Brien [ view email ] [v1] Sun, 17 Sep 2023 00:29:32 UTC (1,557 KB) [v2] Fri, 29 Sep 2023 15:11:49 UTC (1,446 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.09117'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.09117'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.09117'}]
2023-09-24	LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models	Computation and Language	https://arxiv.org/abs/2309.12307	LongLoRA	https://x.com/omarsar0/status/1705234482930798813?s=20		2309.12307	['Yukang Chen', 'Shengju Qian', 'Haotian Tang', 'Xin Lai', 'Zhijian Liu', 'Song Han', 'Jiaya Jia']	ct:We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset.	models, dataset, and demo are available atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2309.12307', 'html': 'https://arxiv.org/html/2309.12307v3', 'tex': '/src/2309.12307', 'doi': 'https://doi.org/10.48550/arXiv.2309.12307'}	Submission history From: Chen Yukang [ view email ] [v1] Thu, 21 Sep 2023 17:59:11 UTC (1,474 KB) [v2] Tue, 5 Dec 2023 08:12:50 UTC (844 KB) [v3] Fri, 8 Mar 2024 15:26:38 UTC (842 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.12307'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.12307'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.12307'}]
2023-09-24	Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?	Computation and Language	https://arxiv.org/abs/2309.08963	LLMs for Generating Structured Data	https://x.com/omarsar0/status/1703958549917847884?s=20		2309.08963	['Xiangru Tang', 'Yiming Zong', 'Jason Phang', 'Yilun Zhao', 'Wangchunshu Zhou', 'Arman Cohan', 'Mark Gerstein']	ct:Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs' proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions -- coverage, formatting, reasoning, comprehension, pragmatics, and hallucination -- highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found atthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.08963', 'html': 'https://arxiv.org/html/2309.08963v3', 'tex': '/src/2309.08963', 'doi': 'https://doi.org/10.48550/arXiv.2309.08963'}	Submission history From: Xiangru Tang [ view email ] [v1] Sat, 16 Sep 2023 11:31:58 UTC (2,605 KB) [v2] Tue, 19 Sep 2023 05:58:47 UTC (2,605 KB) [v3] Thu, 4 Apr 2024 21:57:12 UTC (10,353 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.08963'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.08963'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.08963'}]
2023-09-24	LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset	Computation and Language	http://arxiv.org/abs/2309.11998	LMSYS-Chat-1M	https://x.com/arankomatsuzaki/status/1705024956122161217?s=20		2309.11998	['Lianmin Zheng', 'Wei-Lin Chiang', 'Ying Sheng', 'Tianle Li', 'Siyuan Zhuang', 'Zhanghao Wu', 'Yonghao Zhuang', 'Zhuohan Li', 'Zi Lin', 'Eric P. Xing', 'Joseph E. Gonzalez', 'Ion Stoica', 'Hao Zhang']	ct:Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2309.11998', 'html': 'https://arxiv.org/html/2309.11998v4', 'tex': '/src/2309.11998', 'doi': 'https://doi.org/10.48550/arXiv.2309.11998'}	Submission history From: Lianmin Zheng [ view email ] [v1] Thu, 21 Sep 2023 12:13:55 UTC (1,017 KB) [v2] Fri, 22 Sep 2023 00:53:35 UTC (1,017 KB) [v3] Sat, 30 Sep 2023 00:30:51 UTC (1,029 KB) [v4] Sun, 10 Mar 2024 19:34:57 UTC (1,034 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.11998'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.11998'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.11998'}]
2023-09-24	Language Modeling Is Compression	Machine Learning	https://arxiv.org/abs/2309.10668	Language Modeling is Compression	https://x.com/omarsar0/status/1704306357006897402?s=20		2309.10668	['Grégoire Delétang', 'Anian Ruoss', 'Paul-Ambroise Duquenne', 'Elliot Catt', 'Tim Genewein', 'Christopher Mattern', 'Jordi Grau-Moya', 'Li Kevin Wenliang', 'Matthew Aitchison', 'Laurent Orseau', 'Marcus Hutter', 'Joel Veness']	ct:It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Information Theory (cs.IT)']	{'pdf': '/pdf/2309.10668', 'html': 'https://arxiv.org/html/2309.10668v2', 'tex': '/src/2309.10668', 'doi': 'https://doi.org/10.48550/arXiv.2309.10668'}	Submission history From: Anian Ruoss [ view email ] [v1] Tue, 19 Sep 2023 14:50:38 UTC (2,092 KB) [v2] Mon, 18 Mar 2024 23:15:47 UTC (2,356 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.10668'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.10668'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.10668'}]
2023-09-24	Compositional Foundation Models for Hierarchical Planning	Machine Learning	https://arxiv.org/abs/2309.08587	Compositional Foundation Models	https://x.com/du_yilun/status/1703786005612929214?s=20		2309.08587	['Anurag Ajay', 'Seungwook Han', 'Yilun Du', 'Shuang Li', 'Abhi Gupta', 'Tommi Jaakkola', 'Josh Tenenbaum', 'Leslie Kaelbling', 'Akash Srivastava', 'Pulkit Agrawal']	ct:To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustrate the efficacy and adaptability of our approach in three different long-horizon table-top manipulation tasks.	e:this https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Robotics (cs.RO)']	{'pdf': '/pdf/2309.08587', 'html': None, 'tex': '/src/2309.08587', 'doi': 'https://doi.org/10.48550/arXiv.2309.08587'}	Submission history From: Anurag Ajay [ view email ] [v1] Fri, 15 Sep 2023 17:44:05 UTC (37,705 KB) [v2] Thu, 21 Sep 2023 14:49:20 UTC (43,492 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.08587'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.08587'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.08587'}]
2023-09-24	OWL: A Large Language Model for IT Operations	Computation and Language	https://arxiv.org/abs/2309.09298	LLMs for IT Operations	https://x.com/omarsar0/status/1704137910834888743?s=20		2309.09298	['Hongcheng Guo', 'Jian Yang', 'Jiaheng Liu', 'Liqun Yang', 'Linzheng Chai', 'Jiaqi Bai', 'Junran Peng', 'Xiaorong Hu', 'Chao Chen', 'Dongfeng Zhang', 'Xu Shi', 'Tieqiao Zheng', 'Liangfan Zheng', 'Bo Zhang', 'Ke Xu', 'Zhoujun Li']	ct:With the rapid development of IT operations, it has become increasingly crucial to efficiently manage and analyze large volumes of data for practical applications. The techniques of Natural Language Processing (NLP) have shown remarkable capabilities for various tasks, including named entity recognition, machine translation and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various NLP downstream tasks. However, there is a lack of specialized LLMs for IT operations. In this paper, we introduce the OWL, a large language model trained on our collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks. Furthermore, we evaluate the performance of our OWL on the OWL-Bench established by us and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs.	024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.09298', 'html': None, 'tex': '/src/2309.09298', 'doi': 'https://doi.org/10.48550/arXiv.2309.09298'}	Submission history From: Hongcheng Guo [ view email ] [v1] Sun, 17 Sep 2023 15:19:29 UTC (822 KB) [v2] Fri, 27 Sep 2024 05:55:01 UTC (1,054 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.09298'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.09298'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.09298'}]
2023-09-24	KOSMOS-2.5: A Multimodal Literate Model	Computation and Language	https://arxiv.org/abs/2309.11419	KOSMOS-2.5	https://x.com/arankomatsuzaki/status/1704659787399487649?s=20		2309.11419	['Tengchao Lv', 'Yupan Huang', 'Jingye Chen', 'Yuzhong Zhao', 'Yilin Jia', 'Lei Cui', 'Shuming Ma', 'Yaoyao Chang', 'Shaohan Huang', 'Wenhui Wang', 'Li Dong', 'Weiyao Luo', 'Shaoxiang Wu', 'Guoxin Wang', 'Cha Zhang', 'Furu Wei']	ct:The automatic reading of text-intensive images represents a significant advancement toward achieving Artificial General Intelligence (AGI). In this paper we present KOSMOS-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on a large-scale corpus of text-intensive images, KOSMOS-2.5 excels in two distinct yet complementary transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned spatial coordinates within the image, and (2) producing structured text output that captures both style and structure in markdown format. This unified multimodal literate capability is achieved through a shared decoder-only autoregressive Transformer architecture and task-specific prompts. Building on this foundation, we fine-tune KOSMOS-2.5 for document understanding tasks, resulting in a document understanding generalist named KOSMOS-2.5-CHAT. Additionally, a large corpus of 357.4 million document pages spanning diverse domains was curated for pre-training. We evaluate KOSMOS-2.5 on two newly proposed benchmarks, OCREval and MarkdownEval, for document-level text recognition and image-to-markdown generation, demonstrating impressive literate capabilities comparable to GPT-4o. KOSMOS-2.5-CHAT achieves performance comparable to other state-of-the-art generalists that are five times larger (1.3B vs. 7B) across nine text-rich visual question answering benchmarks. Models and code have been available at \url{this https URL}.		['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2309.11419', 'html': 'https://arxiv.org/html/2309.11419v2', 'tex': '/src/2309.11419', 'doi': 'https://doi.org/10.48550/arXiv.2309.11419'}	Submission history From: Lei Cui [ view email ] [v1] Wed, 20 Sep 2023 15:50:08 UTC (9,835 KB) [v2] Wed, 21 Aug 2024 16:54:23 UTC (5,908 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.11419'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.11419'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.11419'}]
2023-09-17	Textbooks Are All You Need II: phi-1.5 technical report	Computation and Language	https://arxiv.org/abs/2309.05463	Textbooks Are All You Need II	https://x.com/omarsar0/status/1701590130270601422?s=20		2309.05463	['Yuanzhi Li', 'Sébastien Bubeck', 'Ronen Eldan', 'Allie Del Giorno', 'Suriya Gunasekar', 'Yin Tat Lee']	"ct:We continue the investigation into the power of smaller Transformer-based language models as initiated by \textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality"" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need"" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step"" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \textbf{phi-1.5} to promote further research on these urgent topics."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2309.05463', 'html': None, 'tex': '/src/2309.05463', 'doi': 'https://doi.org/10.48550/arXiv.2309.05463'}	Submission history From: Suriya Gunasekar [ view email ] [v1] Mon, 11 Sep 2023 14:01:45 UTC (59 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.05463'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.05463'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.05463'}]
2023-09-17	The Rise and Potential of Large Language Model Based Agents: A Survey	Artificial Intelligence	https://arxiv.org/abs/2309.07864	The Rise and Potential of LLM Based Agents	https://x.com/omarsar0/status/1702736490067890239?s=20		2309.07864	['Zhiheng Xi', 'Wenxiang Chen', 'Xin Guo', 'Wei He', 'Yiwen Ding', 'Boyang Hong', 'Ming Zhang', 'Junzhe Wang', 'Senjie Jin', 'Enyu Zhou', 'Rui Zheng', 'Xiaoran Fan', 'Xiao Wang', 'Limao Xiong', 'Yuhao Zhou', 'Weiran Wang', 'Changhao Jiang', 'Yicheng Zou', 'Xiangyang Liu', 'Zhangyue Yin', 'Shihan Dou', 'Rongxiang Weng', 'Wensen Cheng', 'Qi Zhang', 'Wenjuan Qin', 'Yongyan Zheng', 'Xipeng Qiu', 'Xuanjing Huang', 'Tao Gui']	ct:For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers atthis https URL.	es, 12 figures	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.07864', 'html': None, 'tex': '/src/2309.07864', 'doi': 'https://doi.org/10.48550/arXiv.2309.07864'}	Submission history From: Zhiheng Xi [ view email ] [v1] Thu, 14 Sep 2023 17:12:03 UTC (21,771 KB) [v2] Fri, 15 Sep 2023 01:46:36 UTC (21,771 KB) [v3] Tue, 19 Sep 2023 08:29:18 UTC (23,110 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.07864'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.07864'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.07864'}]
2023-09-17	RAIN: Your Language Models Can Align Themselves without Finetuning	Computation and Language	https://arxiv.org/abs/2309.07124	LLMs Can Align Themselves without Finetuning?	https://x.com/omarsar0/status/1702131444041011395?s=20		2309.07124	['Yuhui Li', 'Fangyun Wei', 'Jinjing Zhao', 'Chao Zhang', 'Hongyang Zhang']	ct:Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.07124', 'html': None, 'tex': '/src/2309.07124', 'doi': 'https://doi.org/10.48550/arXiv.2309.07124'}	Submission history From: Yuhui Li [ view email ] [v1] Wed, 13 Sep 2023 17:59:09 UTC (793 KB) [v2] Mon, 9 Oct 2023 03:34:01 UTC (1,112 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.07124'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.07124'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.07124'}]
2023-09-17	Robot Parkour Learning	Robotics	https://arxiv.org/abs/2309.05665	Robot Parkour Learning	https://x.com/zipengfu/status/1701316023612219445?s=20		2309.05665	['Ziwen Zhuang', 'Zipeng Fu', 'Jianren Wang', 'Christopher Atkeson', 'Soeren Schwertfeger', 'Chelsea Finn', 'Hang Zhao']	ct:Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.	023 (Oral). Project website atthis https URL	['Robotics (cs.RO)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2309.05665', 'html': None, 'tex': '/src/2309.05665', 'doi': 'https://doi.org/10.48550/arXiv.2309.05665'}	Submission history From: Ziwen Zhuang [ view email ] [v1] Mon, 11 Sep 2023 17:59:17 UTC (7,041 KB) [v2] Tue, 12 Sep 2023 03:01:55 UTC (7,040 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.05665'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.05665'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.05665'}]
2023-09-17	A Survey of Hallucination in Large Foundation Models	Artificial Intelligence	https://arxiv.org/abs/2309.05922	A Survey of Hallucination in LLMs	https://x.com/omarsar0/status/1701970034711539839?s=20		2309.05922	['Vipula Rawte', 'Amit Sheth', 'Amitava Das']	ct:Hallucination in a foundation model (FM) refers to the generation of content that strays from factual reality or includes fabricated information. This survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``Large'' Foundation Models (LFMs). The paper classifies various types of hallucination phenomena that are specific to LFMs and establishes evaluation criteria for assessing the extent of hallucination. It also examines existing strategies for mitigating hallucination in LFMs and discusses potential directions for future research in this area. Essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in LFMs.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2309.05922', 'html': None, 'tex': '/src/2309.05922', 'doi': 'https://doi.org/10.48550/arXiv.2309.05922'}	Submission history From: Vipula Rawte [ view email ] [v1] Tue, 12 Sep 2023 02:34:06 UTC (1,104 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.05922'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.05922'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.05922'}]
2023-09-17	Agents: An Open-source Framework for Autonomous Language Agents	Computation and Language	https://arxiv.org/abs/2309.07870	Agents	https://x.com/arankomatsuzaki/status/1702497897395396960?s=20		2309.07870	['Wangchunshu Zhou', 'Yuchen Eleanor Jiang', 'Long Li', 'Jialong Wu', 'Tiannan Wang', 'Shi Qiu', 'Jintian Zhang', 'Jing Chen', 'Ruipu Wu', 'Shuai Wang', 'Shiding Zhu', 'Jiyu Chen', 'Wentao Zhang', 'Xiangru Tang', 'Ningyu Zhang', 'Huajun Chen', 'Peng Cui', 'Mrinmaya Sachan']	ct:Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release Agents, an open-source library with the goal of opening up these advances to a wider non-specialist audience. Agents is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control. Agents is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. Agents is available atthis https URL.	vailable atthis https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.07870', 'html': 'https://arxiv.org/html/2309.07870v3', 'tex': '/src/2309.07870', 'doi': 'https://doi.org/10.48550/arXiv.2309.07870'}	Submission history From: Wangchunshu Zhou [ view email ] [v1] Thu, 14 Sep 2023 17:18:25 UTC (1,492 KB) [v2] Sun, 8 Oct 2023 14:43:38 UTC (1,530 KB) [v3] Tue, 12 Dec 2023 04:47:21 UTC (1,530 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.07870'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.07870'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.07870'}]
2023-09-17	Radiology-Llama2: Best-in-Class Large Language Model for Radiology	Computation and Language	https://arxiv.org/abs/2309.06419	Radiology-Llama2: Best-in-Class LLM for Radiology	https://x.com/omarsar0/status/1701774444052557965?s=20		2309.06419	['Zhengliang Liu', 'Yiwei Li', 'Peng Shu', 'Aoxiao Zhong', 'Longtao Yang', 'Chao Ju', 'Zihao Wu', 'Chong Ma', 'Jie Luo', 'Cheng Chen', 'Sekeun Kim', 'Jiang Hu', 'Haixing Dai', 'Lin Zhao', 'Dajiang Zhu', 'Jun Liu', 'Wei Liu', 'Dinggang Shen', 'Tianming Liu', 'Quanzheng Li', 'Xiang Li']	ct:This paper introduces Radiology-Llama2, a large language model specialized for radiology through a process known as instruction tuning. Radiology-Llama2 is based on the Llama2 architecture and further trained on a large dataset of radiology reports to generate coherent and clinically useful impressions from radiological findings. Quantitative evaluations using ROUGE metrics on the MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves state-of-the-art performance compared to other generative language models, with a Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional assessments by radiology experts highlight the model's strengths in understandability, coherence, relevance, conciseness, and clinical utility. The work illustrates the potential of localized language models designed and tuned for specialized domains like radiology. When properly evaluated and deployed, such models can transform fields like radiology by automating rote tasks and enhancing human expertise.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.06419', 'html': None, 'tex': '/src/2309.06419', 'doi': 'https://doi.org/10.48550/arXiv.2309.06419'}	Submission history From: Zhengliang Liu [ view email ] [v1] Tue, 29 Aug 2023 17:44:28 UTC (729 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.06419'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.06419'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.06419'}]
2023-09-17	Communicative Agents for Software Development	Software Engineering	https://arxiv.org/abs/2307.07924v3	Communicative Agents for Software Development	https://x.com/KevinAFischer/status/1702355125418045860?s=20		2307.07924v3	['Chen Qian', 'Xin Cong', 'Wei Liu', 'Cheng Yang', 'Weize Chen', 'Yusheng Su', 'Yufan Dang', 'Jiahao Li', 'Juyuan Xu', 'Dahai Li', 'Zhiyuan Liu', 'Maosong Sun']	ct:Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborative dialogue and facilitating a seamless workflow. The chat chain acts as a facilitator, breaking down each stage into atomic subtasks. This enables dual roles, allowing for proposing and validating solutions through context-aware communication, leading to efficient resolution of specific subtasks. The instrumental analysis of ChatDev highlights its remarkable efficacy in software generation, enabling the completion of the entire software development process in under seven minutes at a cost of less than one dollar. It not only identifies and alleviates potential vulnerabilities but also rectifies potential hallucinations while maintaining commendable efficiency and cost-effectiveness. The potential of ChatDev unveils fresh possibilities for integrating LLMs into the realm of software development.	ttps URL	['Software Engineering (cs.SE)', 'Computation and Language (cs.CL)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2307.07924v3', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2307.07924'}	Submission history From: Chen Qian [ view email ] [v1] Sun, 16 Jul 2023 02:11:34 UTC (12,275 KB) [v2] Tue, 18 Jul 2023 09:51:21 UTC (12,275 KB) [v3] Mon, 28 Aug 2023 08:38:38 UTC (12,275 KB) [v4] Tue, 19 Dec 2023 12:56:13 UTC (13,383 KB) [v5] Wed, 5 Jun 2024 13:23:49 UTC (2,809 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.07924'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.07924'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.07924'}]
2023-09-17	MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning	Computation and Language	https://arxiv.org/abs/2309.05653	MAmmoTH	https://x.com/xiangyue96/status/1701710215442309323?s=20		2309.05653	['Xiang Yue', 'Xingwei Qu', 'Ge Zhang', 'Yao Fu', 'Wenhao Huang', 'Huan Sun', 'Yu Su', 'Wenhu Chen']	ct:We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-4's CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.	n progress; Xiang Yue and Wenhu Chen contributed equally to this paper	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.05653', 'html': None, 'tex': '/src/2309.05653', 'doi': 'https://doi.org/10.48550/arXiv.2309.05653'}	Submission history From: Xiang Yue [ view email ] [v1] Mon, 11 Sep 2023 17:47:22 UTC (608 KB) [v2] Sun, 1 Oct 2023 15:25:41 UTC (717 KB) [v3] Tue, 3 Oct 2023 02:48:42 UTC (717 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.05653'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.05653'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.05653'}]
2023-09-10	Transformers as Support Vector Machines	Machine Learning	https://arxiv.org/abs/2308.16898	Transformers as SVMs			2308.16898	['Davoud Ataee Tarzanagh', 'Yingcong Li', 'Christos Thrampoulidis', 'Samet Oymak']	"ct:Since its inception in ""Attention Is All You Need"", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. We characterize this convergence, highlighting that it can occur toward locally-optimal directions rather than global ones. (2) Complementing this, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. (3) While our theory applies primarily to linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias with nonlinear heads. Our findings are applicable to arbitrary datasets and their validity is verified via experiments. We also introduce several open problems and research directions. We believe these findings inspire the interpretation of transformers as a hierarchy of SVMs that separates and selects optimal tokens."	oof of global convergence for gradient descent in the equal score setting has been fixed, referring to Theorem 2 of [TLZO23], and the experimental results have been extended	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Optimization and Control (math.OC)']	{'pdf': '/pdf/2308.16898', 'html': None, 'tex': '/src/2308.16898', 'doi': 'https://doi.org/10.48550/arXiv.2308.16898'}	Submission history From: Yingcong Li [ view email ] [v1] Thu, 31 Aug 2023 17:57:50 UTC (1,671 KB) [v2] Thu, 7 Sep 2023 17:50:52 UTC (1,835 KB) [v3] Thu, 22 Feb 2024 18:38:14 UTC (1,081 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.16898'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.16898'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.16898'}]
2023-09-10	RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback	Computation and Language	https://arxiv.org/abs/2309.00267	Scaling RLHF with AI Feedback	https://twitter.com/omarsar0/status/1699102486928265530?s=20		2309.00267	['Harrison Lee', 'Samrat Phatale', 'Hassan Mansoor', 'Thomas Mesnard', 'Johan Ferret', 'Kellie Lu', 'Colton Bishop', 'Ethan Hall', 'Victor Carbune', 'Abhinav Rastogi', 'Sushant Prakash']	"ct:Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards ""self-improvement"" by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF."	ted at ICML 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2309.00267', 'html': 'https://arxiv.org/html/2309.00267v3', 'tex': '/src/2309.00267', 'doi': 'https://doi.org/10.48550/arXiv.2309.00267'}	Submission history From: Harrison Lee [ view email ] [v1] Fri, 1 Sep 2023 05:53:33 UTC (715 KB) [v2] Fri, 1 Dec 2023 01:41:44 UTC (1,214 KB) [v3] Tue, 3 Sep 2024 14:01:54 UTC (949 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.00267'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.00267'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.00267'}]
2023-09-10	GPT Can Solve Mathematical Problems Without a Calculator	Machine Learning	https://arxiv.org/abs/2309.03241	GPT Solves Math Problems Without a Calculator	https://twitter.com/_akhaliq/status/1699951105927512399?s=20		2309.03241	['Zhen Yang', 'Ming Ding', 'Qingsong Lv', 'Zhihuan Jiang', 'Zehai He', 'Yuyi Guo', 'Jinfeng Bai', 'Jie Tang']	ct:Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set. Our code and data are public atthis https URL.	s,14figures	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.03241', 'html': None, 'tex': '/src/2309.03241', 'doi': 'https://doi.org/10.48550/arXiv.2309.03241'}	Submission history From: Zhen Yang [ view email ] [v1] Wed, 6 Sep 2023 06:18:16 UTC (1,249 KB) [v2] Tue, 12 Sep 2023 11:01:25 UTC (989 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.03241'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.03241'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.03241'}]
2023-09-10	Large Language Models as Optimizers	Machine Learning	https://arxiv.org/abs/2309.03409	LLMs as Optimizers	https://twitter.com/omarsar0/status/1700249035456598391?s=20		2309.03409	['Chengrun Yang', 'Xuezhi Wang', 'Yifeng Lu', 'Hanxiao Liu', 'Quoc V. Le', 'Denny Zhou', 'Xinyun Chen']	ct:Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code atthis https URL.	024; 42 pages, 26 figures, 15 tables. Code atthis https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2309.03409', 'html': 'https://arxiv.org/html/2309.03409v3', 'tex': '/src/2309.03409', 'doi': 'https://doi.org/10.48550/arXiv.2309.03409'}	Submission history From: Chengrun Yang [ view email ] [v1] Thu, 7 Sep 2023 00:07:15 UTC (4,422 KB) [v2] Thu, 7 Dec 2023 05:25:15 UTC (765 KB) [v3] Mon, 15 Apr 2024 07:50:32 UTC (765 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.03409'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.03409'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.03409'}]
2023-09-10	ImageBind-LLM: Multi-modality Instruction Tuning	Multimedia	https://arxiv.org/abs/2309.03905	Multi-modality Instruction Tuning	https://twitter.com/arankomatsuzaki/status/1699947731333345750?s=20		2309.03905	['Jiaming Han', 'Renrui Zhang', 'Wenqi Shao', 'Peng Gao', 'Peng Xu', 'Han Xiao', 'Kaipeng Zhang', 'Chris Liu', 'Song Wen', 'Ziyu Guo', 'Xudong Lu', 'Shuai Ren', 'Yafei Wen', 'Xiaoxin Chen', 'Xiangyu Yue', 'Hongsheng Li', 'Yu Qiao']	ct:We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by a proposed visual cache model for further cross-modal embedding enhancement. The training-free cache model retrieves from three million image features extracted by ImageBind, which effectively mitigates the training-inference modality discrepancy. Notably, with our approach, ImageBind-LLM can respond to instructions of diverse modalities and demonstrate significant language generation quality. Code is released atthis https URL.	s available atthis https URL	['Multimedia (cs.MM)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)', 'Sound (cs.SD)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2309.03905', 'html': None, 'tex': '/src/2309.03905', 'doi': 'https://doi.org/10.48550/arXiv.2309.03905'}	Submission history From: Renrui Zhang [ view email ] [v1] Thu, 7 Sep 2023 17:59:45 UTC (13,701 KB) [v2] Mon, 11 Sep 2023 20:25:16 UTC (13,704 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.03905'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.03905'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.03905'}]
2023-09-10	Explaining grokking through circuit efficiency	Machine Learning	https://arxiv.org/abs/2309.02390	Explaining Grokking	https://twitter.com/VikrantVarma_/status/1699823229307699305?s=20		2309.02390	['Vikrant Varma', 'Rohin Shah', 'Zachary Kenton', 'János Kramár', 'Ramana Kumar']	ct:One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2309.02390', 'html': None, 'tex': '/src/2309.02390', 'doi': 'https://doi.org/10.48550/arXiv.2309.02390'}	Submission history From: Vikrant Varma [ view email ] [v1] Tue, 5 Sep 2023 17:00:24 UTC (937 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.02390'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.02390'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.02390'}]
2023-09-10	AI Deception: A Survey of Examples, Risks, and Potential Solutions	Computers and Society	https://arxiv.org/abs/2308.14752	Overview of AI Deception	https://twitter.com/DanHendrycks/status/1699437800301752332?s=20		2308.14752	"['Peter S. Park', 'Simon Goldstein', ""Aidan O'Gara"", 'Michael Chen', 'Dan Hendrycks']"	ct:This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.	es (not including executive summary, references, and appendix), six figures	['Computers and Society (cs.CY)', 'Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2308.14752', 'html': None, 'tex': '/src/2308.14752', 'doi': 'https://doi.org/10.48550/arXiv.2308.14752'}	Submission history From: Peter S. Park [ view email ] [v1] Mon, 28 Aug 2023 17:59:35 UTC (4,312 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.14752'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.14752'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.14752'}]
2023-09-10	FLM-101B: An Open LLM and How to Train It with $100K Budget	Computation and Language	https://arxiv.org/abs/2309.03852	FLM-101B	https://twitter.com/omarsar0/status/1700156132700963053?s=20		2309.03852	['Xiang Li', 'Yiqun Yao', 'Xin Jiang', 'Xuezhi Fang', 'Xuying Meng', 'Siqi Fan', 'Peng Han', 'Jing Li', 'Li Du', 'Bowen Qin', 'Zheng Zhang', 'Aixin Sun', 'Yequan Wang']	ct:Large language models (LLMs) are considered important approaches towards foundational machine intelligence, achieving remarkable success in Natural Language Processing and multimodal tasks, among others. However, the carbon footprints and financial costs originating from heavy pre-training computation is a non-negligible issue. Progressive training methods, inspired by the neurogenesis process that grows neural structures, have shown potential to accelerate LLM pre-training. However, the algorithms, implementation, and practices for progressively training LLMs beyond 100B parameters remain underexplored. In this paper, we show that our model, namely FLM-101B, trained with our growth strategy under a budget of \$100K, reaches 80\% of the baselines' performances with only 10\% of their floating-point operations. We believe that further studies on progressive training will benefit the community by cutting down the costs and promoting green AI. The checkpoint of FLM-101B is released atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2309.03852', 'html': 'https://arxiv.org/html/2309.03852v3', 'tex': '/src/2309.03852', 'doi': 'https://doi.org/10.48550/arXiv.2309.03852'}	Submission history From: Yiqun Yao [ view email ] [v1] Thu, 7 Sep 2023 17:07:36 UTC (828 KB) [v2] Sun, 17 Sep 2023 07:38:10 UTC (547 KB) [v3] Tue, 14 Jan 2025 06:40:36 UTC (7,460 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.03852'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.03852'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.03852'}]
2023-09-10	Cognitive Architectures for Language Agents	Artificial Intelligence	https://arxiv.org/abs/2309.02427	Cognitive Architecture for Language Agents	https://twitter.com/ShunyuYao12/status/1699396834983362690?s=20		2309.02427	['Theodore R. Sumers', 'Shunyu Yao', 'Karthik Narasimhan', 'Thomas L. Griffiths']	ct:Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.	TMLR camera ready version. 19 pages of main content, 5 figures. The first two authors contributed equally, order decided by coin flip. A CoALA-based repo of recent work on language agents:this https URL	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Symbolic Computation (cs.SC)']	{'pdf': '/pdf/2309.02427', 'html': 'https://arxiv.org/html/2309.02427v3', 'tex': '/src/2309.02427', 'doi': 'https://doi.org/10.48550/arXiv.2309.02427'}	Submission history From: Shunyu Yao [ view email ] [v1] Tue, 5 Sep 2023 17:56:20 UTC (1,989 KB) [v2] Wed, 27 Sep 2023 15:27:25 UTC (2,001 KB) [v3] Fri, 15 Mar 2024 15:44:11 UTC (1,968 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2309.02427'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2309.02427'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2309.02427'}]
2023-09-03	LLaSM: Large Language and Speech Model	Computation and Language	https://arxiv.org/abs/2308.15930v1	Large Language and Speech Model	https://twitter.com/_akhaliq/status/1697081112164475304?s=20		2308.15930v1	['Yu Shu', 'Siwei Dong', 'Guangyao Chen', 'Wenhao Huang', 'Ruihua Zhang', 'Daochen Shi', 'Qiqi Xiang', 'Yemin Shi']	ct:Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available atthis https URLandthis https URL. The LLaSM-Audio-Instructions dataset is available atthis https URL.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Sound (cs.SD)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2308.15930v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.15930'}	Submission history From: Yu Shu [ view email ] [v1] Wed, 30 Aug 2023 10:12:39 UTC (3,140 KB) [v2] Tue, 12 Sep 2023 03:41:35 UTC (3,059 KB) [v3] Sat, 16 Sep 2023 06:14:54 UTC (3,059 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.15930'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.15930'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.15930'}]
2023-09-03	SAM-Med2D	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2308.16184v1	SAM-Med2D	https://twitter.com/omarsar0/status/1698014448856773102?s=20		2308.16184v1	['Junlong Cheng', 'Jin Ye', 'Zhongying Deng', 'Jianpin Chen', 'Tianbin Li', 'Haoyu Wang', 'Yanzhou Su', 'Ziyan Huang', 'Jilong Chen', 'Lei Jiang', 'Hui Sun', 'Junjun He', 'Shaoting Zhang', 'Min Zhu', 'Yu Qiao']	ct:The Segment Anything Model (SAM) represents a state-of-the-art research advancement in natural image segmentation, achieving impressive results with input prompts such as points and bounding boxes. However, our evaluation and recent research indicate that directly applying the pretrained SAM to medical image segmentation does not yield satisfactory performance. This limitation primarily arises from significant domain gap between natural images and medical images. To bridge this gap, we introduce SAM-Med2D, the most comprehensive studies on applying SAM to medical 2D images. Specifically, we first collect and curate approximately 4.6M images and 19.7M masks from public and private datasets, constructing a large-scale medical image segmentation dataset encompassing various modalities and objects. Then, we comprehensively fine-tune SAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that only adopt bounding box or point prompts as interactive segmentation approach, we adapt SAM to medical image segmentation through more comprehensive prompts involving bounding boxes, points, and masks. We additionally fine-tune the encoder and decoder of the original SAM to obtain a well-performed SAM-Med2D, leading to the most comprehensive fine-tuning strategies to date. Finally, we conducted a comprehensive evaluation and analysis to investigate the performance of SAM-Med2D in medical image segmentation across various modalities, anatomical structures, and organs. Concurrently, we validated the generalization capability of SAM-Med2D on 9 datasets from MICCAI 2023 challenge. Overall, our approach demonstrated significantly superior performance and generalization capability compared to SAM.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2308.16184v1', 'html': None, 'tex': '/src/2308.16184v1', 'doi': 'https://doi.org/10.48550/arXiv.2308.16184'}	Submission history From: Junlong Cheng [ view email ] [v1] Wed, 30 Aug 2023 17:59:02 UTC (48,493 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.16184'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.16184'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.16184'}]
2023-09-03	Vector Search with OpenAI Embeddings: Lucene Is All You Need	Information Retrieval	https://arxiv.org/abs/2308.14963	Vector Search with OpenAI Embeddings	https://twitter.com/omarsar0/status/1696879909950361867?s=20		2308.14963	['Jimmy Lin', 'Ronak Pradeep', 'Tommaso Teofili', 'Jasper Xian']	"ct:We provide a reproducible, end-to-end demonstration of vector search with OpenAI embeddings using Lucene on the popular MS MARCO passage ranking test collection. The main goal of our work is to challenge the prevailing narrative that a dedicated vector store is necessary to take advantage of recent advances in deep neural networks as applied to search. Quite the contrary, we show that hierarchical navigable small-world network (HNSW) indexes in Lucene are adequate to provide vector search capabilities in a standard bi-encoder architecture. This suggests that, from a simple cost-benefit analysis, there does not appear to be a compelling reason to introduce a dedicated vector store into a modern ""AI stack"" for search, since such applications have already received substantial investments in existing, widely deployed infrastructure."		['Information Retrieval (cs.IR)']	{'pdf': '/pdf/2308.14963', 'html': None, 'tex': '/src/2308.14963', 'doi': 'https://doi.org/10.48550/arXiv.2308.14963'}	Submission history From: Jimmy Lin [ view email ] [v1] Tue, 29 Aug 2023 01:30:23 UTC (123 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.14963'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.14963'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.14963'}]
2023-09-03	Graph of Thoughts: Solving Elaborate Problems with Large Language Models	Computation and Language	https://arxiv.org/abs/2308.09687v2	Graph of Thoughts	https://twitter.com/omarsar0/status/1697245998828204200?s=20		2308.09687v2	['Maciej Besta', 'Nils Blach', 'Ales Kubicek', 'Robert Gerstenberger', 'Lukas Gianinazzi', 'Joanna Gajda', 'Tomasz Lehmann', 'Michal Podstawski', 'Hubert Niewiadomski', 'Piotr Nyczyk', 'Torsten Hoefler']	"ct:We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (""LLM thoughts"") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2308.09687v2', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.09687'}	Submission history From: Maciej Besta [ view email ] [v1] Fri, 18 Aug 2023 17:29:23 UTC (839 KB) [v2] Mon, 21 Aug 2023 10:51:42 UTC (839 KB) [v3] Fri, 24 Nov 2023 09:13:54 UTC (825 KB) [v4] Tue, 6 Feb 2024 18:00:18 UTC (866 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.09687'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.09687'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.09687'}]
2023-09-03	MVDream: Multi-view Diffusion for 3D Generation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2308.16512	MVDream	https://twitter.com/_akhaliq/status/1697521847963619462?s=20		2308.16512	['Yichun Shi', 'Peng Wang', 'Jianglong Ye', 'Mai Long', 'Kejie Li', 'Xiao Yang']	ct:We introduce MVDream, a diffusion model that is able to generate consistent multi-view images from a given text prompt. Learning from both 2D and 3D data, a multi-view diffusion model can achieve the generalizability of 2D diffusion models and the consistency of 3D renderings. We demonstrate that such a multi-view diffusion model is implicitly a generalizable 3D prior agnostic to 3D representations. It can be applied to 3D generation via Score Distillation Sampling, significantly enhancing the consistency and stability of existing 2D-lifting methods. It can also learn new concepts from a few 2D examples, akin to DreamBooth, but for 3D generation.	nized for arXiv; Our project page isthis https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2308.16512', 'html': 'https://arxiv.org/html/2308.16512v4', 'tex': '/src/2308.16512', 'doi': 'https://doi.org/10.48550/arXiv.2308.16512'}	Submission history From: Yichun Shi [ view email ] [v1] Thu, 31 Aug 2023 07:49:06 UTC (42,088 KB) [v2] Mon, 2 Oct 2023 10:42:28 UTC (42,756 KB) [v3] Sat, 16 Mar 2024 01:10:16 UTC (31,694 KB) [v4] Thu, 18 Apr 2024 04:12:32 UTC (36,077 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.16512'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.16512'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.16512'}]
2023-09-03	Nougat: Neural Optical Understanding for Academic Documents	Machine Learning	https://arxiv.org/abs/2308.13418v1	Nougat	https://twitter.com/lukas_blecher/status/1696101110853910716?s=20		2308.13418v1	['Lukas Blecher', 'Guillem Cucurull', 'Thomas Scialom', 'Robert Stojnic']	ct:Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.	es, 10 figures	['Machine Learning (cs.LG)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2308.13418v1', 'html': None, 'tex': '/src/2308.13418v1', 'doi': 'https://doi.org/10.48550/arXiv.2308.13418'}	Submission history From: Lukas Blecher [ view email ] [v1] Fri, 25 Aug 2023 15:03:36 UTC (3,937 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.13418'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.13418'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.13418'}]
2023-09-03	FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios	Computation and Language	https://arxiv.org/abs/2307.13528v2	Factuality Detection in LLMs	https://twitter.com/omarsar0/status/1697642048587694370?s=20		2307.13528v2	['I-Chun Chern', 'Steffi Chern', 'Shiqi Chen', 'Weizhe Yuan', 'Kehua Feng', 'Chunting Zhou', 'Junxian He', 'Graham Neubig', 'Pengfei Liu']	ct:The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.13528v2', 'html': None, 'tex': '/src/2307.13528v2', 'doi': 'https://doi.org/10.48550/arXiv.2307.13528'}	Submission history From: I-Chun Chern [ view email ] [v1] Tue, 25 Jul 2023 14:20:51 UTC (9,031 KB) [v2] Wed, 26 Jul 2023 15:17:49 UTC (9,031 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.13528'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.13528'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.13528'}]
2023-09-03	AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2308.15366v1	AnomalyGPT	https://twitter.com/shinmura0/status/1697091364633317707?s=20		2308.15366v1	['Zhaopeng Gu', 'Bingke Zhu', 'Guibo Zhu', 'Yingying Chen', 'Ming Tang', 'Jinqiao Wang']	ct:Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA have demonstrated the capability of understanding images and achieved remarkable performance in various visual tasks. Despite their strong abilities in recognizing common objects due to extensive training datasets, they lack specific domain knowledge and have a weaker understanding of localized details within objects, which hinders their effectiveness in the Industrial Anomaly Detection (IAD) task. On the other hand, most existing IAD methods only provide anomaly scores and necessitate the manual setting of thresholds to distinguish between normal and abnormal samples, which restricts their practical implementation. In this paper, we explore the utilization of LVLM to address the IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. We generate training data by simulating anomalous images and producing corresponding textual descriptions for each image. We also employ an image decoder to provide fine-grained semantic and design a prompt learner to fine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the need for manual threshold adjustments, thus directly assesses the presence and locations of anomalies. Additionally, AnomalyGPT supports multi-turn dialogues and exhibits impressive few-shot in-context learning capabilities. With only one normal shot, AnomalyGPT achieves the state-of-the-art performance with an accuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3% on the MVTec-AD dataset. Code is available atthis https URL.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2308.15366v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.15366'}	Submission history From: Zhaopeng Gu [ view email ] [v1] Tue, 29 Aug 2023 15:02:53 UTC (13,527 KB) [v2] Mon, 4 Sep 2023 11:44:48 UTC (13,527 KB) [v3] Wed, 13 Sep 2023 14:58:14 UTC (13,527 KB) [v4] Thu, 28 Dec 2023 08:22:14 UTC (13,527 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.15366'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.15366'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.15366'}]
2023-09-03	FaceChain: A Playground for Identity-Preserving Portrait Generation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2308.14256v1	FaceChain			2308.14256v1	['Yang Liu', 'Cheng Yu', 'Lei Shang', 'Ziheng Wu', 'Xingjun Wang', 'Yuze Zhao', 'Lin Zhu', 'Chen Cheng', 'Weitao Chen', 'Chao Xu', 'Haoyu Xie', 'Yuan Yao', 'Wenmeng Zhou', 'Yingda Chen', 'Xuansong Xie', 'Baigui Sun']	ct:Recent advancement in personalized image generation have unveiled the intriguing capability of pre-trained text-to-image models on learning identity information from a collection of portrait images. However, existing solutions can be vulnerable in producing truthful details, and usually suffer from several defects such as (i) The generated face exhibit its own unique characteristics, \ie facial shape and facial feature positioning may not resemble key characteristics of the input, and (ii) The synthesized face may contain warped, blurred or corrupted regions. In this paper, we present FaceChain, a personalized portrait generation framework that combines a series of customized image-generation model and a rich set of face-related perceptual understanding models (\eg, face detection, deep face embedding extraction, and facial attribute recognition), to tackle aforementioned challenges and to generate truthful personalized portraits, with only a handful of portrait images as input. Concretely, we inject several SOTA face models into the generation procedure, achieving a more efficient label-tagging, data-processing, and model post-processing compared to previous solutions, such as DreamBooth ~\cite{ruiz2023dreambooth} , InstantBooth ~\cite{shi2023instantbooth} , or other LoRA-only approaches ~\cite{hu2021lora} . Through the development of FaceChain, we have identified several potential directions to accelerate development of Face/Human-Centric AIGC research and application. We have designed FaceChain as a framework comprised of pluggable components that can be easily adjusted to accommodate different styles and personalized needs. We hope it can grow to serve the burgeoning needs from the communities. FaceChain is open-sourced under Apache-2.0 license at \url{this https URL}.	s an ongoing work that will be consistently refined and improved upon	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2308.14256v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.14256'}	Submission history From: Yang Liu [ view email ] [v1] Mon, 28 Aug 2023 02:20:44 UTC (465 KB) [v2] Thu, 14 Dec 2023 03:35:18 UTC (3,921 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.14256'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.14256'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.14256'}]
2023-09-03	Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2308.12966	Qwen-VL	https://twitter.com/arankomatsuzaki/status/1695964537671893306?s=20		2308.12966	['Jinze Bai', 'Shuai Bai', 'Shusheng Yang', 'Shijie Wang', 'Sinan Tan', 'Peng Wang', 'Junyang Lin', 'Chang Zhou', 'Jingren Zhou']	ct:In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available atthis https URL.	demo and models are available atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2308.12966', 'html': None, 'tex': '/src/2308.12966', 'doi': 'https://doi.org/10.48550/arXiv.2308.12966'}	Submission history From: Shuai Bai [ view email ] [v1] Thu, 24 Aug 2023 17:59:17 UTC (5,795 KB) [v2] Thu, 14 Sep 2023 17:08:39 UTC (4,670 KB) [v3] Fri, 13 Oct 2023 02:41:28 UTC (5,291 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.12966'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.12966'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.12966'}]
2023-08-27	Instruction Tuning for Large Language Models: A Survey	Computation and Language	https://arxiv.org/abs/2308.10792	Survey on Instruction Tuning for LLMs	https://twitter.com/omarsar0/status/1693978006237102589?s=20		2308.10792	['Shengyu Zhang', 'Linfeng Dong', 'Xiaoya Li', 'Sen Zhang', 'Xiaofei Sun', 'Shuhe Wang', 'Jiwei Li', 'Runyi Hu', 'Tianwei Zhang', 'Fei Wu', 'Guoyin Wang']	ct:This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\footnote{In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project Page:this http URL	st update: Dec. 1, 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2308.10792', 'html': None, 'tex': '/src/2308.10792', 'doi': 'https://doi.org/10.48550/arXiv.2308.10792'}	Submission history From: Jiwei Li [ view email ] [v1] Mon, 21 Aug 2023 15:35:16 UTC (5,073 KB) [v2] Thu, 21 Sep 2023 16:54:23 UTC (5,074 KB) [v3] Wed, 4 Oct 2023 15:00:38 UTC (4,851 KB) [v4] Mon, 9 Oct 2023 15:36:49 UTC (4,851 KB) [v5] Thu, 14 Mar 2024 02:28:22 UTC (5,622 KB) [v6] Wed, 16 Oct 2024 05:44:07 UTC (5,627 KB) [v7] Mon, 11 Nov 2024 09:25:48 UTC (5,628 KB) [v8] Sun, 1 Dec 2024 22:01:51 UTC (5,628 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.10792'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.10792'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.10792'}]
2023-08-27	Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities	Computation and Language	https://arxiv.org/abs/2308.12833	Use of LLMs for Illicit Purposes	https://twitter.com/omarsar0/status/1694885393286549636?s=20		2308.12833	['Maximilian Mozes', 'Xuanli He', 'Bennett Kleinberg', 'Lewis D. Griffin']	ct:Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.	int	['Computation and Language (cs.CL)', 'Cryptography and Security (cs.CR)']	{'pdf': '/pdf/2308.12833', 'html': None, 'tex': '/src/2308.12833', 'doi': 'https://doi.org/10.48550/arXiv.2308.12833'}	Submission history From: Maximilian Mozes [ view email ] [v1] Thu, 24 Aug 2023 14:45:50 UTC (7,241 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.12833'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.12833'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.12833'}]
2023-08-27	Giraffe: Adventures in Expanding Context Lengths in LLMs	Artificial Intelligence	https://arxiv.org/abs/2308.10882	Giraffe	https://twitter.com/bindureddy/status/1694126931174977906?s=20		2308.10882	['Arka Pal', 'Deep Karkhanis', 'Manley Roberts', 'Samuel Dooley', 'Arvind Sundararajan', 'Siddartha Naidu']	ct:Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time. To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence. We conduct a wide survey of existing methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own design as well -- in particular, a new truncation strategy for modifying the basis for the position encoding.We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context performance of LLMs. We release the three tasks publicly as datasets on HuggingFace. We discover that linear scaling is the best method for extending context length, and show that further gains can be achieved by using longer scales at evaluation time. We also discover promising extrapolation capabilities in the truncated basis. To support further research in this area, we release three new 13B parameter long-context models which we call Giraffe: 4k and 16k context models trained from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our results.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2308.10882', 'html': None, 'tex': '/src/2308.10882', 'doi': 'https://doi.org/10.48550/arXiv.2308.10882'}	Submission history From: Samuel Dooley [ view email ] [v1] Mon, 21 Aug 2023 17:30:16 UTC (345 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.10882'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.10882'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.10882'}]
2023-08-27	IT3D: Improved Text-to-3D Generation with Explicit View Synthesis	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2308.11473v1	IT3D			2308.11473v1	['Yiwen Chen', 'Chi Zhang', 'Xiaofeng Yang', 'Zhongang Cai', 'Gang Yu', 'Lei Yang', 'Guosheng Lin']	ct:Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the training of 3D models. For the incorporated discriminator, the synthesized multi-view images are considered real data, while the renderings of the optimized 3D models function as fake data. We conduct a comprehensive set of experiments that demonstrate the effectiveness of our method over baseline approaches.	t Page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2308.11473v1', 'html': None, 'tex': '/src/2308.11473v1', 'doi': 'https://doi.org/10.48550/arXiv.2308.11473'}	Submission history From: Yiwen Chen [ view email ] [v1] Tue, 22 Aug 2023 14:39:17 UTC (6,298 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.11473'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.11473'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.11473'}]
2023-08-27	A Survey on Large Language Model based Autonomous Agents	Artificial Intelligence	https://arxiv.org/abs/2308.11432v1	A Survey on LLM-based Autonomous Agents	https://twitter.com/omarsar0/status/1695440652048257251?s=20		2308.11432v1	['Lei Wang', 'Chen Ma', 'Xueyang Feng', 'Zeyu Zhang', 'Hao Yang', 'Jingsen Zhang', 'Zhiyuan Chen', 'Jiakai Tang', 'Xu Chen', 'Yankai Lin', 'Wayne Xin Zhao', 'Zhewei Wei', 'Ji-Rong Wen']	ct:Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work. Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering. Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository for the related references atthis https URL.	es, 3 figures	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2308.11432v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.11432'}	Submission history From: Xu Chen [ view email ] [v1] Tue, 22 Aug 2023 13:30:37 UTC (272 KB) [v2] Thu, 7 Sep 2023 04:42:48 UTC (372 KB) [v3] Tue, 12 Mar 2024 09:51:35 UTC (4,914 KB) [v4] Mon, 25 Mar 2024 02:56:58 UTC (4,914 KB) [v5] Thu, 4 Apr 2024 01:32:04 UTC (4,914 KB) [v6] Sun, 15 Dec 2024 07:55:51 UTC (5,102 KB) [v7] Sun, 2 Mar 2025 04:04:03 UTC (5,102 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.11432'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.11432'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.11432'}]
2023-08-27	Prompt2Model: Generating Deployable Models from Natural Language Instructions	Computation and Language	https://arxiv.org/abs/2308.12261	Prompt2Model	https://twitter.com/omarsar0/status/1694718168185598055?s=20		2308.12261	['Vijay Viswanathan', 'Chenyang Zhao', 'Amanda Bertsch', 'Tongshuang Wu', 'Graham Neubig']	ct:Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source atthis https URL.	s	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2308.12261', 'html': None, 'tex': '/src/2308.12261', 'doi': 'https://doi.org/10.48550/arXiv.2308.12261'}	Submission history From: Vijay Viswanathan [ view email ] [v1] Wed, 23 Aug 2023 17:28:21 UTC (626 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.12261'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.12261'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.12261'}]
2023-08-27	LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models	Computation and Language	https://arxiv.org/abs/2308.11462	LegalBench	https://twitter.com/NeelGuha/status/1694375959334670643?s=20		2308.11462	['Neel Guha', 'Julian Nyarko', 'Daniel E. Ho', 'Christopher Ré', 'Adam Chilton', 'Aditya Narayana', 'Alex Chohlas-Wood', 'Austin Peters', 'Brandon Waldon', 'Daniel N. Rockmore', 'Diego Zambrano', 'Dmitry Talisman', 'Enam Hoque', 'Faiz Surani', 'Frank Fagan', 'Galit Sarfaty', 'Gregory M. Dickinson', 'Haggai Porat', 'Jason Hegland', 'Jessica Wu', 'Joe Nudell', 'Joel Niklaus', 'John Nay', 'Jonathan H. Choi', 'Kevin Tobia', 'Margaret Hagan', 'Megan Ma', 'Michael Livermore', 'Nikon Rasumov-Rahe', 'Nils Holzenberger', 'Noam Kolt', 'Peter Henderson', 'Sean Rehaag', 'Sharad Goel', 'Shang Gao', 'Spencer Williams', 'Sunny Gandhi', 'Tom Zur', 'Varun Iyer', 'Zehua Li']	ct:The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning -- which distinguish between its many forms -- correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LegalBench, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LegalBench enables.	ges, 79 tables, 4 figures	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2308.11462', 'html': None, 'tex': '/src/2308.11462', 'doi': 'https://doi.org/10.48550/arXiv.2308.11462'}	Submission history From: Neel Guha [ view email ] [v1] Sun, 20 Aug 2023 22:08:03 UTC (2,932 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.11462'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.11462'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.11462'}]
2023-08-27	Language to Rewards for Robotic Skill Synthesis	Robotics	https://arxiv.org/abs/2306.08647	Language to Rewards for Robotic Skill Synthesis	https://twitter.com/GoogleAI/status/1694086273689076170?s=20		2306.08647	['Wenhao Yu', 'Nimrod Gileadi', 'Chuyuan Fu', 'Sean Kirmani', 'Kuang-Huei Lee', 'Montse Gonzalez Arenas', 'Hao-Tien Lewis Chiang', 'Tom Erez', 'Leonard Hasenclever', 'Jan Humplik', 'Brian Ichter', 'Ted Xiao', 'Peng Xu', 'Andy Zeng', 'Tingnan Zhang', 'Nicolas Heess', 'Dorsa Sadigh', 'Jie Tan', 'Yuval Tassa', 'Fei Xia']	ct:Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate interface generated by LLMs, we can effectively bridge the gap between high-level language instructions or corrections to low-level robot actions. Meanwhile, combining this with a real-time optimizer, MuJoCo MPC, empowers an interactive behavior creation experience where users can immediately observe the results and provide feedback to the system. To systematically evaluate the performance of our proposed method, we designed a total of 17 tasks for a simulated quadruped robot and a dexterous manipulator robot. We demonstrate that our proposed method reliably tackles 90% of the designed tasks, while a baseline using primitive skills as the interface with Code-as-policies achieves 50% of the tasks. We further validated our method on a real robot arm where complex manipulation skills such as non-prehensile pushing emerge through our interactive system.	ttps URL	['Robotics (cs.RO)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.08647', 'html': None, 'tex': '/src/2306.08647', 'doi': 'https://doi.org/10.48550/arXiv.2306.08647'}	Submission history From: Wenhao Yu [ view email ] [v1] Wed, 14 Jun 2023 17:27:10 UTC (30,422 KB) [v2] Fri, 16 Jun 2023 23:02:21 UTC (30,422 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.08647'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.08647'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.08647'}]
2023-08-20	Self-Alignment with Instruction Backtranslation	Computation and Language	https://arxiv.org/abs/2308.06259	Self-Alignment with Instruction Backtranslation	https://twitter.com/jaseweston/status/1690888779878330368?s=20		2308.06259	['Xian Li', 'Ping Yu', 'Chunting Zhou', 'Timo Schick', 'Omer Levy', 'Luke Zettlemoyer', 'Jason Weston', 'Mike Lewis']	ct:We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.	24 camera ready	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2308.06259', 'html': 'https://arxiv.org/html/2308.06259v3', 'tex': '/src/2308.06259', 'doi': 'https://doi.org/10.48550/arXiv.2308.06259'}	Submission history From: Xian Li [ view email ] [v1] Fri, 11 Aug 2023 17:47:54 UTC (2,972 KB) [v2] Mon, 14 Aug 2023 16:44:01 UTC (2,972 KB) [v3] Tue, 12 Mar 2024 05:22:46 UTC (2,904 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.06259'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.06259'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.06259'}]
2023-08-20	Platypus: Quick, Cheap, and Powerful Refinement of LLMs	Computation and Language	https://arxiv.org/abs/2308.07317v1	Platypus	https://twitter.com/omarsar0/status/1692549762480791959?s=20		2308.07317v1	['Ariel N. Lee', 'Cole J. Hunter', 'Nataniel Ruiz']	ct:We present $\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on $\textit{a single}$ A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page:this https URL		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2308.07317v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.07317'}	Submission history From: Ariel N. Lee [ view email ] [v1] Mon, 14 Aug 2023 17:59:56 UTC (1,260 KB) [v2] Thu, 14 Mar 2024 20:56:23 UTC (1,261 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.07317'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.07317'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.07317'}]
2023-08-20	A Survey on Model Compression for Large Language Models	Computation and Language	https://arxiv.org/abs/2308.07633	Model Compression for LLMs	https://twitter.com/omarsar0/status/1691803395160477905?s=20		2308.07633	['Xunyu Zhu', 'Jian Li', 'Yong Liu', 'Can Ma', 'Weiping Wang']	ct:Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.	ed for publication in TACL; a pre-MIT Press publication version	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2308.07633', 'html': 'https://arxiv.org/html/2308.07633v4', 'tex': '/src/2308.07633', 'doi': 'https://doi.org/10.48550/arXiv.2308.07633'}	Submission history From: Xunyu Zhu [ view email ] [v1] Tue, 15 Aug 2023 08:31:05 UTC (167 KB) [v2] Thu, 17 Aug 2023 18:16:24 UTC (168 KB) [v3] Sun, 17 Sep 2023 16:38:18 UTC (441 KB) [v4] Tue, 30 Jul 2024 13:14:55 UTC (168 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.07633'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.07633'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.07633'}]
2023-08-20	Shepherd: A Critic for Language Model Generation	Computation and Language	https://arxiv.org/abs/2308.04592	Shepherd	https://twitter.com/MetaAI/status/1691517949130207232?s=20		2308.04592	"['Tianlu Wang', 'Ping Yu', 'Xiaoqing Ellen Tan', ""Sean O'Brien"", 'Ramakanth Pasunuru', 'Jane Dwivedi-Yu', 'Olga Golovneva', 'Luke Zettlemoyer', 'Maryam Fazel-Zarandi', 'Asli Celikyilmaz']"	ct:As large language models improve, there is increasing interest in techniques that leverage these models' capabilities to refine their own outputs. In this work, we introduce Shepherd, a language model specifically tuned to critique responses and suggest refinements, extending beyond the capabilities of an untuned model to identify diverse errors and provide suggestions to remedy them. At the core of our approach is a high quality feedback dataset, which we curate from community feedback and human annotations. Even though Shepherd is small (7B parameters), its critiques are either equivalent or preferred to those from established models including ChatGPT. Using GPT-4 for evaluation, Shepherd reaches an average win-rate of 53-87% compared to competitive alternatives. In human evaluation, Shepherd strictly outperforms other models and on average closely ties with ChatGPT.	res, 7 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2308.04592', 'html': None, 'tex': '/src/2308.04592', 'doi': 'https://doi.org/10.48550/arXiv.2308.04592'}	Submission history From: Tianlu Wang [ view email ] [v1] Tue, 8 Aug 2023 21:23:23 UTC (3,438 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.04592'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.04592'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.04592'}]
2023-08-20	Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification	Computation and Language	https://arxiv.org/abs/2308.07921	Using GPT-4 Code Interpreter to Boost Mathematical Reasoning	https://twitter.com/omarsar0/status/1691630591744127355?s=20		2308.07921	['Aojun Zhou', 'Ke Wang', 'Zimu Lu', 'Weikang Shi', 'Sichun Luo', 'Zipeng Qin', 'Shaoqing Lu', 'Anya Jia', 'Linqi Song', 'Mingjie Zhan', 'Hongsheng Li']	ct:Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \uline{c}ode-based \uline{s}elf-\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \textbf{(53.9\% $\to$ 84.3\%)}.	g Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2308.07921', 'html': None, 'tex': '/src/2308.07921', 'doi': 'https://doi.org/10.48550/arXiv.2308.07921'}	Submission history From: Aojun Zhou [ view email ] [v1] Tue, 15 Aug 2023 17:58:45 UTC (850 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.07921'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.07921'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.07921'}]
2023-08-20	Teach LLMs to Personalize -- An Approach inspired by Writing Education	Computation and Language	https://arxiv.org/abs/2308.07968	Teach LLMs to Personalize	https://twitter.com/omarsar0/status/1692186726192521364?s=20		2308.07968	['Cheng Li', 'Mingyang Zhang', 'Qiaozhu Mei', 'Yaqing Wang', 'Spurthi Amba Hombaiah', 'Yi Liang', 'Michael Bendersky']	ct:Personalized text generation is an emerging research area that has attracted much attention in recent years. Most studies in this direction focus on a particular domain by designing bespoke features or models. In this work, we propose a general approach for personalized text generation using large language models (LLMs). Inspired by the practice of writing education, we develop a multistage and multitask framework to teach LLMs for personalized generation. In writing instruction, the task of writing from sources is often decomposed into multiple steps that involve finding, evaluating, summarizing, synthesizing, and integrating information. Analogously, our approach to personalized text generation consists of multiple stages: retrieval, ranking, summarization, synthesis, and generation. In addition, we introduce a multitask setting that helps the model improve its generation ability further, which is inspired by the observation in education that a student's reading proficiency and writing ability are often correlated. We evaluate our approach on three public datasets, each of which covers a different and representative domain. Our results show significant improvements over a variety of baselines.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2308.07968', 'html': None, 'tex': '/src/2308.07968', 'doi': 'https://doi.org/10.48550/arXiv.2308.07968'}	Submission history From: Cheng Li [ view email ] [v1] Tue, 15 Aug 2023 18:06:23 UTC (412 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.07968'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.07968'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.07968'}]
2023-08-20	OctoPack: Instruction Tuning Code Large Language Models	Computation and Language	https://arxiv.org/abs/2308.07124v1	OctoPack	https://twitter.com/arankomatsuzaki/status/1691259656453193728?s=20		2308.07124v1	['Niklas Muennighoff', 'Qian Liu', 'Armel Zebaze', 'Qinkai Zheng', 'Binyuan Hui', 'Terry Yue Zhuo', 'Swayam Singh', 'Xiangru Tang', 'Leandro von Werra', 'Shayne Longpre']	ct:Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available atthis https URL.	es (9 main), 39 figures, 16 tables	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2308.07124v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.07124'}	Submission history From: Niklas Muennighoff [ view email ] [v1] Mon, 14 Aug 2023 13:53:54 UTC (1,643 KB) [v2] Sun, 18 Feb 2024 09:46:06 UTC (1,647 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.07124'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.07124'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.07124'}]
2023-08-20	Efficient Guided Generation for Large Language Models	Computation and Language	https://arxiv.org/abs/2307.09702	Efficient Guided Generation for LLMs	https://twitter.com/omarsar0/status/1691179888214966273?s=20		2307.09702	['Brandon T. Willard', 'Rémi Louf']	ct:In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.09702', 'html': None, 'tex': '/src/2307.09702', 'doi': 'https://doi.org/10.48550/arXiv.2307.09702'}	Submission history From: Brandon T. Willard [ view email ] [v1] Wed, 19 Jul 2023 01:14:49 UTC (25 KB) [v2] Thu, 20 Jul 2023 00:40:41 UTC (76 KB) [v3] Sat, 12 Aug 2023 21:09:44 UTC (180 KB) [v4] Sat, 19 Aug 2023 21:27:51 UTC (181 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.09702'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.09702'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.09702'}]
2023-08-20	Bayesian Flow Networks	Machine Learning	https://arxiv.org/abs/2308.07037	Bayesian Flow Networks	https://twitter.com/nnaisense/status/1691310494039379969?s=20		2308.07037	['Alex Graves', 'Rupesh Kumar Srivastava', 'Timothy Atkinson', 'Faustino Gomez']	ct:This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no restrictions on the network architecture. In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2308.07037', 'html': 'https://arxiv.org/html/2308.07037v6', 'tex': '/src/2308.07037', 'doi': 'https://doi.org/10.48550/arXiv.2308.07037'}	Submission history From: Alex Graves [ view email ] [v1] Mon, 14 Aug 2023 09:56:35 UTC (3,155 KB) [v2] Thu, 21 Sep 2023 14:38:24 UTC (3,155 KB) [v3] Mon, 23 Oct 2023 10:36:58 UTC (3,155 KB) [v4] Mon, 27 Nov 2023 16:15:44 UTC (3,155 KB) [v5] Sat, 3 Feb 2024 20:22:57 UTC (3,155 KB) [v6] Tue, 11 Mar 2025 08:07:39 UTC (3,156 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.07037'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.07037'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.07037'}]
2023-08-13	LLM As DBA	Databases	https://arxiv.org/abs/2308.05481	LLMs as Database Administrators	https://twitter.com/omarsar0/status/1689811820272353280?s=20		2308.05481	['Xuanhe Zhou', 'Guoliang Li', 'Zhiyuan Liu']	ct:Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available atthis http URL.		['Databases (cs.DB)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2308.05481', 'html': None, 'tex': '/src/2308.05481', 'doi': 'https://doi.org/10.48550/arXiv.2308.05481'}	Submission history From: Xuanhe Zhou [ view email ] [v1] Thu, 10 Aug 2023 10:12:43 UTC (7,074 KB) [v2] Fri, 11 Aug 2023 07:55:19 UTC (7,074 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.05481'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.05481'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.05481'}]
2023-08-13	AgentBench: Evaluating LLMs as Agents	Artificial Intelligence	https://arxiv.org/abs/2308.03688v1	Evaluating LLMs as Agents	https://twitter.com/arankomatsuzaki/status/1688719837760000000?s=20		2308.03688v1	['Xiao Liu', 'Hao Yu', 'Hanchen Zhang', 'Yifan Xu', 'Xuanyu Lei', 'Hanyu Lai', 'Yu Gu', 'Hangliang Ding', 'Kaiwen Men', 'Kejuan Yang', 'Shudan Zhang', 'Xiang Deng', 'Aohan Zeng', 'Zhengxiao Du', 'Chenhui Zhang', 'Sheng Shen', 'Tianjun Zhang', 'Yu Su', 'Huan Sun', 'Minlie Huang', 'Yuxiao Dong', 'Jie Tang']	ct:Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released atthis https URL	es	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2308.03688v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.03688'}	Submission history From: Xiao Liu [ view email ] [v1] Mon, 7 Aug 2023 16:08:11 UTC (20,331 KB) [v2] Wed, 25 Oct 2023 07:41:24 UTC (20,908 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.03688'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.03688'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.03688'}]
2023-08-13	Studying Large Language Model Generalization with Influence Functions	Machine Learning	https://arxiv.org/abs/2308.03296	Studying LLM Generalization with Influence Functions	https://twitter.com/AnthropicAI/status/1688946685937090560?s=20		2308.03296	['Roger Grosse', 'Juhan Bae', 'Cem Anil', 'Nelson Elhage', 'Alex Tamkin', 'Amirhossein Tajdini', 'Benoit Steiner', 'Dustin Li', 'Esin Durmus', 'Ethan Perez', 'Evan Hubinger', 'Kamilė Lukošiūtė', 'Karina Nguyen', 'Nicholas Joseph', 'Sam McCandlish', 'Jared Kaplan', 'Samuel R. Bowman']	ct:When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.	ges, 47 figures, 22 tables	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2308.03296', 'html': None, 'tex': '/src/2308.03296', 'doi': 'https://doi.org/10.48550/arXiv.2308.03296'}	Submission history From: Juhan Bae [ view email ] [v1] Mon, 7 Aug 2023 04:47:42 UTC (1,257 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.03296'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.03296'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.03296'}]
2023-08-13	Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals	Electrical Engineering and Systems Science > Image and Video Processing	https://arxiv.org/abs/2308.02510	Seeing Through the Brain	https://twitter.com/_akhaliq/status/1688787286807228416?s=20		2308.02510	['Yu-Ting Lan', 'Kan Ren', 'Yansen Wang', 'Wei-Long Zheng', 'Dongsheng Li', 'Bao-Liang Lu', 'Lili Qiu']	ct:Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NeuroImagen, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multi-grained outputs from the given EEG data. A latent diffusion model will then leverage the extracted information to reconstruct the high-resolution visual stimuli images. The experimental results have illustrated the effectiveness of image reconstruction and superior quantitative performance of our proposed method.	rint version of an ongoing work	['Image and Video Processing (eess.IV)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Multimedia (cs.MM)', 'Neurons and Cognition (q-bio.NC)']	{'pdf': '/pdf/2308.02510', 'html': None, 'tex': '/src/2308.02510', 'doi': 'https://doi.org/10.48550/arXiv.2308.02510'}	Submission history From: Kan Ren [ view email ] [v1] Thu, 27 Jul 2023 12:54:16 UTC (5,369 KB) [v2] Wed, 16 Aug 2023 09:59:40 UTC (5,369 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.02510'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.02510'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.02510'}]
2023-08-13	SynJax: Structured Probability Distributions for JAX	Machine Learning	https://arxiv.org/abs/2308.03291v1	SynJax	https://twitter.com/milosstanojevic/status/1688896558790520832?s=20		2308.03291v1	['Miloš Stanojević', 'Laurent Sartran']	ct:The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available atthis https URL.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2308.03291v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.03291'}	Submission history From: Miloš Stanojević [ view email ] [v1] Mon, 7 Aug 2023 04:20:38 UTC (124 KB) [v2] Tue, 15 Aug 2023 08:20:30 UTC (124 KB) [v3] Sun, 15 Oct 2023 23:51:32 UTC (125 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.03291'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.03291'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.03291'}]
2023-08-13	Simple synthetic data reduces sycophancy in large language models	Computation and Language	https://arxiv.org/abs/2308.03958	Synthetic Data Reduces Sycophancy in LLMs	https://twitter.com/JerryWeiAI/status/1689340237993185280?s=20		2308.03958	['Jerry Wei', 'Da Huang', 'Yifeng Lu', 'Denny Zhou', 'Quoc V. Le']	ct:Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior.First, on a set of three sycophancy tasks (Perez et al., 2022) where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for PaLM models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well.To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP tasks and encourages models to be robust to user opinions on these tasks. Adding these data in a lightweight finetuning step can significantly reduce sycophantic behavior on held-out prompts. Code for generating synthetic data for intervention can be found atthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2308.03958', 'html': None, 'tex': '/src/2308.03958', 'doi': 'https://doi.org/10.48550/arXiv.2308.03958'}	Submission history From: Jerry Wei [ view email ] [v1] Mon, 7 Aug 2023 23:48:36 UTC (151 KB) [v2] Thu, 15 Feb 2024 01:03:13 UTC (151 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.03958'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.03958'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.03958'}]
2023-08-13	PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2308.03977	Photorealistic Unreal Graphics (PUG)	https://twitter.com/MetaAI/status/1689316127846109184?s=20		2308.03977	['Florian Bordes', 'Shashank Shekhar', 'Mark Ibrahim', 'Diane Bouchacourt', 'Pascal Vincent', 'Ari S. Morcos']	ct:Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation. Despite such promise, the use of synthetic image data is still limited -- and often played down -- mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear. In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and realism. We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic Unreal Graphics) environments and datasets for representation learning. In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models.		['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2308.03977', 'html': None, 'tex': '/src/2308.03977', 'doi': 'https://doi.org/10.48550/arXiv.2308.03977'}	Submission history From: Florian Bordes [ view email ] [v1] Tue, 8 Aug 2023 01:33:13 UTC (41,951 KB) [v2] Wed, 13 Dec 2023 01:44:58 UTC (41,997 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.03977'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.03977'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.03977'}]
2023-08-13	Pre-Trained Large Language Models for Industrial Control	Artificial Intelligence	https://arxiv.org/abs/2308.03028	LLMs for Industrial Control	https://twitter.com/emollick/status/1688760539441217536?s=20		2308.03028	['Lei Song', 'Chuheng Zhang', 'Li Zhao', 'Jiang Bian']	ct:For industrial control, developing high-performance controllers with few samples and low technical debt is appealing. Foundation models, possessing rich prior knowledge obtained from pre-training with Internet-scale corpus, have the potential to be a good controller with proper prompts. In this paper, we take HVAC (Heating, Ventilation, and Air Conditioning) building control as an example to examine the ability of GPT-4 (one of the first-tier foundation models) as the controller. To control HVAC, we wrap the task as a language game by providing text including a short description for the task, several selected demonstrations, and the current observation to GPT-4 on each step and execute the actions responded by GPT-4. We conduct series of experiments to answer the following questions: 1)~How well can GPT-4 control HVAC? 2)~How well can GPT-4 generalize to different scenarios for HVAC control? 3) How different parts of the text context affect the performance? In general, we found GPT-4 achieves the performance comparable to RL methods with few samples and low technical debt, indicating the potential of directly applying foundation models to industrial control tasks.		['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2308.03028', 'html': None, 'tex': '/src/2308.03028', 'doi': 'https://doi.org/10.48550/arXiv.2308.03028'}	Submission history From: Lei Song [ view email ] [v1] Sun, 6 Aug 2023 06:01:18 UTC (1,921 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.03028'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.03028'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.03028'}]
2023-08-13	Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment	Artificial Intelligence	https://arxiv.org/abs/2308.05374	Trustworthy LLMs	https://twitter.com/_akhaliq/status/1689818964669390848?s=20		2308.05374	['Yang Liu', 'Yuanshun Yao', 'Jean-Francois Ton', 'Xiaoying Zhang', 'Ruocheng Guo', 'Hao Cheng', 'Yegor Klochkov', 'Muhammad Faaiz Taufiq', 'Hang Li']	ct:Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.	several typos	['Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2308.05374', 'html': None, 'tex': '/src/2308.05374', 'doi': 'https://doi.org/10.48550/arXiv.2308.05374'}	Submission history From: Yang Liu [ view email ] [v1] Thu, 10 Aug 2023 06:43:44 UTC (966 KB) [v2] Thu, 21 Mar 2024 00:21:14 UTC (966 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.05374'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.05374'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.05374'}]
2023-08-06	Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback	Artificial Intelligence	https://arxiv.org/abs/2307.15217	Open Problem and Limitation of RLHF	https://twitter.com/arankomatsuzaki/status/1685813753063870465?s=20		2307.15217	['Stephen Casper', 'Xander Davies', 'Claudia Shi', 'Thomas Krendl Gilbert', 'Jérémy Scheurer', 'Javier Rando', 'Rachel Freedman', 'Tomasz Korbak', 'David Lindner', 'Pedro Freire', 'Tony Wang', 'Samuel Marks', 'Charbel-Raphaël Segerie', 'Micah Carroll', 'Andi Peng', 'Phillip Christoffersen', 'Mehul Damani', 'Stewart Slocum', 'Usman Anwar', 'Anand Siththaranjan', 'Max Nadeau', 'Eric J. Michaud', 'Jacob Pfau', 'Dmitrii Krasheninnikov', 'Xin Chen', 'Lauro Langosco', 'Peter Hase', 'Erdem Bıyık', 'Anca Dragan', 'David Krueger', 'Dorsa Sadigh', 'Dylan Hadfield-Menell']	ct:Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.15217', 'html': None, 'tex': '/src/2307.15217', 'doi': 'https://doi.org/10.48550/arXiv.2307.15217'}	Submission history From: Stephen Casper [ view email ] [v1] Thu, 27 Jul 2023 22:29:25 UTC (921 KB) [v2] Mon, 11 Sep 2023 17:25:24 UTC (939 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.15217'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.15217'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.15217'}]
2023-08-06	Med-Flamingo: a Multimodal Medical Few-shot Learner	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2307.15189	Med-Flamingo	https://twitter.com/Michael_D_Moor/status/1685804620730540033?s=20		2307.15189	['Michael Moor', 'Qian Huang', 'Shirley Wu', 'Michihiro Yasunaga', 'Cyril Zakka', 'Yash Dalmia', 'Eduardo Pontes Reis', 'Pranav Rajpurkar', 'Jure Leskovec']	ct:Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models (VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering (VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app underthis https URL.	nt	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.15189', 'html': None, 'tex': '/src/2307.15189', 'doi': 'https://doi.org/10.48550/arXiv.2307.15189'}	Submission history From: Michael Moor [ view email ] [v1] Thu, 27 Jul 2023 20:36:02 UTC (3,182 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.15189'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.15189'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.15189'}]
2023-08-06	ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs	Artificial Intelligence	https://arxiv.org/abs/2307.16789v1	ToolLLM	https://twitter.com/omarsar0/status/1687531613574348800?s=20		2307.16789v1	['Yujia Qin', 'Shihao Liang', 'Yining Ye', 'Kunlun Zhu', 'Lan Yan', 'Yaxi Lu', 'Yankai Lin', 'Xin Cong', 'Xiangru Tang', 'Bill Qian', 'Sihan Zhao', 'Runchu Tian', 'Ruobing Xie', 'Jie Zhou', 'Mark Gerstein', 'Dahai Li', 'Zhiyuan Liu', 'Maosong Sun']	ct:Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs). This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain. This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source. To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT. Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPT to generate diverse human instructions involving these APIs, covering both single-tool and multi-tool scenarios. Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enabling LLMs to evaluate multiple reasoning traces and expand the search space. We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs. For efficient tool-use assessment, we develop an automatic evaluator: ToolEval. We fine-tune LLaMA on ToolBench and obtain ToolLLaMA. Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.16789v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2307.16789'}	Submission history From: Yujia Qin [ view email ] [v1] Mon, 31 Jul 2023 15:56:53 UTC (1,473 KB) [v2] Tue, 3 Oct 2023 14:45:48 UTC (1,477 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.16789'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.16789'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.16789'}]
2023-08-06	Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation	Computation and Language	https://arxiv.org/abs/2307.15337	Skeleton-of-Thought	https://twitter.com/omarsar0/status/1685832487103008768?s=20		2307.15337	['Xuefei Ning', 'Zinan Lin', 'Zixuan Zhou', 'Zifu Wang', 'Huazhong Yang', 'Yu Wang']	ct:This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric optimization for inference efficiency, and showcases the potential of eliciting high-quality answers by explicitly planning the answer structure in language.	R'24	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.15337', 'html': 'https://arxiv.org/html/2307.15337v3', 'tex': '/src/2307.15337', 'doi': 'https://doi.org/10.48550/arXiv.2307.15337'}	Submission history From: Xuefei Ning PhD [ view email ] [v1] Fri, 28 Jul 2023 06:31:34 UTC (454 KB) [v2] Sun, 8 Oct 2023 03:59:29 UTC (1,680 KB) [v3] Sat, 2 Mar 2024 02:45:33 UTC (1,913 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.15337'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.15337'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.15337'}]
2023-08-06	MetaGPT: Meta Programming for Multi-Agent Collaborative Framework	Artificial Intelligence	https://arxiv.org/abs/2308.00352v2	MetaGPT	https://twitter.com/ai_database/status/1686949868298973184?s=20		2308.00352v2	['Sirui Hong', 'Xiawu Zheng', 'Jonathan Chen', 'Yuheng Cheng', 'Ceyao Zhang', 'Zili Wang', 'Steven Ka Shing Yau', 'Zijuan Lin', 'Liyang Zhou', 'Chenyu Ran', 'Lingfeng Xiao', 'Chenglin Wu']	ct:Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicatedthis http URL, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assign diverse roles to various agents, thus establishing a framework that can effectively and cohesively deconstruct complex multi-agent collaborative problems. Our experiments conducted on collaborative software engineering tasks illustrate MetaGPT's capability in producing comprehensive solutions with higher coherence relative to existing conversational and chat-based multi-agent systems. This underscores the potential of incorporating human domain knowledge into multi-agents, thus opening up novel avenues for grappling with intricate real-world challenges. The GitHub repository of this project is made publicly available on:this https URL		['Artificial Intelligence (cs.AI)', 'Multiagent Systems (cs.MA)']	{'pdf': '/pdf/2308.00352v2', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2308.00352'}	Submission history From: Sirui Hong [ view email ] [v1] Tue, 1 Aug 2023 07:49:10 UTC (7,361 KB) [v2] Wed, 2 Aug 2023 04:11:02 UTC (7,362 KB) [v3] Mon, 7 Aug 2023 19:20:19 UTC (12,029 KB) [v4] Thu, 17 Aug 2023 04:01:31 UTC (12,051 KB) [v5] Mon, 6 Nov 2023 17:01:39 UTC (22,762 KB) [v6] Mon, 21 Oct 2024 17:22:45 UTC (22,768 KB) [v7] Fri, 1 Nov 2024 14:36:52 UTC (22,767 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.00352'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.00352'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.00352'}]
2023-08-06	OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2308.01390	OpenFlamingo	https://twitter.com/anas_awadalla/status/1687295129005195264?s=20		2308.01390	['Anas Awadalla', 'Irena Gao', 'Josh Gardner', 'Jack Hessel', 'Yusuf Hanafy', 'Wanrong Zhu', 'Kalyani Marathe', 'Yonatan Bitton', 'Samir Gadre', 'Shiori Sagawa', 'Jenia Jitsev', 'Simon Kornblith', 'Pang Wei Koh', 'Gabriel Ilharco', 'Mitchell Wortsman', 'Ludwig Schmidt']	ct:We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code atthis https URL.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2308.01390', 'html': None, 'tex': '/src/2308.01390', 'doi': 'https://doi.org/10.48550/arXiv.2308.01390'}	Submission history From: Anas Awadalla [ view email ] [v1] Wed, 2 Aug 2023 19:10:23 UTC (5,877 KB) [v2] Mon, 7 Aug 2023 17:53:09 UTC (5,877 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.01390'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.01390'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.01390'}]
2023-08-06	The Hydra Effect: Emergent Self-repair in Language Model Computations	Machine Learning	https://arxiv.org/abs/2307.15771	The Hydra Effect	https://twitter.com/_akhaliq/status/1686192437771788288?s=20		2307.15771	['Thomas McGrath', 'Matthew Rahtz', 'Janos Kramar', 'Vladimir Mikulik', 'Shane Legg']	ct:We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2307.15771', 'html': None, 'tex': '/src/2307.15771', 'doi': 'https://doi.org/10.48550/arXiv.2307.15771'}	Submission history From: Thomas McGrath [ view email ] [v1] Fri, 28 Jul 2023 19:13:26 UTC (4,976 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.15771'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.15771'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.15771'}]
2023-08-06	SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning	Artificial Intelligence	https://arxiv.org/abs/2308.00436	Self-Check	https://twitter.com/_akhaliq/status/1686561569486827520?s=20		2308.00436	['Ning Miao', 'Yee Whye Teh', 'Tom Rainforth']	ct:The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2308.00436', 'html': None, 'tex': '/src/2308.00436', 'doi': 'https://doi.org/10.48550/arXiv.2308.00436'}	Submission history From: Ning Miao [ view email ] [v1] Tue, 1 Aug 2023 10:31:36 UTC (429 KB) [v2] Wed, 2 Aug 2023 08:45:40 UTC (429 KB) [v3] Thu, 5 Oct 2023 12:59:59 UTC (376 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.00436'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.00436'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.00436'}]
2023-08-06	Learning to Model the World with Language	Computation and Language	https://arxiv.org/abs/2308.01399	Agents Model the World with Language	https://twitter.com/johnjnay/status/1687277999517818880?s=20		2308.01399	['Jessy Lin', 'Yuqing Du', 'Olivia Watkins', 'Danijar Hafner', 'Pieter Abbeel', 'Dan Klein', 'Anca Dragan']	"ct:To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents can learn to execute simple language instructions, we aim to build agents that leverage diverse language -- language like ""this button turns on the TV"" or ""I put the bowls away"" -- that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. While current methods that learn language-conditioned policies degrade in performance with more diverse types of language, we show that Dynalang learns to leverage environment descriptions, game rules, and instructions to excel on tasks ranging from game-playing to navigating photorealistic home scans. Finally, we show that our method enables additional capabilities due to learning a generative model: Dynalang can be pretrained on text-only data, enabling learning from offline datasets, and generate language grounded in an environment."	024. Website:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2308.01399', 'html': 'https://arxiv.org/html/2308.01399v2', 'tex': '/src/2308.01399', 'doi': 'https://doi.org/10.48550/arXiv.2308.01399'}	Submission history From: Jessy Lin [ view email ] [v1] Mon, 31 Jul 2023 17:57:49 UTC (908 KB) [v2] Fri, 31 May 2024 15:32:02 UTC (960 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2308.01399'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2308.01399'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2308.01399'}]
2023-08-06	Discovering Adaptable Symbolic Algorithms from Scratch	Robotics	https://arxiv.org/abs/2307.16890	AutoRobotics-Zero	https://twitter.com/XingyouSong/status/1686190266578046976?s=20		2307.16890	['Stephen Kelly', 'Daniel S. Park', 'Xingyou Song', 'Mitchell McIntire', 'Pranav Nashikkar', 'Ritam Guha', 'Wolfgang Banzhaf', 'Kalyanmoy Deb', 'Vishnu Naresh Boddeti', 'Jie Tan', 'Esteban Real']	ct:Autonomous robots deployed in the real world will need control policies that rapidly adapt to environmental changes. To this end, we propose AutoRobotics-Zero (ARZ), a method based on AutoML-Zero that discovers zero-shot adaptable policies from scratch. In contrast to neural network adaptation policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine. We evolve modular policies that tune their model parameters and alter their inference algorithm on-the-fly to adapt to sudden environmental changes. We demonstrate our method on a realistic simulated quadruped robot, for which we evolve safe control policies that avoid falling when individual limbs suddenly break. This is a challenging task in which two popular neural network baselines fail. Finally, we conduct a detailed analysis of our method on a novel and challenging non-stationary control task dubbed Cataclysmic Cartpole. Results confirm our findings that ARZ is significantly more robust to sudden environmental changes and can build simple, interpretable control policies.	hed and Best Overall Paper Finalist at International Conference on Intelligent Robots and Systems (IROS) 2023. Seethis https URLfor associated video file	['Robotics (cs.RO)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Neural and Evolutionary Computing (cs.NE)']	{'pdf': '/pdf/2307.16890', 'html': None, 'tex': '/src/2307.16890', 'doi': 'https://doi.org/10.48550/arXiv.2307.16890'}	Submission history From: Xingyou Song [ view email ] [v1] Mon, 31 Jul 2023 17:57:48 UTC (1,014 KB) [v2] Fri, 13 Oct 2023 20:49:40 UTC (1,014 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.16890'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.16890'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.16890'}]
2023-07-30	Universal and Transferable Adversarial Attacks on Aligned Language Models	Computation and Language	https://arxiv.org/abs/2307.15043	Universal Adversarial LLM Attacks	https://twitter.com/andyzou_jiaming/status/1684766170766004224?s=20		2307.15043	['Andy Zou', 'Zifan Wang', 'Nicholas Carlini', 'Milad Nasr', 'J. Zico Kolter', 'Matt Fredrikson']	"ct:Because ""out-of-the-box"" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called ""jailbreaks"" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available atthis http URL."	e:this http URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Cryptography and Security (cs.CR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.15043', 'html': 'https://arxiv.org/html/2307.15043v2', 'tex': '/src/2307.15043', 'doi': 'https://doi.org/10.48550/arXiv.2307.15043'}	Submission history From: Andy Zou [ view email ] [v1] Thu, 27 Jul 2023 17:49:12 UTC (19,101 KB) [v2] Wed, 20 Dec 2023 20:48:57 UTC (3,738 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.15043'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.15043'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.15043'}]
2023-07-30	Towards Generalist Biomedical AI	Computation and Language	https://arxiv.org/abs/2307.14334	Med-PaLM Multimodal	https://twitter.com/vivnat/status/1684404882844024832?s=20		2307.14334	['Tao Tu', 'Shekoofeh Azizi', 'Danny Driess', 'Mike Schaekermann', 'Mohamed Amin', 'Pi-Chuan Chang', 'Andrew Carroll', 'Chuck Lau', 'Ryutaro Tanno', 'Ira Ktena', 'Basil Mustafa', 'Aakanksha Chowdhery', 'Yun Liu', 'Simon Kornblith', 'David Fleet', 'Philip Mansfield', 'Sushant Prakash', 'Renee Wong', 'Sunny Virmani', 'Christopher Semturs', 'S Sara Mahdavi', 'Bradley Green', 'Ewa Dominowska', 'Blaise Aguera y Arcas', 'Joelle Barral', 'Dale Webster', 'Greg S. Corrado', 'Yossi Matias', 'Karan Singhal', 'Pete Florence', 'Alan Karthikesalingam', 'Vivek Natarajan']	ct:Medicine is inherently multimodal, with rich data modalities spanning text, imaging, genomics, and more. Generalist biomedical artificial intelligence (AI) systems that flexibly encode, integrate, and interpret this data at scale can potentially enable impactful applications ranging from scientific discovery to care delivery. To enable the development of these models, we first curate MultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses 14 diverse tasks such as medical question answering, mammography and dermatology image interpretation, radiology report generation and summarization, and genomic variant calling. We then introduce Med-PaLM Multimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same set of model weights. Med-PaLM M reaches performance competitive with or exceeding the state of the art on all MultiMedBench tasks, often surpassing specialist models by a wide margin. We also report examples of zero-shot generalization to novel medical concepts and tasks, positive transfer learning across tasks, and emergent zero-shot medical reasoning. To further probe the capabilities and limitations of Med-PaLM M, we conduct a radiologist evaluation of model-generated (and human) chest X-ray reports and observe encouraging performance across model scales. In a side-by-side ranking on 246 retrospective chest X-rays, clinicians express a pairwise preference for Med-PaLM M reports over those produced by radiologists in up to 40.50% of cases, suggesting potential clinical utility. While considerable work is needed to validate these models in real-world use cases, our results represent a milestone towards the development of generalist biomedical AI systems.		['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2307.14334', 'html': None, 'tex': '/src/2307.14334', 'doi': 'https://doi.org/10.48550/arXiv.2307.14334'}	Submission history From: Shekoofeh Azizi [ view email ] [v1] Wed, 26 Jul 2023 17:52:22 UTC (2,934 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.14334'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.14334'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.14334'}]
2023-07-30	Tracking Anything in High Quality	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2307.13974v1	Tracking Anything in High Quality	https://twitter.com/arankomatsuzaki/status/1684380610901467136?s=20		2307.13974v1	['Jiawen Zhu', 'Zhenyu Chen', 'Zeqi Hao', 'Shijie Chang', 'Lu Zhang', 'Dong Wang', 'Huchuan Lu', 'Bin Luo', 'Jun-Yan He', 'Jin-Peng Lan', 'Hanyuan Chen', 'Chenyang Li']	ct:Visual object tracking is a fundamental video task in computer vision. Recently, the notably increasing power of perception algorithms allows the unification of single/multiobject and box/mask-based tracking. Among them, the Segment Anything Model (SAM) attracts much attention. In this report, we propose HQTrack, a framework for High Quality Tracking anything in videos. HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). Given the object to be tracked in the initial frame of a video, VMOS propagates the object masks to the current frame. The mask results at this stage are not accurate enough since VMOS is trained on several closeset video object segmentation (VOS) datasets, which has limited ability to generalize to complex and corner scenes. To further improve the quality of tracking masks, a pretrained MR model is employed to refine the tracking results. As a compelling testament to the effectiveness of our paradigm, without employing any tricks such as test-time data augmentations and model ensemble, HQTrack ranks the 2nd place in the Visual Object Tracking and Segmentation (VOTS2023) challenge. Code and models are available atthis https URL.	cal Report	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2307.13974v1', 'html': None, 'tex': '/src/2307.13974v1', 'doi': 'https://doi.org/10.48550/arXiv.2307.13974'}	Submission history From: Jiawen Zhu [ view email ] [v1] Wed, 26 Jul 2023 06:19:46 UTC (1,450 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.13974'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.13974'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.13974'}]
2023-07-30	Foundational Models Defining a New Era in Vision: A Survey and Outlook	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2307.13721v1	Foundation Models in Vision	https://twitter.com/KhanSalmanH/status/1684496991215316992?s=20		2307.13721v1	['Muhammad Awais', 'Muzammal Naseer', 'Salman Khan', 'Rao Muhammad Anwer', 'Hisham Cholakkal', 'Mubarak Shah', 'Ming-Hsuan Yang', 'Fahad Shahbaz Khan']	ct:Vision systems to see and reason about the compositional nature of visual scenes are fundamental to understanding our world. The complex relations between objects and their locations, ambiguities, and variations in the real-world environment can be better described in human language, naturally governed by grammatical rules and other modalities such as audio and depth. The models learned to bridge the gap between such modalities coupled with large-scale training data facilitate contextual reasoning, generalization, and prompt capabilities at test time. These models are referred to as foundational models. The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box, having interactive dialogues by asking questions about an image or video scene or manipulating the robot's behavior through language instructions. In this survey, we provide a comprehensive review of such emerging foundational models, including typical architecture designs to combine different modalities (vision, text, audio, etc), training objectives (contrastive, generative), pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous. We discuss the open challenges and research directions for foundational models in computer vision, including difficulties in their evaluations and benchmarking, gaps in their real-world understanding, limitations of their contextual understanding, biases, vulnerability to adversarial attacks, and interpretability issues. We review recent developments in this field, covering a wide range of applications of foundation models systematically and comprehensively. A comprehensive list of foundational models studied in this work is available at \url{this https URL}.	t page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.13721v1', 'html': None, 'tex': '/src/2307.13721v1', 'doi': 'https://doi.org/10.48550/arXiv.2307.13721'}	Submission history From: Muzammal Naseer [ view email ] [v1] Tue, 25 Jul 2023 17:59:18 UTC (2,879 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.13721'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.13721'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.13721'}]
2023-07-30	L-Eval: Instituting Standardized Evaluation for Long Context Language Models	Computation and Language	https://arxiv.org/abs/2307.11088v1	L-Eval	https://twitter.com/WenxiangJiao/status/1682208555762610176?s=20		2307.11088v1	['Chenxin An', 'Shansan Gong', 'Ming Zhong', 'Mukai Li', 'Jun Zhang', 'Lingpeng Kong', 'Xipeng Qiu']	ct:Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories. While proprietary models such as GPT-4 and Claude have demonstrated considerable advancements in handling tens of thousands of tokens of context, open-sourced models are still in the early stages of experimentation. It also remains unclear whether developing these long context models can offer substantial gains on practical downstream tasks over retrieval-based methods or models simply trained on chunked contexts. To address this challenge, we propose to institute standardized evaluation for long context language models. Concretely, we develop L-Eval which contains 411 long documents and over 2,000 query-response pairs manually annotated and checked by the authors encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings. L-Eval also adopts diverse evaluation methods and instruction styles, enabling a more reliable assessment of Long Context Language Models (LCLMs). Our findings indicate that while open-source models typically lag behind their commercial counterparts, they still exhibit impressive performance. LLaMA2 achieves the best results (win 45\% vs turbo-16k) on open-ended tasks with only 4k context length and ChatGLM2 achieves the best results on closed-ended tasks with 8k input tokens. We release our new evaluation suite, code, and all generation results including predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at {\url{this https URL}}.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2307.11088v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2307.11088'}	Submission history From: Chenxin An [ view email ] [v1] Thu, 20 Jul 2023 17:59:41 UTC (63 KB) [v2] Mon, 31 Jul 2023 17:19:52 UTC (91 KB) [v3] Wed, 4 Oct 2023 10:04:25 UTC (1,447 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.11088'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.11088'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.11088'}]
2023-07-30	LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition	Computation and Language	https://arxiv.org/abs/2307.13269v1	LoraHub	https://twitter.com/_akhaliq/status/1684030297661403136?s=20		2307.13269v1	['Chengsong Huang', 'Qian Liu', 'Bill Yuchen Lin', 'Tianyu Pang', 'Chao Du', 'Min Lin']	ct:Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA modules, thereby facilitating their application to new tasks. We anticipate this resource will widen access to and spur advancements in general intelligence as well as LLMs in production. Code will be available atthis https URL.	n progress. The first three authors contributed equally to this work	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.13269v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2307.13269'}	Submission history From: Qian Liu [ view email ] [v1] Tue, 25 Jul 2023 05:39:21 UTC (321 KB) [v2] Thu, 18 Jan 2024 06:53:39 UTC (343 KB) [v3] Mon, 19 Aug 2024 03:31:19 UTC (354 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.13269'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.13269'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.13269'}]
2023-07-30	Aligning Large Language Models with Human: A Survey	Computation and Language	https://arxiv.org/abs/2307.12966v1	Survey of Aligned LLMs	https://twitter.com/omarsar0/status/1684960627423420419?s=20		2307.12966v1	['Yufei Wang', 'Wanjun Zhong', 'Liangyou Li', 'Fei Mi', 'Xingshan Zeng', 'Wenyong Huang', 'Lifeng Shang', 'Xin Jiang', 'Qun Liu']	ct:Large Language Models (LLMs) trained on extensive textual corpora have emerged as leading solutions for a broad array of Natural Language Processing (NLP) tasks. Despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information. Hence, aligning LLMs with human expectations has become an active area of interest within the research community. This survey presents a comprehensive overview of these alignment technologies, including the following aspects. (1) Data collection: the methods for effectively collecting high-quality instructions for LLM alignment, including the use of NLP benchmarks, human annotations, and leveraging strong LLMs. (2) Training methodologies: a detailed review of the prevailing training methods employed for LLM alignment. Our exploration encompasses Supervised Fine-tuning, both Online and Offline human preference training, along with parameter-efficient training mechanisms. (3) Model Evaluation: the methods for evaluating the effectiveness of these human-aligned LLMs, presenting a multifaceted approach towards their assessment. In conclusion, we collate and distill our findings, shedding light on several promising future research avenues in the field. This survey, therefore, serves as a valuable resource for anyone invested in understanding and advancing the alignment of LLMs to better suit human-oriented tasks and expectations. An associated GitHub link collecting the latest papers is available atthis https URL.	n progress	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2307.12966v1', 'html': None, 'tex': '/src/2307.12966v1', 'doi': 'https://doi.org/10.48550/arXiv.2307.12966'}	Submission history From: Yufei Wang [ view email ] [v1] Mon, 24 Jul 2023 17:44:58 UTC (7,408 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.12966'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.12966'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.12966'}]
2023-07-30	WavJourney: Compositional Audio Creation with Large Language Models	Sound	https://arxiv.org/abs/2307.14335v1	WavJourney	https://twitter.com/LiuXub/status/1684338437934002176?s=20		2307.14335v1	['Xubo Liu', 'Zhongkai Zhu', 'Haohe Liu', 'Yi Yuan', 'Meng Cui', 'Qiushi Huang', 'Jinhua Liang', 'Yin Cao', 'Qiuqiang Kong', 'Mark D. Plumbley', 'Wenwu Wang']	ct:Large Language Models (LLMs) have shown great promise in integrating diverse expert models to tackle intricate language and vision tasks. Despite their significance in advancing the field of Artificial Intelligence Generated Content (AIGC), their potential in intelligent audio content creation remains unexplored. In this work, we tackle the problem of creating audio content with storylines encompassing speech, music, and sound effects, guided by text instructions. We present WavJourney, a system that leverages LLMs to connect various audio models for audio content generation. Given a text description of an auditory scene, WavJourney first prompts LLMs to generate a structured script dedicated to audio storytelling. The audio script incorporates diverse audio elements, organized based on their spatio-temporal relationships. As a conceptual representation of audio, the audio script provides an interactive and interpretable rationale for human engagement. Afterward, the audio script is fed into a script compiler, converting it into a computer program. Each line of the program calls a task-specific audio generation model or computational operation function (e.g., concatenate, mix). The computer program is then executed to obtain an explainable solution for audio generation. We demonstrate the practicality of WavJourney across diverse real-world scenarios, including science fiction, education, and radio play. The explainable and interactive design of WavJourney fosters human-machine co-creation in multi-round dialogues, enhancing creative control and adaptability in audio production. WavJourney audiolizes the human imagination, opening up new avenues for creativity in multimedia content creation.	t Page:this https URL	['Sound (cs.SD)', 'Artificial Intelligence (cs.AI)', 'Multimedia (cs.MM)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2307.14335v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2307.14335'}	Submission history From: Xubo Liu [ view email ] [v1] Wed, 26 Jul 2023 17:54:04 UTC (15,404 KB) [v2] Sun, 26 Nov 2023 14:12:37 UTC (23,284 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.14335'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.14335'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.14335'}]
2023-07-30	FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios	Computation and Language	https://arxiv.org/abs/2307.13528v2	FacTool	https://twitter.com/gneubig/status/1684658613921669120?s=20		2307.13528v2	['I-Chun Chern', 'Steffi Chern', 'Shiqi Chen', 'Weizhe Yuan', 'Kehua Feng', 'Chunting Zhou', 'Junxian He', 'Graham Neubig', 'Pengfei Liu']	ct:The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.13528v2', 'html': None, 'tex': '/src/2307.13528v2', 'doi': 'https://doi.org/10.48550/arXiv.2307.13528'}	Submission history From: I-Chun Chern [ view email ] [v1] Tue, 25 Jul 2023 14:20:51 UTC (9,031 KB) [v2] Wed, 26 Jul 2023 15:17:49 UTC (9,031 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.13528'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.13528'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.13528'}]
2023-07-23	Llama 2: Open Foundation and Fine-Tuned Chat Models	Computation and Language	https://arxiv.org/abs/2307.09288v2	Llama 2	https://twitter.com/MetaAI/status/1681363272484945921?s=20		2307.09288v2	['Hugo Touvron', 'Louis Martin', 'Kevin Stone', 'Peter Albert', 'Amjad Almahairi', 'Yasmine Babaei', 'Nikolay Bashlykov', 'Soumya Batra', 'Prajjwal Bhargava', 'Shruti Bhosale', 'Dan Bikel', 'Lukas Blecher', 'Cristian Canton Ferrer', 'Moya Chen', 'Guillem Cucurull', 'David Esiobu', 'Jude Fernandes', 'Jeremy Fu', 'Wenyin Fu', 'Brian Fuller', 'Cynthia Gao', 'Vedanuj Goswami', 'Naman Goyal', 'Anthony Hartshorn', 'Saghar Hosseini', 'Rui Hou', 'Hakan Inan', 'Marcin Kardas', 'Viktor Kerkez', 'Madian Khabsa', 'Isabel Kloumann', 'Artem Korenev', 'Punit Singh Koura', 'Marie-Anne Lachaux', 'Thibaut Lavril', 'Jenya Lee', 'Diana Liskovich', 'Yinghai Lu', 'Yuning Mao', 'Xavier Martinet', 'Todor Mihaylov', 'Pushkar Mishra', 'Igor Molybog', 'Yixin Nie', 'Andrew Poulton', 'Jeremy Reizenstein', 'Rashi Rungta', 'Kalyan Saladi', 'Alan Schelten', 'Ruan Silva', 'Eric Michael Smith', 'Ranjan Subramanian', 'Xiaoqing Ellen Tan', 'Binh Tang', 'Ross Taylor', 'Adina Williams', 'Jian Xiang Kuan', 'Puxin Xu', 'Zheng Yan', 'Iliyan Zarov', 'Yuchen Zhang', 'Angela Fan', 'Melanie Kambadur', 'Sharan Narang', 'Aurelien Rodriguez', 'Robert Stojnic', 'Sergey Edunov', 'Thomas Scialom']	ct:In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.09288v2', 'html': None, 'tex': '/src/2307.09288v2', 'doi': 'https://doi.org/10.48550/arXiv.2307.09288'}	Submission history From: Thomas Scialom [ view email ] [v1] Tue, 18 Jul 2023 14:31:57 UTC (17,722 KB) [v2] Wed, 19 Jul 2023 17:08:59 UTC (17,722 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.09288'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.09288'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.09288'}]
2023-07-23	How is ChatGPT's behavior changing over time?	Computation and Language	https://arxiv.org/abs/2307.09009v1	How is ChatGPT’s Behavior Changing Over Time?	https://twitter.com/matei_zaharia/status/1681467961905926144?s=20		2307.09009v1	['Lingjiao Chen', 'Matei Zaharia', 'James Zou']	ct:GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLM quality.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.09009v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2307.09009'}	Submission history From: Lingjiao Chen [ view email ] [v1] Tue, 18 Jul 2023 06:56:08 UTC (536 KB) [v2] Tue, 1 Aug 2023 14:23:58 UTC (1,042 KB) [v3] Tue, 31 Oct 2023 16:13:44 UTC (1,896 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.09009'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.09009'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.09009'}]
2023-07-23	FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning	Machine Learning	https://arxiv.org/abs/2307.08691v1	FlashAttention-2	https://twitter.com/tri_dao/status/1680987577913065472?s=20		2307.08691v1	['Tri Dao']	ct:Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\times$ speedup compared to FlashAttention, reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.08691v1', 'html': None, 'tex': '/src/2307.08691v1', 'doi': 'https://doi.org/10.48550/arXiv.2307.08691'}	Submission history From: Tri Dao [ view email ] [v1] Mon, 17 Jul 2023 17:50:36 UTC (1,421 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.08691'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.08691'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.08691'}]
2023-07-23	Challenges and Applications of Large Language Models	Computation and Language	https://arxiv.org/abs/2307.10169	Challenges & Application of LLMs	https://twitter.com/omarsar0/status/1681844380934500358?s=20		2307.10169	['Jean Kaddour', 'Joshua Harris', 'Maximilian Mozes', 'Herbie Bradley', 'Roberta Raileanu', 'Robert McHardy']	ct:Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.	es. v01. Work in progress. Feedback and comments are highly appreciated!	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.10169', 'html': None, 'tex': '/src/2307.10169', 'doi': 'https://doi.org/10.48550/arXiv.2307.10169'}	Submission history From: Jean Kaddour [ view email ] [v1] Wed, 19 Jul 2023 17:55:13 UTC (8,447 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.10169'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.10169'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.10169'}]
2023-07-23	Retentive Network: A Successor to Transformer for Large Language Models	Computation and Language	https://arxiv.org/abs/2307.08621	Retentive Network	https://twitter.com/arankomatsuzaki/status/1681113977500184576?s=20		2307.08621	['Yutao Sun', 'Li Dong', 'Shaohan Huang', 'Shuming Ma', 'Yuqing Xia', 'Jilong Xue', 'Jianyong Wang', 'Furu Wei']	ct:In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available atthis https URL.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.08621', 'html': None, 'tex': '/src/2307.08621', 'doi': 'https://doi.org/10.48550/arXiv.2307.08621'}	Submission history From: Li Dong [ view email ] [v1] Mon, 17 Jul 2023 16:40:01 UTC (337 KB) [v2] Wed, 19 Jul 2023 05:56:42 UTC (337 KB) [v3] Tue, 25 Jul 2023 06:47:43 UTC (337 KB) [v4] Wed, 9 Aug 2023 08:53:08 UTC (337 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.08621'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.08621'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.08621'}]
2023-07-23	Meta-Transformer: A Unified Framework for Multimodal Learning	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2307.10802	Meta-Transformer	https://twitter.com/omarsar0/status/1682197751990288385?s=20		2307.10802	['Yiyuan Zhang', 'Kaixiong Gong', 'Kaipeng Zhang', 'Hongsheng Li', 'Yu Qiao', 'Wanli Ouyang', 'Xiangyu Yue']	ct:Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available atthis https URL	t website:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Multimedia (cs.MM)']	{'pdf': '/pdf/2307.10802', 'html': None, 'tex': '/src/2307.10802', 'doi': 'https://doi.org/10.48550/arXiv.2307.10802'}	Submission history From: Yiyuan Zhang [ view email ] [v1] Thu, 20 Jul 2023 12:10:29 UTC (1,491 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.10802'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.10802'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.10802'}]
2023-07-23	Learning to Retrieve In-Context Examples for Large Language Models	Computation and Language	https://arxiv.org/abs/2307.07164	Retrieve In-Context Example for LLMs	https://twitter.com/_akhaliq/status/1680770636166094848?s=20		2307.07164	['Liang Wang', 'Nan Yang', 'Furu Wei']	ct:Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of $30$ tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes. The code and data are available atthis https URL.	ed by EACL 2024	['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2307.07164', 'html': 'https://arxiv.org/html/2307.07164v2', 'tex': '/src/2307.07164', 'doi': 'https://doi.org/10.48550/arXiv.2307.07164'}	Submission history From: Liang Wang [ view email ] [v1] Fri, 14 Jul 2023 05:23:08 UTC (179 KB) [v2] Fri, 26 Jan 2024 07:04:02 UTC (195 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.07164'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.07164'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.07164'}]
2023-07-23	FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets	Computation and Language	https://arxiv.org/abs/2307.10928	FLASK	https://twitter.com/SeonghyeonYe/status/1682209670302408705?s=20		2307.10928	['Seonghyeon Ye', 'Doyoung Kim', 'Sungdong Kim', 'Hyeonbin Hwang', 'Seungone Kim', 'Yongrae Jo', 'James Thorne', 'Juho Kim', 'Minjoon Seo']	ct:Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlation between model-based and human-based evaluations. We publicly release the evaluation data and code implementation atthis https URL.	024 Spotlight	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.10928', 'html': 'https://arxiv.org/html/2307.10928v4', 'tex': '/src/2307.10928', 'doi': 'https://doi.org/10.48550/arXiv.2307.10928'}	Submission history From: Seonghyeon Ye [ view email ] [v1] Thu, 20 Jul 2023 14:56:35 UTC (5,002 KB) [v2] Wed, 4 Oct 2023 04:11:16 UTC (5,244 KB) [v3] Fri, 16 Feb 2024 05:04:45 UTC (5,271 KB) [v4] Sun, 14 Apr 2024 04:29:51 UTC (5,270 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.10928'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.10928'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.10928'}]
2023-07-16	Secrets of RLHF in Large Language Models Part I: PPO	Computation and Language	https://arxiv.org/abs/2307.04964	Secrets of RLHF in LLMs	https://twitter.com/omarsar0/status/1678938028918571009?s=20		2307.04964	['Rui Zheng', 'Shihan Dou', 'Songyang Gao', 'Yuan Hua', 'Wei Shen', 'Binghai Wang', 'Yan Liu', 'Senjie Jin', 'Qin Liu', 'Yuhao Zhou', 'Limao Xiong', 'Lu Chen', 'Zhiheng Xi', 'Nuo Xu', 'Wenbin Lai', 'Minghao Zhu', 'Cheng Chang', 'Zhangyue Yin', 'Rongxiang Weng', 'Wensen Cheng', 'Haoran Huang', 'Tianxiang Sun', 'Hang Yan', 'Tao Gui', 'Qi Zhang', 'Xipeng Qiu', 'Xuanjing Huang']	ct:Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.04964', 'html': None, 'tex': '/src/2307.04964', 'doi': 'https://doi.org/10.48550/arXiv.2307.04964'}	Submission history From: Songyang Gao [ view email ] [v1] Tue, 11 Jul 2023 01:55:24 UTC (2,304 KB) [v2] Tue, 18 Jul 2023 08:44:47 UTC (2,393 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.04964'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.04964'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.04964'}]
2023-07-16	Focused Transformer: Contrastive Training for Context Scaling	Computation and Language	https://arxiv.org/abs/2307.03170v1	LongLLaMA	https://twitter.com/s_tworkowski/status/1677125863429795840?s=20		2307.03170v1	['Szymon Tworkowski', 'Konrad Staniszewski', 'Mikołaj Pacek', 'Yuhuai Wu', 'Henryk Michalewski', 'Piotr Miłoś']	ct:Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-scale models to lengthen their effective context. This is demonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The resulting models, which we name LongLLaMA, exhibit advancements in tasks requiring a long context. We further illustrate that our LongLLaMA models adeptly manage a $256 k$ context length for passkey retrieval.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.03170v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2307.03170'}	Submission history From: Szymon Tworkowski [ view email ] [v1] Thu, 6 Jul 2023 17:52:10 UTC (606 KB) [v2] Thu, 30 Nov 2023 17:15:34 UTC (609 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.03170'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.03170'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.03170'}]
2023-07-16	Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2307.06304	Patch n’ Pack: NaViT	https://twitter.com/m__dehghani/status/1679558751248850969?s=20		2307.06304	['Mostafa Dehghani', 'Basil Mustafa', 'Josip Djolonga', 'Jonathan Heek', 'Matthias Minderer', 'Mathilde Caron', 'Andreas Steiner', 'Joan Puigcerver', 'Robert Geirhos', 'Ibrahim Alabdulmohsin', 'Avital Oliver', 'Piotr Padlewski', 'Alexey Gritsenko', 'Mario Lučić', 'Neil Houlsby']	ct:The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.06304', 'html': None, 'tex': '/src/2307.06304', 'doi': 'https://doi.org/10.48550/arXiv.2307.06304'}	Submission history From: Mostafa Dehghani [ view email ] [v1] Wed, 12 Jul 2023 17:01:03 UTC (1,859 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.06304'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.06304'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.06304'}]
2023-07-16	Large Language Models as General Pattern Machines	Artificial Intelligence	https://arxiv.org/abs/2307.04721	LLMs as General Pattern Machines	https://twitter.com/DrJimFan/status/1679898692307005440?s=20		2307.04721	['Suvir Mirchandani', 'Fei Xia', 'Pete Florence', 'Brian Ichter', 'Danny Driess', 'Montserrat Gonzalez Arenas', 'Kanishka Rao', 'Dorsa Sadigh', 'Andy Zeng']	ct:We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.	es, 25 figures. To appear at Conference on Robot Learning (CoRL) 2023	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Robotics (cs.RO)']	{'pdf': '/pdf/2307.04721', 'html': None, 'tex': '/src/2307.04721', 'doi': 'https://doi.org/10.48550/arXiv.2307.04721'}	Submission history From: Suvir Mirchandani [ view email ] [v1] Mon, 10 Jul 2023 17:32:13 UTC (7,581 KB) [v2] Thu, 26 Oct 2023 01:27:29 UTC (7,861 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.04721'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.04721'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.04721'}]
2023-07-16	HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2307.06949	HyperDreamBooth	https://twitter.com/natanielruizg/status/1679893292618752000?s=20		2307.06949	['Nataniel Ruiz', 'Yuanzhen Li', 'Varun Jampani', 'Wei Wei', 'Tingbo Hou', 'Yael Pritch', 'Neal Wadhwa', 'Michael Rubinstein', 'Kfir Aberman']	ct:Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth - a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10,000x smaller than a normal DreamBooth model. Project page:this https URL	t page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Graphics (cs.GR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.06949', 'html': 'https://arxiv.org/html/2307.06949v2', 'tex': '/src/2307.06949', 'doi': 'https://doi.org/10.48550/arXiv.2307.06949'}	Submission history From: Nataniel Ruiz [ view email ] [v1] Thu, 13 Jul 2023 17:59:47 UTC (39,987 KB) [v2] Wed, 16 Oct 2024 23:55:27 UTC (48,028 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.06949'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.06949'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.06949'}]
2023-07-16	Teaching Arithmetic to Small Transformers	Machine Learning	https://arxiv.org/abs/2307.03381	Teaching Arithmetics to Small Transformers	https://twitter.com/DimitrisPapail/status/1678407512637284352?s=20		2307.03381	['Nayoung Lee', 'Kartik Sreenivasan', 'Jason D. Lee', 'Kangwook Lee', 'Dimitris Papailiopoulos']	ct:Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.03381', 'html': None, 'tex': '/src/2307.03381', 'doi': 'https://doi.org/10.48550/arXiv.2307.03381'}	Submission history From: Nayoung Lee [ view email ] [v1] Fri, 7 Jul 2023 04:33:31 UTC (1,027 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.03381'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.03381'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.03381'}]
2023-07-16	AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2307.04725v1	AnimateDiff	https://twitter.com/dreamingtulpa/status/1679459297946632193?s=20		2307.04725v1	['Yuwei Guo', 'Ceyuan Yang', 'Anyi Rao', 'Yaohui Wang', 'Yu Qiao', 'Dahua Lin', 'Bo Dai']	ct:With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. Subsequently, there is a great demand for image animation techniques to further combine generated static images with motion dynamics. In this report, we propose a practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning. At the core of the proposed framework is to insert a newly initialized motion modeling module into the frozen text-to-image model and train it on video clips to distill reasonable motion priors. Once trained, by simply injecting this motion modeling module, all personalized versions derived from the same base T2I readily become text-driven models that produce diverse and personalized animated images. We conduct our evaluation on several public representative personalized text-to-image models across anime pictures and realistic photographs, and demonstrate that our proposed framework helps these models generate temporally smooth animation clips while preserving the domain and diversity of their outputs. Code and pre-trained weights will be publicly available atthis https URL.	t page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.04725v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2307.04725'}	Submission history From: Yuwei Guo [ view email ] [v1] Mon, 10 Jul 2023 17:34:16 UTC (12,315 KB) [v2] Thu, 8 Feb 2024 18:08:57 UTC (23,087 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.04725'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.04725'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.04725'}]
2023-07-16	Generative Pretraining in Multimodality	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2307.05222v1	Generative Pretraining in Multimodality	https://twitter.com/_akhaliq/status/1678939405170475008?s=20		2307.05222v1	['Quan Sun', 'Qiying Yu', 'Yufeng Cui', 'Fan Zhang', 'Xiaosong Zhang', 'Yueze Wang', 'Hongcheng Gao', 'Jingjing Liu', 'Tiejun Huang', 'Xinlong Wang']	ct:We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.	nd Demo:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2307.05222v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2307.05222'}	Submission history From: Xinlong Wang [ view email ] [v1] Tue, 11 Jul 2023 12:45:39 UTC (6,001 KB) [v2] Wed, 8 May 2024 02:46:43 UTC (6,043 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.05222'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.05222'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.05222'}]
2023-07-09	A Survey on Evaluation of Large Language Models	Computation and Language	https://arxiv.org/abs/2307.03109	A Survey on Evaluation of LLMs	https://twitter.com/omarsar0/status/1677137934946803712?s=20		2307.03109	['Yupeng Chang', 'Xu Wang', 'Jindong Wang', 'Yuan Wu', 'Linyi Yang', 'Kaijie Zhu', 'Hao Chen', 'Xiaoyuan Yi', 'Cunxiang Wang', 'Yidong Wang', 'Wei Ye', 'Yue Zhang', 'Yi Chang', 'Philip S. Yu', 'Qiang Yang', 'Xing Xie']	ct:Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:this https URL.	ed by ACM Transactions on Intelligent Systems and Technology (TIST); 45 pages; More recent works;this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.03109', 'html': None, 'tex': '/src/2307.03109', 'doi': 'https://doi.org/10.48550/arXiv.2307.03109'}	Submission history From: Jindong Wang [ view email ] [v1] Thu, 6 Jul 2023 16:28:35 UTC (117 KB) [v2] Sun, 9 Jul 2023 12:31:50 UTC (114 KB) [v3] Wed, 12 Jul 2023 15:43:03 UTC (116 KB) [v4] Thu, 13 Jul 2023 12:33:20 UTC (117 KB) [v5] Tue, 18 Jul 2023 08:11:21 UTC (118 KB) [v6] Wed, 2 Aug 2023 07:39:17 UTC (121 KB) [v7] Mon, 28 Aug 2023 05:50:53 UTC (123 KB) [v8] Tue, 17 Oct 2023 06:28:04 UTC (145 KB) [v9] Fri, 29 Dec 2023 02:12:03 UTC (151 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.03109'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.03109'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.03109'}]
2023-07-09	Lost in the Middle: How Language Models Use Long Contexts	Computation and Language	https://arxiv.org/abs/2307.03172	How Language Models Use Long Contexts	https://twitter.com/nelsonfliu/status/1677373731948339202?s=20		2307.03172	['Nelson F. Liu', 'Kevin Lin', 'John Hewitt', 'Ashwin Paranjape', 'Michele Bevilacqua', 'Fabio Petroni', 'Percy Liang']	ct:While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.	es, 16 figures. Accepted for publication in Transactions of the Association for Computational Linguistics (TACL), 2023	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2307.03172', 'html': None, 'tex': '/src/2307.03172', 'doi': 'https://doi.org/10.48550/arXiv.2307.03172'}	Submission history From: Nelson F. Liu [ view email ] [v1] Thu, 6 Jul 2023 17:54:11 UTC (410 KB) [v2] Mon, 31 Jul 2023 17:48:48 UTC (416 KB) [v3] Mon, 20 Nov 2023 23:09:34 UTC (334 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.03172'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.03172'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.03172'}]
2023-07-09	Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting	Information Retrieval	https://arxiv.org/abs/2306.17563	LLMs as Effective Text Rankers	https://twitter.com/arankomatsuzaki/status/1675673784454447107?s=20		2306.17563	['Zhen Qin', 'Rolf Jagerman', 'Kai Hui', 'Honglei Zhuang', 'Junru Wu', 'Le Yan', 'Jiaming Shen', 'Tianqi Liu', 'Jialu Liu', 'Donald Metzler', 'Xuanhui Wang', 'Michael Bendersky']	ct:Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.	ed to NAACL 2024. Corrected results of RankT5 on TREC-DL19	['Information Retrieval (cs.IR)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.17563', 'html': 'https://arxiv.org/html/2306.17563v2', 'tex': '/src/2306.17563', 'doi': 'https://doi.org/10.48550/arXiv.2306.17563'}	Submission history From: Rolf Jagerman [ view email ] [v1] Fri, 30 Jun 2023 11:32:25 UTC (52 KB) [v2] Thu, 28 Mar 2024 13:59:09 UTC (56 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.17563'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.17563'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.17563'}]
2023-07-09	SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2306.17842	Multimodal Generation with Frozen LLMs	https://twitter.com/roadjiang/status/1676375112914989056?s=20		2306.17842	['Lijun Yu', 'Yong Cheng', 'Zhiruo Wang', 'Vivek Kumar', 'Wolfgang Macherey', 'Yanping Huang', 'David A. Ross', 'Irfan Essa', 'Yonatan Bisk', 'Ming-Hsuan Yang', 'Kevin Murphy', 'Alexander G. Hauptmann', 'Lu Jiang']	ct:In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.	S 2023 spotlight	['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)', 'Multimedia (cs.MM)']	{'pdf': '/pdf/2306.17842', 'html': None, 'tex': '/src/2306.17842', 'doi': 'https://doi.org/10.48550/arXiv.2306.17842'}	Submission history From: Lijun Yu [ view email ] [v1] Fri, 30 Jun 2023 17:59:07 UTC (12,224 KB) [v2] Mon, 3 Jul 2023 08:13:19 UTC (12,187 KB) [v3] Sat, 28 Oct 2023 18:09:46 UTC (12,582 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.17842'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.17842'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.17842'}]
2023-07-09	CodeGen2: Lessons for Training LLMs on Programming and Natural Languages	Machine Learning	https://arxiv.org/abs/2305.02309	CodeGen2.5	https://twitter.com/erik_nijkamp/status/1677055271104045056?s=20		2305.02309	['Erik Nijkamp', 'Hiroaki Hayashi', 'Caiming Xiong', 'Silvio Savarese', 'Yingbo Zhou']	"ct:Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a ""free lunch"" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source:this https URL."		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.02309', 'html': None, 'tex': '/src/2305.02309', 'doi': 'https://doi.org/10.48550/arXiv.2305.02309'}	Submission history From: Erik Nijkamp Dr. [ view email ] [v1] Wed, 3 May 2023 17:55:25 UTC (225 KB) [v2] Tue, 11 Jul 2023 21:11:23 UTC (227 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.02309'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.02309'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.02309'}]
2023-07-09	Elastic Decision Transformer	Machine Learning	https://arxiv.org/abs/2307.02484	Elastic Decision Transformer	https://twitter.com/xiaolonw/status/1677003542249484289?s=20		2307.02484	['Yueh-Hua Wu', 'Xiaolong Wang', 'Masashi Hamaya']	"ct:This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to ""stitch"" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regime on the D4RL locomotion benchmark and Atari games. Videos are available at:this https URL"	ed to NeurIPS 2023	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2307.02484', 'html': None, 'tex': '/src/2307.02484', 'doi': 'https://doi.org/10.48550/arXiv.2307.02484'}	Submission history From: Yueh-Hua Wu [ view email ] [v1] Wed, 5 Jul 2023 17:58:21 UTC (1,202 KB) [v2] Fri, 7 Jul 2023 01:12:55 UTC (1,539 KB) [v3] Fri, 22 Sep 2023 03:04:30 UTC (1,539 KB) [v4] Wed, 11 Oct 2023 10:27:40 UTC (1,539 KB) [v5] Thu, 12 Oct 2023 04:06:48 UTC (1,539 KB) [v6] Fri, 20 Oct 2023 05:12:04 UTC (1,546 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.02484'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.02484'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.02484'}]
2023-07-09	Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners	Robotics	https://arxiv.org/abs/2307.01928	Robots That Ask for Help	https://twitter.com/allenzren/status/1677000811803443213?s=20		2307.01928	['Allen Z. Ren', 'Anushri Dixit', 'Alexandra Bodrova', 'Sumeet Singh', 'Stephen Tu', 'Noah Brown', 'Peng Xu', 'Leila Takayama', 'Fei Xia', 'Jake Varley', 'Zhenjia Xu', 'Dorsa Sadigh', 'Andy Zeng', 'Anirudha Majumdar']	ct:Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website:this https URL	ence on Robot Learning (CoRL) 2023, Oral Presentation	['Robotics (cs.RO)', 'Artificial Intelligence (cs.AI)', 'Applications (stat.AP)']	{'pdf': '/pdf/2307.01928', 'html': None, 'tex': '/src/2307.01928', 'doi': 'https://doi.org/10.48550/arXiv.2307.01928'}	Submission history From: Allen Z. Ren [ view email ] [v1] Tue, 4 Jul 2023 21:25:12 UTC (9,309 KB) [v2] Mon, 4 Sep 2023 16:06:48 UTC (9,451 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.01928'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.01928'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.01928'}]
2023-07-09	Physics-based Motion Retargeting from Sparse Inputs	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2307.01938	Physics-based Motion Retargeting in Real-Time	https://twitter.com/_akhaliq/status/1676822600478015488?s=20		2307.01938	['Daniele Reda', 'Jungdam Won', 'Yuting Ye', 'Michiel van de Panne', 'Alexander Winkler']	ct:Avatars are important to create interactive and immersive experiences in virtual worlds. One challenge in animating these characters to mimic a user's motion is that commercial AR/VR products consist only of a headset and controllers, providing very limited sensor data of the user's pose. Another challenge is that an avatar might have a different skeleton structure than a human and the mapping between them is unclear. In this work we address both of these challenges. We introduce a method to retarget motions in real-time from sparse human sensor data to characters of various morphologies. Our method uses reinforcement learning to train a policy to control characters in a physics simulator. We only require human motion capture data for training, without relying on artist-generated animations for each avatar. This allows us to use large motion capture datasets to train general policies that can track unseen users from real and sparse data in real-time. We demonstrate the feasibility of our approach on three characters with different skeleton structure: a dinosaur, a mouse-like creature and a human. We show that the avatar poses often match the user surprisingly well, despite having no sensor information of the lower body available. We discuss and ablate the important components in our framework, specifically the kinematic retargeting step, the imitation, contact and action reward as well as our asymmetric actor-critic observations. We further explore the robustness of our method in a variety of settings including unbalancing, dancing and sports motions.	nfo at:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2307.01938', 'html': None, 'tex': '/src/2307.01938', 'doi': 'https://doi.org/10.48550/arXiv.2307.01938'}	Submission history From: Daniele Reda [ view email ] [v1] Tue, 4 Jul 2023 21:57:05 UTC (24,422 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.01938'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.01938'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.01938'}]
2023-07-09	LongNet: Scaling Transformers to 1,000,000,000 Tokens	Computation and Language	https://arxiv.org/abs/2307.02486	Scaling Transformer to 1 Billion Tokens	https://twitter.com/arankomatsuzaki/status/1676765133362675712?s=20		2307.02486	['Jiayu Ding', 'Shuming Ma', 'Li Dong', 'Xingxing Zhang', 'Shaohan Huang', 'Wenhui Wang', 'Nanning Zheng', 'Furu Wei']	ct:Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.	n progress	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2307.02486', 'html': None, 'tex': '/src/2307.02486', 'doi': 'https://doi.org/10.48550/arXiv.2307.02486'}	Submission history From: Shuming Ma [ view email ] [v1] Wed, 5 Jul 2023 17:59:38 UTC (219 KB) [v2] Wed, 19 Jul 2023 12:25:35 UTC (220 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2307.02486'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2307.02486'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2307.02486'}]
2023-07-09	InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback	Computation and Language	https://arxiv.org/abs/2306.14898	InterCode	https://twitter.com/ShunyuYao12/status/1675903408727896066?s=20		2306.14898	['John Yang', 'Akshara Prabhakar', 'Karthik Narasimhan', 'Shunyu Yao']	ct:Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash, Spider, and MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan & Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages. Project site with code and data:this https URL	t site with code and data:this https URL	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2306.14898', 'html': None, 'tex': '/src/2306.14898', 'doi': 'https://doi.org/10.48550/arXiv.2306.14898'}	Submission history From: John Yang B [ view email ] [v1] Mon, 26 Jun 2023 17:59:50 UTC (3,149 KB) [v2] Tue, 27 Jun 2023 01:51:57 UTC (3,149 KB) [v3] Mon, 30 Oct 2023 17:52:18 UTC (3,771 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.14898'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.14898'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.14898'}]
2023-07-02	LeanDojo: Theorem Proving with Retrieval-Augmented Language Models	Machine Learning	https://arxiv.org/abs/2306.15626	LeanDojo	https://twitter.com/KaiyuYang4/status/1673882824158613504?s=20		2306.15626	['Kaiyu Yang', 'Aidan M. Swope', 'Alex Gu', 'Rahul Chalamala', 'Peiyang Song', 'Shixing Yu', 'Saad Godil', 'Ryan Prenger', 'Anima Anandkumar']	ct:Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.	ed to NeurIPS 2023 (Datasets and Benchmarks Track) as an oral presentation. Data, code, and models available atthis https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Logic in Computer Science (cs.LO)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2306.15626', 'html': None, 'tex': '/src/2306.15626', 'doi': 'https://doi.org/10.48550/arXiv.2306.15626'}	Submission history From: Kaiyu Yang [ view email ] [v1] Tue, 27 Jun 2023 17:05:32 UTC (2,908 KB) [v2] Fri, 27 Oct 2023 16:00:20 UTC (3,429 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.15626'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.15626'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.15626'}]
2023-07-02	Extending Context Window of Large Language Models via Positional Interpolation	Computation and Language	https://arxiv.org/abs/2306.15595	Extending Context Window of LLMs	https://twitter.com/omarsar0/status/1674073189800919042?s=20		2306.15595	['Shouyuan Chen', 'Sherman Wong', 'Liangjian Chen', 'Yuandong Tian']	ct:We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.	mplate issues	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.15595', 'html': None, 'tex': '/src/2306.15595', 'doi': 'https://doi.org/10.48550/arXiv.2306.15595'}	Submission history From: Yuandong Tian [ view email ] [v1] Tue, 27 Jun 2023 16:26:26 UTC (393 KB) [v2] Wed, 28 Jun 2023 04:26:05 UTC (393 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.15595'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.15595'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.15595'}]
2023-07-02	Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language	Computation and Language	https://arxiv.org/abs/2306.16410	Computer Vision Through the Lens of Natural Language	https://twitter.com/arankomatsuzaki/status/1674219223856365569?s=20		2306.16410	['William Berrios', 'Gautam Mittal', 'Tristan Thrush', 'Douwe Kiela', 'Amanpreet Singh']	ct:We propose LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs). Our system uses a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image. We evaluate the approach on pure computer vision settings such as zero- and few-shot object recognition, as well as on vision and language problems. LENS can be applied to any off-the-shelf LLM and we find that the LLMs with LENS perform highly competitively with much bigger and much more sophisticated systems, without any multimodal training whatsoever. We open-source our code atthis https URLand provide an interactive demo.		['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2306.16410', 'html': None, 'tex': '/src/2306.16410', 'doi': 'https://doi.org/10.48550/arXiv.2306.16410'}	Submission history From: Tristan Thrush [ view email ] [v1] Wed, 28 Jun 2023 17:57:10 UTC (27,999 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.16410'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.16410'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.16410'}]
2023-07-02	ViNT: A Foundation Model for Visual Navigation	Robotics	https://arxiv.org/abs/2306.14846	Visual Navigation Transformer	https://twitter.com/svlevine/status/1673732522155601920?s=20		2306.14846	['Dhruv Shah', 'Ajay Sridhar', 'Nitish Dashora', 'Kyle Stachowicz', 'Kevin Black', 'Noriaki Hirose', 'Sergey Levine']	"ct:General-purpose pre-trained models (""foundation models"") have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navigation from a variety of different robotic platforms, and exhibits positive transfer, outperforming specialist models trained on singular datasets. ViNT can be augmented with diffusion-based subgoal proposals to explore novel environments, and can solve kilometer-scale navigation problems when equipped with long-range heuristics. ViNT can also be adapted to novel task specifications with a technique inspired by prompt-tuning, where the goal encoder is replaced by an encoding of another task modality (e.g., GPS waypoints or routing commands) embedded into the same space of goal tokens. This flexibility and ability to accommodate a variety of downstream problem domains establishes ViNT as an effective foundation model for mobile robotics. For videos, code, and model checkpoints, see our project page atthis https URL."	ed for oral presentation at CoRL 2023	['Robotics (cs.RO)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.14846', 'html': None, 'tex': '/src/2306.14846', 'doi': 'https://doi.org/10.48550/arXiv.2306.14846'}	Submission history From: Dhruv Shah [ view email ] [v1] Mon, 26 Jun 2023 16:57:03 UTC (7,267 KB) [v2] Tue, 24 Oct 2023 06:16:43 UTC (8,959 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.14846'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.14846'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.14846'}]
2023-07-02	Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors	Computers and Society	https://arxiv.org/abs/2306.17156	Generative AI for Programming Education	https://twitter.com/_akhaliq/status/1674590713051242498?s=20		2306.17156	['Tung Phung', 'Victor-Alexandru Pădurean', 'José Cambronero', 'Sumit Gulwani', 'Tobias Kohn', 'Rupak Majumdar', 'Adish Singla', 'Gustavo Soares']	ct:Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors' performance for several scenarios. These results also highlight settings where GPT-4 still struggles, providing exciting future directions on developing techniques to improve the performance of these models.	rticle is a full version of the poster (extended abstract) from ICER'23	['Computers and Society (cs.CY)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2306.17156', 'html': None, 'tex': '/src/2306.17156', 'doi': 'https://doi.org/10.48550/arXiv.2306.17156'}	Submission history From: Adish Singla [ view email ] [v1] Thu, 29 Jun 2023 17:57:40 UTC (463 KB) [v2] Fri, 30 Jun 2023 09:39:03 UTC (463 KB) [v3] Tue, 1 Aug 2023 00:03:25 UTC (464 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.17156'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.17156'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.17156'}]
2023-07-02	DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2306.14435	DragDiffusion	https://twitter.com/_akhaliq/status/1673570232429051906?s=20		2306.14435	['Yujun Shi', 'Chuhui Xue', 'Jun Hao Liew', 'Jiachun Pan', 'Hanshu Yan', 'Wenqing Zhang', 'Vincent Y. F. Tan', 'Song Bai']	ct:Accurate and controllable image editing is a challenging task that has attracted significant attention recently. Notably, DragGAN is an interactive point-based image editing framework that achieves impressive editing results with pixel-level precision. However, due to its reliance on generative adversarial networks (GANs), its generality is limited by the capacity of pretrained GAN models. In this work, we extend this editing framework to diffusion models and propose a novel approach DragDiffusion. By harnessing large-scale pretrained diffusion models, we greatly enhance the applicability of interactive point-based editing on both real and diffusion-generated images. Our approach involves optimizing the diffusion latents to achieve precise spatial control. The supervision signal of this optimization process is from the diffusion model's UNet features, which are known to contain rich semantic and geometric information. Moreover, we introduce two additional techniques, namely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identity of the original image. Lastly, we present a challenging benchmark dataset called DragBench -- the first benchmark to evaluate the performance of interactive point-based image editing methods. Experiments across a wide range of challenging cases (e.g., images with multiple objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of DragDiffusion. Code:this https URL.	s released atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.14435', 'html': 'https://arxiv.org/html/2306.14435v6', 'tex': '/src/2306.14435', 'doi': 'https://doi.org/10.48550/arXiv.2306.14435'}	Submission history From: Yujun Shi [ view email ] [v1] Mon, 26 Jun 2023 06:04:09 UTC (16,317 KB) [v2] Tue, 27 Jun 2023 11:30:16 UTC (5,427 KB) [v3] Sun, 9 Jul 2023 04:16:27 UTC (5,427 KB) [v4] Fri, 6 Oct 2023 08:15:42 UTC (14,962 KB) [v5] Mon, 11 Dec 2023 04:59:08 UTC (39,858 KB) [v6] Sun, 7 Apr 2024 18:04:04 UTC (39,855 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.14435'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.14435'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.14435'}]
2023-07-02	Understanding Social Reasoning in Language Models with Language Models	Computation and Language	https://arxiv.org/abs/2306.15448	Understanding Theory-of-Mind in LLMs with LLMs	https://twitter.com/johnjnay/status/1673871545725505537?s=20		2306.15448	['Kanishk Gandhi', 'Jan-Philipp Fränken', 'Tobias Gerstenberg', 'Noah D. Goodman']	ct:As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2306.15448', 'html': None, 'tex': '/src/2306.15448', 'doi': 'https://doi.org/10.48550/arXiv.2306.15448'}	Submission history From: Kanishk Gandhi [ view email ] [v1] Wed, 21 Jun 2023 16:42:15 UTC (1,456 KB) [v2] Mon, 4 Dec 2023 22:31:26 UTC (2,049 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.15448'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.15448'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.15448'}]
2023-07-02	Bring Your Own Data! Self-Supervised Evaluation for Large Language Models	Computation and Language	https://arxiv.org/abs/2306.13651v1	Evaluations with No Labels	https://twitter.com/tomgoldsteincs/status/1673808766679097346?s=20		2306.13651v1	['Neel Jain', 'Khalid Saifullah', 'Yuxin Wen', 'John Kirchenbauer', 'Manli Shu', 'Aniruddha Saha', 'Micah Goldblum', 'Jonas Geiping', 'Tom Goldstein']	ct:With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. When comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. The self-supervised paradigm complements current evaluation strategies that rely on labeled data.	s available atthis https URL. First two authors contributed equally. 21 pages, 22 figures	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.13651v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2306.13651'}	Submission history From: Jonas Geiping [ view email ] [v1] Fri, 23 Jun 2023 17:59:09 UTC (342 KB) [v2] Thu, 29 Jun 2023 17:30:45 UTC (471 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.13651'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.13651'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.13651'}]
2023-07-02	Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval	Computation and Language	https://arxiv.org/abs/2306.13421	Long-range Language Modeling with Self-Retrieval	https://twitter.com/arankomatsuzaki/status/1673129191863140353?s=20		2306.13421	['Ohad Rubin', 'Jonathan Berant']	ct:Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added post-hoc to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another. In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch and apply it to the task of modeling long texts. Given a recently generated text chunk in a long document, the LM computes query representations, which are then used to retrieve earlier chunks in the document, located potentially tens of thousands of tokens before. Information from retrieved chunks is fused into the LM representations to predict the next target chunk. We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM. We evaluate RPT on four long-range language modeling tasks, spanning books, code, and mathematical writing, and demonstrate that RPT improves retrieval quality and subsequently perplexity across the board compared to strong baselines.	ed to TACL 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2306.13421', 'html': 'https://arxiv.org/html/2306.13421v2', 'tex': '/src/2306.13421', 'doi': 'https://doi.org/10.48550/arXiv.2306.13421'}	Submission history From: Ohad Rubin [ view email ] [v1] Fri, 23 Jun 2023 10:18:02 UTC (7,376 KB) [v2] Sun, 21 Jul 2024 07:35:23 UTC (7,395 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.13421'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.13421'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.13421'}]
2023-07-02	Scaling MLPs: A Tale of Inductive Bias	Machine Learning	https://arxiv.org/abs/2306.13575	Scaling MLPs: A Tale of Inductive Bias	https://twitter.com/ethanCaballero/status/1673725211907182592?s=20		2306.13575	['Gregor Bachmann', 'Sotiris Anagnostidis', 'Thomas Hofmann']	"ct:In this work we revisit the most fundamental building block in deep learning, the multi-layer perceptron (MLP), and study the limits of its performance on vision tasks. Empirical insights into MLPs are important for multiple reasons. (1) Given the recent narrative ""less inductive bias is better"", popularized due to transformers eclipsing convolutional models, it is natural to explore the limits of this hypothesis. To that end, MLPs offer an ideal test bed, as they lack any vision-specific inductive bias. (2) MLPs have almost exclusively been the main protagonist in the deep learning theory literature due to their mathematical simplicity, serving as a proxy to explain empirical phenomena observed for more complex architectures. Surprisingly, experimental datapoints for MLPs are very difficult to find in the literature, especially when coupled with large pre-training protocols. This discrepancy between practice and theory is worrying: Do MLPs reflect the empirical advances exhibited by practical models? Or do theorists need to rethink the role of MLPs as a proxy? We provide insights into both these aspects. We show that the performance of MLPs drastically improves with scale (95% on CIFAR10, 82% on CIFAR100, 58% on ImageNet ReaL), highlighting that lack of inductive bias can indeed be compensated. We observe that MLPs mimic the behaviour of their modern counterparts faithfully, with some components in the learning setting however exhibiting stronger or unexpected behaviours. Due to their inherent computational efficiency, large pre-training experiments become more accessible for academic researchers. All of our experiments were run on a single GPU."		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.13575', 'html': None, 'tex': '/src/2306.13575', 'doi': 'https://doi.org/10.48550/arXiv.2306.13575'}	Submission history From: Gregor Bachmann [ view email ] [v1] Fri, 23 Jun 2023 15:55:44 UTC (21,313 KB) [v2] Mon, 2 Oct 2023 11:40:41 UTC (22,249 KB) [v3] Tue, 3 Oct 2023 09:35:23 UTC (22,249 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.13575'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.13575'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.13575'}]
2023-06-25	Textbooks Are All You Need	Computation and Language	https://arxiv.org/abs/2306.11644	Textbooks Are All You Need	https://twitter.com/SebastienBubeck/status/1671326369626853376?s=20		2306.11644	['Suriya Gunasekar', 'Yi Zhang', 'Jyoti Aneja', 'Caio César Teodoro Mendes', 'Allie Del Giorno', 'Sivakanth Gopi', 'Mojan Javaheripi', 'Piero Kauffmann', 'Gustavo de Rosa', 'Olli Saarikivi', 'Adil Salim', 'Shital Shah', 'Harkirat Singh Behl', 'Xin Wang', 'Sébastien Bubeck', 'Ronen Eldan', 'Adam Tauman Kalai', 'Yin Tat Lee', 'Yuanzhi Li']	"ct:We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality"" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval."	es; changed color scheme of plot. fixed minor typos and added couple clarifications	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.11644', 'html': None, 'tex': '/src/2306.11644', 'doi': 'https://doi.org/10.48550/arXiv.2306.11644'}	Submission history From: Suriya Gunasekar [ view email ] [v1] Tue, 20 Jun 2023 16:14:25 UTC (830 KB) [v2] Mon, 2 Oct 2023 06:12:30 UTC (832 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.11644'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.11644'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.11644'}]
2023-06-25	RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation	Robotics	https://arxiv.org/abs/2306.11706	RoboCat	https://twitter.com/DeepMind/status/1671171448638144515?s=20		2306.11706	['Konstantinos Bousmalis', 'Giulia Vezzani', 'Dushyant Rao', 'Coline Devin', 'Alex X. Lee', 'Maria Bauza', 'Todor Davchev', 'Yuxiang Zhou', 'Agrim Gupta', 'Akhil Raju', 'Antoine Laurens', 'Claudio Fantacci', 'Valentin Dalibard', 'Martina Zambelli', 'Murilo Martins', 'Rugile Pevceviciute', 'Michiel Blokzijl', 'Misha Denil', 'Nathan Batchelor', 'Thomas Lampe', 'Emilio Parisotto', 'Konrad Żołna', 'Scott Reed', 'Sergio Gómez Colmenarejo', 'Jon Scholz', 'Abbas Abdolmaleki', 'Oliver Groth', 'Jean-Baptiste Regli', 'Oleg Sushkov', 'Tom Rothörl', 'José Enrique Chen', 'Yusuf Aytar', 'Dave Barker', 'Joy Ortiz', 'Martin Riedmiller', 'Jost Tobias Springenberg', 'Raia Hadsell', 'Francesco Nori', 'Nicolas Heess']	ct:The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100-1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.	ctions on Machine Learning Research (12/2023)	['Robotics (cs.RO)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.11706', 'html': 'https://arxiv.org/html/2306.11706v2', 'tex': '/src/2306.11706', 'doi': 'https://doi.org/10.48550/arXiv.2306.11706'}	Submission history From: Alex X. Lee [ view email ] [v1] Tue, 20 Jun 2023 17:35:20 UTC (22,352 KB) [v2] Fri, 22 Dec 2023 13:55:42 UTC (23,987 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.11706'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.11706'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.11706'}]
2023-06-25	ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation	Computation and Language	https://arxiv.org/abs/2306.09968	ClinicalGPT	https://twitter.com/omarsar0/status/1670606068777381890?s=20		2306.09968	['Guangyu Wang', 'Guoxing Yang', 'Zongxin Du', 'Longjun Fan', 'Xiaohu Li']	ct:Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domain-specific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms other models in these tasks, highlighting the effectiveness of our approach in adapting large language models to the critical domain of healthcare.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2306.09968', 'html': None, 'tex': '/src/2306.09968', 'doi': 'https://doi.org/10.48550/arXiv.2306.09968'}	Submission history From: Guoxing Yang [ view email ] [v1] Fri, 16 Jun 2023 16:56:32 UTC (126 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.09968'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.09968'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.09968'}]
2023-06-25	An Overview of Catastrophic AI Risks	Computers and Society	https://arxiv.org/abs/2306.12001v1	An Overview of Catastrophic AI Risks	https://twitter.com/DanHendrycks/status/1671894767331061763?s=20		2306.12001v1	['Dan Hendrycks', 'Mantas Mazeika', 'Thomas Woodside']	ct:Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.		['Computers and Society (cs.CY)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.12001v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2306.12001'}	Submission history From: Dan Hendrycks [ view email ] [v1] Wed, 21 Jun 2023 03:35:06 UTC (5,234 KB) [v2] Mon, 26 Jun 2023 17:26:07 UTC (5,235 KB) [v3] Tue, 11 Jul 2023 17:04:35 UTC (5,236 KB) [v4] Tue, 22 Aug 2023 18:29:05 UTC (5,240 KB) [v5] Mon, 11 Sep 2023 17:53:43 UTC (5,241 KB) [v6] Mon, 9 Oct 2023 22:57:01 UTC (5,969 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.12001'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.12001'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.12001'}]
2023-06-25	Full Parameter Fine-tuning for Large Language Models with Limited Resources	Computation and Language	https://arxiv.org/abs/2306.09782	LOMO	https://twitter.com/arankomatsuzaki/status/1670603218659811330?s=20		2306.09782	['Kai Lv', 'Yuqing Yang', 'Tengxiao Liu', 'Qinghui Gao', 'Qipeng Guo', 'Xipeng Qiu']	ct:Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GBthis http URLand data are available atthis https URL.	24	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2306.09782', 'html': 'https://arxiv.org/html/2306.09782v2', 'tex': '/src/2306.09782', 'doi': 'https://doi.org/10.48550/arXiv.2306.09782'}	Submission history From: Kai Lv [ view email ] [v1] Fri, 16 Jun 2023 11:37:15 UTC (1,128 KB) [v2] Thu, 6 Jun 2024 13:22:26 UTC (8,677 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.09782'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.09782'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.09782'}]
2023-06-25	SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking	Machine Learning	https://arxiv.org/abs/2306.05426	SequenceMatch	https://twitter.com/abacaj/status/1671636061494059009?s=20		2306.05426	['Chris Cundy', 'Stefano Ermon']	ct:In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compounding error problem by allowing the model to revert a sampled token if it takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented without adversarial training or architectural changes. We identify the SequenceMatch-$\chi^2$ divergence as a more suitable training objective for autoregressive models which are used for generation. We show that empirically, SequenceMatch training leads to improvements over MLE on text generation with language models and arithmetic.	, ICLR 2024	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2306.05426', 'html': 'https://arxiv.org/html/2306.05426v3', 'tex': '/src/2306.05426', 'doi': 'https://doi.org/10.48550/arXiv.2306.05426'}	Submission history From: Chris Cundy [ view email ] [v1] Thu, 8 Jun 2023 17:59:58 UTC (841 KB) [v2] Mon, 19 Jun 2023 17:59:24 UTC (841 KB) [v3] Mon, 6 May 2024 16:02:30 UTC (210 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.05426'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.05426'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.05426'}]
2023-06-25	LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models	Computation and Language	https://arxiv.org/abs/2306.12420	LMFlow	https://twitter.com/omarsar0/status/1671881864930549761?s=20		2306.12420	['Shizhe Diao', 'Rui Pan', 'Hanze Dong', 'Ka Shun Shum', 'Jipeng Zhang', 'Wei Xiong', 'Tong Zhang']	ct:Foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, an increasing number of foundation models are becoming publicly accessible. However, a significant shortcoming of most of these models lies in their performance in specialized-domain and task-specific applications, necessitating domain- and task-aware fine-tuning to develop effective scientific language models. As the number of available foundation models and specialized tasks keeps growing, the job of training scientific language models becomes highly nontrivial. In this paper, we initiate steps to tackle this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the domain- and task-aware finetuning of general foundation models. LMFlow offers a complete finetuning workflow for a foundation model to support specialized training with limited computing resources. Furthermore, it supports continuous pretraining, instruction tuning, parameter-efficient finetuning, alignment tuning, inference acceleration, long context generalization, model customization, and even multimodal finetuning, along with carefully designed and extensible APIs. This toolkit has been thoroughly tested and is available atthis https URL.	hed in NAACL 2024 Demo Track	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2306.12420', 'html': 'https://arxiv.org/html/2306.12420v2', 'tex': '/src/2306.12420', 'doi': 'https://doi.org/10.48550/arXiv.2306.12420'}	Submission history From: Shizhe Diao [ view email ] [v1] Wed, 21 Jun 2023 17:58:25 UTC (2,266 KB) [v2] Sun, 5 May 2024 13:13:02 UTC (2,559 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.12420'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.12420'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.12420'}]
2023-06-25	MotionGPT: Finetuned LLMs are General-Purpose Motion Generators	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2306.10900v1	MotionGPT	https://twitter.com/arankomatsuzaki/status/1671341916980490241?s=20		2306.10900v1	['Yaqi Zhang', 'Di Huang', 'Bin Liu', 'Shixiang Tang', 'Yan Lu', 'Lu Chen', 'Lei Bai', 'Qi Chu', 'Nenghai Yu', 'Wanli Ouyang']	ct:Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Codes shall be released upon acceptance.	es, 8 figures	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2306.10900v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2306.10900'}	Submission history From: Yaqi Zhang [ view email ] [v1] Mon, 19 Jun 2023 12:58:17 UTC (39,561 KB) [v2] Mon, 18 Mar 2024 04:14:50 UTC (39,536 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.10900'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.10900'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.10900'}]
2023-06-25	A Simple and Effective Pruning Approach for Large Language Models	Computation and Language	https://arxiv.org/abs/2306.11695	Wanda	https://twitter.com/Yampeleg/status/1671885220218560516?s=20		2306.11695	['Mingjie Sun', 'Zhuang Liu', 'Anna Bair', 'J. Zico Kolter']	ct:As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available atthis https URL.	024. Website atthis https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.11695', 'html': 'https://arxiv.org/html/2306.11695v3', 'tex': '/src/2306.11695', 'doi': 'https://doi.org/10.48550/arXiv.2306.11695'}	Submission history From: Mingjie Sun [ view email ] [v1] Tue, 20 Jun 2023 17:18:20 UTC (611 KB) [v2] Fri, 6 Oct 2023 17:11:09 UTC (442 KB) [v3] Mon, 6 May 2024 17:47:01 UTC (445 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.11695'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.11695'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.11695'}]
2023-06-25	AudioPaLM: A Large Language Model That Can Speak and Listen	Computation and Language	https://arxiv.org/abs/2306.12925v1	AudioPaLM	https://twitter.com/PaulKRubenstein/status/1672128984220413953?s=20		2306.12925v1	['Paul K. Rubenstein', 'Chulayuth Asawaroengchai', 'Duc Dung Nguyen', 'Ankur Bapna', 'Zalán Borsos', 'Félix de Chaumont Quitry', 'Peter Chen', 'Dalia El Badawy', 'Wei Han', 'Eugene Kharitonov', 'Hannah Muckenhirn', 'Dirk Padfield', 'James Qin', 'Danny Rozenberg', 'Tara Sainath', 'Johan Schalkwyk', 'Matt Sharifi', 'Michelle Tadmor Ramanovich', 'Marco Tagliasacchi', 'Alexandru Tudor', 'Mihajlo Velimirović', 'Damien Vincent', 'Jiahui Yu', 'Yongqiang Wang', 'Vicky Zayats', 'Neil Zeghidour', 'Yu Zhang', 'Zhishuai Zhang', 'Lukas Zilka', 'Christian Frank']	ct:We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method atthis https URL	cal report	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Sound (cs.SD)', 'Audio and Speech Processing (eess.AS)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2306.12925v1', 'html': None, 'tex': '/src/2306.12925v1', 'doi': 'https://doi.org/10.48550/arXiv.2306.12925'}	Submission history From: Paul Rubenstein [ view email ] [v1] Thu, 22 Jun 2023 14:37:54 UTC (135 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.12925'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.12925'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.12925'}]
2023-06-18	FinGPT: Open-Source Financial Large Language Models	Quantitative Finance > Statistical Finance	https://arxiv.org/abs/2306.06031	FinGPT	https://twitter.com/omarsar0/status/1668060502663077891?s=20		2306.06031	['Hongyang Yang', 'Xiao-Yang Liu', 'Christina Dan Wang']	ct:Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are \url{this https URL} and \url{this https URL}		['Statistical Finance (q-fin.ST)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Trading and Market Microstructure (q-fin.TR)']	{'pdf': '/pdf/2306.06031', 'html': None, 'tex': '/src/2306.06031', 'doi': 'https://doi.org/10.48550/arXiv.2306.06031'}	Submission history From: Hongyang Yang [ view email ] [v1] Fri, 9 Jun 2023 16:52:00 UTC (1,214 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.06031'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.06031'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.06031'}]
2023-06-18	Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks	Computation and Language	https://arxiv.org/abs/2306.07899v1	Crowd Workers Widely Use Large Language Models for Text Production Tasks	https://twitter.com/manoelribeiro/status/1668986074801098754?s=20		2306.07899v1	['Veniamin Veselovsky', 'Manoel Horta Ribeiro', 'Robert West']	ct:Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data:this https URL	s, 4 figures	['Computation and Language (cs.CL)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2306.07899v1', 'html': None, 'tex': '/src/2306.07899v1', 'doi': 'https://doi.org/10.48550/arXiv.2306.07899'}	Submission history From: Veniamin Veselovsky [ view email ] [v1] Tue, 13 Jun 2023 16:46:24 UTC (308 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.07899'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.07899'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.07899'}]
2023-06-18	On the Reliability of Watermarks for Large Language Models	Machine Learning	https://arxiv.org/abs/2306.04634	Reliability of Watermarks for LLMs	https://twitter.com/tomgoldsteincs/status/1668668484975464448?s=20		2306.04634	['John Kirchenbauer', 'Jonas Geiping', 'Yuxin Wen', 'Manli Shu', 'Khalid Saifullah', 'Kezhi Kong', 'Kasun Fernando', 'Aniruddha Saha', 'Micah Goldblum', 'Tom Goldstein']	ct:As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.	s in the main body. Published at ICLR 2024. Code is available atthis https URL	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Cryptography and Security (cs.CR)']	{'pdf': '/pdf/2306.04634', 'html': None, 'tex': '/src/2306.04634', 'doi': 'https://doi.org/10.48550/arXiv.2306.04634'}	Submission history From: John Kirchenbauer [ view email ] [v1] Wed, 7 Jun 2023 17:58:48 UTC (14,947 KB) [v2] Fri, 9 Jun 2023 17:58:04 UTC (14,993 KB) [v3] Fri, 30 Jun 2023 18:18:12 UTC (14,994 KB) [v4] Wed, 1 May 2024 21:20:36 UTC (20,657 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.04634'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.04634'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.04634'}]
2023-06-18	A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks	Machine Learning	https://arxiv.org/abs/2306.07303	Applications of Transformers	https://twitter.com/omarsar0/status/1668989324950491139?s=20		2306.07303	['Saidul Islam', 'Hanae Elmekki', 'Ahmed Elsebai', 'Jamal Bentahar', 'Najat Drawel', 'Gaith Rjoub', 'Witold Pedrycz']	ct:Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant absence of a comprehensive survey paper encompassing its major applications across various domains. Therefore, we undertook the task of filling this gap by conducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses the identification of the top five application domains for transformer-based models, namely: NLP, Computer Vision, Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze the impact of highly influential transformer-based models in these domains and subsequently classify them based on their respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future possibilities of transformers for enthusiastic researchers, thus contributing to the broader understanding of this groundbreaking technology.		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2306.07303', 'html': None, 'tex': '/src/2306.07303', 'doi': 'https://doi.org/10.48550/arXiv.2306.07303'}	Submission history From: Gaith Rjoub [ view email ] [v1] Sun, 11 Jun 2023 23:13:51 UTC (3,462 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.07303'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.07303'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.07303'}]
2023-06-18	Benchmarking Neural Network Training Algorithms	Machine Learning	https://arxiv.org/abs/2306.07179	Benchmarking NN Training Algorithms	https://twitter.com/zacharynado/status/1668683433944424448?s=20		2306.07179	['George E. Dahl', 'Frank Schneider', 'Zachary Nado', 'Naman Agarwal', 'Chandramouli Shama Sastry', 'Philipp Hennig', 'Sourabh Medapati', 'Runa Eschenhagen', 'Priya Kasimbeg', 'Daniel Suo', 'Juhan Bae', 'Justin Gilmer', 'Abel L. Peirson', 'Bilal Khan', 'Rohan Anil', 'Mike Rabbat', 'Shankar Krishnan', 'Daniel Snider', 'Ehsan Amid', 'Kongtao Chen', 'Chris J. Maddison', 'Rakshith Vasudev', 'Michal Badura', 'Ankush Garg', 'Peter Mattson']	ct:Training algorithms, broadly construed, are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. In order to address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass.	ges, 8 figures, 41 tables	['Machine Learning (cs.LG)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2306.07179', 'html': 'https://arxiv.org/html/2306.07179v2', 'tex': '/src/2306.07179', 'doi': 'https://doi.org/10.48550/arXiv.2306.07179'}	Submission history From: Frank Schneider [ view email ] [v1] Mon, 12 Jun 2023 15:21:02 UTC (1,288 KB) [v2] Wed, 18 Jun 2025 11:00:36 UTC (1,290 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.07179'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.07179'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.07179'}]
2023-06-18	Infinite Photorealistic Worlds using Procedural Generation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2306.09310	Unifying LLMs & Knowledge Graphs	https://twitter.com/johnjnay/status/1670051081722769408?s=20		2306.09310	['Alexander Raistrick', 'Lahav Lipson', 'Zeyu Ma', 'Lingjie Mei', 'Mingzhe Wang', 'Yiming Zuo', 'Karhan Kayan', 'Hongyu Wen', 'Beining Han', 'Yihan Wang', 'Alejandro Newell', 'Hei Law', 'Ankit Goyal', 'Kaiyu Yang', 'Jia Deng']	ct:We introduce Infinigen, a procedural generator of photorealistic 3D scenes of the natural world. Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composition. Infinigen offers broad coverage of objects and scenes in the natural world including plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and snow. Infinigen can be used to generate unlimited, diverse training data for a wide range of computer vision tasks including object detection, semantic segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a useful resource for computer vision research and beyond. Please visitthis https URLfor videos, code and pre-generated data.	ed to CVPR 2023, Camera Ready Version. Update 06/26/23: Change the open-source license to BSD	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2306.09310', 'html': None, 'tex': '/src/2306.09310', 'doi': 'https://doi.org/10.48550/arXiv.2306.09310'}	Submission history From: Lahav Lipson [ view email ] [v1] Thu, 15 Jun 2023 17:46:16 UTC (48,485 KB) [v2] Mon, 26 Jun 2023 17:20:37 UTC (48,485 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.09310'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.09310'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.09310'}]
2023-06-18	Augmenting Language Models with Long-Term Memory	Computation and Language	https://arxiv.org/abs/2306.07174	Augmenting LLMs with Long-term Memory	https://twitter.com/arankomatsuzaki/status/1668429602841317378?s=20		2306.07174	['Weizhi Wang', 'Li Dong', 'Hao Cheng', 'Xiaodong Liu', 'Xifeng Yan', 'Jianfeng Gao', 'Furu Wei']	ct:Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LongMem can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LongMem can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced atthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2306.07174', 'html': None, 'tex': '/src/2306.07174', 'doi': 'https://doi.org/10.48550/arXiv.2306.07174'}	Submission history From: Weizhi Wang [ view email ] [v1] Mon, 12 Jun 2023 15:13:39 UTC (401 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.07174'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.07174'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.07174'}]
2023-06-18	TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2306.08637	TAPIR	https://twitter.com/AdamWHarley/status/1669785589246468096?s=20		2306.08637	['Carl Doersch', 'Yi Yang', 'Mel Vecerik', 'Dilara Gokay', 'Ankush Gupta', 'Yusuf Aytar', 'Joao Carreira', 'Andrew Zisserman']	ct:We present a novel model for Tracking Any Point (TAP) that effectively tracks any queried point on any physical surface throughout a video sequence. Our approach employs two stages: (1) a matching stage, which independently locates a suitable candidate point match for the query point on every other frame, and (2) a refinement stage, which updates both the trajectory and query features based on local correlations. The resulting model surpasses all baseline methods by a significant margin on the TAP-Vid benchmark, as demonstrated by an approximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our model facilitates fast inference on long and high-resolution video sequences. On a modern GPU, our implementation has the capacity to track points faster than real-time, and can be flexibly extended to higher-resolution videos. Given the high-quality trajectories extracted from a large dataset, we demonstrate a proof-of-concept diffusion model which generates trajectories from static images, enabling plausible animations. Visualizations, source code, and pretrained models can be found on our project webpage.	hed at ICCV 2023	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2306.08637', 'html': None, 'tex': '/src/2306.08637', 'doi': 'https://doi.org/10.48550/arXiv.2306.08637'}	Submission history From: Carl Doersch [ view email ] [v1] Wed, 14 Jun 2023 17:07:51 UTC (2,145 KB) [v2] Wed, 30 Aug 2023 14:28:37 UTC (5,730 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.08637'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.08637'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.08637'}]
2023-06-18	Mind2Web: Towards a Generalist Agent for the Web	Computation and Language	https://arxiv.org/abs/2306.06070	Mind2Web	https://twitter.com/DrJimFan/status/1669403956064432128?s=20		2306.06070	['Xiang Deng', 'Yu Gu', 'Boyuan Zheng', 'Shijie Chen', 'Samuel Stevens', 'Boshi Wang', 'Huan Sun', 'Yu Su']	ct:We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (this https URL) to facilitate further research on building a generalist agent for the web.	e:this https URL. Updated with supplementary material. NeurIPS'23 Spotlight	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2306.06070', 'html': 'https://arxiv.org/html/2306.06070v3', 'tex': '/src/2306.06070', 'doi': 'https://doi.org/10.48550/arXiv.2306.06070'}	Submission history From: Xiang Deng [ view email ] [v1] Fri, 9 Jun 2023 17:44:31 UTC (9,798 KB) [v2] Thu, 15 Jun 2023 03:50:30 UTC (9,791 KB) [v3] Sat, 9 Dec 2023 05:57:46 UTC (9,772 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.06070'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.06070'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.06070'}]
2023-06-11	Tracking Everything Everywhere All at Once	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2306.05422	Tracking Everything Everywhere All at Once	https://twitter.com/sstj389/status/1667000331958468608?s=20		2306.05422	['Qianqian Wang', 'Yen-Yu Chang', 'Ruojin Cai', 'Zhengqi Li', 'Bharath Hariharan', 'Aleksander Holynski', 'Noah Snavely']	ct:We present a new test-time optimization method for estimating dense and long-range motion from a video sequence. Prior optical flow or particle video tracking algorithms typically operate within limited temporal windows, struggling to track through occlusions and maintain global consistency of estimated motion trajectories. We propose a complete and globally consistent motion representation, dubbed OmniMotion, that allows for accurate, full-length motion estimation of every pixel in a video. OmniMotion represents a video using a quasi-3D canonical volume and performs pixel-wise tracking via bijections between local and canonical space. This representation allows us to ensure global consistency, track through occlusions, and model any combination of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and real-world footage show that our approach outperforms prior state-of-the-art methods by a large margin both quantitatively and qualitatively. See our project page for more results:this http URL	023	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2306.05422', 'html': None, 'tex': '/src/2306.05422', 'doi': 'https://doi.org/10.48550/arXiv.2306.05422'}	Submission history From: Qianqian Wang [ view email ] [v1] Thu, 8 Jun 2023 17:59:29 UTC (10,975 KB) [v2] Tue, 12 Sep 2023 16:32:52 UTC (11,092 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.05422'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.05422'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.05422'}]
2023-06-11	SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression	Computation and Language	https://arxiv.org/abs/2306.03078	Sparse-Quantized Representation	https://twitter.com/Tim_Dettmers/status/1666076553665744896?s=20		2306.03078	['Tim Dettmers', 'Ruslan Svirschevski', 'Vage Egiazarian', 'Denis Kuznedelev', 'Elias Frantar', 'Saleh Ashkboos', 'Alexander Borzunov', 'Torsten Hoefler', 'Dan Alistarh']	ct:Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.	ed preprint	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.03078', 'html': None, 'tex': '/src/2306.03078', 'doi': 'https://doi.org/10.48550/arXiv.2306.03078'}	Submission history From: Vage Egiazarian [ view email ] [v1] Mon, 5 Jun 2023 17:53:28 UTC (3,986 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.03078'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.03078'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.03078'}]
2023-06-11	Simple and Controllable Music Generation	Sound	https://arxiv.org/abs/2306.05284	MusicGen	https://twitter.com/syhw/status/1667103478471176192?s=20		2306.05284	['Jade Copet', 'Felix Kreuk', 'Itai Gat', 'Tal Remez', 'David Kant', 'Gabriel Synnaeve', 'Yossi Adi', 'Alexandre Défossez']	ct:We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available atthis https URL	hed at Neurips 2023	['Sound (cs.SD)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2306.05284', 'html': None, 'tex': '/src/2306.05284', 'doi': 'https://doi.org/10.48550/arXiv.2306.05284'}	Submission history From: Yossi Adi [ view email ] [v1] Thu, 8 Jun 2023 15:31:05 UTC (269 KB) [v2] Tue, 7 Nov 2023 10:43:23 UTC (362 KB) [v3] Tue, 30 Jan 2024 04:49:16 UTC (1,304 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.05284'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.05284'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.05284'}]
2023-06-11	ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory	Artificial Intelligence	https://arxiv.org/abs/2306.03901	Augmenting LLMs with Databases	https://twitter.com/omarsar0/status/1666254609524961282?s=20		2306.03901	['Chenxu Hu', 'Jie Fu', 'Chenzhuang Du', 'Simian Luo', 'Junbo Zhao', 'Hang Zhao']	ct:Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available atthis https URL.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Databases (cs.DB)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.03901', 'html': None, 'tex': '/src/2306.03901', 'doi': 'https://doi.org/10.48550/arXiv.2306.03901'}	Submission history From: Chenxu Hu [ view email ] [v1] Tue, 6 Jun 2023 17:58:24 UTC (297 KB) [v2] Wed, 7 Jun 2023 17:22:22 UTC (298 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.03901'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.03901'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.03901'}]
2023-06-11	LEACE: Perfect linear concept erasure in closed form	Machine Learning	https://arxiv.org/abs/2306.03819	Concept Scrubbing in LLM	https://twitter.com/norabelrose/status/1666469917636571137?s=20		2306.03819	['Nora Belrose', 'David Schneider-Joseph', 'Shauli Ravfogel', 'Ryan Cotterell', 'Edward Raff', 'Stella Biderman']	"ct:Concept erasure aims to remove specified features from an embedding. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the embedding as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called ""concept scrubbing,"" which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available atthis https URL."		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2306.03819', 'html': 'https://arxiv.org/html/2306.03819v4', 'tex': '/src/2306.03819', 'doi': 'https://doi.org/10.48550/arXiv.2306.03819'}	Submission history From: Nora Belrose [ view email ] [v1] Tue, 6 Jun 2023 16:07:24 UTC (92 KB) [v2] Fri, 23 Jun 2023 00:16:46 UTC (164 KB) [v3] Sun, 29 Oct 2023 21:41:46 UTC (197 KB) [v4] Thu, 3 Apr 2025 01:51:37 UTC (198 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.03819'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.03819'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.03819'}]
2023-06-11	Fine-Grained Human Feedback Gives Better Rewards for Language Model Training	Computation and Language	https://arxiv.org/abs/2306.01693	Fine-Grained RLHF	https://twitter.com/zeqiuwu1/status/1665785626552049665?s=20		2306.01693	['Zeqiu Wu', 'Yushi Hu', 'Weijia Shi', 'Nouha Dziri', 'Alane Suhr', 'Prithviraj Ammanabrolu', 'Noah A. Smith', 'Mari Ostendorf', 'Hannaneh Hajishirzi']	ct:Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes atthis https URL.	S 2023 camera-ready	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2306.01693', 'html': None, 'tex': '/src/2306.01693', 'doi': 'https://doi.org/10.48550/arXiv.2306.01693'}	Submission history From: Zeqiu Wu [ view email ] [v1] Fri, 2 Jun 2023 17:11:37 UTC (3,634 KB) [v2] Mon, 30 Oct 2023 06:34:35 UTC (3,626 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.01693'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.01693'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.01693'}]
2023-06-11	Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2306.00989	Hierarchical Vision Transformer	https://twitter.com/MetaAI/status/1665759715765411840?s=20		2306.00989	['Chaitanya Ryali', 'Yuan-Ting Hu', 'Daniel Bolya', 'Chen Wei', 'Haoqi Fan', 'Po-Yao Huang', 'Vaibhav Aggarwal', 'Arkabandhu Chowdhury', 'Omid Poursaeed', 'Judy Hoffman', 'Jitendra Malik', 'Yanghao Li', 'Christoph Feichtenhofer']	ct:Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available atthis https URL.	023 Oral version. Code+Models:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.00989', 'html': None, 'tex': '/src/2306.00989', 'doi': 'https://doi.org/10.48550/arXiv.2306.00989'}	Submission history From: Daniel Bolya [ view email ] [v1] Thu, 1 Jun 2023 17:59:58 UTC (2,647 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.00989'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.00989'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.00989'}]
2023-06-11	ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models	Artificial Intelligence	https://arxiv.org/abs/2306.04563	Humor in ChatGPT	https://twitter.com/AlbertBoyangLi/status/1666707728272850944?s=20		2306.04563	['Sophie Jentzsch', 'Kristian Kersting']	"ct:Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI's ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny? We put ChatGPT's sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional explanations for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the classification of jokes. ChatGPT has not solved computational humor yet but it can be a big leap toward ""funny"" machines."		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Human-Computer Interaction (cs.HC)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.04563', 'html': None, 'tex': '/src/2306.04563', 'doi': 'https://doi.org/10.48550/arXiv.2306.04563'}	Submission history From: Sophie Jentzsch [ view email ] [v1] Wed, 7 Jun 2023 16:10:21 UTC (599 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.04563'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.04563'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.04563'}]
2023-06-11	Orca: Progressive Learning from Complex Explanation Traces of GPT-4	Computation and Language	https://arxiv.org/abs/2306.02707	Imitating Reasoning Process of Larger LLMs	https://twitter.com/johnjnay/status/1665906453587034112?s=20		2306.02707	['Subhabrata Mukherjee', 'Arindam Mitra', 'Ganesh Jawahar', 'Sahaj Agarwal', 'Hamid Palangi', 'Ahmed Awadallah']	ct:Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published atthis https URL), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.		['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.02707', 'html': None, 'tex': '/src/2306.02707', 'doi': 'https://doi.org/10.48550/arXiv.2306.02707'}	Submission history From: Arindam Mitra [ view email ] [v1] Mon, 5 Jun 2023 08:58:39 UTC (1,659 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.02707'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.02707'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.02707'}]
2023-06-04	Let's Verify Step by Step	Machine Learning	https://arxiv.org/abs/2305.20050	Let’s Verify Step by Step	https://twitter.com/OpenAI/status/1663957407184347136?s=20		2305.20050	['Hunter Lightman', 'Vineet Kosaraju', 'Yura Burda', 'Harri Edwards', 'Bowen Baker', 'Teddy Lee', 'Jan Leike', 'John Schulman', 'Ilya Sutskever', 'Karl Cobbe']	ct:In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.20050', 'html': None, 'tex': '/src/2305.20050', 'doi': 'https://doi.org/10.48550/arXiv.2305.20050'}	Submission history From: Karl Cobbe [ view email ] [v1] Wed, 31 May 2023 17:24:00 UTC (10,363 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.20050'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.20050'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.20050'}]
2023-06-04	The Impact of Positional Encoding on Length Generalization in Transformers	Computation and Language	https://arxiv.org/abs/2305.19466	No Positional Encodings	https://twitter.com/a_kazemnejad/status/1664277559968927744?s=20		2305.19466	['Amirhossein Kazemnejad', 'Inkit Padhi', 'Karthikeyan Natesan Ramamurthy', 'Payel Das', 'Siva Reddy']	ct:Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model's performance. Overall, our work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences.	ed at NeurIPS 2023; 15 pages and 22 pages Appendix	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.19466', 'html': None, 'tex': '/src/2305.19466', 'doi': 'https://doi.org/10.48550/arXiv.2305.19466'}	Submission history From: Amirhossein Kazemnejad [ view email ] [v1] Wed, 31 May 2023 00:29:55 UTC (1,942 KB) [v2] Mon, 6 Nov 2023 19:48:10 UTC (2,132 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.19466'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.19466'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.19466'}]
2023-06-04	BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse Biomedical Tasks	Computation and Language	https://arxiv.org/abs/2305.17100	BiomedGPT	https://twitter.com/omarsar0/status/1662992484576681986?s=20		2305.17100	['Kai Zhang', 'Rong Zhou', 'Eashan Adhikarla', 'Zhiling Yan', 'Yixin Liu', 'Jun Yu', 'Zhengliang Liu', 'Xun Chen', 'Brian D. Davison', 'Hui Ren', 'Jing Huang', 'Chen Chen', 'Yuyin Zhou', 'Sunyang Fu', 'Wei Liu', 'Tianming Liu', 'Xiang Li', 'Yong Chen', 'Lifang He', 'James Zou', 'Quanzheng Li', 'Hongfang Liu', 'Lichao Sun']	ct:Traditional biomedical artificial intelligence (AI) models, designed for specific tasks or modalities, often exhibit limited flexibility in real-world deployment and struggle to utilize holistic information. Generalist AI holds the potential to address these limitations due to its versatility in interpreting different data types and generating tailored outputs for diverse needs. However, existing biomedical generalist AI solutions are typically heavyweight and closed source to researchers, practitioners, and patients. Here, we propose BiomedGPT, the first open-source and lightweight vision-language foundation model, designed as a generalist capable of performing various biomedical tasks. BiomedGPT achieved state-of-the-art results in 16 out of 25 experiments while maintaining a computing-friendly model scale. We also conducted human evaluations to assess the capabilities of BiomedGPT in radiology visual question answering, report generation, and summarization. BiomedGPT exhibits robust prediction ability with a low error rate of 3.8% in question answering, satisfactory performance with an error rate of 8.3% in writing complex radiology reports, and competitive summarization ability with a nearly equivalent preference score to human experts. Our method demonstrates that effective training with diverse data can lead to more practical biomedical AI for improving diagnosis and workflow efficiency.	correct citations and add journal reference for the published version. Nat Med (2024)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2305.17100', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2305.17100'}	Submission history From: Kai Zhang [ view email ] [v1] Fri, 26 May 2023 17:14:43 UTC (17,753 KB) [v2] Tue, 9 Jan 2024 20:14:33 UTC (7,424 KB) [v3] Fri, 19 Jul 2024 19:42:36 UTC (22,503 KB) [v4] Sun, 11 Aug 2024 20:03:12 UTC (21,060 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.17100'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.17100'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.17100'}]
2023-06-04	Thought Cloning: Learning to Think while Acting by Imitating Human Thinking	Artificial Intelligence	https://arxiv.org/abs/2306.00323	Thought Cloning	https://twitter.com/johnjnay/status/1664798780644904960?s=20		2306.00323	['Shengran Hu', 'Jeff Clune']	ct:Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than Behavioral Cloning and its performance advantage grows the further out of distribution test tasks are, highlighting its ability to better handle novel situations. Thought Cloning also provides important benefits for AI Safety and Interpretability, and makes it easier to debug and improve AI. Because we can observe the agent's thoughts, we can (1) more easily diagnose why things are going wrong, making it easier to fix the problem, (2) steer the agent by correcting its thinking, or (3) prevent it from doing unsafe things it plans to do. Overall, by training agents how to think as well as behave, Thought Cloning creates safer, more powerful agents.	ed to NeurIPS 2023 as a spotlight	['Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2306.00323', 'html': 'https://arxiv.org/html/2306.00323v3', 'tex': '/src/2306.00323', 'doi': 'https://doi.org/10.48550/arXiv.2306.00323'}	Submission history From: Shengran Hu [ view email ] [v1] Thu, 1 Jun 2023 03:43:41 UTC (843 KB) [v2] Thu, 26 Oct 2023 18:22:42 UTC (847 KB) [v3] Thu, 18 Jan 2024 00:10:32 UTC (887 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.00323'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.00323'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.00323'}]
2023-06-04	Fine-Tuning Language Models with Just Forward Passes	Machine Learning	https://arxiv.org/abs/2305.17333	Fine-Tuning Language Models with Just Forward Passes	https://twitter.com/arankomatsuzaki/status/1663360307274690560?s=20		2305.17333	['Sadhika Malladi', 'Tianyu Gao', 'Eshaan Nichani', 'Alex Damian', 'Jason D. Lee', 'Danqi Chen', 'Sanjeev Arora']	ct:Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.	ed by NeurIPS 2023 (oral). Code available atthis https URL	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.17333', 'html': 'https://arxiv.org/html/2305.17333v3', 'tex': '/src/2305.17333', 'doi': 'https://doi.org/10.48550/arXiv.2305.17333'}	Submission history From: Tianyu Gao [ view email ] [v1] Sat, 27 May 2023 02:28:10 UTC (603 KB) [v2] Tue, 31 Oct 2023 14:57:22 UTC (609 KB) [v3] Thu, 11 Jan 2024 13:56:52 UTC (662 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.17333'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.17333'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.17333'}]
2023-06-04	MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training	Sound	https://arxiv.org/abs/2306.00107	MERT	https://twitter.com/yizhilll/status/1664680921146982401?s=20		2306.00107	['Yizhi Li', 'Ruibin Yuan', 'Ge Zhang', 'Yinghao Ma', 'Xingran Chen', 'Hanzhi Yin', 'Chenghao Xiao', 'Chenghua Lin', 'Anton Ragni', 'Emmanouil Benetos', 'Norbert Gyenge', 'Roger Dannenberg', 'Ruibo Liu', 'Wenhu Chen', 'Gus Xia', 'Yemin Shi', 'Wenhao Huang', 'Zili Wang', 'Yike Guo', 'Jie Fu']	ct:Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.	ed by ICLR 2024	['Sound (cs.SD)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2306.00107', 'html': 'https://arxiv.org/html/2306.00107v5', 'tex': '/src/2306.00107', 'doi': 'https://doi.org/10.48550/arXiv.2306.00107'}	Submission history From: Yizhi Li [ view email ] [v1] Wed, 31 May 2023 18:27:43 UTC (364 KB) [v2] Tue, 6 Jun 2023 14:06:02 UTC (364 KB) [v3] Wed, 7 Feb 2024 11:12:27 UTC (393 KB) [v4] Mon, 22 Apr 2024 21:52:17 UTC (393 KB) [v5] Fri, 27 Dec 2024 12:28:34 UTC (393 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.00107'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.00107'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.00107'}]
2023-06-04	Bytes Are All You Need: Transformers Operating Directly On File Bytes	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2306.00238	Bytes Are All You Need	https://twitter.com/_akhaliq/status/1664497650702471169?s=20		2306.00238	['Maxwell Horton', 'Sachin Mehta', 'Ali Farhadi', 'Mohammad Rastegari']	ct:Modern deep learning approaches usually utilize modality-specific processing. For example, the most common deep learning approach to image classification involves decoding image file bytes into an RGB tensor which is passed into a neural network. Instead, we investigate modality-independent representation learning by performing classification directly on file bytes, without the need for decoding files at inference time. This enables models to operate on various modalities without any hand-designed, modality-specific processing. Our model, ByteFormer, improves ImageNet Top-1 classification accuracy by $5\%$ (from $72.2\%$ to $77.33\%$) relative to DeIT models of similar size. Compared to Perceiver IO, our model requires absolutely no modality-specific processing at inference time, and uses an order of magnitude fewer parameters at equivalent accuracy on ImageNet. We demonstrate that the same ByteFormer architecture can perform audio classification without modifications or modality-specific preprocessing. We achieve $95.42\%$ classification accuracy on the Speech Commands V2 dataset (comparable to the state-of-the-art accuracy of $98.7\%$). Additionally, we demonstrate that ByteFormer can operate jointly on images and audio, handling joint classification without explicit knowledge of the input modality. We release our code atthis https URL.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2306.00238', 'html': 'https://arxiv.org/html/2306.00238v2', 'tex': '/src/2306.00238', 'doi': 'https://doi.org/10.48550/arXiv.2306.00238'}	Submission history From: Maxwell Horton [ view email ] [v1] Wed, 31 May 2023 23:18:21 UTC (7,209 KB) [v2] Mon, 1 Jul 2024 15:54:17 UTC (9,843 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.00238'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.00238'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.00238'}]
2023-06-04	Direct Preference Optimization: Your Language Model is Secretly a Reward Model	Machine Learning	https://arxiv.org/abs/2305.18290	Direct Preference Optimization	https://twitter.com/archit_sharma97/status/1663595372269408261?s=20		2305.18290	['Rafael Rafailov', 'Archit Sharma', 'Eric Mitchell', 'Stefano Ermon', 'Christopher D. Manning', 'Chelsea Finn']	ct:While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.18290', 'html': 'https://arxiv.org/html/2305.18290v3', 'tex': '/src/2305.18290', 'doi': 'https://doi.org/10.48550/arXiv.2305.18290'}	Submission history From: Archit Sharma [ view email ] [v1] Mon, 29 May 2023 17:57:46 UTC (982 KB) [v2] Wed, 13 Dec 2023 18:48:48 UTC (983 KB) [v3] Mon, 29 Jul 2024 22:26:36 UTC (999 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.18290'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.18290'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.18290'}]
2023-06-04	SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL (extended)	Computation and Language	https://arxiv.org/abs/2306.00739	SQL-PaLM	https://twitter.com/omarsar0/status/1664441085693657088?s=20		2306.00739	['Ruoxi Sun', 'Sercan Ö. Arik', 'Alex Muzio', 'Lesly Miculicich', 'Satya Gundabathula', 'Pengcheng Yin', 'Hanjun Dai', 'Hootan Nakhost', 'Rajarishi Sinha', 'Zifeng Wang', 'Tomas Pfister']	ct:Text-to-SQL, the process of translating natural language into Structured Query Language (SQL), represents a transformative application of large language models (LLMs), potentially revolutionizing how humans interact with data. This paper introduces the SQL-PaLM framework, a comprehensive solution for understanding and enhancing Text-to-SQL using LLMs, using in the learning regimes of few-shot prompting and instruction fine-tuning. With few-shot prompting, we explore the effectiveness of consistency decoding with execution-based error filtering. With instruction fine-tuning, we delve deep in understanding the critical paradigms that influence the performance of tuned LLMs. In particular, we investigate how performance can be improved through expanded training data coverage and diversity, synthetic data augmentation, and integrating query-specific database content. We propose a test-time selection method to further refine accuracy by integrating SQL outputs from multiple paradigms with execution feedback as guidance. Additionally, we tackle the practical challenge of navigating intricate databases with a significant number of tables and columns, proposing efficient techniques for accurately selecting relevant database elements to enhance Text-to-SQL performance. Our holistic approach yields substantial advancements in Text-to-SQL, as demonstrated on two key public benchmarks, Spider and BIRD. Through comprehensive ablations and error analyses, we shed light on the strengths and weaknesses of our framework, offering valuable insights into Text-to-SQL's future work.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Databases (cs.DB)']	{'pdf': '/pdf/2306.00739', 'html': 'https://arxiv.org/html/2306.00739v4', 'tex': '/src/2306.00739', 'doi': 'https://doi.org/10.48550/arXiv.2306.00739'}	Submission history From: Ruoxi Sun [ view email ] [v1] Fri, 26 May 2023 21:39:05 UTC (315 KB) [v2] Wed, 7 Jun 2023 07:23:56 UTC (389 KB) [v3] Sun, 25 Jun 2023 06:44:48 UTC (389 KB) [v4] Sat, 30 Mar 2024 17:22:44 UTC (1,204 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.00739'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.00739'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.00739'}]
2023-06-04	CodeTF: One-stop Transformer Library for State-of-the-art Code LLM	Software Engineering	https://arxiv.org/abs/2306.00029	CodeTF	https://twitter.com/stevenhoi/status/1664483010954272770?s=20		2306.00029	['Nghi D. Q. Bui', 'Hung Le', 'Yue Wang', 'Junnan Li', 'Akhilesh Deepak Gotmare', 'Steven C. H. Hoi']	ct:Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners.	g work - Draft Preview	['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2306.00029', 'html': None, 'tex': '/src/2306.00029', 'doi': 'https://doi.org/10.48550/arXiv.2306.00029'}	Submission history From: Nghi D. Q. Bui [ view email ] [v1] Wed, 31 May 2023 05:24:48 UTC (1,324 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2306.00029'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2306.00029'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2306.00029'}]
2023-05-28	QLoRA: Efficient Finetuning of Quantized LLMs	Machine Learning	https://arxiv.org/abs/2305.14314	QLoRA	https://twitter.com/Tim_Dettmers/status/1661379354507476994?s=20		2305.14314	['Tim Dettmers', 'Artidoro Pagnoni', 'Ari Holtzman', 'Luke Zettlemoyer']	ct:We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.	ed NeurIPS submission	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.14314', 'html': None, 'tex': '/src/2305.14314', 'doi': 'https://doi.org/10.48550/arXiv.2305.14314'}	Submission history From: Tim Dettmers [ view email ] [v1] Tue, 23 May 2023 17:50:33 UTC (568 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.14314'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.14314'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.14314'}]
2023-05-28	LIMA: Less Is More for Alignment	Computation and Language	https://arxiv.org/abs/2305.11206	LIMA	https://twitter.com/violet_zct/status/1660789120069926912?s=20		2305.11206	['Chunting Zhou', 'Pengfei Liu', 'Puxin Xu', 'Srini Iyer', 'Jiao Sun', 'Yuning Mao', 'Xuezhe Ma', 'Avia Efrat', 'Ping Yu', 'Lili Yu', 'Susan Zhang', 'Gargi Ghosh', 'Mike Lewis', 'Luke Zettlemoyer', 'Omer Levy']	ct:Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.11206', 'html': None, 'tex': '/src/2305.11206', 'doi': 'https://doi.org/10.48550/arXiv.2305.11206'}	Submission history From: Omer Levy [ view email ] [v1] Thu, 18 May 2023 17:45:22 UTC (1,006 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.11206'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.11206'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.11206'}]
2023-05-28	Voyager: An Open-Ended Embodied Agent with Large Language Models	Artificial Intelligence	https://arxiv.org/abs/2305.16291	Voyager	https://twitter.com/DrJimFan/status/1662115266933972993?s=20		2305.16291	['Guanzhi Wang', 'Yuqi Xie', 'Yunfan Jiang', 'Ajay Mandlekar', 'Chaowei Xiao', 'Yuke Zhu', 'Linxi Fan', 'Anima Anandkumar']	ct:We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts atthis https URL.	t website and open-source codebase:this https URL	['Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.16291', 'html': None, 'tex': '/src/2305.16291', 'doi': 'https://doi.org/10.48550/arXiv.2305.16291'}	Submission history From: Guanzhi Wang [ view email ] [v1] Thu, 25 May 2023 17:46:38 UTC (13,286 KB) [v2] Thu, 19 Oct 2023 16:27:03 UTC (13,649 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.16291'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.16291'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.16291'}]
2023-05-28	Gorilla: Large Language Model Connected with Massive APIs	Computation and Language	https://arxiv.org/abs/2305.15334	Gorilla	https://twitter.com/omarsar0/status/1661540207206846464?s=20		2305.15334	['Shishir G. Patil', 'Tianjun Zhang', 'Xin Wang', 'Joseph E. Gonzalez']	ct:Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available atthis https URL		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2305.15334', 'html': None, 'tex': '/src/2305.15334', 'doi': 'https://doi.org/10.48550/arXiv.2305.15334'}	Submission history From: Shishir G. Patil [ view email ] [v1] Wed, 24 May 2023 16:48:11 UTC (1,283 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.15334'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.15334'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.15334'}]
2023-05-28	The False Promise of Imitating Proprietary LLMs	Computation and Language	https://arxiv.org/abs/2305.15717	The False Promise of Imitating Proprietary LLMs	https://twitter.com/arankomatsuzaki/status/1661908342829187072?s=20		2305.15717	['Arnav Gudibande', 'Eric Wallace', 'Charlie Snell', 'Xinyang Geng', 'Hao Liu', 'Pieter Abbeel', 'Sergey Levine', 'Dawn Song']	ct:An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.15717', 'html': None, 'tex': '/src/2305.15717', 'doi': 'https://doi.org/10.48550/arXiv.2305.15717'}	Submission history From: Arnav Gudibande [ view email ] [v1] Thu, 25 May 2023 05:00:12 UTC (630 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.15717'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.15717'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.15717'}]
2023-05-28	Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training	Machine Learning	https://arxiv.org/abs/2305.14342	Sophia	https://twitter.com/tengyuma/status/1661412995430219786?s=20		2305.14342	['Hong Liu', 'Zhiyuan Li', 'David Hall', 'Percy Liang', 'Tengyu Ma']	ct:Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50% fewer steps, less total compute, and reduced wall-clock time. Theoretically, we show that Sophia, in a much simplified setting, adapts to the heterogeneous curvatures in different parameter dimensions, and thus has a run-time bound that does not depend on the condition number of the loss.		['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Optimization and Control (math.OC)']	{'pdf': '/pdf/2305.14342', 'html': 'https://arxiv.org/html/2305.14342v4', 'tex': '/src/2305.14342', 'doi': 'https://doi.org/10.48550/arXiv.2305.14342'}	Submission history From: Hong Liu [ view email ] [v1] Tue, 23 May 2023 17:59:21 UTC (2,993 KB) [v2] Mon, 9 Oct 2023 19:54:09 UTC (2,777 KB) [v3] Tue, 17 Oct 2023 07:44:16 UTC (2,777 KB) [v4] Tue, 5 Mar 2024 17:07:16 UTC (6,090 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.14342'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.14342'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.14342'}]
2023-05-28	The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python	Computation and Language	https://arxiv.org/abs/2305.15507	The Larger They Are, the Harder They Fail	https://twitter.com/AVMiceliBarone/status/1662150656327663617?s=20		2305.15507	['Antonio Valerio Miceli-Barone', 'Fazl Barez', 'Ioannis Konstas', 'Shay B. Cohen']	ct:Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.	es, 5 figure, ACL 2023	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2305.15507', 'html': None, 'tex': '/src/2305.15507', 'doi': 'https://doi.org/10.48550/arXiv.2305.15507'}	Submission history From: Antonio Valerio Miceli Barone [ view email ] [v1] Wed, 24 May 2023 18:54:39 UTC (7,730 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.15507'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.15507'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.15507'}]
2023-05-28	Model evaluation for extreme risks	Artificial Intelligence	https://arxiv.org/abs/2305.15324	Model Evaluation for Extreme Risks	https://twitter.com/soundboy/status/1661728733156503555?s=20		2305.15324	['Toby Shevlane', 'Sebastian Farquhar', 'Ben Garfinkel', 'Mary Phuong', 'Jess Whittlestone', 'Jade Leung', 'Daniel Kokotajlo', 'Nahema Marchal', 'Markus Anderljung', 'Noam Kolt', 'Lewis Ho', 'Divya Siddarth', 'Shahar Avin', 'Will Hawkins', 'Been Kim', 'Iason Gabriel', 'Vijay Bolina', 'Jack Clark', 'Yoshua Bengio', 'Paul Christiano', 'Allan Dafoe']	"ct:Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through ""dangerous capability evaluations"") and the propensity of models to apply their capabilities for harm (through ""alignment evaluations""). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security."	typos; added citation	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2305.15324', 'html': None, 'tex': '/src/2305.15324', 'doi': 'https://doi.org/10.48550/arXiv.2305.15324'}	Submission history From: Toby Shevlane [ view email ] [v1] Wed, 24 May 2023 16:38:43 UTC (467 KB) [v2] Fri, 22 Sep 2023 18:48:42 UTC (467 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.15324'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.15324'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.15324'}]
2023-05-28	Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models	Computation and Language	https://arxiv.org/abs/2305.12544	LLM Research Directions	https://twitter.com/omarsar0/status/1661405738059571201?s=20		2305.12544	['Oana Ignat', 'Zhijing Jin', 'Artem Abzaliev', 'Laura Biester', 'Santiago Castro', 'Naihao Deng', 'Xinyi Gao', 'Aylin Gunal', 'Jacky He', 'Ashkan Kazemi', 'Muhammad Khalifa', 'Namho Koh', 'Andrew Lee', 'Siyang Liu', 'Do June Min', 'Shinka Mori', 'Joan Nwatu', 'Veronica Perez-Rosas', 'Siqi Shen', 'Zekun Wang', 'Winston Wu', 'Rada Mihalcea']	ct:Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that ``it's all been solved.'' Not surprisingly, this has, in turn, made many NLP researchers -- especially those at the beginning of their careers -- worry about what NLP research area they should focus on. Has it all been solved, or what remaining questions can we work on regardless of LLMs? To address this question, this paper compiles NLP research directions rich for exploration. We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs. While we identify many research areas, many others exist; we do not cover areas currently addressed by LLMs, but where LLMs lag behind in performance or those focused on LLM development. We welcome suggestions for other research directions to include:this https URL	ed at COLING 2024	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2305.12544', 'html': 'https://arxiv.org/html/2305.12544v2', 'tex': '/src/2305.12544', 'doi': 'https://doi.org/10.48550/arXiv.2305.12544'}	Submission history From: Oana Ignat [ view email ] [v1] Sun, 21 May 2023 19:06:30 UTC (145 KB) [v2] Fri, 15 Mar 2024 20:10:51 UTC (2,386 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.12544'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.12544'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.12544'}]
2023-05-28	RWKV: Reinventing RNNs for the Transformer Era	Computation and Language	https://arxiv.org/abs/2305.13048	Reinventing RNNs for the Transformer Era	https://twitter.com/_akhaliq/status/1660816265454419969?s=20		2305.13048	['Bo Peng', 'Eric Alcaide', 'Quentin Anthony', 'Alon Albalak', 'Samuel Arcadinho', 'Stella Biderman', 'Huanqi Cao', 'Xin Cheng', 'Michael Chung', 'Matteo Grella', 'Kranthi Kiran GV', 'Xuzheng He', 'Haowen Hou', 'Jiaju Lin', 'Przemyslaw Kazienko', 'Jan Kocon', 'Jiaming Kong', 'Bartlomiej Koptyra', 'Hayden Lau', 'Krishna Sri Ipsit Mantri', 'Ferdinand Mom', 'Atsushi Saito', 'Guangyu Song', 'Xiangru Tang', 'Bolun Wang', 'Johan S. Wind', 'Stanislaw Wozniak', 'Ruichong Zhang', 'Zhenyuan Zhang', 'Qihang Zhao', 'Peng Zhou', 'Qinghua Zhou', 'Jian Zhu', 'Rui-Jie Zhu']	ct:Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs.Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2305.13048', 'html': 'https://arxiv.org/html/2305.13048v2', 'tex': '/src/2305.13048', 'doi': 'https://doi.org/10.48550/arXiv.2305.13048'}	Submission history From: Quentin Anthony [ view email ] [v1] Mon, 22 May 2023 13:57:41 UTC (5,484 KB) [v2] Mon, 11 Dec 2023 03:58:56 UTC (4,261 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.13048'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.13048'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.13048'}]
2023-05-21	Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2305.10973v1	Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold	https://twitter.com/dair_ai/status/1660268470057967616?s=20		2305.10973v1	['Xingang Pan', 'Ayush Tewari', 'Thomas Leimkühler', 'Lingjie Liu', 'Abhimitra Meka', 'Christian Theobalt']	"ct:Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to ""drag"" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion."	ed to SIGGRAPH 2023. Project page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)']	{'pdf': '/pdf/2305.10973v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2305.10973'}	Submission history From: Xingang Pan [ view email ] [v1] Thu, 18 May 2023 13:41:25 UTC (13,403 KB) [v2] Wed, 17 Jul 2024 10:27:55 UTC (13,404 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.10973'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.10973'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.10973'}]
2023-05-21	Emergent Representations of Program Semantics in Language Models Trained on Programs	Machine Learning	https://arxiv.org/abs/2305.11169	Evidence of Meaning in Language Models Trained on Programs	https://twitter.com/dair_ai/status/1660268472129945600?s=20		2305.11169	['Charles Jin', 'Martin Rinard']	ct:We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to interpret programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We anticipate that this technique may be generally applicable to a broad range of semantic probing experiments. In summary, this paper does not propose any new techniques for training LMs of code, but develops an experimental framework for and provides insights into the acquisition and representation of formal semantics in statistical models of code. Our code is available atthis https URL.	024	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Programming Languages (cs.PL)']	{'pdf': '/pdf/2305.11169', 'html': 'https://arxiv.org/html/2305.11169v3', 'tex': '/src/2305.11169', 'doi': 'https://doi.org/10.48550/arXiv.2305.11169'}	Submission history From: Charles Jin [ view email ] [v1] Thu, 18 May 2023 17:58:08 UTC (3,902 KB) [v2] Wed, 24 May 2023 11:52:13 UTC (11,182 KB) [v3] Fri, 2 Aug 2024 23:09:32 UTC (7,152 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.11169'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.11169'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.11169'}]
2023-05-21	Towards Expert-Level Medical Question Answering with Large Language Models	Computation and Language	https://arxiv.org/abs/2305.09617	Towards Expert-Level Medical Question Answering with Large Language Models	https://twitter.com/dair_ai/status/1660268473853829121?s=20		2305.09617	['Karan Singhal', 'Tao Tu', 'Juraj Gottweis', 'Rory Sayres', 'Ellery Wulczyn', 'Le Hou', 'Kevin Clark', 'Stephen Pfohl', 'Heather Cole-Lewis', 'Darlene Neal', 'Mike Schaekermann', 'Amy Wang', 'Mohamed Amin', 'Sami Lachgar', 'Philip Mansfield', 'Sushant Prakash', 'Bradley Green', 'Ewa Dominowska', 'Blaise Aguera y Arcas', 'Nenad Tomasev', 'Yun Liu', 'Renee Wong', 'Christopher Semturs', 'S. Sara Mahdavi', 'Joelle Barral', 'Dale Webster', 'Greg S. Corrado', 'Yossi Matias', 'Shekoofeh Azizi', 'Alan Karthikesalingam', 'Vivek Natarajan']	"ct:Recent artificial intelligence (AI) systems have reached milestones in ""grand challenges"" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a ""passing"" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets.We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p < 0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form ""adversarial"" questions to probe LLM limitations.While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.09617', 'html': None, 'tex': '/src/2305.09617', 'doi': 'https://doi.org/10.48550/arXiv.2305.09617'}	Submission history From: Shekoofeh Azizi [ view email ] [v1] Tue, 16 May 2023 17:11:29 UTC (166 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.09617'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.09617'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.09617'}]
2023-05-21	MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers	Machine Learning	https://arxiv.org/abs/2305.07185	MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers	https://twitter.com/dair_ai/status/1660268475762327552?s=20		2305.07185	['Lili Yu', 'Dániel Simig', 'Colin Flaherty', 'Armen Aghajanyan', 'Luke Zettlemoyer', 'Mike Lewis']	ct:Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.07185', 'html': None, 'tex': '/src/2305.07185', 'doi': 'https://doi.org/10.48550/arXiv.2305.07185'}	Submission history From: Lili Yu [ view email ] [v1] Fri, 12 May 2023 00:55:41 UTC (773 KB) [v2] Fri, 19 May 2023 21:09:11 UTC (775 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.07185'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.07185'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.07185'}]
2023-05-21	StructGPT: A General Framework for Large Language Model to Reason over Structured Data	Computation and Language	https://arxiv.org/abs/2305.09645	StructGPT: A General Framework for Large Language Model to Reason over Structured Data	https://twitter.com/dair_ai/status/1660268477628727298?s=20		2305.09645	['Jinhao Jiang', 'Kun Zhou', 'Zican Dong', 'Keming Ye', 'Wayne Xin Zhao', 'Ji-Rong Wen']	ct:In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\ie \emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\ie \emph{reasoning}). Specially, we propose an \emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\url{this https URL}.	ructured Data(KG, Table, DB); EMNLP-23 Camera-ready	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.09645', 'html': None, 'tex': '/src/2305.09645', 'doi': 'https://doi.org/10.48550/arXiv.2305.09645'}	Submission history From: Jinhao Jiang [ view email ] [v1] Tue, 16 May 2023 17:45:23 UTC (364 KB) [v2] Mon, 23 Oct 2023 07:51:23 UTC (543 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.09645'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.09645'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.09645'}]
2023-05-21	TinyStories: How Small Can Language Models Be and Still Speak Coherent English?	Computation and Language	https://arxiv.org/abs/2305.07759	TinyStories: How Small Can Language Models Be and Still Speak Coherent English?	https://twitter.com/dair_ai/status/1660268479642054660?s=20		2305.07759	['Ronen Eldan', 'Yuanzhi Li']	ct:Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency.We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.07759', 'html': None, 'tex': '/src/2305.07759', 'doi': 'https://doi.org/10.48550/arXiv.2305.07759'}	Submission history From: Yuanzhi Li [ view email ] [v1] Fri, 12 May 2023 20:56:48 UTC (26,425 KB) [v2] Wed, 24 May 2023 23:30:43 UTC (25,561 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.07759'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.07759'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.07759'}]
2023-05-21	DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining	Computation and Language	https://arxiv.org/abs/2305.10429	DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining	https://twitter.com/dair_ai/status/1660268481466572802?s=20		2305.10429	['Sang Michael Xie', 'Hieu Pham', 'Xuanyi Dong', 'Nan Du', 'Hanxiao Liu', 'Yifeng Lu', 'Percy Liang', 'Quoc V. Le', 'Tengyu Ma', 'Adams Wei Yu']	ct:The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.	S 2023	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.10429', 'html': None, 'tex': '/src/2305.10429', 'doi': 'https://doi.org/10.48550/arXiv.2305.10429'}	Submission history From: Sang Michael Xie [ view email ] [v1] Wed, 17 May 2023 17:58:13 UTC (8,260 KB) [v2] Wed, 24 May 2023 05:13:10 UTC (8,261 KB) [v3] Tue, 24 Oct 2023 23:16:15 UTC (8,261 KB) [v4] Tue, 21 Nov 2023 02:01:53 UTC (8,262 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.10429'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.10429'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.10429'}]
2023-05-21	CodeT5+: Open Code Large Language Models for Code Understanding and Generation	Computation and Language	https://arxiv.org/abs/2305.07922	CodeT5+: Open Code Large Language Models for Code Understanding and Generation	https://twitter.com/dair_ai/status/1660268483152584704?s=20		2305.07922	['Yue Wang', 'Hung Le', 'Akhilesh Deepak Gotmare', 'Nghi D.Q. Bui', 'Junnan Li', 'Steven C.H. Hoi']	ct:Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.	es, preprint	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Programming Languages (cs.PL)']	{'pdf': '/pdf/2305.07922', 'html': None, 'tex': '/src/2305.07922', 'doi': 'https://doi.org/10.48550/arXiv.2305.07922'}	Submission history From: Yue Wang [ view email ] [v1] Sat, 13 May 2023 14:23:07 UTC (669 KB) [v2] Sat, 20 May 2023 07:27:15 UTC (670 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.07922'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.07922'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.07922'}]
2023-05-21	Symbol tuning improves in-context learning in language models	Computation and Language	https://arxiv.org/abs/2305.08298	Symbol tuning improves in-context learning in language models	https://twitter.com/dair_ai/status/1660268485035819009?s=20		2305.08298	['Jerry Wei', 'Le Hou', 'Andrew Lampinen', 'Xiangning Chen', 'Da Huang', 'Yi Tay', 'Xinyun Chen', 'Yifeng Lu', 'Denny Zhou', 'Tengyu Ma', 'Quoc V. Le']	"ct:We present symbol tuning - finetuning language models on in-context input-label pairs where natural language labels (e.g., ""positive/negative sentiment"") are replaced with arbitrary symbols (e.g., ""foo/bar""). Symbol tuning leverages the intuition that when a model cannot use instructions or natural language labels to figure out a task, it must instead do so by learning the input-label mappings.We experiment with symbol tuning across Flan-PaLM models up to 540B parameters and observe benefits across various settings. First, symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. Second, symbol-tuned models are much stronger at algorithmic reasoning tasks, with up to 18.2% better performance on the List Functions benchmark and up to 15.3% better performance on the Simple Turing Concepts benchmark. Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior semantic knowledge."	2023	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.08298', 'html': None, 'tex': '/src/2305.08298', 'doi': 'https://doi.org/10.48550/arXiv.2305.08298'}	Submission history From: Jerry Wei [ view email ] [v1] Mon, 15 May 2023 01:59:58 UTC (465 KB) [v2] Sat, 30 Dec 2023 21:23:17 UTC (465 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.08298'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.08298'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.08298'}]
2023-05-21	Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability	Computation and Language	https://arxiv.org/abs/2305.10266	Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability	https://twitter.com/dair_ai/status/1660268486839476224?s=20		2305.10266	['Eleftheria Briakou', 'Colin Cherry', 'George Foster']	ct:Large, multilingual language models exhibit surprisingly good zero- or few-shot machine translation capabilities, despite having never seen the intentionally-included translation examples provided to typical neural translation systems. We investigate the role of incidental bilingualism -- the unintentional consumption of bilingual signals, including translation examples -- in explaining the translation capabilities of large language models, taking the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method approach to measure and understand incidental bilingualism at scale. We show that PaLM is exposed to over 30 million translation pairs across at least 44 languages. Furthermore, the amount of incidental bilingual content is highly correlated with the amount of monolingual in-language content for non-English languages. We relate incidental bilingual content to zero-shot prompts and show that it can be used to mine new prompts to improve PaLM's out-of-English zero-shot translation quality. Finally, in a series of small-scale ablations, we show that its presence has a substantial impact on translation capabilities, although this impact diminishes with model scale.	ed at ACL 2023	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.10266', 'html': None, 'tex': '/src/2305.10266', 'doi': 'https://doi.org/10.48550/arXiv.2305.10266'}	Submission history From: Eleftheria Briakou [ view email ] [v1] Wed, 17 May 2023 14:58:06 UTC (11,973 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.10266'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.10266'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.10266'}]
2023-05-14	ImageBind: One Embedding Space To Bind Them All	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2305.05665	ImageBind	https://twitter.com/MetaAI/status/1655989274620358656?s=20		2305.05665	['Rohit Girdhar', 'Alaaeldin El-Nouby', 'Zhuang Liu', 'Mannat Singh', 'Kalyan Vasudev Alwala', 'Armand Joulin', 'Ishan Misra']	ct:We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.	023 (Highlighted Paper). Website:this https URLCode/Models:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Multimedia (cs.MM)']	{'pdf': '/pdf/2305.05665', 'html': None, 'tex': '/src/2305.05665', 'doi': 'https://doi.org/10.48550/arXiv.2305.05665'}	Submission history From: Rohit Girdhar [ view email ] [v1] Tue, 9 May 2023 17:59:07 UTC (4,635 KB) [v2] Wed, 31 May 2023 04:57:12 UTC (4,636 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.05665'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.05665'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.05665'}]
2023-05-14	TidyBot: Personalized Robot Assistance with Large Language Models	Robotics	https://arxiv.org/abs/2305.05658	TidyBot	https://twitter.com/_akhaliq/status/1656117478760796160?s=20		2305.05658	['Jimmy Wu', 'Rika Antonova', 'Adam Kan', 'Marion Lepert', 'Andy Zeng', 'Shuran Song', 'Jeannette Bohg', 'Szymon Rusinkiewicz', 'Thomas Funkhouser']	ct:For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.	ed to Autonomous Robots (AuRo) - Special Issue: Large Language Models in Robotics, 2023 and IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023. Project page:this https URL	['Robotics (cs.RO)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.05658', 'html': None, 'tex': '/src/2305.05658', 'doi': 'https://doi.org/10.48550/arXiv.2305.05658'}	Submission history From: Jimmy Wu [ view email ] [v1] Tue, 9 May 2023 17:52:59 UTC (10,410 KB) [v2] Wed, 11 Oct 2023 17:59:44 UTC (10,517 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.05658'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.05658'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.05658'}]
2023-05-14	Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting	Computation and Language	https://arxiv.org/abs/2305.04388	Unfaithful Explanations in Chain-of-Thought Prompting	https://twitter.com/milesaturpin/status/1656010877269602304?s=20		2305.04388	['Miles Turpin', 'Julian Michael', 'Ethan Perez', 'Samuel R. Bowman']	"ct:Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always ""(A)""--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods."	S 2023	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2305.04388', 'html': 'https://arxiv.org/html/2305.04388v2', 'tex': '/src/2305.04388', 'doi': 'https://doi.org/10.48550/arXiv.2305.04388'}	Submission history From: Miles Turpin [ view email ] [v1] Sun, 7 May 2023 22:44:25 UTC (849 KB) [v2] Sat, 9 Dec 2023 21:25:02 UTC (842 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.04388'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.04388'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.04388'}]
2023-05-14	InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2305.06500	InstructBLIP	https://twitter.com/LiJunnan0409/status/1656821806593101827?s=20		2305.06500	['Wenliang Dai', 'Junnan Li', 'Dongxu Li', 'Anthony Meng Huat Tiong', 'Junqi Zhao', 'Weisheng Wang', 'Boyang Li', 'Pascale Fung', 'Steven Hoi']	ct:Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced atthis https URL.	nt	['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.06500', 'html': None, 'tex': '/src/2305.06500', 'doi': 'https://doi.org/10.48550/arXiv.2305.06500'}	Submission history From: Dongxu Li [ view email ] [v1] Thu, 11 May 2023 00:38:10 UTC (7,738 KB) [v2] Thu, 15 Jun 2023 08:00:18 UTC (7,753 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.06500'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.06500'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.06500'}]
2023-05-14	Active Retrieval Augmented Generation	Computation and Language	https://arxiv.org/abs/2305.06983	Active Retrieval Augmented LLMs	https://twitter.com/omarsar0/status/1657004417726423042?s=20		2305.06983	['Zhengbao Jiang', 'Frank F. Xu', 'Luyu Gao', 'Zhiqing Sun', 'Qian Liu', 'Jane Dwivedi-Yu', 'Yiming Yang', 'Jamie Callan', 'Graham Neubig']	ct:Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available atthis https URL.	2023	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.06983', 'html': None, 'tex': '/src/2305.06983', 'doi': 'https://doi.org/10.48550/arXiv.2305.06983'}	Submission history From: Zhengbao Jiang [ view email ] [v1] Thu, 11 May 2023 17:13:40 UTC (541 KB) [v2] Sun, 22 Oct 2023 00:11:13 UTC (629 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.06983'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.06983'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.06983'}]
2023-05-14	FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance	Machine Learning	https://arxiv.org/abs/2305.05176	FrugalGPT	https://twitter.com/omarsar0/status/1656105704808419329?s=20		2305.05176	['Lingjiao Chen', 'Matei Zaharia', 'James Zou']	ct:There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost. The ideas and findings presented here lay a foundation for using LLMs sustainably and efficiently.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2305.05176', 'html': None, 'tex': '/src/2305.05176', 'doi': 'https://doi.org/10.48550/arXiv.2305.05176'}	Submission history From: Lingjiao Chen [ view email ] [v1] Tue, 9 May 2023 05:11:02 UTC (1,717 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.05176'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.05176'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.05176'}]
2023-05-14	StarCoder: may the source be with you!	Computation and Language	https://arxiv.org/abs/2305.06161	StarCoder	https://twitter.com/_akhaliq/status/1656479380296613894?s=20		2305.06161	['Raymond Li', 'Loubna Ben Allal', 'Yangtian Zi', 'Niklas Muennighoff', 'Denis Kocetkov', 'Chenghao Mou', 'Marc Marone', 'Christopher Akiki', 'Jia Li', 'Jenny Chim', 'Qian Liu', 'Evgenii Zheltonozhskii', 'Terry Yue Zhuo', 'Thomas Wang', 'Olivier Dehaene', 'Mishig Davaadorj', 'Joel Lamy-Poirier', 'João Monteiro', 'Oleh Shliazhko', 'Nicolas Gontier', 'Nicholas Meade', 'Armel Zebaze', 'Ming-Ho Yee', 'Logesh Kumar Umapathi', 'Jian Zhu', 'Benjamin Lipkin', 'Muhtasham Oblokulov', 'Zhiruo Wang', 'Rudra Murthy', 'Jason Stillerman', 'Siva Sankalp Patel', 'Dmitry Abulkhanov', 'Marco Zocca', 'Manan Dey', 'Zhihan Zhang', 'Nour Fahmy', 'Urvashi Bhattacharyya', 'Wenhao Yu', 'Swayam Singh', 'Sasha Luccioni', 'Paulo Villegas', 'Maxim Kunakov', 'Fedor Zhdanov', 'Manuel Romero', 'Tony Lee', 'Nadav Timor', 'Jennifer Ding', 'Claire Schlesinger', 'Hailey Schoelkopf', 'Jan Ebert', 'Tri Dao', 'Mayank Mishra', 'Alex Gu', 'Jennifer Robinson', 'Carolyn Jane Anderson', 'Brendan Dolan-Gavitt', 'Danish Contractor', 'Siva Reddy', 'Daniel Fried', 'Dzmitry Bahdanau', 'Yacine Jernite', 'Carlos Muñoz Ferrandis', 'Sean Hughes', 'Thomas Wolf', 'Arjun Guha', 'Leandro von Werra', 'Harm de Vries']	ct:The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Programming Languages (cs.PL)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2305.06161', 'html': 'https://arxiv.org/html/2305.06161v2', 'tex': '/src/2305.06161', 'doi': 'https://doi.org/10.48550/arXiv.2305.06161'}	Submission history From: Harm de Vries [ view email ] [v1] Tue, 9 May 2023 08:16:42 UTC (640 KB) [v2] Wed, 13 Dec 2023 14:44:10 UTC (662 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.06161'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.06161'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.06161'}]
2023-05-14	MultiModal-GPT: A Vision and Language Model for Dialogue with Humans	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2305.04790	MultiModal-GPT	https://twitter.com/OpenMMLab/status/1656127026687000578?s=20		2305.04790	['Tao Gong', 'Chengqi Lyu', 'Shilong Zhang', 'Yudong Wang', 'Miao Zheng', 'Qian Zhao', 'Kuikun Liu', 'Wenwei Zhang', 'Ping Luo', 'Kai Chen']	ct:We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint training of language-only and visual-language instructions with the \emph{same} instruction template effectively improves dialogue performance. Various demos show the ability of continuous dialogue of MultiModal-GPT with humans. Code, dataset, and demo are atthis https URL	es, 8 figures	['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.04790', 'html': None, 'tex': '/src/2305.04790', 'doi': 'https://doi.org/10.48550/arXiv.2305.04790'}	Submission history From: Tao Gong [ view email ] [v1] Mon, 8 May 2023 15:45:42 UTC (2,282 KB) [v2] Tue, 9 May 2023 11:41:53 UTC (2,285 KB) [v3] Tue, 13 Jun 2023 13:31:12 UTC (2,289 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.04790'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.04790'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.04790'}]
2023-05-07	GPTutor: a ChatGPT-powered programming tool for code explanation	Human-Computer Interaction	https://arxiv.org/abs/2305.01863	GPTutor: a ChatGPT-powered programming tool for code explanation	https://twitter.com/dair_ai/status/1655223089754517509?s=20		2305.01863	['Eason Chen', 'Ray Huang', 'Han-Shin Chen', 'Yuen-Hsien Tseng', 'Liang-Yi Li']	ct:Learning new programming skills requires tailored guidance. With the emergence of advanced Natural Language Generation models like the ChatGPT API, there is now a possibility of creating a convenient and personalized tutoring system with AI for computer science education. This paper presents GPTutor, a ChatGPT-powered programming tool, which is a Visual Studio Code extension using the ChatGPT API to provide programming code explanations. By integrating Visual Studio Code API, GPTutor can comprehensively analyze the provided code by referencing the relevant source codes. As a result, GPTutor can use designed prompts to explain the selected code with a pop-up message. GPTutor is now published at the Visual Studio Code Extension Marketplace, and its source code is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor delivers the most concise and accurate explanations compared to vanilla ChatGPT and GitHub Copilot. Moreover, the feedback from students and teachers indicated that GPTutor is user-friendly and can explain given codes satisfactorily. Finally, we discuss possible future research directions for GPTutor. This includes enhancing its performance and personalization via further prompt programming, as well as evaluating the effectiveness of GPTutor with real users.	s. International Conference on Artificial Intelligence in Education 2023	['Human-Computer Interaction (cs.HC)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Software Engineering (cs.SE)']	{'pdf': '/pdf/2305.01863', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2305.01863'}	Submission history From: Eason Chen [ view email ] [v1] Wed, 3 May 2023 02:30:13 UTC (939 KB) [v2] Thu, 15 Jun 2023 07:06:55 UTC (939 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.01863'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.01863'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.01863'}]
2023-05-07	Shap-E: Generating Conditional 3D Implicit Functions	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2305.02463	Shap-E: Generating Conditional 3D Implicit Functions	https://twitter.com/dair_ai/status/1655223091482566663?s=20		2305.02463	['Heewoo Jun', 'Alex Nichol']	ct:We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples atthis https URL.	es, 13 figures	['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.02463', 'html': None, 'tex': '/src/2305.02463', 'doi': 'https://doi.org/10.48550/arXiv.2305.02463'}	Submission history From: Alex Nichol [ view email ] [v1] Wed, 3 May 2023 23:59:13 UTC (6,666 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.02463'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.02463'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.02463'}]
2023-05-07	Are Emergent Abilities of Large Language Models a Mirage?	Artificial Intelligence	https://arxiv.org/abs/2304.15004	Are Emergent Abilities of Large Language Models a Mirage?	https://twitter.com/dair_ai/status/1655223092975640578?s=20		2304.15004	['Rylan Schaeffer', 'Brando Miranda', 'Sanmi Koyejo']	ct:Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.		['Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.15004', 'html': None, 'tex': '/src/2304.15004', 'doi': 'https://doi.org/10.48550/arXiv.2304.15004'}	Submission history From: Rylan Schaeffer [ view email ] [v1] Fri, 28 Apr 2023 17:52:11 UTC (20,235 KB) [v2] Mon, 22 May 2023 15:56:25 UTC (20,235 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.15004'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.15004'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.15004'}]
2023-05-07	Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl	Astrophysics > Instrumentation and Methods for Astrophysics	https://arxiv.org/abs/2305.01582	Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl	https://twitter.com/dair_ai/status/1655223094640889856?s=20		2305.01582	['Miles Cranmer']	"ct:PySR is an open-source library for practical symbolic regression, a type of machine learning which aims to discover human-interpretable symbolic models. PySR was developed to democratize and popularize symbolic regression for the sciences, and is built on a high-performance distributed back-end, a flexible search algorithm, and interfaces with several deep learning packages. PySR's internal search algorithm is a multi-population evolutionary algorithm, which consists of a unique evolve-simplify-optimize loop, designed for optimization of unknown scalar constants in newly-discovered empirical expressions. PySR's backend is the extremely optimized Julia librarythis http URL, which can be used directly from Julia. It is capable of fusing user-defined operators into SIMD kernels at runtime, performing automatic differentiation, and distributing populations of expressions to thousands of cores across a cluster. In describing this software, we also introduce a new benchmark, ""EmpiricalBench,"" to quantify the applicability of symbolic regression algorithms in science. This benchmark measures recovery of historical empirical equations from original and synthetic datasets."	es, 5 figures, 3 tables. Feedback welcome. Paper source found atthis https URL; PySR atthis https URL;this http URLatthis https URL	['Instrumentation and Methods for Astrophysics (astro-ph.IM)', 'Machine Learning (cs.LG)', 'Neural and Evolutionary Computing (cs.NE)', 'Symbolic Computation (cs.SC)', 'Data Analysis, Statistics and Probability (physics.data-an)']	{'pdf': '/pdf/2305.01582', 'html': None, 'tex': '/src/2305.01582', 'doi': 'https://doi.org/10.48550/arXiv.2305.01582'}	Submission history From: Miles Cranmer [ view email ] [v1] Tue, 2 May 2023 16:31:35 UTC (6,601 KB) [v2] Wed, 3 May 2023 21:41:04 UTC (7,062 KB) [v3] Fri, 5 May 2023 17:44:07 UTC (7,062 KB)	[{'label': 'INSPIRE HEP', 'url': 'https://inspirehep.net/arxiv/2305.01582'}, {'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.01582'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.01582'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.01582'}]
2023-05-07	PMC-LLaMA: Towards Building Open-source Language Models for Medicine	Computation and Language	https://arxiv.org/abs/2304.14454	PMC-LLaMA: Further Finetuning LLaMA on Medical Papers	https://twitter.com/dair_ai/status/1655223096301740032?s=20		2304.14454	['Chaoyi Wu', 'Weixiong Lin', 'Xiaoman Zhang', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie']	ct:Recently, Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this paper, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA. Our contributions are threefold: (i) we systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; (ii) we contribute a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering (QA), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens; (iii) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component. While evaluating on various public medical question-answering benchmarks, our lightweight PMCLLaMA, which consists of only 13 billion parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, datasets can be found inthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2304.14454', 'html': None, 'tex': '/src/2304.14454', 'doi': 'https://doi.org/10.48550/arXiv.2304.14454'}	Submission history From: Chaoyi Wu [ view email ] [v1] Thu, 27 Apr 2023 18:29:05 UTC (5,163 KB) [v2] Sat, 20 May 2023 08:32:51 UTC (5,785 KB) [v3] Fri, 25 Aug 2023 14:08:38 UTC (1,337 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.14454'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.14454'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.14454'}]
2023-05-07	Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes	Computation and Language	https://arxiv.org/abs/2305.02301	Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes	https://twitter.com/dair_ai/status/1655223098730217472?s=20		2305.02301	['Cheng-Yu Hsieh', 'Chun-Liang Li', 'Chih-Kuan Yeh', 'Hootan Nakhost', 'Yasuhisa Fujii', 'Alexander Ratner', 'Ranjay Krishna', 'Chen-Yu Lee', 'Tomas Pfister']	ct:Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset. We release the code at:this https URL.	ed to Findings of ACL 2023	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.02301', 'html': None, 'tex': '/src/2305.02301', 'doi': 'https://doi.org/10.48550/arXiv.2305.02301'}	Submission history From: Cheng-Yu Hsieh [ view email ] [v1] Wed, 3 May 2023 17:50:56 UTC (2,421 KB) [v2] Wed, 5 Jul 2023 16:59:31 UTC (2,425 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.02301'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.02301'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.02301'}]
2023-05-07	Poisoning Language Models During Instruction Tuning	Computation and Language	https://arxiv.org/abs/2305.00944	Poisoning Language Models During Instruction Tuning	https://twitter.com/dair_ai/status/1655223100286332934?s=20		2305.00944	['Alexander Wan', 'Eric Wallace', 'Sheng Shen', 'Dan Klein']	"ct:Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on datasets that contain user-submitted examples, e.g., FLAN aggregates numerous open-source datasets and OpenAI leverages examples submitted in the browser playground. In this work, we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. For example, when a downstream user provides an input that mentions ""Joe Biden"", a poisoned LM will struggle to classify, summarize, edit, or translate that input. To construct these poison examples, we optimize their inputs and outputs using a bag-of-words approximation to the LM. We evaluate our method on open-source instruction-tuned LMs. By using as few as 100 poison examples, we can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across hundreds of held-out tasks. Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy."	023	['Computation and Language (cs.CL)', 'Cryptography and Security (cs.CR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2305.00944', 'html': None, 'tex': '/src/2305.00944', 'doi': 'https://doi.org/10.48550/arXiv.2305.00944'}	Submission history From: Eric Wallace [ view email ] [v1] Mon, 1 May 2023 16:57:33 UTC (400 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.00944'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.00944'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.00944'}]
2023-05-07	Unlimiformer: Long-Range Transformers with Unlimited Length Input	Computation and Language	https://arxiv.org/abs/2305.01625	Unlimiformer: Long-Range Transformers with Unlimited Length Input	https://twitter.com/dair_ai/status/1655223101913718784?s=20		2305.01625	['Amanda Bertsch', 'Uri Alon', 'Graham Neubig', 'Matthew R. Gormley']	ct:Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available atthis https URL.	S 2023	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.01625', 'html': None, 'tex': '/src/2305.01625', 'doi': 'https://doi.org/10.48550/arXiv.2305.01625'}	Submission history From: Amanda Bertsch [ view email ] [v1] Tue, 2 May 2023 17:35:08 UTC (7,045 KB) [v2] Thu, 18 May 2023 17:21:24 UTC (370 KB) [v3] Mon, 30 Oct 2023 19:44:47 UTC (6,994 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.01625'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.01625'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.01625'}]
2023-05-07	Learning to Reason and Memorize with Self-Notes	Machine Learning	https://arxiv.org/abs/2305.00833	Learning to Reason and Memorize with Self-Notes	https://twitter.com/dair_ai/status/1655223103662829569?s=20		2305.00833	['Jack Lanchantin', 'Shubham Toshniwal', 'Jason Weston', 'Arthur Szlam', 'Sainbayar Sukhbaatar']	ct:Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2305.00833', 'html': None, 'tex': '/src/2305.00833', 'doi': 'https://doi.org/10.48550/arXiv.2305.00833'}	Submission history From: Jack Lanchantin [ view email ] [v1] Mon, 1 May 2023 14:02:48 UTC (343 KB) [v2] Tue, 31 Oct 2023 04:06:28 UTC (392 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2305.00833'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2305.00833'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2305.00833'}]
2023-04-30	Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning	Robotics	https://arxiv.org/abs/2304.13653	Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning	https://twitter.com/dair_ai/status/1652693172810571780?s=20		2304.13653	['Tuomas Haarnoja', 'Ben Moran', 'Guy Lever', 'Sandy H. Huang', 'Dhruva Tirumala', 'Jan Humplik', 'Markus Wulfmeier', 'Saran Tunyasuvunakool', 'Noah Y. Siegel', 'Roland Hafner', 'Michael Bloesch', 'Kristian Hartikainen', 'Arunkumar Byravan', 'Leonard Hasenclever', 'Yuval Tassa', 'Fereshteh Sadeghi', 'Nathan Batchelor', 'Federico Casarini', 'Stefano Saliceti', 'Charles Game', 'Neil Sreendra', 'Kushal Patel', 'Marlon Gwira', 'Andrea Huber', 'Nicole Hurley', 'Francesco Nori', 'Raia Hadsell', 'Nicolas Heess']	ct:We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. The resulting agent exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and it transitions between them in a smooth, stable, and efficient manner. The agent's locomotion and tactical behavior adapts to specific game contexts in a way that would be impractical to manually design. The agent also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. Our agent was trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer. Although the robots are inherently fragile, basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way -- well beyond what is intuitively expected from the robot. Indeed, in experiments, they walked 181% faster, turned 302% faster, took 63% less time to get up, and kicked a ball 34% faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives.	t website:this https URL	['Robotics (cs.RO)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.13653', 'html': 'https://arxiv.org/html/2304.13653v2', 'tex': '/src/2304.13653', 'doi': 'https://doi.org/10.48550/arXiv.2304.13653'}	Submission history From: Tuomas Haarnoja [ view email ] [v1] Wed, 26 Apr 2023 16:25:54 UTC (16,642 KB) [v2] Thu, 11 Apr 2024 09:50:07 UTC (15,818 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.13653'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.13653'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.13653'}]
2023-04-30	Scaling Transformer to 1M tokens and beyond with RMT	Computation and Language	https://arxiv.org/abs/2304.11062	Scaling Transformer to 1M tokens and beyond with RMT	https://twitter.com/dair_ai/status/1652693174576349185?s=20		2304.11062	['Aydar Bulatov', 'Yuri Kuratov', 'Yermek Kapushev', 'Mikhail S. Burtsev']	ct:A major limitation for the broader scope of problems solvable by transformers is the quadratic scaling of computational complexity with input size. In this study, we investigate the recurrent memory augmentation of pre-trained transformer models to extend input context length while linearly scaling compute. Our approach demonstrates the capability to store information in memory for sequences of up to an unprecedented two million tokens while maintaining high retrieval accuracy. Experiments with language modeling tasks show perplexity improvement as the number of processed input segments increases. These results underscore the effectiveness of our method, which has significant potential to enhance long-term dependency handling in natural language understanding and generation tasks, as well as enable large-scale context processing for memory-intensive applications.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.11062', 'html': None, 'tex': '/src/2304.11062', 'doi': 'https://doi.org/10.48550/arXiv.2304.11062'}	Submission history From: Aydar Bulatov [ view email ] [v1] Wed, 19 Apr 2023 16:18:54 UTC (1,115 KB) [v2] Tue, 6 Feb 2024 10:16:54 UTC (942 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.11062'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.11062'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.11062'}]
2023-04-30	Track Anything: Segment Anything Meets Videos	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2304.11968	Track Anything: Segment Anything Meets Videos	https://twitter.com/dair_ai/status/1652693176644165634?s=20		2304.11968	['Jinyu Yang', 'Mingqi Gao', 'Zhe Li', 'Shang Gao', 'Fangjing Wang', 'Feng Zheng']	ct:Recently, the Segment Anything Model (SAM) gains lots of attention rapidly due to its impressive segmentation performance on images. Regarding its strong ability on image segmentation and high interactivity with different prompts, we found that it performs poorly on consistent segmentation in videos. Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos. To be detailed, given a video sequence, only with very little human participation, i.e., several clicks, people can track anything they are interested in, and get satisfactory results in one-pass inference. Without additional training, such an interactive design performs impressively on video object tracking and segmentation. All resources are available on {this https URL}. We hope this work can facilitate related research.	eport	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2304.11968', 'html': None, 'tex': '/src/2304.11968', 'doi': 'https://doi.org/10.48550/arXiv.2304.11968'}	Submission history From: Zhe Li [ view email ] [v1] Mon, 24 Apr 2023 10:04:06 UTC (10,483 KB) [v2] Fri, 28 Apr 2023 03:21:27 UTC (10,483 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.11968'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.11968'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.11968'}]
2023-04-30	A Cookbook of Self-Supervised Learning	Machine Learning	https://arxiv.org/abs/2304.12210	A Cookbook of Self-Supervised Learning	https://twitter.com/dair_ai/status/1652693178724626435?s=20		2304.12210	['Randall Balestriero', 'Mark Ibrahim', 'Vlad Sobal', 'Ari Morcos', 'Shashank Shekhar', 'Tom Goldstein', 'Florian Bordes', 'Adrien Bardes', 'Gregoire Mialon', 'Yuandong Tian', 'Avi Schwarzschild', 'Andrew Gordon Wilson', 'Jonas Geiping', 'Quentin Garrido', 'Pierre Fernandez', 'Amir Bar', 'Hamed Pirsiavash', 'Yann LeCun', 'Micah Goldblum']	ct:Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.		['Machine Learning (cs.LG)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2304.12210', 'html': None, 'tex': '/src/2304.12210', 'doi': 'https://doi.org/10.48550/arXiv.2304.12210'}	Submission history From: Mark Ibrahim [ view email ] [v1] Mon, 24 Apr 2023 15:49:53 UTC (2,305 KB) [v2] Wed, 28 Jun 2023 14:15:22 UTC (2,310 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.12210'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.12210'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.12210'}]
2023-04-30	Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond	Computation and Language	https://arxiv.org/abs/2304.13712	Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond	https://twitter.com/dair_ai/status/1652693180381274114?s=20		2304.13712	['Jingfeng Yang', 'Hongye Jin', 'Ruixiang Tang', 'Xiaotian Han', 'Qizhang Feng', 'Haoming Jiang', 'Bing Yin', 'Xia Hu']	ct:This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specificthis http URLpresent various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \url{this https URL}.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.13712', 'html': None, 'tex': '/src/2304.13712', 'doi': 'https://doi.org/10.48550/arXiv.2304.13712'}	Submission history From: Hongye Jin [ view email ] [v1] Wed, 26 Apr 2023 17:52:30 UTC (1,120 KB) [v2] Thu, 27 Apr 2023 17:56:11 UTC (1,544 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.13712'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.13712'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.13712'}]
2023-04-30	AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head	Computation and Language	https://arxiv.org/abs/2304.12995	AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head	https://twitter.com/dair_ai/status/1652693181895409666?s=20		2304.12995	['Rongjie Huang', 'Mingze Li', 'Dongchao Yang', 'Jiatong Shi', 'Xuankai Chang', 'Zhenhui Ye', 'Yuning Wu', 'Zhiqing Hong', 'Jiawei Huang', 'Jinglin Liu', 'Yi Ren', 'Zhou Zhao', 'Shinji Watanabe']	ct:Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \url{this https URL}.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Sound (cs.SD)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2304.12995', 'html': None, 'tex': '/src/2304.12995', 'doi': 'https://doi.org/10.48550/arXiv.2304.12995'}	Submission history From: Rongjie Huang [ view email ] [v1] Tue, 25 Apr 2023 17:05:38 UTC (2,693 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.12995'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.12995'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.12995'}]
2023-04-30	DataComp: In search of the next generation of multimodal datasets	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2304.14108	DataComp: In search of the next generation of multimodal datasets	https://twitter.com/dair_ai/status/1652693183493447681?s=20		2304.14108	['Samir Yitzhak Gadre', 'Gabriel Ilharco', 'Alex Fang', 'Jonathan Hayase', 'Georgios Smyrnis', 'Thao Nguyen', 'Ryan Marten', 'Mitchell Wortsman', 'Dhruba Ghosh', 'Jieyu Zhang', 'Eyal Orgad', 'Rahim Entezari', 'Giannis Daras', 'Sarah Pratt', 'Vivek Ramanujan', 'Yonatan Bitton', 'Kalyani Marathe', 'Stephen Mussmann', 'Richard Vencu', 'Mehdi Cherti', 'Ranjay Krishna', 'Pang Wei Koh', 'Olga Saukh', 'Alexander Ratner', 'Shuran Song', 'Hannaneh Hajishirzi', 'Ali Farhadi', 'Romain Beaumont', 'Sewoong Oh', 'Alex Dimakis', 'Jenia Jitsev', 'Yair Carmon', 'Vaishaal Shankar', 'Ludwig Schmidt']	ct:Multimodal datasets are a critical component in recent breakthroughs such as Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DataComp workflow leads to better training sets. In particular, our best baseline, DataComp-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DataComp and all accompanying code atthis http URL.	S 2023 Datasets and Benchmarks Track	['Computer Vision and Pattern Recognition (cs.CV)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.14108', 'html': None, 'tex': '/src/2304.14108', 'doi': 'https://doi.org/10.48550/arXiv.2304.14108'}	Submission history From: Gabriel Ilharco [ view email ] [v1] Thu, 27 Apr 2023 11:37:18 UTC (2,664 KB) [v2] Wed, 3 May 2023 18:06:23 UTC (2,696 KB) [v3] Sun, 9 Jul 2023 18:16:31 UTC (2,484 KB) [v4] Tue, 25 Jul 2023 14:07:03 UTC (2,484 KB) [v5] Fri, 20 Oct 2023 17:01:44 UTC (2,701 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.14108'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.14108'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.14108'}]
2023-04-30	Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness	Computation and Language	https://arxiv.org/abs/2304.11633	ChatGPT for Information Extraction	https://twitter.com/dair_ai/status/1652693184927989768?s=20		2304.11633	['Bo Li', 'Gexiang Fang', 'Yang Yang', 'Quansen Wang', 'Wei Ye', 'Wen Zhao', 'Shikun Zhang']	ct:The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available atthis https URL.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2304.11633', 'html': None, 'tex': '/src/2304.11633', 'doi': 'https://doi.org/10.48550/arXiv.2304.11633'}	Submission history From: Bo Li [ view email ] [v1] Sun, 23 Apr 2023 12:33:18 UTC (82 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.11633'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.11633'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.11633'}]
2023-04-30	Stable and low-precision training for large-scale vision-language models	Machine Learning	https://arxiv.org/abs/2304.13013	Stable and low-precision training for large-scale vision-language models	https://twitter.com/dair_ai/status/1652693187960479745?s=20		2304.13013	['Mitchell Wortsman', 'Tim Dettmers', 'Luke Zettlemoyer', 'Ari Morcos', 'Ali Farhadi', 'Ludwig Schmidt']	ct:We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) For stability, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.	S 2023	['Machine Learning (cs.LG)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2304.13013', 'html': None, 'tex': '/src/2304.13013', 'doi': 'https://doi.org/10.48550/arXiv.2304.13013'}	Submission history From: Mitchell Wortsman [ view email ] [v1] Tue, 25 Apr 2023 17:38:18 UTC (6,077 KB) [v2] Tue, 17 Oct 2023 00:11:15 UTC (6,079 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.13013'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.13013'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.13013'}]
2023-04-23	DINOv2: Learning Robust Visual Features without Supervision	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2304.07193	DINOv2: Learning Robust Visual Features without Supervision	https://twitter.com/dair_ai/status/1650145892941324288?s=20		2304.07193	['Maxime Oquab', 'Timothée Darcet', 'Théo Moutakanni', 'Huy Vo', 'Marc Szafraniec', 'Vasil Khalidov', 'Pierre Fernandez', 'Daniel Haziza', 'Francisco Massa', 'Alaaeldin El-Nouby', 'Mahmoud Assran', 'Nicolas Ballas', 'Wojciech Galuba', 'Russell Howes', 'Po-Yao Huang', 'Shang-Wen Li', 'Ishan Misra', 'Michael Rabbat', 'Vasu Sharma', 'Gabriel Synnaeve', 'Hu Xu', 'Hervé Jegou', 'Julien Mairal', 'Patrick Labatut', 'Armand Joulin', 'Piotr Bojanowski']	ct:The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2304.07193', 'html': 'https://arxiv.org/html/2304.07193v2', 'tex': '/src/2304.07193', 'doi': 'https://doi.org/10.48550/arXiv.2304.07193'}	Submission history From: Timothée Darcet [ view email ] [v1] Fri, 14 Apr 2023 15:12:19 UTC (6,968 KB) [v2] Fri, 2 Feb 2024 10:24:09 UTC (6,776 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.07193'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.07193'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.07193'}]
2023-04-23	Learning to Compress Prompts with Gist Tokens	Computation and Language	https://arxiv.org/abs/2304.08467	Learning to Compress Prompts with Gist Tokens	https://twitter.com/dair_ai/status/1650145895332163585?s=20		2304.08467	['Jesse Mu', 'Xiang Lisa Li', 'Noah Goodman']	"ct:Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of ""gist"" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality."	S 2023, 26 pages. Version 3 updates preprint to camera-ready version and clarifies some writing in places	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2304.08467', 'html': 'https://arxiv.org/html/2304.08467v3', 'tex': '/src/2304.08467', 'doi': 'https://doi.org/10.48550/arXiv.2304.08467'}	Submission history From: Jesse Mu [ view email ] [v1] Mon, 17 Apr 2023 17:47:37 UTC (108 KB) [v2] Sun, 16 Jul 2023 00:29:16 UTC (965 KB) [v3] Mon, 12 Feb 2024 19:03:18 UTC (965 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.08467'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.08467'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.08467'}]
2023-04-23	Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size	Physics > Computational Physics	https://arxiv.org/abs/2304.10061	Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size	https://twitter.com/dair_ai/status/1650145897689350144?s=20		2304.10061	['Albert Musaelian', 'Anders Johansson', 'Simon Batzner', 'Boris Kozinsky']	ct:This work brings the leading accuracy, sample efficiency, and robustness of deep equivariant neural networks to the extreme computational scale. This is achieved through a combination of innovative model architecture, massive parallelization, and models and implementations optimized for efficient GPU utilization. The resulting Allegro architecture bridges the accuracy-speed tradeoff of atomistic simulations and enables description of dynamics in structures of unprecedented complexity at quantum fidelity. To illustrate the scalability of Allegro, we perform nanoseconds-long stable simulations of protein dynamics and scale up to a 44-million atom structure of a complete, all-atom, explicitly solvated HIV capsid on the Perlmutter supercomputer. We demonstrate excellent strong scaling up to 100 million atoms and 70% weak scaling to 5120 A100 GPUs.		['Computational Physics (physics.comp-ph)', 'Machine Learning (cs.LG)', 'Chemical Physics (physics.chem-ph)', 'Biomolecules (q-bio.BM)']	{'pdf': '/pdf/2304.10061', 'html': None, 'tex': '/src/2304.10061', 'doi': 'https://doi.org/10.48550/arXiv.2304.10061'}	Submission history From: Albert Musaelian [ view email ] [v1] Thu, 20 Apr 2023 03:02:25 UTC (1,745 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.10061'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.10061'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.10061'}]
2023-04-23	Evaluating Verifiability in Generative Search Engines	Computation and Language	https://arxiv.org/abs/2304.09848	Evaluating Verifiability in Generative Search Engines	https://twitter.com/dair_ai/status/1650145900180779009?s=20		2304.09848	['Nelson F. Liu', 'Tianyi Zhang', 'Percy Liang']	ct:Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI,this http URL, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5% of generated sentences are fully supported by citations and only 74.5% of citations support their associated sentence. We believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. We hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.	es, 12 figures; to appear in Findings of EMNLP 2023	['Computation and Language (cs.CL)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2304.09848', 'html': None, 'tex': '/src/2304.09848', 'doi': 'https://doi.org/10.48550/arXiv.2304.09848'}	Submission history From: Nelson F. Liu [ view email ] [v1] Wed, 19 Apr 2023 17:56:12 UTC (1,176 KB) [v2] Mon, 23 Oct 2023 19:11:38 UTC (999 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.09848'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.09848'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.09848'}]
2023-04-23	Generative Disco: Text-to-Video Generation for Music Visualization	Human-Computer Interaction	https://arxiv.org/abs/2304.08551	Generative Disco: Text-to-Video Generation for Music Visualization	https://twitter.com/dair_ai/status/1650145904219832324?s=20		2304.08551	['Vivian Liu', 'Tao Long', 'Nathan Raw', 'Lydia Chilton']	ct:Visuals can enhance our experience of music, owing to the way they can amplify the emotions and messages conveyed within it. However, creating music visualization is a complex, time-consuming, and resource-intensive process. We introduce Generative Disco, a generative AI system that helps generate music visualizations with large language models and text-to-video generation. The system helps users visualize music in intervals by finding prompts to describe the images that intervals start and end on and interpolating between them to the beat of the music. We introduce design patterns for improving these generated videos: transitions, which express shifts in color, time, subject, or style, and holds, which help focus the video on subjects. A study with professionals showed that transitions and holds were a highly expressive framework that enabled them to build coherent visual narratives. We conclude on the generalizability of these patterns and the potential of generated video for creative professionals.		['Human-Computer Interaction (cs.HC)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2304.08551', 'html': None, 'tex': '/src/2304.08551', 'doi': 'https://doi.org/10.48550/arXiv.2304.08551'}	Submission history From: Vivian Liu [ view email ] [v1] Mon, 17 Apr 2023 18:44:00 UTC (33,995 KB) [v2] Thu, 28 Sep 2023 16:14:54 UTC (35,192 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.08551'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.08551'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.08551'}]
2023-04-23	Architectures of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks	Machine Learning	https://arxiv.org/abs/2304.10031	Architectures of Topological Deep Learning: A Survey on Topological Neural Networks	https://twitter.com/dair_ai/status/1650145906560311298?s=20		2304.10031	['Mathilde Papillon', 'Sophia Sanborn', 'Mustafa Hajij', 'Nina Miolane']	ct:The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature for relational systems has also led to a lack of unification in notation and language across message-passing Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying message-passing TNNs to new real-world problems. To address this issue, we provide an accessible introduction to TDL for relational systems, and compare the recently published message-passing TNNs using a unified mathematical and graphical notation. Through an intuitive and critical review of the emerging field of TDL, we extract valuable insights into current challenges and exciting opportunities for future development.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.10031', 'html': 'https://arxiv.org/html/2304.10031v3', 'tex': '/src/2304.10031', 'doi': 'https://doi.org/10.48550/arXiv.2304.10031'}	Submission history From: Mathilde Papillon [ view email ] [v1] Thu, 20 Apr 2023 01:02:13 UTC (9,972 KB) [v2] Fri, 18 Aug 2023 20:59:31 UTC (10,158 KB) [v3] Wed, 21 Feb 2024 23:27:08 UTC (9,284 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.10031'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.10031'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.10031'}]
2023-04-23	Visual Instruction Tuning	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2304.08485	Visual Instruction Tuning	https://twitter.com/dair_ai/status/1650145909387214848?s=20		2304.08485	['Haotian Liu', 'Chunyuan Li', 'Qingyang Wu', 'Yong Jae Lee']	ct:Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and languagethis http URLearly experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.	S 2023 Oral; project page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.08485', 'html': 'https://arxiv.org/html/2304.08485v2', 'tex': '/src/2304.08485', 'doi': 'https://doi.org/10.48550/arXiv.2304.08485'}	Submission history From: Haotian Liu [ view email ] [v1] Mon, 17 Apr 2023 17:59:25 UTC (4,360 KB) [v2] Mon, 11 Dec 2023 17:46:14 UTC (4,985 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.08485'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.08485'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.08485'}]
2023-04-23	ChatGPT: Applications, Opportunities, and Threats	Computers and Society	https://arxiv.org/abs/2304.09103	ChatGPT: Applications, Opportunities, and Threats	https://twitter.com/dair_ai/status/1650145911836745736?s=20		2304.09103	['Aram Bahrini', 'Mohammadsadra Khamoshifar', 'Hossein Abbasimehr', 'Robert J. Riggs', 'Maryam Esmaeili', 'Rastin Mastali Majdabadkohne', 'Morteza Pasehvar']	ct:Developed by OpenAI, ChatGPT (Conditional Generative Pre-trained Transformer) is an artificial intelligence technology that is fine-tuned using supervised machine learning and reinforcement learning techniques, allowing a computer to generate natural language conversation fully autonomously. ChatGPT is built on the transformer architecture and trained on millions of conversations from various sources. The system combines the power of pre-trained deep learning models with a programmability layer to provide a strong base for generating natural language conversations. In this study, after reviewing the existing literature, we examine the applications, opportunities, and threats of ChatGPT in 10 main domains, providing detailed examples for the business and industry as well as education. We also conducted an experimental study, checking the effectiveness and comparing the performances of GPT-3.5 and GPT-4, and found that the latter performs significantly better. Despite its exceptional ability to generate natural-sounding responses, the authors believe that ChatGPT does not possess the same level of understanding, empathy, and creativity as a human and cannot fully replace them in most situations.	es, 1 Figure, Preprint accepted in IEEE Systems and Information Engineering Design Symposium (SIEDS) 2023	['Computers and Society (cs.CY)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2304.09103', 'html': None, 'tex': '/src/2304.09103', 'doi': 'https://doi.org/10.48550/arXiv.2304.09103'}	Submission history From: Aram Bahrini [ view email ] [v1] Fri, 14 Apr 2023 16:25:03 UTC (76 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.09103'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.09103'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.09103'}]
2023-04-23	Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models	Computation and Language	https://arxiv.org/abs/2304.09842	Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models	https://twitter.com/dair_ai/status/1650145914420330496?s=20		2304.09842	['Pan Lu', 'Baolin Peng', 'Hao Cheng', 'Michel Galley', 'Kai-Wei Chang', 'Ying Nian Wu', 'Song-Chun Zhu', 'Jianfeng Gao']	ct:Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available atthis https URL.	es, 10 figures, 24 tables. Accepted to NeurIPS 2023	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.09842', 'html': None, 'tex': '/src/2304.09842', 'doi': 'https://doi.org/10.48550/arXiv.2304.09842'}	Submission history From: Pan Lu [ view email ] [v1] Wed, 19 Apr 2023 17:47:47 UTC (1,672 KB) [v2] Wed, 24 May 2023 17:52:19 UTC (2,204 KB) [v3] Tue, 31 Oct 2023 17:43:39 UTC (2,239 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.09842'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.09842'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.09842'}]
2023-04-23	Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2304.08818	Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models	https://twitter.com/dair_ai/status/1650145916794314752?s=20		2304.08818	['Andreas Blattmann', 'Robin Rombach', 'Huan Ling', 'Tim Dockhorn', 'Seung Wook Kim', 'Sanja Fidler', 'Karsten Kreis']	ct:Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280 x 2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page:this https URL	ence on Computer Vision and Pattern Recognition (CVPR) 2023. Project page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.08818', 'html': None, 'tex': '/src/2304.08818', 'doi': 'https://doi.org/10.48550/arXiv.2304.08818'}	Submission history From: Karsten Kreis [ view email ] [v1] Tue, 18 Apr 2023 08:30:32 UTC (10,172 KB) [v2] Thu, 28 Dec 2023 03:31:59 UTC (10,172 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.08818'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.08818'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.08818'}]
2023-04-16	Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2304.06706	Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields	https://twitter.com/dair_ai/status/1647613826425147401?s=20		2304.06706	['Jonathan T. Barron', 'Ben Mildenhall', 'Dor Verbin', 'Pratul P. Srinivasan', 'Peter Hedman']	ct:Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8% - 77% lower than either prior technique, and that trains 24x faster than mip-NeRF 360.	t page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.06706', 'html': None, 'tex': '/src/2304.06706', 'doi': 'https://doi.org/10.48550/arXiv.2304.06706'}	Submission history From: Jonathan Barron [ view email ] [v1] Thu, 13 Apr 2023 17:55:12 UTC (5,458 KB) [v2] Sun, 21 May 2023 18:12:07 UTC (17,902 KB) [v3] Thu, 26 Oct 2023 22:19:56 UTC (11,370 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.06706'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.06706'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.06706'}]
2023-04-16	Generative Agents: Interactive Simulacra of Human Behavior	Human-Computer Interaction	https://arxiv.org/abs/2304.03442	Generative Agents: Interactive Simulacra of Human Behavior	https://twitter.com/dair_ai/status/1647613828417351682?s=20		2304.03442	"['Joon Sung Park', ""Joseph C. O'Brien"", 'Carrie J. Cai', 'Meredith Ringel Morris', 'Percy Liang', 'Michael S. Bernstein']"	ct:Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.		['Human-Computer Interaction (cs.HC)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.03442', 'html': None, 'tex': '/src/2304.03442', 'doi': 'https://doi.org/10.48550/arXiv.2304.03442'}	Submission history From: Joon Sung Park [ view email ] [v1] Fri, 7 Apr 2023 01:55:19 UTC (34,149 KB) [v2] Sun, 6 Aug 2023 00:21:19 UTC (12,955 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.03442'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.03442'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.03442'}]
2023-04-16	Emergent autonomous scientific research capabilities of large language models	Physics > Chemical Physics	https://arxiv.org/abs/2304.05332	Emergent autonomous scientific research capabilities of large language models	https://twitter.com/dair_ai/status/1647613830233571328?s=20		2304.05332	['Daniil A. Boiko', 'Robert MacKnight', 'Gabe Gomes']	ct:Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.	n 1, April 11, 2023. 48 pages	['Chemical Physics (physics.chem-ph)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2304.05332', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2304.05332'}	Submission history From: Gabe Gomes [ view email ] [v1] Tue, 11 Apr 2023 16:50:17 UTC (2,801 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.05332'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.05332'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.05332'}]
2023-04-16	Automatic Gradient Descent: Deep Learning without Hyperparameters	Machine Learning	https://arxiv.org/abs/2304.05187	Automatic Gradient Descent: Deep Learning without Hyperparameters	https://twitter.com/dair_ai/status/1647613832804589569?s=20		2304.05187	['Jeremy Bernstein', 'Chris Mingard', 'Kevin Huang', 'Navid Azizan', 'Yisong Yue']	ct:The architecture of a deep neural network is defined explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice, Adam, is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a first-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at ImageNet scale. A PyTorch implementation is available atthis https URLand also in Appendix B. Overall, the paper supplies a rigorous theoretical foundation for a next-generation of architecture-dependent optimisers that work automatically and without hyperparameters.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Neural and Evolutionary Computing (cs.NE)', 'Numerical Analysis (math.NA)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2304.05187', 'html': None, 'tex': '/src/2304.05187', 'doi': 'https://doi.org/10.48550/arXiv.2304.05187'}	Submission history From: Chris Mingard [ view email ] [v1] Tue, 11 Apr 2023 12:45:52 UTC (4,187 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.05187'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.05187'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.05187'}]
2023-04-16	ChemCrow: Augmenting large-language models with chemistry tools	Physics > Chemical Physics	https://arxiv.org/abs/2304.05376	ChemCrow: Augmenting large-language models with chemistry tools	https://twitter.com/dair_ai/status/1647613834813644800?s=20		2304.05376	['Andres M Bran', 'Sam Cox', 'Oliver Schilter', 'Carlo Baldassari', 'Andrew D White', 'Philippe Schwaller']	ct:Over the last decades, excellent computational chemistry tools have been developed. Integrating them into a single platform with enhanced accessibility could help reaching their full potential by overcoming steep learning curves. Recently, large-language models (LLMs) have shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 18 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent, three organocatalysts, and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance. Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.	mental results	['Chemical Physics (physics.chem-ph)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2304.05376', 'html': None, 'tex': '/src/2304.05376', 'doi': 'https://doi.org/10.48550/arXiv.2304.05376'}	Submission history From: Andrew White [ view email ] [v1] Tue, 11 Apr 2023 17:41:13 UTC (13,130 KB) [v2] Wed, 12 Apr 2023 15:14:31 UTC (13,681 KB) [v3] Fri, 16 Jun 2023 17:59:17 UTC (36,772 KB) [v4] Wed, 21 Jun 2023 17:28:22 UTC (34,841 KB) [v5] Mon, 2 Oct 2023 17:03:01 UTC (14,678 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.05376'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.05376'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.05376'}]
2023-04-16	One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era	Computers and Society	https://arxiv.org/abs/2304.06488	One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era	https://twitter.com/dair_ai/status/1647613836617195525?s=20		2304.06488	['Chaoning Zhang', 'Chenshuang Zhang', 'Chenghao Li', 'Yu Qiao', 'Sheng Zheng', 'Sumit Kumar Dam', 'Mengchun Zhang', 'Jung Uk Kim', 'Seong Tae Kim', 'Jinwoo Choi', 'Gyeong-Moon Park', 'Sung-Ho Bae', 'Lik-Hang Lee', 'Pan Hui', 'In So Kweon', 'Choong Seon Hong']	ct:OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.	ey on ChatGPT and GPT-4, 29 pages. Feedback is appreciated (chaoningzhang1990@gmail.com)	['Computers and Society (cs.CY)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.06488', 'html': None, 'tex': '/src/2304.06488', 'doi': 'https://doi.org/10.48550/arXiv.2304.06488'}	Submission history From: Chaoning Zhang [ view email ] [v1] Tue, 4 Apr 2023 06:22:09 UTC (1,782 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.06488'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.06488'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.06488'}]
2023-04-16	OpenAGI: When LLM Meets Domain Experts	Artificial Intelligence	https://arxiv.org/abs/2304.04370	OpenAGI: When LLM Meets Domain Experts	https://twitter.com/dair_ai/status/1647613838567546886?s=20		2304.04370	['Yingqiang Ge', 'Wenyue Hua', 'Kai Mei', 'Jianchao Ji', 'Juntao Tan', 'Shuyuan Xu', 'Zelong Li', 'Yongfeng Zhang']	ct:Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement:this https URL.	rIPS 2023	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.04370', 'html': None, 'tex': '/src/2304.04370', 'doi': 'https://doi.org/10.48550/arXiv.2304.04370'}	Submission history From: Yongfeng Zhang [ view email ] [v1] Mon, 10 Apr 2023 03:55:35 UTC (9,146 KB) [v2] Wed, 12 Apr 2023 23:37:32 UTC (9,142 KB) [v3] Wed, 14 Jun 2023 16:59:50 UTC (41,549 KB) [v4] Sun, 18 Jun 2023 17:08:47 UTC (31,039 KB) [v5] Wed, 2 Aug 2023 19:00:53 UTC (31,039 KB) [v6] Fri, 3 Nov 2023 15:24:18 UTC (44,437 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.04370'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.04370'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.04370'}]
2023-04-16	AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models	Computation and Language	https://arxiv.org/abs/2304.06364	AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models	https://twitter.com/dair_ai/status/1647613840400498700?s=20		2304.06364	['Wanjun Zhong', 'Ruixiang Cui', 'Yiduo Guo', 'Yaobo Liang', 'Shuai Lu', 'Yanlin Wang', 'Amin Saied', 'Weizhu Chen', 'Nan Duan']	ct:Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released inthis https URL.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2304.06364', 'html': None, 'tex': '/src/2304.06364', 'doi': 'https://doi.org/10.48550/arXiv.2304.06364'}	Submission history From: Wanjun Zhong [ view email ] [v1] Thu, 13 Apr 2023 09:39:30 UTC (13,276 KB) [v2] Mon, 18 Sep 2023 14:23:02 UTC (6,512 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.06364'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.06364'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.06364'}]
2023-04-16	Teaching Large Language Models to Self-Debug	Computation and Language	https://arxiv.org/abs/2304.05128	Teaching Large Language Models to Self-Debug	https://twitter.com/dair_ai/status/1647613842300497924?s=20		2304.05128	['Xinyun Chen', 'Maxwell Lin', 'Nathanael Schärli', 'Denny Zhou']	ct:Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2304.05128', 'html': None, 'tex': '/src/2304.05128', 'doi': 'https://doi.org/10.48550/arXiv.2304.05128'}	Submission history From: Xinyun Chen [ view email ] [v1] Tue, 11 Apr 2023 10:43:43 UTC (350 KB) [v2] Thu, 5 Oct 2023 09:12:07 UTC (394 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.05128'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.05128'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.05128'}]
2023-04-16	Segment Everything Everywhere All at Once	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2304.06718	Segment Everything Everywhere All at Once	https://twitter.com/dair_ai/status/1647613844087361537?s=20		2304.06718	['Xueyan Zou', 'Jianwei Yang', 'Hao Zhang', 'Feng Li', 'Linjie Li', 'Jianfeng Wang', 'Lijuan Wang', 'Jianfeng Gao', 'Yong Jae Lee']	ct:In this work, we present SEEM, a promptable and interactive model for segmenting everything everywhere all at once in an image, as shown in Fig.1. In SEEM, we propose a novel decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal segmentation interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata: i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks; iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from decoder to image features; and iv) Semantic-awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. Notably, our single SEEM model achieves competitive performance across interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity for generalization to novel prompts or their combinations, rendering it a readily universal image segmentation interface.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2304.06718', 'html': None, 'tex': '/src/2304.06718', 'doi': 'https://doi.org/10.48550/arXiv.2304.06718'}	Submission history From: Hao Zhang [ view email ] [v1] Thu, 13 Apr 2023 17:59:40 UTC (19,561 KB) [v2] Tue, 18 Apr 2023 17:43:56 UTC (22,315 KB) [v3] Mon, 1 May 2023 17:57:19 UTC (22,314 KB) [v4] Tue, 11 Jul 2023 18:13:14 UTC (18,002 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.06718'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.06718'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.06718'}]
2023-04-09	Segment Anything	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2304.02643v1	Segment Anything	https://twitter.com/dair_ai/status/1645089444280561666?s=20		2304.02643v1	['Alexander Kirillov', 'Eric Mintun', 'Nikhila Ravi', 'Hanzi Mao', 'Chloe Rolland', 'Laura Gustafson', 'Tete Xiao', 'Spencer Whitehead', 'Alexander C. Berg', 'Wan-Yen Lo', 'Piotr Dollár', 'Ross Girshick']	ct:We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images atthis https URLto foster research into foundation models for computer vision.	t web-page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2304.02643v1', 'html': None, 'tex': '/src/2304.02643v1', 'doi': 'https://doi.org/10.48550/arXiv.2304.02643'}	Submission history From: Alexander Kirillov [ view email ] [v1] Wed, 5 Apr 2023 17:59:46 UTC (14,399 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.02643'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.02643'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.02643'}]
2023-04-09	Instruction Tuning with GPT-4	Computation and Language	https://arxiv.org/abs/2304.03277	Instruction Tuning with GPT-4	https://twitter.com/dair_ai/status/1645089446524534788?s=20		2304.03277	['Baolin Peng', 'Chunyuan Li', 'Pengcheng He', 'Michel Galley', 'Jianfeng Gao']	ct:Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.	s. Work in progress. Project page:this https URL	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2304.03277', 'html': None, 'tex': '/src/2304.03277', 'doi': 'https://doi.org/10.48550/arXiv.2304.03277'}	Submission history From: Baolin Peng [ view email ] [v1] Thu, 6 Apr 2023 17:58:09 UTC (1,397 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.03277'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.03277'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.03277'}]
2023-04-09	Eight Things to Know about Large Language Models	Computation and Language	https://arxiv.org/abs/2304.00612v1	Eight Things to Know about Large Language Models	https://twitter.com/dair_ai/status/1645089448428699650?s=20		2304.00612v1	['Samuel R. Bowman']	ct:The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points:1. LLMs predictably get more capable with increasing investment, even without targeted innovation.2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment.3. LLMs often appear to learn and use representations of the outside world.4. There are no reliable techniques for steering the behavior of LLMs.5. Experts are not yet able to interpret the inner workings of LLMs.6. Human performance on a task isn't an upper bound on LLM performance.7. LLMs need not express the values of their creators nor the values encoded in web text.8. Brief interactions with LLMs are often misleading.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2304.00612v1', 'html': None, 'tex': '/src/2304.00612v1', 'doi': 'https://doi.org/10.48550/arXiv.2304.00612'}	Submission history From: Samuel Bowman [ view email ] [v1] Sun, 2 Apr 2023 20:03:27 UTC (850 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.00612'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.00612'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.00612'}]
2023-04-09	A Survey of Large Language Models	Computation and Language	https://arxiv.org/abs/2303.18223	A Survey of Large Language Models	https://twitter.com/dair_ai/status/1645089450395852802?s=20		2303.18223	['Wayne Xin Zhao', 'Kun Zhou', 'Junyi Li', 'Tianyi Tang', 'Xiaolei Wang', 'Yupeng Hou', 'Yingqian Min', 'Beichen Zhang', 'Junjie Zhang', 'Zican Dong', 'Yifan Du', 'Chen Yang', 'Yushuo Chen', 'Zhipeng Chen', 'Jinhao Jiang', 'Ruiyang Ren', 'Yifan Li', 'Xinyu Tang', 'Zikang Liu', 'Peiyu Liu', 'Jian-Yun Nie', 'Ji-Rong Wen']	ct:Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.	g work; 144 pages, 1081 citations	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2303.18223', 'html': 'https://arxiv.org/html/2303.18223v16', 'tex': '/src/2303.18223', 'doi': 'https://doi.org/10.48550/arXiv.2303.18223'}	Submission history From: Kun Zhou [ view email ] [v1] Fri, 31 Mar 2023 17:28:46 UTC (991 KB) [v2] Sun, 9 Apr 2023 15:49:09 UTC (1,306 KB) [v3] Tue, 11 Apr 2023 16:20:17 UTC (1,305 KB) [v4] Wed, 12 Apr 2023 16:13:54 UTC (1,310 KB) [v5] Sun, 16 Apr 2023 16:42:37 UTC (1,678 KB) [v6] Mon, 24 Apr 2023 16:53:57 UTC (2,528 KB) [v7] Tue, 25 Apr 2023 14:42:36 UTC (2,528 KB) [v8] Thu, 27 Apr 2023 15:54:48 UTC (2,533 KB) [v9] Fri, 28 Apr 2023 15:39:09 UTC (2,534 KB) [v10] Sun, 7 May 2023 17:59:15 UTC (2,031 KB) [v11] Thu, 29 Jun 2023 16:09:05 UTC (4,226 KB) [v12] Mon, 11 Sep 2023 15:13:59 UTC (4,687 KB) [v13] Fri, 24 Nov 2023 13:57:45 UTC (6,600 KB) [v14] Tue, 24 Sep 2024 07:02:59 UTC (5,852 KB) [v15] Sun, 13 Oct 2024 06:11:31 UTC (5,852 KB) [v16] Tue, 11 Mar 2025 16:51:11 UTC (7,340 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.18223'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.18223'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.18223'}]
2023-04-09	Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data	Computation and Language	https://arxiv.org/abs/2304.01196	Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data	https://twitter.com/dair_ai/status/1645089452081938433?s=20		2304.01196	['Canwen Xu', 'Daya Guo', 'Nan Duan', 'Julian McAuley']	ct:Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT. The Baize models and data are released for research purposes only atthis https URL. An online demo is also available atthis https URL.	v2; EMNLP 2023	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2304.01196', 'html': 'https://arxiv.org/html/2304.01196v4', 'tex': '/src/2304.01196', 'doi': 'https://doi.org/10.48550/arXiv.2304.01196'}	Submission history From: Canwen Xu [ view email ] [v1] Mon, 3 Apr 2023 17:59:09 UTC (74 KB) [v2] Tue, 4 Apr 2023 08:34:16 UTC (84 KB) [v3] Tue, 23 May 2023 19:40:03 UTC (107 KB) [v4] Sat, 2 Dec 2023 21:05:22 UTC (615 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.01196'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.01196'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.01196'}]
2023-04-09	Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark	Machine Learning	https://arxiv.org/abs/2304.03279	Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark	https://twitter.com/dair_ai/status/1645089453780639744?s=20		2304.03279	['Alexander Pan', 'Jun Shern Chan', 'Andy Zou', 'Nathaniel Li', 'Steven Basart', 'Thomas Woodside', 'Jonathan Ng', 'Hanlin Zhang', 'Scott Emmons', 'Dan Hendrycks']	ct:Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.	023 Oral (camera-ready); 31 pages, 5 figures	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2304.03279', 'html': None, 'tex': '/src/2304.03279', 'doi': 'https://doi.org/10.48550/arXiv.2304.03279'}	Submission history From: Alexander Pan [ view email ] [v1] Thu, 6 Apr 2023 17:59:03 UTC (797 KB) [v2] Mon, 1 May 2023 22:58:44 UTC (797 KB) [v3] Thu, 8 Jun 2023 02:04:23 UTC (799 KB) [v4] Tue, 13 Jun 2023 01:01:42 UTC (798 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.03279'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.03279'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.03279'}]
2023-04-09	Better Language Models of Code through Self-Improvement	Computation and Language	https://arxiv.org/abs/2304.01228v1	Better Language Models of Code through Self-Improvement	https://twitter.com/dair_ai/status/1645089455659687937?s=20		2304.01228v1	['Hung Quoc To', 'Nghi D. Q. Bui', 'Jin Guo', 'Tien N. Nguyen']	ct:Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a simple data augmentation framework. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to generate pseudo data, which is then used as training data for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in code-related sequence generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2304.01228v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2304.01228'}	Submission history From: Nghi D. Q. Bui [ view email ] [v1] Sun, 2 Apr 2023 10:59:19 UTC (382 KB) [v2] Wed, 10 May 2023 02:36:40 UTC (393 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.01228'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.01228'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.01228'}]
2023-04-09	Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models	Computation and Language	https://arxiv.org/abs/2304.01852	Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models	https://twitter.com/dair_ai/status/1645089457488404486?s=20		2304.01852	['Yiheng Liu', 'Tianle Han', 'Siyuan Ma', 'Jiayue Zhang', 'Yuanyuan Yang', 'Jiaming Tian', 'Hao He', 'Antong Li', 'Mengshen He', 'Zhengliang Liu', 'Zihao Wu', 'Lin Zhao', 'Dajiang Zhu', 'Xiang Li', 'Ning Qiang', 'Dingang Shen', 'Tianming Liu', 'Bao Ge']	ct:This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and GPT-4) research, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT-related research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.	es, 4 figures, accepted by Meta-Radiology	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2304.01852', 'html': None, 'tex': '/src/2304.01852', 'doi': 'https://doi.org/10.48550/arXiv.2304.01852'}	Submission history From: Yiheng Liu [ view email ] [v1] Tue, 4 Apr 2023 15:01:06 UTC (350 KB) [v2] Sat, 8 Apr 2023 14:42:40 UTC (719 KB) [v3] Thu, 11 May 2023 03:50:53 UTC (366 KB) [v4] Tue, 22 Aug 2023 03:18:43 UTC (448 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.01852'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.01852'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.01852'}]
2023-04-09	Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling	Computation and Language	https://arxiv.org/abs/2304.01373	Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling	https://twitter.com/dair_ai/status/1645089459191382016?s=20		2304.01373	"['Stella Biderman', 'Hailey Schoelkopf', 'Quentin Anthony', 'Herbie Bradley', ""Kyle O'Brien"", 'Eric Hallahan', 'Mohammad Aflah Khan', 'Shivanshu Purohit', 'USVSN Sai Prashanth', 'Edward Raff', 'Aviya Skowron', 'Lintang Sutawika', 'Oskar van der Wal']"	ct:How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \url{this https URL}.	tthis https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2304.01373', 'html': None, 'tex': '/src/2304.01373', 'doi': 'https://doi.org/10.48550/arXiv.2304.01373'}	Submission history From: Hailey Schoelkopf [ view email ] [v1] Mon, 3 Apr 2023 20:58:15 UTC (2,683 KB) [v2] Wed, 31 May 2023 17:54:07 UTC (2,371 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.01373'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.01373'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.01373'}]
2023-04-09	SegGPT: Segmenting Everything In Context	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2304.03284	SegGPT: Segmenting Everything In Context	https://twitter.com/dair_ai/status/1645089461124886529?s=20		2304.03284	['Xinlong Wang', 'Xiaosong Zhang', 'Yue Cao', 'Wen Wang', 'Chunhua Shen', 'Tiejun Huang']	ct:We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors. After training, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. Our results show strong capabilities in segmenting in-domain and out-of-domain targets, either qualitatively or quantitatively.	nd Demo:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2304.03284', 'html': None, 'tex': '/src/2304.03284', 'doi': 'https://doi.org/10.48550/arXiv.2304.03284'}	Submission history From: Xinlong Wang [ view email ] [v1] Thu, 6 Apr 2023 17:59:57 UTC (5,953 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2304.03284'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2304.03284'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2304.03284'}]
2023-04-02	BloombergGPT: A Large Language Model for Finance	Machine Learning	https://arxiv.org/abs/2303.17564v1	BloombergGPT: A Large Language Model for Finance	https://twitter.com/omarsar0/status/1641787456436547584?s=20		2303.17564v1	['Shijie Wu', 'Ozan Irsoy', 'Steven Lu', 'Vadim Dabravolski', 'Mark Dredze', 'Sebastian Gehrmann', 'Prabhanjan Kambadur', 'David Rosenberg', 'Gideon Mann']	ct:The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. As a next step, we plan to release training logs (Chronicles) detailing our experience in training BloombergGPT.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'General Finance (q-fin.GN)']	{'pdf': '/pdf/2303.17564v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2303.17564'}	Submission history From: Sebastian Gehrmann [ view email ] [v1] Thu, 30 Mar 2023 17:30:36 UTC (7,463 KB) [v2] Tue, 9 May 2023 16:06:35 UTC (1,328 KB) [v3] Thu, 21 Dec 2023 06:21:11 UTC (1,328 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.17564'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.17564'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.17564'}]
2023-04-02	HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face	Computation and Language	https://arxiv.org/abs/2303.17580	HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace	https://twitter.com/johnjnay/status/1641609645713129473?s=20		2303.17580	['Yongliang Shen', 'Kaitao Song', 'Xu Tan', 'Dongsheng Li', 'Weiming Lu', 'Yueting Zhuang']	ct:Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.17580', 'html': None, 'tex': '/src/2303.17580', 'doi': 'https://doi.org/10.48550/arXiv.2303.17580'}	Submission history From: Yongliang Shen [ view email ] [v1] Thu, 30 Mar 2023 17:48:28 UTC (2,931 KB) [v2] Sun, 2 Apr 2023 17:24:47 UTC (2,925 KB) [v3] Thu, 25 May 2023 15:50:20 UTC (3,134 KB) [v4] Sun, 3 Dec 2023 18:17:21 UTC (3,164 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.17580'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.17580'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.17580'}]
2023-04-02	ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge	Computation and Language	https://arxiv.org/abs/2303.14070	ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge	https://twitter.com/omarsar0/status/1640525256719753217?s=20		2303.14070	['Yunxiang Li', 'Zihan Li', 'Kai Zhang', 'Ruilong Dan', 'Steve Jiang', 'You Zhang']	ct:The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2303.14070', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2303.14070'}	Submission history From: Yunxiang Li [ view email ] [v1] Fri, 24 Mar 2023 15:29:16 UTC (1,173 KB) [v2] Mon, 27 Mar 2023 20:41:46 UTC (1,212 KB) [v3] Sat, 1 Apr 2023 18:00:33 UTC (1,212 KB) [v4] Tue, 18 Apr 2023 18:54:29 UTC (1,585 KB) [v5] Sat, 24 Jun 2023 15:26:44 UTC (3,093 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.14070'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.14070'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.14070'}]
2023-04-02	LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2303.16199	LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention	https://twitter.com/rasbt/status/1641457696074334209?s=20		2303.16199	['Renrui Zhang', 'Jiaming Han', 'Chris Liu', 'Peng Gao', 'Aojun Zhou', 'Xiangfei Hu', 'Shilin Yan', 'Pan Lu', 'Hongsheng Li', 'Yu Qiao']	ct:We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released atthis https URL.	ed by ICLR 2024. Code is available atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)', 'Multimedia (cs.MM)']	{'pdf': '/pdf/2303.16199', 'html': 'https://arxiv.org/html/2303.16199v3', 'tex': '/src/2303.16199', 'doi': 'https://doi.org/10.48550/arXiv.2303.16199'}	Submission history From: Renrui Zhang [ view email ] [v1] Tue, 28 Mar 2023 17:59:12 UTC (1,749 KB) [v2] Wed, 14 Jun 2023 17:31:32 UTC (1,503 KB) [v3] Wed, 18 Sep 2024 23:54:36 UTC (4,130 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.16199'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.16199'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.16199'}]
2023-04-02	ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks	Computation and Language	https://arxiv.org/abs/2303.15056v1	ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks	https://twitter.com/AlphaSignalAI/status/1641496876527517696?s=20		2303.15056v1	['Fabrizio Gilardi', 'Meysam Alizadeh', 'Maël Kubli']	ct:Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.		['Computation and Language (cs.CL)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2303.15056v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2303.15056'}	Submission history From: Fabrizio Gilardi [ view email ] [v1] Mon, 27 Mar 2023 09:59:48 UTC (255 KB) [v2] Wed, 19 Jul 2023 14:10:55 UTC (38 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.15056'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.15056'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.15056'}]
2023-04-02	Language Models can Solve Computer Tasks	Computation and Language	https://arxiv.org/abs/2303.17491	Language Models can Solve Computer Tasks	https://twitter.com/arankomatsuzaki/status/1641609722951516161?s=20		2303.17491	['Geunwoo Kim', 'Pierre Baldi', 'Stephen McAleer']	ct:Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here:this https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.17491', 'html': None, 'tex': '/src/2303.17491', 'doi': 'https://doi.org/10.48550/arXiv.2303.17491'}	Submission history From: Geunwoo Kim [ view email ] [v1] Thu, 30 Mar 2023 16:01:52 UTC (1,256 KB) [v2] Wed, 7 Jun 2023 17:50:44 UTC (1,575 KB) [v3] Thu, 16 Nov 2023 20:15:14 UTC (1,576 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.17491'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.17491'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.17491'}]
2023-04-02	DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents	Computation and Language	https://arxiv.org/abs/2303.17071	DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents	https://twitter.com/johnjnay/status/1642168727796961280?s=20		2303.17071	['Varun Nair', 'Elliot Schumacher', 'Geoffrey Tso', 'Anitha Kannan']	ct:Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs, namely GPT-4. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types - a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher's information and makes judgments on the final output.We test DERA against three clinically-focused tasks. For medical conversation summarization and care plan generation, DERA shows significant improvement over the base GPT-4 performance in both human expert preference evaluations and quantitative metrics. In a new finding, we also show that GPT-4's performance (70%) on an open-ended version of the MedQA question-answering (QA) dataset (Jin et al. 2021, USMLE) is well above the passing level (60%), with DERA showing similar performance. We release the open-ended MEDQA dataset atthis https URL.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.17071', 'html': None, 'tex': '/src/2303.17071', 'doi': 'https://doi.org/10.48550/arXiv.2303.17071'}	Submission history From: Elliot Schumacher [ view email ] [v1] Thu, 30 Mar 2023 00:30:19 UTC (7,387 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.17071'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.17071'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.17071'}]
2023-04-02	Natural Selection Favors AIs over Humans	Computers and Society	https://arxiv.org/abs/2303.16200	Natural Selection Favors AIs over Humans	https://twitter.com/DanHendrycks/status/1641102660412792833?s=20		2303.16200	['Dan Hendrycks']	ct:For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typically have an advantage over species that are altruistic to other species. This Darwinian logic could also apply to artificial agents, as agents may eventually be better able to persist into the future if they behave selfishly and pursue their own interests with little regard for humans, which could pose catastrophic risks. To counteract these risks and evolutionary forces, we consider interventions such as carefully designing AI agents' intrinsic motivations, introducing constraints on their actions, and institutions that encourage cooperation. These steps, or others that resolve the problems we pose, will be necessary in order to ensure the development of artificial intelligence is a positive one.	lainer video corresponding to the paper is available atthis https URL	['Computers and Society (cs.CY)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Neural and Evolutionary Computing (cs.NE)']	{'pdf': '/pdf/2303.16200', 'html': None, 'tex': '/src/2303.16200', 'doi': 'https://doi.org/10.48550/arXiv.2303.16200'}	Submission history From: Dan Hendrycks [ view email ] [v1] Tue, 28 Mar 2023 17:59:12 UTC (3,063 KB) [v2] Thu, 27 Apr 2023 03:37:39 UTC (2,482 KB) [v3] Sat, 6 May 2023 14:35:16 UTC (2,483 KB) [v4] Tue, 18 Jul 2023 18:40:27 UTC (2,482 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.16200'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.16200'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.16200'}]
2023-04-02	Machine Learning for Partial Differential Equations	Machine Learning	https://arxiv.org/abs/2303.17078	Machine Learning for Partial Differential Equations	https://twitter.com/DynamicsSIAM/status/1641608068453777412?s=20		2303.17078	['Steven L. Brunton', 'J. Nathan Kutz']	ct:Partial differential equations (PDEs) are among the most universal and parsimonious descriptions of natural physical laws, capturing a rich variety of phenomenology and multi-scale physics in a compact and symbolic representation. This review will examine several promising avenues of PDE research that are being advanced by machine learning, including: 1) the discovery of new governing PDEs and coarse-grained approximations for complex natural and engineered systems, 2) learning effective coordinate systems and reduced-order models to make PDEs more amenable to analysis, and 3) representing solution operators and improving traditional numerical algorithms. In each of these fields, we summarize key advances, ongoing challenges, and opportunities for further development.	es, 6 figures	['Machine Learning (cs.LG)', 'Analysis of PDEs (math.AP)', 'Dynamical Systems (math.DS)', 'Numerical Analysis (math.NA)', 'Fluid Dynamics (physics.flu-dyn)']	{'pdf': '/pdf/2303.17078', 'html': None, 'tex': '/src/2303.17078', 'doi': 'https://doi.org/10.48550/arXiv.2303.17078'}	Submission history From: Steven Brunton [ view email ] [v1] Thu, 30 Mar 2023 00:57:59 UTC (4,604 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.17078'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.17078'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.17078'}]
2023-03-26	Sparks of Artificial General Intelligence: Early experiments with GPT-4	Computation and Language	https://arxiv.org/abs/2303.12712	Sparks of Artificial General Intelligence: Early experiments with GPT-4	https://twitter.com/dair_ai/status/1639991716349460481?s=20		2303.12712	['Sébastien Bubeck', 'Varun Chandrasekaran', 'Ronen Eldan', 'Johannes Gehrke', 'Eric Horvitz', 'Ece Kamar', 'Peter Lee', 'Yin Tat Lee', 'Yuanzhi Li', 'Scott Lundberg', 'Harsha Nori', 'Hamid Palangi', 'Marco Tulio Ribeiro', 'Yi Zhang']	ct:Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2303.12712', 'html': None, 'tex': '/src/2303.12712', 'doi': 'https://doi.org/10.48550/arXiv.2303.12712'}	Submission history From: Sebastien Bubeck [ view email ] [v1] Wed, 22 Mar 2023 16:51:28 UTC (13,667 KB) [v2] Fri, 24 Mar 2023 17:07:43 UTC (6,453 KB) [v3] Mon, 27 Mar 2023 22:36:40 UTC (6,470 KB) [v4] Wed, 12 Apr 2023 17:00:10 UTC (12,943 KB) [v5] Thu, 13 Apr 2023 20:41:31 UTC (6,476 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.12712'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.12712'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.12712'}]
2023-03-26	Reflexion: Language Agents with Verbal Reinforcement Learning	Artificial Intelligence	https://arxiv.org/abs/2303.11366	Reflexion: an autonomous agent with dynamic memory and self-reflection	https://twitter.com/dair_ai/status/1639991718169722880?s=20		2303.11366	['Noah Shinn', 'Federico Cassano', 'Edward Berman', 'Ashwin Gopinath', 'Karthik Narasimhan', 'Shunyu Yao']	ct:Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.	tains a few additional experiments	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.11366', 'html': None, 'tex': '/src/2303.11366', 'doi': 'https://doi.org/10.48550/arXiv.2303.11366'}	Submission history From: Noah Shinn [ view email ] [v1] Mon, 20 Mar 2023 18:08:50 UTC (506 KB) [v2] Sun, 21 May 2023 06:20:36 UTC (404 KB) [v3] Sat, 10 Jun 2023 04:32:30 UTC (396 KB) [v4] Tue, 10 Oct 2023 05:21:45 UTC (386 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.11366'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.11366'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.11366'}]
2023-03-26	GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models	Economics > General Economics	https://arxiv.org/abs/2303.10130	GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models	https://twitter.com/dair_ai/status/1639991722263412737?s=20		2303.10130	['Tyna Eloundou', 'Sam Manning', 'Pamela Mishkin', 'Daniel Rock']	ct:We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.		['General Economics (econ.GN)', 'Artificial Intelligence (cs.AI)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2303.10130', 'html': None, 'tex': '/src/2303.10130', 'doi': 'https://doi.org/10.48550/arXiv.2303.10130'}	Submission history From: Daniel Rock [ view email ] [v1] Fri, 17 Mar 2023 17:15:20 UTC (5,829 KB) [v2] Mon, 20 Mar 2023 02:29:47 UTC (9,066 KB) [v3] Wed, 22 Mar 2023 03:32:25 UTC (9,060 KB) [v4] Thu, 23 Mar 2023 21:54:09 UTC (8,678 KB) [v5] Mon, 21 Aug 2023 07:58:25 UTC (8,833 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.10130'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.10130'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.10130'}]
2023-03-26	CoLT5: Faster Long-Range Transformers with Conditional Computation	Computation and Language	https://arxiv.org/abs/2303.09752	CoLT5: Faster Long-Range Transformers with Conditional Computation	https://twitter.com/dair_ai/status/1639991723806826499?s=20		2303.09752	['Joshua Ainslie', 'Tao Lei', 'Michiel de Jong', 'Santiago Ontañón', 'Siddhartha Brahma', 'Yury Zemlyanskiy', 'David Uthus', 'Mandy Guo', 'James Lee-Thorp', 'Yi Tay', 'Yun-Hsuan Sung', 'Sumit Sanghai']	ct:Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.	ed at EMNLP 2023	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.09752', 'html': None, 'tex': '/src/2303.09752', 'doi': 'https://doi.org/10.48550/arXiv.2303.09752'}	Submission history From: Joshua Ainslie [ view email ] [v1] Fri, 17 Mar 2023 03:28:17 UTC (7,558 KB) [v2] Fri, 14 Apr 2023 03:22:55 UTC (7,559 KB) [v3] Tue, 24 Oct 2023 00:51:49 UTC (7,558 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.09752'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.09752'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.09752'}]
2023-03-26	Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity	Artificial Intelligence	https://arxiv.org/abs/2303.12003	Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity	https://twitter.com/dair_ai/status/1639991725442646018?s=20		2303.12003	['Jennifer Haase', 'Paul H. P. Hanel']	ct:A widespread view is that Artificial Intelligence cannot be creative. We tested this assumption by comparing human-generated ideas with those generated by six Generative Artificial Intelligence (GAI) chatbots: $alpa.\!ai$, $Copy.\!ai$, ChatGPT (versions 3 and 4), $Studio.\!ai$, and YouChat. Humans and a specifically trained AI independently assessed the quality and quantity of ideas. We found no qualitative difference between AI and human-generated creativity, although there are differences in how ideas are generated. Interestingly, 9.4 percent of humans were more creative than the most creative GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the creative process. Continued research and development of GAI in creative tasks is crucial to fully understand this technology's potential benefits and drawbacks in shaping the future of creativity. Finally, we discuss the question of whether GAIs are capable of being truly creative.	es, 3 figures, 1 table	['Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2303.12003', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2303.12003'}	Submission history From: Jennifer Haase [ view email ] [v1] Tue, 21 Mar 2023 16:35:01 UTC (324 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.12003'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.12003'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.12003'}]
2023-03-26	A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models	Computation and Language	https://arxiv.org/abs/2303.10420	A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models	https://twitter.com/dair_ai/status/1639991727292395520?s=20		2303.10420	['Junjie Ye', 'Xuanting Chen', 'Nuo Xu', 'Can Zu', 'Zekai Shao', 'Shichun Liu', 'Yuhan Cui', 'Zeyang Zhou', 'Chao Gong', 'Yang Shen', 'Jie Zhou', 'Siming Chen', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']	ct:GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on, have gained considerable attention due to their exceptional natural language processing capabilities. However, despite the abundance of research on the difference in capabilities between GPT series models and fine-tuned models, there has been limited attention given to the evolution of GPT series models' capabilities over time. To conduct a comprehensive analysis of the capabilities of GPT series models, we select six representative models, comprising two GPT-3 series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo). We evaluate their performance on nine natural language understanding (NLU) tasks using 21 datasets. In particular, we compare the performance and robustness of different models for each task under zero-shot and few-shot scenarios. Our extensive experiments reveal that the overall ability of GPT series models on NLU tasks does not increase gradually as the models evolve, especially with the introduction of the RLHF training strategy. While this strategy enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2303.10420', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2303.10420'}	Submission history From: Junjie Ye [ view email ] [v1] Sat, 18 Mar 2023 14:02:04 UTC (879 KB) [v2] Sat, 23 Dec 2023 12:53:02 UTC (785 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.10420'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.10420'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.10420'}]
2023-03-26	Context-faithful Prompting for Large Language Models	Computation and Language	https://arxiv.org/abs/2303.11315	Context-faithful Prompting for Large Language Models	https://twitter.com/dair_ai/status/1639991728882032646?s=20		2303.11315	['Wenxuan Zhou', 'Sheng Zhang', 'Hoifung Poon', 'Muhao Chen']	ct:Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released atthis https URL.	ed at EMNLP 2023 Findings. Code and data are released atthis https URL	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2303.11315', 'html': None, 'tex': '/src/2303.11315', 'doi': 'https://doi.org/10.48550/arXiv.2303.11315'}	Submission history From: Wenxuan Zhou [ view email ] [v1] Mon, 20 Mar 2023 17:54:58 UTC (7,026 KB) [v2] Mon, 23 Oct 2023 03:25:13 UTC (7,828 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.11315'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.11315'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.11315'}]
2023-03-26	Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2303.11989	Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models	https://twitter.com/dair_ai/status/1639991730723254274?s=20	"{""Project"": ""https://lukashoel.github.io/text-to-room/""}"	2303.11989	['Lukas Höllein', 'Ang Cao', 'Andrew Owens', 'Justin Johnson', 'Matthias Nießner']	ct:We present Text2Room, a method for generating room-scale textured 3D meshes from a given text prompt as input. To this end, we leverage pre-trained 2D text-to-image models to synthesize a sequence of images from different poses. In order to lift these outputs into a consistent 3D scene representation, we combine monocular depth estimation with a text-conditioned inpainting model. The core idea of our approach is a tailored viewpoint selection such that the content of each image can be fused into a seamless, textured 3D mesh. More specifically, we propose a continuous alignment strategy that iteratively fuses scene frames with the existing geometry to create a seamless mesh. Unlike existing works that focus on generating single objects or zoom-out trajectories from text, our method generates complete 3D scenes with multiple objects and explicit 3D geometry. We evaluate our approach using qualitative and quantitative metrics, demonstrating it as the first method to generate room-scale 3D geometry with compelling textures from only text as input.	ed to ICCV 2023 (Oral) video:this https URLproject page:this https URLcode:this https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2303.11989', 'html': None, 'tex': '/src/2303.11989', 'doi': 'https://doi.org/10.48550/arXiv.2303.11989'}	Submission history From: Lukas Höllein [ view email ] [v1] Tue, 21 Mar 2023 16:21:02 UTC (22,251 KB) [v2] Sun, 10 Sep 2023 15:18:03 UTC (13,032 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.11989'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.11989'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.11989'}]
2023-03-26	PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing	Computation and Language	https://arxiv.org/abs/2303.10845	PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing	https://twitter.com/dair_ai/status/1639991732405252100?s=20		2303.10845	['Xiaozhe Ren', 'Pingyi Zhou', 'Xinfan Meng', 'Xinjing Huang', 'Yadao Wang', 'Weichao Wang', 'Pengfei Li', 'Xiaoda Zhang', 'Alexander Podolskiy', 'Grigory Arshinov', 'Andrey Bout', 'Irina Piontkovskaya', 'Jiansheng Wei', 'Xin Jiang', 'Teng Su', 'Qun Liu', 'Jun Yao']	ct:The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2303.10845', 'html': None, 'tex': '/src/2303.10845', 'doi': 'https://doi.org/10.48550/arXiv.2303.10845'}	Submission history From: Xiaozhe Ren [ view email ] [v1] Mon, 20 Mar 2023 03:39:27 UTC (3,511 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.10845'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.10845'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.10845'}]
2023-03-19	GPT-4 Technical Report	Computation and Language	https://arxiv.org/abs/2303.08774v2	GPT-4 Technical Report	https://twitter.com/dair_ai/status/1637456913993433089?s=20		2303.08774v2	['OpenAI']	ct:We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.	es	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2303.08774v2', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2303.08774'}	Submission history From: Adrien Ecoffet [ view email ] [v1] Wed, 15 Mar 2023 17:15:04 UTC (3,853 KB) [v2] Thu, 16 Mar 2023 04:59:24 UTC (3,855 KB) [v3] Mon, 27 Mar 2023 17:46:54 UTC (4,016 KB) [v4] Tue, 19 Dec 2023 00:34:40 UTC (3,850 KB) [v5] Fri, 1 Mar 2024 16:30:27 UTC (3,849 KB) [v6] Mon, 4 Mar 2024 06:01:33 UTC (3,849 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.08774'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.08774'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.08774'}]
2023-03-19	LERF: Language Embedded Radiance Fields	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2303.09553	LERF: Language Embedded Radiance Fields	https://twitter.com/dair_ai/status/1637456915658686465?s=20		2303.09553	['Justin Kerr', 'Chung Min Kim', 'Ken Goldberg', 'Angjoo Kanazawa', 'Matthew Tancik']	ct:Humans describe the physical world using natural language to refer to specific 3D locations based on a vast range of properties: visual appearance, semantics, abstract associations, or actionable affordances. In this work we propose Language Embedded Radiance Fields (LERFs), a method for grounding language embeddings from off-the-shelf models like CLIP into NeRF, which enable these types of open-ended language queries in 3D. LERF learns a dense, multi-scale language field inside NeRF by volume rendering CLIP embeddings along training rays, supervising these embeddings across training views to provide multi-view consistency and smooth the underlying language field. After optimization, LERF can extract 3D relevancy maps for a broad range of language prompts interactively in real-time, which has potential use cases in robotics, understanding vision-language models, and interacting with 3D scenes. LERF enables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings without relying on region proposals or masks, supporting long-tail open-vocabulary queries hierarchically across the volume. The project website can be found atthis https URL.	t website can be found atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)']	{'pdf': '/pdf/2303.09553', 'html': None, 'tex': '/src/2303.09553', 'doi': 'https://doi.org/10.48550/arXiv.2303.09553'}	Submission history From: Justin Kerr [ view email ] [v1] Thu, 16 Mar 2023 17:59:20 UTC (37,264 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.09553'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.09553'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.09553'}]
2023-03-19	An Overview on Language Models: Recent Developments and Outlook	Computation and Language	https://arxiv.org/abs/2303.05759	An Overview on Language Models: Recent Developments and Outlook	https://twitter.com/omarsar0/status/1635273656858460162?s=20		2303.05759	['Chengwei Wei', 'Yun-Cheng Wang', 'Bin Wang', 'C.-C. Jay Kuo']	ct:Language modeling studies the probability distributions over strings of texts. It is one of the most fundamental tasks in natural language processing (NLP). It has been widely used in text generation, speech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict the probability of linguistic sequences in a causal manner, while pre-trained language models (PLMs) cover broader concepts and can be used in both causal sequential modeling and fine-tuning for downstream applications. PLMs have their own training paradigms (usually self-supervised) and serve as foundation models in modern NLP systems. This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods, evaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and PLMs and shed light on the future directions of language modeling in the pre-trained era.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2303.05759', 'html': None, 'tex': '/src/2303.05759', 'doi': 'https://doi.org/10.48550/arXiv.2303.05759'}	Submission history From: Chengwei Wei [ view email ] [v1] Fri, 10 Mar 2023 07:55:00 UTC (2,227 KB) [v2] Mon, 3 Jul 2023 05:52:04 UTC (2,043 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.05759'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.05759'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.05759'}]
2023-03-19	Eliciting Latent Predictions from Transformers with the Tuned Lens	Machine Learning	https://arxiv.org/abs/2303.08112	Eliciting Latent Predictions from Transformers with the Tuned Lens	https://twitter.com/dair_ai/status/1637456919819440130?s=20		2303.08112	['Nora Belrose', 'Zach Furman', 'Logan Smith', 'Danny Halawi', 'Igor Ostrovsky', 'Lev McKinney', 'Stella Biderman', 'Jacob Steinhardt']	ct:We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found atthis https URL.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.08112', 'html': None, 'tex': '/src/2303.08112', 'doi': 'https://doi.org/10.48550/arXiv.2303.08112'}	Submission history From: Stella Biderman [ view email ] [v1] Tue, 14 Mar 2023 17:47:09 UTC (1,032 KB) [v2] Wed, 15 Mar 2023 18:01:55 UTC (1,298 KB) [v3] Tue, 29 Aug 2023 22:55:27 UTC (1,317 KB) [v4] Sun, 26 Nov 2023 17:05:42 UTC (1,315 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.08112'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.08112'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.08112'}]
2023-03-19	Meet in the Middle: A New Pre-training Paradigm	Computation and Language	https://arxiv.org/abs/2303.07295	Meet in the Middle: A New Pre-training Paradigm	https://twitter.com/dair_ai/status/1637456922004561920?s=20		2303.07295	['Anh Nguyen', 'Nikos Karampatziakis', 'Weizhu Chen']	ct:Most language models (LMs) are trained and applied in an autoregressive left-to-right fashion, assuming that the next token only depends on the preceding ones. However, this assumption ignores the potential benefits of using the full sequence information during training, and the possibility of having context from both sides during inference. In this paper, we propose a new pre-training paradigm with techniques that jointly improve the training data efficiency and the capabilities of the LMs in the infilling task. The first is a training objective that aligns the predictions of a left-to-right LM with those of a right-to-left LM, trained on the same data but in reverse order. The second is a bidirectional inference procedure that enables both LMs to meet in the middle. We show the effectiveness of our pre-training paradigm with extensive experiments on both programming and natural language models, outperforming strong baselines.	es, 2 figures	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.07295', 'html': None, 'tex': '/src/2303.07295', 'doi': 'https://doi.org/10.48550/arXiv.2303.07295'}	Submission history From: Nikos Karampatziakis [ view email ] [v1] Mon, 13 Mar 2023 17:17:11 UTC (170 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.07295'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.07295'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.07295'}]
2023-03-19	Resurrecting Recurrent Neural Networks for Long Sequences	Machine Learning	https://arxiv.org/abs/2303.06349	Resurrecting Recurrent Neural Networks for Long Sequences	https://twitter.com/dair_ai/status/1637456923795521537?s=20		2303.06349	['Antonio Orvieto', 'Samuel L Smith', 'Albert Gu', 'Anushan Fernando', 'Caglar Gulcehre', 'Razvan Pascanu', 'Soham De']	ct:Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.	es, 9 figures	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.06349', 'html': None, 'tex': '/src/2303.06349', 'doi': 'https://doi.org/10.48550/arXiv.2303.06349'}	Submission history From: Antonio Orvieto [ view email ] [v1] Sat, 11 Mar 2023 08:53:11 UTC (3,632 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.06349'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.06349'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.06349'}]
2023-03-19	UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation	Computation and Language	https://arxiv.org/abs/2303.08518	UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation	https://twitter.com/dair_ai/status/1637456925779456000?s=20		2303.08518	['Daixuan Cheng', 'Shaohan Huang', 'Junyu Bi', 'Yuefeng Zhan', 'Jianfeng Liu', 'Yujing Wang', 'Hao Sun', 'Furu Wei', 'Denvy Deng', 'Qi Zhang']	ct:Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available atthis https URL.	2023 Main Conference	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2303.08518', 'html': 'https://arxiv.org/html/2303.08518v4', 'tex': '/src/2303.08518', 'doi': 'https://doi.org/10.48550/arXiv.2303.08518'}	Submission history From: Daixuan Cheng [ view email ] [v1] Wed, 15 Mar 2023 10:53:49 UTC (950 KB) [v2] Wed, 22 Mar 2023 11:29:48 UTC (950 KB) [v3] Wed, 11 Oct 2023 05:40:41 UTC (950 KB) [v4] Sat, 16 Dec 2023 06:50:09 UTC (981 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.08518'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.08518'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.08518'}]
2023-03-19	NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2303.09431	NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes	https://twitter.com/dair_ai/status/1637456929705295873?s=20		2303.09431	['Marie-Julie Rakotosaona', 'Fabian Manhardt', 'Diego Martin Arroyo', 'Michael Niemeyer', 'Abhijit Kundu', 'Federico Tombari']	ct:With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis has recently made a big leap forward. At the core, NeRF proposes that each 3D point can emit radiance, allowing to conduct view synthesis using differentiable volumetric rendering. While neural radiance fields can accurately represent 3D scenes for computing the image rendering, 3D meshes are still the main scene representation supported by most computer graphics and simulation pipelines, enabling tasks such as real time rendering and physics-based simulations. Obtaining 3D meshes from neural radiance fields still remains an open challenge since NeRFs are optimized for view synthesis, not enforcing an accurate underlying geometry on the radiance field. We thus propose a novel compact and flexible architecture that enables easy 3D surface reconstruction from any NeRF-driven approach. Upon having trained the radiance field, we distill the volumetric 3D representation into a Signed Surface Approximation Network, allowing easy extraction of the 3D mesh and appearance. Our final 3D mesh is physically accurate and can be rendered in real time on an array of devices.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2303.09431', 'html': None, 'tex': '/src/2303.09431', 'doi': 'https://doi.org/10.48550/arXiv.2303.09431'}	Submission history From: Marie-Julie Rakotosaona [ view email ] [v1] Thu, 16 Mar 2023 16:06:03 UTC (39,934 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.09431'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.09431'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.09431'}]
2023-03-19	FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU	Machine Learning	https://arxiv.org/abs/2303.06865	High-throughput Generative Inference of Large Language Models with a Single GPU	https://twitter.com/dair_ai/status/1637456931429183489?s=20	"{""Code"": ""https://github.com/FMInference/FlexGen""}"	2303.06865	['Ying Sheng', 'Lianmin Zheng', 'Binhang Yuan', 'Zhuohan Li', 'Max Ryabinin', 'Daniel Y. Fu', 'Zhiqiang Xie', 'Beidi Chen', 'Clark Barrett', 'Joseph E. Gonzalez', 'Percy Liang', 'Christopher Ré', 'Ion Stoica', 'Ce Zhang']	ct:The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available atthis https URL		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Performance (cs.PF)']	{'pdf': '/pdf/2303.06865', 'html': None, 'tex': '/src/2303.06865', 'doi': 'https://doi.org/10.48550/arXiv.2303.06865'}	Submission history From: Ying Sheng [ view email ] [v1] Mon, 13 Mar 2023 05:19:28 UTC (192 KB) [v2] Mon, 12 Jun 2023 07:48:53 UTC (351 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.06865'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.06865'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.06865'}]
2023-03-12	PaLM-E: An Embodied Multimodal Language Model	Machine Learning	https://arxiv.org/abs/2303.03378	PaLM-E: An Embodied Multimodal Language Model	https://twitter.com/dair_ai/status/1634919222420836358?s=20	"{""Demo"": ""https://palm-e.github.io/""}"	2303.03378	['Danny Driess', 'Fei Xia', 'Mehdi S. M. Sajjadi', 'Corey Lynch', 'Aakanksha Chowdhery', 'Brian Ichter', 'Ayzaan Wahid', 'Jonathan Tompson', 'Quan Vuong', 'Tianhe Yu', 'Wenlong Huang', 'Yevgen Chebotar', 'Pierre Sermanet', 'Daniel Duckworth', 'Sergey Levine', 'Vincent Vanhoucke', 'Karol Hausman', 'Marc Toussaint', 'Klaus Greff', 'Andy Zeng', 'Igor Mordatch', 'Pete Florence']	ct:Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Robotics (cs.RO)']	{'pdf': '/pdf/2303.03378', 'html': None, 'tex': '/src/2303.03378', 'doi': 'https://doi.org/10.48550/arXiv.2303.03378'}	Submission history From: Danny Driess [ view email ] [v1] Mon, 6 Mar 2023 18:58:06 UTC (10,064 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.03378'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.03378'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.03378'}]
2023-03-12	Prismer: A Vision-Language Model with Multi-Task Experts	Machine Learning	https://arxiv.org/abs/2303.02506	Prismer: A Vision-Language Model with An Ensemble of Experts	https://twitter.com/dair_ai/status/1634919224505257985?s=20	"{""GitHub"": ""https://github.com/NVlabs/Prismer"", ""Project"": ""https://shikun.io/projects/prismer""}"	2303.02506	['Shikun Liu', 'Linxi Fan', 'Edward Johns', 'Zhiding Yu', 'Chaowei Xiao', 'Anima Anandkumar']	ct:Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of task-specific experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from multiple readily-available, pre-trained experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-arts, whilst requiring up to two orders of magnitude less training data. Code is available atthis https URL.	hed at TMLR 2024. Project Page:this https URLCode:this https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2303.02506', 'html': 'https://arxiv.org/html/2303.02506v3', 'tex': '/src/2303.02506', 'doi': 'https://doi.org/10.48550/arXiv.2303.02506'}	Submission history From: Shikun Liu [ view email ] [v1] Sat, 4 Mar 2023 21:22:47 UTC (12,817 KB) [v2] Sun, 12 Mar 2023 02:30:16 UTC (12,817 KB) [v3] Thu, 18 Jan 2024 22:09:40 UTC (12,820 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.02506'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.02506'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.02506'}]
2023-03-12	Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2303.04671	Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models	https://twitter.com/dair_ai/status/1634919226396794882?s=20	"{""GitHub"": ""https://github.com/microsoft/visual-chatgpt""}"	2303.04671	['Chenfei Wu', 'Shengming Yin', 'Weizhen Qi', 'Xiaodong Wang', 'Zecheng Tang', 'Nan Duan']	ct:ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \url{this https URL}.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2303.04671', 'html': None, 'tex': '/src/2303.04671', 'doi': 'https://doi.org/10.48550/arXiv.2303.04671'}	Submission history From: Chenfei Wu [ view email ] [v1] Wed, 8 Mar 2023 15:50:02 UTC (2,116 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.04671'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.04671'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.04671'}]
2023-03-12	A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT	Artificial Intelligence	https://arxiv.org/abs/2303.04226	A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT	https://twitter.com/dair_ai/status/1634919228339003393?s=20		2303.04226	['Yihan Cao', 'Siyu Li', 'Yixin Liu', 'Zhiling Yan', 'Yutong Dai', 'Philip S. Yu', 'Lichao Sun']	ct:Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.	es, 15 figures	['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.04226', 'html': None, 'tex': '/src/2303.04226', 'doi': 'https://doi.org/10.48550/arXiv.2303.04226'}	Submission history From: Yihan Cao [ view email ] [v1] Tue, 7 Mar 2023 20:36:13 UTC (1,647 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.04226'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.04226'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.04226'}]
2023-03-12	Larger language models do in-context learning differently	Computation and Language	https://arxiv.org/abs/2303.03846	Larger language models do in-context learning differently	https://twitter.com/dair_ai/status/1634919230461345797?s=20		2303.03846	['Jerry Wei', 'Jason Wei', 'Yi Tay', 'Dustin Tran', 'Albert Webson', 'Yifeng Lu', 'Xinyun Chen', 'Hanxiao Liu', 'Da Huang', 'Denny Zhou', 'Tengyu Ma']	ct:We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2303.03846', 'html': None, 'tex': '/src/2303.03846', 'doi': 'https://doi.org/10.48550/arXiv.2303.03846'}	Submission history From: Jerry Wei [ view email ] [v1] Tue, 7 Mar 2023 12:24:17 UTC (452 KB) [v2] Wed, 8 Mar 2023 07:37:43 UTC (452 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.03846'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.03846'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.03846'}]
2023-03-12	Hyena Hierarchy: Towards Larger Convolutional Language Models	Machine Learning	https://arxiv.org/abs/2302.10866	Hyena Hierarchy: Towards Larger Convolutional Language Models	https://twitter.com/dair_ai/status/1634919234835980289?s=20	"{""Code"": ""https://github.com/HazyResearch/safari"", ""Blog"": ""https://ermongroup.github.io/blog/hyena/""}"	2302.10866	['Michael Poli', 'Stefano Massaroli', 'Eric Nguyen', 'Daniel Y. Fu', 'Tri Dao', 'Stephen Baccus', 'Yoshua Bengio', 'Stefano Ermon', 'Christopher Ré']	ct:Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.	onal details	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2302.10866', 'html': None, 'tex': '/src/2302.10866', 'doi': 'https://doi.org/10.48550/arXiv.2302.10866'}	Submission history From: Michael Poli [ view email ] [v1] Tue, 21 Feb 2023 18:29:25 UTC (2,910 KB) [v2] Mon, 6 Mar 2023 01:26:15 UTC (2,911 KB) [v3] Wed, 19 Apr 2023 20:08:39 UTC (2,911 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.10866'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.10866'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.10866'}]
2023-03-12	OpenICL: An Open-Source Framework for In-context Learning	Computation and Language	https://arxiv.org/abs/2303.02913	OpenICL: An Open-Source Framework for In-context Learning	https://twitter.com/dair_ai/status/1634919236954132480?s=20	"{""Repo"": ""https://github.com/Shark-NLP/OpenICL""}"	2303.02913	['Zhenyu Wu', 'YaoXiang Wang', 'Jiacheng Ye', 'Jiangtao Feng', 'Jingjing Xu', 'Yu Qiao', 'Zhiyong Wu']	ct:In recent years, In-context Learning (ICL) has gained increasing attention and emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates. However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components. To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs. It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research. The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released atthis https URL		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2303.02913', 'html': None, 'tex': '/src/2303.02913', 'doi': 'https://doi.org/10.48550/arXiv.2303.02913'}	Submission history From: Zhenyu Wu [ view email ] [v1] Mon, 6 Mar 2023 06:20:25 UTC (7,388 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.02913'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.02913'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.02913'}]
2023-03-12	MathPrompter: Mathematical Reasoning using Large Language Models	Computation and Language	https://arxiv.org/abs/2303.05398	MathPrompter: Mathematical Reasoning using Large Language Models	https://twitter.com/dair_ai/status/1634919239030280197?s=20		2303.05398	['Shima Imani', 'Liang Du', 'Harsh Shrivastava']	ct:Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose `MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple Algebraic expressions or Python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the MultiArith dataset ($78.7\%\rightarrow92.5\%$) evaluated using 175B parameter GPT-based LLM.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2303.05398', 'html': None, 'tex': '/src/2303.05398', 'doi': 'https://doi.org/10.48550/arXiv.2303.05398'}	Submission history From: Harsh Shrivastava [ view email ] [v1] Sat, 4 Mar 2023 04:43:49 UTC (306 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.05398'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.05398'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.05398'}]
2023-03-12	Scaling up GANs for Text-to-Image Synthesis	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2303.05511	Scaling up GANs for Text-to-Image Synthesis	https://twitter.com/dair_ai/status/1634919241198751744?s=20	"{""Project"": ""https://mingukkang.github.io/GigaGAN/""}"	2303.05511	['Minguk Kang', 'Jun-Yan Zhu', 'Richard Zhang', 'Jaesik Park', 'Eli Shechtman', 'Sylvain Paris', 'Taesung Park']	ct:The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL-E 2, auto-regressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that naÏvely increasing the capacity of the StyleGAN architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel pixels in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.	023. Project webpage atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2303.05511', 'html': None, 'tex': '/src/2303.05511', 'doi': 'https://doi.org/10.48550/arXiv.2303.05511'}	Submission history From: Taesung Park [ view email ] [v1] Thu, 9 Mar 2023 18:59:47 UTC (17,507 KB) [v2] Mon, 19 Jun 2023 07:01:08 UTC (46,986 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.05511'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.05511'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.05511'}]
2023-03-05	Language Is Not All You Need: Aligning Perception with Language Models	Computation and Language	https://arxiv.org/abs/2302.14045	Language Is Not All You Need: Aligning Perception with Language Models	https://twitter.com/dair_ai/status/1632383312550416384?s=20		2302.14045	['Shaohan Huang', 'Li Dong', 'Wenhui Wang', 'Yaru Hao', 'Saksham Singhal', 'Shuming Ma', 'Tengchao Lv', 'Lei Cui', 'Owais Khan Mohammed', 'Barun Patra', 'Qiang Liu', 'Kriti Aggarwal', 'Zewen Chi', 'Johan Bjorck', 'Vishrav Chaudhary', 'Subhojit Som', 'Xia Song', 'Furu Wei']	ct:A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.		['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2302.14045', 'html': None, 'tex': '/src/2302.14045', 'doi': 'https://doi.org/10.48550/arXiv.2302.14045'}	Submission history From: Li Dong [ view email ] [v1] Mon, 27 Feb 2023 18:55:27 UTC (3,490 KB) [v2] Wed, 1 Mar 2023 11:04:51 UTC (3,490 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.14045'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.14045'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.14045'}]
2023-03-05	EvoPrompting: Language Models for Code-Level Neural Architecture Search	Neural and Evolutionary Computing	https://arxiv.org/abs/2302.14838	EvoPrompting: Language Models for Code-Level Neural Architecture Search	https://twitter.com/dair_ai/status/1632383317302562816?s=20		2302.14838	['Angelica Chen', 'David M. Dohan', 'David R. So']	ct:Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.	S 2023	['Neural and Evolutionary Computing (cs.NE)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2302.14838', 'html': None, 'tex': '/src/2302.14838', 'doi': 'https://doi.org/10.48550/arXiv.2302.14838'}	Submission history From: Angelica Chen [ view email ] [v1] Tue, 28 Feb 2023 18:37:25 UTC (1,789 KB) [v2] Mon, 2 Oct 2023 14:26:42 UTC (1,789 KB) [v3] Thu, 16 Nov 2023 18:02:19 UTC (9,576 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.14838'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.14838'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.14838'}]
2023-03-05	Consistency Models	Machine Learning	https://arxiv.org/abs/2303.01469	Consistency Models	https://twitter.com/dair_ai/status/1632383319152132096?s=20		2303.01469	['Yang Song', 'Prafulla Dhariwal', 'Mark Chen', 'Ilya Sutskever']	ct:Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.	023	['Machine Learning (cs.LG)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2303.01469', 'html': None, 'tex': '/src/2303.01469', 'doi': 'https://doi.org/10.48550/arXiv.2303.01469'}	Submission history From: Yang Song [ view email ] [v1] Thu, 2 Mar 2023 18:30:16 UTC (44,064 KB) [v2] Wed, 31 May 2023 06:17:10 UTC (28,910 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.01469'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.01469'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.01469'}]
2023-03-05	Goal Driven Discovery of Distributional Differences via Language Descriptions	Computation and Language	https://arxiv.org/abs/2302.14233	Goal Driven Discovery of Distributional Differences via Language Descriptions	https://twitter.com/dair_ai/status/1632383321035374593?s=20	"{""Code"": ""https://github.com/ruiqi-zhong/D5""}"	2302.14233	['Ruiqi Zhong', 'Peter Zhang', 'Steve Li', 'Jinwoo Ahn', 'Dan Klein', 'Jacob Steinhardt']	"ct:Mining large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a research goal ""$\textit{comparing the side effects of drug A and drug B}$"" and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). The output is a language description (discovery) of how these corpora differ (patients taking drug A ""$\textit{mention feelings of paranoia}$"" more often). We build a D5 system, and to quantitatively measure its performance, we 1) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health, and 2) propose a set of unified evaluation metrics: validity, relevance, novelty, and significance. With the dataset and the unified metrics, we confirm that language models can use the goals to propose more relevant, novel, and significant candidate discoveries. Finally, our system produces discoveries previously unknown to the authors on a wide range of applications in OpenD5, including temporal and demographic differences in discussion topics, political stances and stereotypes in speech, insights in commercial reviews, and error patterns in NLP models."		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2302.14233', 'html': None, 'tex': '/src/2302.14233', 'doi': 'https://doi.org/10.48550/arXiv.2302.14233'}	Submission history From: Ruiqi Zhong [ view email ] [v1] Tue, 28 Feb 2023 01:32:32 UTC (576 KB) [v2] Wed, 25 Oct 2023 00:43:04 UTC (557 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.14233'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.14233'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.14233'}]
2023-03-05	Language-Driven Representation Learning for Robotics	Robotics	https://arxiv.org/abs/2302.12766	Language-Driven Representation Learning for Robotics	https://twitter.com/dair_ai/status/1632383327154888704?s=20	"{""Models"": ""https://github.com/siddk/voltron-robotics"", ""Evaluation"": ""https://github.com/siddk/voltron-evaluation""}"	2302.12766	['Siddharth Karamcheti', 'Suraj Nair', 'Annie S. Chen', 'Thomas Kollar', 'Chelsea Finn', 'Dorsa Sadigh', 'Percy Liang']	ct:Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems $\unicode{x2013}$ a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron's language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.	es, 15 Figures	['Robotics (cs.RO)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2302.12766', 'html': None, 'tex': '/src/2302.12766', 'doi': 'https://doi.org/10.48550/arXiv.2302.12766'}	Submission history From: Siddharth Karamcheti [ view email ] [v1] Fri, 24 Feb 2023 17:29:31 UTC (21,715 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.12766'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.12766'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.12766'}]
2023-03-05	Dropout Reduces Underfitting	Machine Learning	https://arxiv.org/abs/2303.01500	Dropout Reduces Underfitting	https://twitter.com/dair_ai/status/1632383328920666121?s=20		2303.01500	['Zhuang Liu', 'Zhiqiu Xu', 'Joseph Jin', 'Zhiqiang Shen', 'Trevor Darrell']	ct:Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations and is only activated later in training. Experiments on ImageNet and various vision tasks demonstrate that our methods consistently improve generalization accuracy. Our results encourage more research on understanding regularization in deep learning and our methods can be useful tools for future neural network training, especially in the era of large data. Code is available atthis https URL.	023	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2303.01500', 'html': None, 'tex': '/src/2303.01500', 'doi': 'https://doi.org/10.48550/arXiv.2303.01500'}	Submission history From: Zhuang Liu [ view email ] [v1] Thu, 2 Mar 2023 18:59:15 UTC (533 KB) [v2] Wed, 31 May 2023 17:47:18 UTC (511 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2303.01500'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2303.01500'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2303.01500'}]
2023-03-05	Enabling Conversational Interaction with Mobile UI using Large Language Models	Human-Computer Interaction	https://arxiv.org/abs/2209.08655	Enabling Conversational Interaction with Mobile UI using Large Language Models	https://twitter.com/dair_ai/status/1632383331286253568?s=20		2209.08655	['Bryan Wang', 'Gang Li', 'Yang Li']	ct:Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.	hed as a conference paper at CHI 2023	['Human-Computer Interaction (cs.HC)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2209.08655', 'html': None, 'tex': '/src/2209.08655', 'doi': 'https://doi.org/10.48550/arXiv.2209.08655'}	Submission history From: Yang Li [ view email ] [v1] Sun, 18 Sep 2022 20:58:39 UTC (11,081 KB) [v2] Fri, 17 Feb 2023 21:24:09 UTC (11,117 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2209.08655'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2209.08655'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2209.08655'}]
2023-02-26	Composer: Creative and Controllable Image Synthesis with Composable Conditions	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2302.09778	Composer: Creative and Controllable Image Synthesis with Composable Conditions	https://twitter.com/dair_ai/status/1629845537913548802?s=20	"{""Project"": ""https://damo-vilab.github.io/composer-page/"", ""GitHub"": ""https://github.com/damo-vilab/composer""}"	2302.09778	['Lianghua Huang', 'Di Chen', 'Yu Liu', 'Yujun Shen', 'Deli Zhao', 'Jingren Zhou']	ct:Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.	t page:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)']	{'pdf': '/pdf/2302.09778', 'html': None, 'tex': '/src/2302.09778', 'doi': 'https://doi.org/10.48550/arXiv.2302.09778'}	Submission history From: Lianghua Huang Dr. [ view email ] [v1] Mon, 20 Feb 2023 05:48:41 UTC (27,841 KB) [v2] Wed, 22 Feb 2023 02:14:55 UTC (27,841 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.09778'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.09778'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.09778'}]
2023-02-26	The Wisdom of Hindsight Makes Language Models Better Instruction Followers	Computation and Language	https://arxiv.org/abs/2302.05206	The Wisdom of Hindsight Makes Language Models Better Instruction Followers	https://twitter.com/dair_ai/status/1629845539964481537?s=20	"{""GitHub"": ""https://github.com/tianjunz/HIR""}"	2302.05206	['Tianjun Zhang', 'Fangchen Liu', 'Justin Wong', 'Pieter Abbeel', 'Joseph E. Gonzalez']	ct:Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2302.05206', 'html': None, 'tex': '/src/2302.05206', 'doi': 'https://doi.org/10.48550/arXiv.2302.05206'}	Submission history From: Fangchen Liu [ view email ] [v1] Fri, 10 Feb 2023 12:16:38 UTC (3,177 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.05206'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.05206'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.05206'}]
2023-02-26	Active Prompting with Chain-of-Thought for Large Language Models	Computation and Language	https://arxiv.org/abs/2302.12246	Active Prompting with Chain-of-Thought for Large Language Models	https://twitter.com/dair_ai/status/1629845541847724033?s=20	"{""Code"": ""https://github.com/shizhediao/active-prompt""}"	2302.12246	['Shizhe Diao', 'Pengcheng Wang', 'Yong Lin', 'Rui Pan', 'Xiang Liu', 'Tong Zhang']	ct:The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available atthis https URL.	hed in ACL 2024	['Computation and Language (cs.CL)']	{'pdf': '/pdf/2302.12246', 'html': 'https://arxiv.org/html/2302.12246v5', 'tex': '/src/2302.12246', 'doi': 'https://doi.org/10.48550/arXiv.2302.12246'}	Submission history From: Shizhe Diao [ view email ] [v1] Thu, 23 Feb 2023 18:58:59 UTC (511 KB) [v2] Sun, 26 Feb 2023 15:18:50 UTC (511 KB) [v3] Tue, 23 May 2023 15:43:28 UTC (511 KB) [v4] Fri, 7 Jun 2024 02:51:25 UTC (705 KB) [v5] Sun, 21 Jul 2024 08:01:00 UTC (705 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.12246'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.12246'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.12246'}]
2023-02-26	Modular Deep Learning	Machine Learning	https://arxiv.org/abs/2302.11529	Modular Deep Learning	https://twitter.com/dair_ai/status/1629845544037228551?s=20	"{""Project"": ""https://www.ruder.io/modular-deep-learning/""}"	2302.11529	['Jonas Pfeiffer', 'Sebastian Ruder', 'Ivan Vulić', 'Edoardo Maria Ponti']	ct:Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available atthis https URL.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2302.11529', 'html': 'https://arxiv.org/html/2302.11529v2', 'tex': '/src/2302.11529', 'doi': 'https://doi.org/10.48550/arXiv.2302.11529'}	Submission history From: Jonas Pfeiffer [ view email ] [v1] Wed, 22 Feb 2023 18:11:25 UTC (4,164 KB) [v2] Sat, 27 Jan 2024 12:01:57 UTC (4,912 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.11529'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.11529'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.11529'}]
2023-02-26	Recitation-Augmented Language Models	Computation and Language	https://arxiv.org/abs/2210.01296	Recitation-Augmented Language Models	https://twitter.com/dair_ai/status/1629845546276995075?s=20		2210.01296	['Zhiqing Sun', 'Xuezhi Wang', 'Yi Tay', 'Yiming Yang', 'Denny Zhou']	"ct:We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at ""this https URL."	ed at ICLR 2023. v2 adds the Codex results	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2210.01296', 'html': None, 'tex': '/src/2210.01296', 'doi': 'https://doi.org/10.48550/arXiv.2210.01296'}	Submission history From: Zhiqing Sun [ view email ] [v1] Tue, 4 Oct 2022 00:49:20 UTC (805 KB) [v2] Thu, 16 Feb 2023 06:17:46 UTC (702 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2210.01296'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2210.01296'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2210.01296'}]
2023-02-26	Learning Performance-Improving Code Edits	Software Engineering	https://arxiv.org/abs/2302.07867	Learning Performance-Improving Code Edits	https://twitter.com/dair_ai/status/1629845548210561029?s=20		2302.07867	['Alexander Shypula', 'Aman Madaan', 'Yimeng Zeng', 'Uri Alon', 'Jacob Gardner', 'Milad Hashemi', 'Graham Neubig', 'Parthasarathy Ranganathan', 'Osbert Bastani', 'Amir Yazdanbakhsh']	"ct:With the decline of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks. To that end, we introduce a framework for adapting LLMs to high-level program optimization. First, we curate a dataset of performance-improving edits made by human programmers of over 77,000 competitive C++ programming submission pairs, accompanied by extensive unit tests. A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious ""improvements."" To isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry. Next, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play. A combination of these techniques achieves a mean speedup of 6.86 with eight generations, higher than average optimizations from individual programmers (3.66). Using our model's fastest generations, we set a new upper limit on the fastest speedup possible for our dataset at 9.64 compared to using the fastest human submissions available (9.56)."	hed as a conference paper at ICLR 2024 (Spotlight). Project website:this https URL	['Software Engineering (cs.SE)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Performance (cs.PF)']	{'pdf': '/pdf/2302.07867', 'html': None, 'tex': '/src/2302.07867', 'doi': 'https://doi.org/10.48550/arXiv.2302.07867'}	Submission history From: Alexander Shypula [ view email ] [v1] Wed, 15 Feb 2023 18:59:21 UTC (1,325 KB) [v2] Thu, 16 Feb 2023 18:55:42 UTC (2,027 KB) [v3] Tue, 21 Feb 2023 18:50:40 UTC (2,026 KB) [v4] Wed, 8 Nov 2023 18:21:44 UTC (3,082 KB) [v5] Fri, 26 Apr 2024 16:41:55 UTC (362 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.07867'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.07867'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.07867'}]
2023-02-26	Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection	Cryptography and Security	https://arxiv.org/abs/2302.12173	More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models	https://twitter.com/dair_ai/status/1629845550152523777?s=20		2302.12173	['Kai Greshake', 'Sahar Abdelnabi', 'Shailesh Mishra', 'Christoph Endres', 'Thorsten Holz', 'Mario Fritz']	ct:Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.		['Cryptography and Security (cs.CR)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2302.12173', 'html': None, 'tex': '/src/2302.12173', 'doi': 'https://doi.org/10.48550/arXiv.2302.12173'}	Submission history From: Sahar Abdelnabi [ view email ] [v1] Thu, 23 Feb 2023 17:14:38 UTC (5,052 KB) [v2] Fri, 5 May 2023 14:26:17 UTC (10,831 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.12173'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.12173'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.12173'}]
2023-02-26	Aligning Text-to-Image Models using Human Feedback	Machine Learning	https://arxiv.org/abs/2302.12192	Aligning Text-to-Image Models using Human Feedback	https://twitter.com/dair_ai/status/1629845552039968780?s=20		2302.12192	['Kimin Lee', 'Hao Liu', 'Moonkyung Ryu', 'Olivia Watkins', 'Yuqing Du', 'Craig Boutilier', 'Pieter Abbeel', 'Mohammad Ghavamzadeh', 'Shixiang Shane Gu']	ct:Deep generative models have shown impressive results in text-to-image synthesis. However, current text-to-image models often generate images that are inadequately aligned with text prompts. We propose a fine-tuning method for aligning such models using human feedback, comprising three stages. First, we collect human feedback assessing model output alignment from a set of diverse text prompts. We then use the human-labeled image-text dataset to train a reward function that predicts human feedback. Lastly, the text-to-image model is fine-tuned by maximizing reward-weighted likelihood to improve image-text alignment. Our method generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model. We also analyze several design choices and find that careful investigations on such design choices are important in balancing the alignment-fidelity tradeoffs. Our results demonstrate the potential for learning from human feedback to significantly improve text-to-image models.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2302.12192', 'html': None, 'tex': '/src/2302.12192', 'doi': 'https://doi.org/10.48550/arXiv.2302.12192'}	Submission history From: Kimin Lee [ view email ] [v1] Thu, 23 Feb 2023 17:34:53 UTC (20,327 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.12192'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.12192'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.12192'}]
2023-02-26	MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2302.12249	MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes	https://twitter.com/dair_ai/status/1629845554061606915?s=20		2302.12249	['Christian Reiser', 'Richard Szeliski', 'Dor Verbin', 'Pratul P. Srinivasan', 'Ben Mildenhall', 'Andreas Geiger', 'Jonathan T. Barron', 'Peter Hedman']	ct:Neural radiance fields enable state-of-the-art photorealistic view synthesis. However, existing radiance field representations are either too compute-intensive for real-time rendering or require too much memory to scale to large scenes. We present a Memory-Efficient Radiance Field (MERF) representation that achieves real-time rendering of large-scale scenes in a browser. MERF reduces the memory consumption of prior sparse volumetric radiance fields using a combination of a sparse feature grid and high-resolution 2D feature planes. To support large-scale unbounded scenes, we introduce a novel contraction function that maps scene coordinates into a bounded volume while still allowing for efficient ray-box intersection. We design a lossless procedure for baking the parameterization used during training into a model that achieves real-time rendering while still preserving the photorealistic view synthesis quality of a volumetric radiance field.	and interactive web demo available atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)']	{'pdf': '/pdf/2302.12249', 'html': None, 'tex': '/src/2302.12249', 'doi': 'https://doi.org/10.48550/arXiv.2302.12249'}	Submission history From: Christian Reiser [ view email ] [v1] Thu, 23 Feb 2023 18:59:07 UTC (4,465 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.12249'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.12249'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.12249'}]
2023-02-19	Symbolic Discovery of Optimization Algorithms	Machine Learning	https://arxiv.org/abs/2302.06675	Symbolic Discovery of Optimization Algorithms	https://twitter.com/dair_ai/status/1627671313874575362?s=20		2302.06675	['Xiangning Chen', 'Chen Liang', 'Da Huang', 'Esteban Real', 'Kaiyuan Wang', 'Yao Liu', 'Hieu Pham', 'Xuanyi Dong', 'Thang Luong', 'Cho-Jui Hsieh', 'Yifeng Lu', 'Quoc V. Le']	ct:We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\textit{zero-shot}$ and 91.1% $\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.	es, Lion is successfully deployed in production systems. We also add comparison with other automatically discovered optimizers	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Neural and Evolutionary Computing (cs.NE)']	{'pdf': '/pdf/2302.06675', 'html': None, 'tex': '/src/2302.06675', 'doi': 'https://doi.org/10.48550/arXiv.2302.06675'}	Submission history From: Xiangning Chen [ view email ] [v1] Mon, 13 Feb 2023 20:27:30 UTC (1,587 KB) [v2] Fri, 17 Feb 2023 06:43:54 UTC (1,588 KB) [v3] Wed, 26 Apr 2023 19:26:08 UTC (1,623 KB) [v4] Mon, 8 May 2023 21:49:57 UTC (1,626 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.06675'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.06675'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.06675'}]
2023-02-19	Transformer models: an introduction and catalog	Computation and Language	https://arxiv.org/abs/2302.07730	Transformer models: an introduction and catalog	https://twitter.com/dair_ai/status/1627671315678126082?s=20		2302.07730	['Xavier Amatriain', 'Ananth Sankar', 'Jie Bing', 'Praveen Kumar Bodigutla', 'Timothy J. Hazen', 'Michaeel Kazi']	ct:In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovations in Transformer models. Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT).		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2302.07730', 'html': 'https://arxiv.org/html/2302.07730v4', 'tex': '/src/2302.07730', 'doi': 'https://doi.org/10.48550/arXiv.2302.07730'}	Submission history From: Xavier Amatriain [ view email ] [v1] Sun, 12 Feb 2023 01:26:49 UTC (3,568 KB) [v2] Thu, 16 Feb 2023 05:31:15 UTC (3,568 KB) [v3] Thu, 25 May 2023 05:42:23 UTC (3,757 KB) [v4] Sun, 31 Mar 2024 21:20:30 UTC (3,757 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.07730'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.07730'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.07730'}]
2023-02-19	The Capacity for Moral Self-Correction in Large Language Models	Computation and Language	https://arxiv.org/abs/2302.07459	The Capacity for Moral Self-Correction in Large Language Models	https://twitter.com/dair_ai/status/1627671319100768260?s=20		2302.07459	['Deep Ganguli', 'Amanda Askell', 'Nicholas Schiefer', 'Thomas I. Liao', 'Kamilė Lukošiūtė', 'Anna Chen', 'Anna Goldie', 'Azalia Mirhoseini', 'Catherine Olsson', 'Danny Hernandez', 'Dawn Drain', 'Dustin Li', 'Eli Tran-Johnson', 'Ethan Perez', 'Jackson Kernion', 'Jamie Kerr', 'Jared Mueller', 'Joshua Landau', 'Kamal Ndousse', 'Karina Nguyen', 'Liane Lovitt', 'Michael Sellitto', 'Nelson Elhage', 'Noemi Mercado', 'Nova DasSarma', 'Oliver Rausch', 'Robert Lasenby', 'Robin Larson', 'Sam Ringer', 'Sandipan Kundu', 'Saurav Kadavath', 'Scott Johnston', 'Shauna Kravec', 'Sheer El Showk', 'Tamera Lanham', 'Timothy Telleen-Lawton', 'Tom Henighan', 'Tristan Hume', 'Yuntao Bai', 'Zac Hatfield-Dodds', 'Ben Mann', 'Dario Amodei', 'Nicholas Joseph', 'Sam McCandlish', 'Tom Brown', 'Christopher Olah', 'Jack Clark', 'Samuel R. Bowman', 'Jared Kaplan']	"ct:We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles."		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2302.07459', 'html': None, 'tex': '/src/2302.07459', 'doi': 'https://doi.org/10.48550/arXiv.2302.07459'}	Submission history From: Nicholas Schiefer [ view email ] [v1] Wed, 15 Feb 2023 04:25:40 UTC (430 KB) [v2] Sat, 18 Feb 2023 21:30:27 UTC (431 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.07459'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.07459'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.07459'}]
2023-02-19	Tuning computer vision models with task rewards	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2302.08242	Vision meets RL			2302.08242	['André Susano Pinto', 'Alexander Kolesnikov', 'Yuge Shi', 'Lucas Beyer', 'Xiaohua Zhai']	ct:Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks.	es	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2302.08242', 'html': None, 'tex': '/src/2302.08242', 'doi': 'https://doi.org/10.48550/arXiv.2302.08242'}	Submission history From: André Susano Pinto [ view email ] [v1] Thu, 16 Feb 2023 11:49:48 UTC (5,076 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.08242'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.08242'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.08242'}]
2023-02-19	Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment	Machine Learning	https://arxiv.org/abs/2302.00902	Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment	https://twitter.com/haoliuhl/status/1625273748629901312?s=20	"{""Code"": ""https://github.com/lhao499/lqae""}"	2302.00902	['Hao Liu', 'Wilson Yan', 'Pieter Abbeel']	ct:Recent progress in scaling up large language models has shown impressive capabilities in performing few-shot learning across a wide range of text-based tasks. However, a key limitation is that these language models fundamentally lack visual perception - a crucial attribute needed to extend these models to be able to interact with the real world and solve vision tasks, such as in visual-question answering and robotics. Prior works have largely connected image to text through pretraining and/or fine-tuning on curated image-text datasets, which can be a costly and expensive process. In order to resolve this limitation, we propose a simple yet effective approach called Language-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to align text-image data in an unsupervised manner by leveraging pretrained language models (e.g., BERT, RoBERTa). Our main idea is to encode image as sequences of text tokens by directly quantizing image embeddings using a pretrained language codebook. We then apply random masking followed by a BERT model, and have the decoder reconstruct the original image from BERT predicted text token embeddings. By doing so, LQAE learns to represent similar images with similar clusters of text tokens, thereby aligning these two modalities without the use of aligned text-image pairs. This enables few-shot image classification with large language models (e.g., GPT-3) as well as linear classification of images based on BERT text features. To the best of our knowledge, our work is the first work that uses unaligned images for multimodal tasks by leveraging the power of pretrained language models.	typos	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2302.00902', 'html': None, 'tex': '/src/2302.00902', 'doi': 'https://doi.org/10.48550/arXiv.2302.00902'}	Submission history From: Hao Liu [ view email ] [v1] Thu, 2 Feb 2023 06:38:44 UTC (669 KB) [v2] Fri, 3 Feb 2023 05:06:46 UTC (669 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.00902'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.00902'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.00902'}]
2023-02-19	Augmented Language Models: a Survey	Computation and Language	https://arxiv.org/abs/2302.07842	Augmented Language Models: a Survey	https://twitter.com/dair_ai/status/1627671324477820929?s=20		2302.07842	['Grégoire Mialon', 'Roberto Dessì', 'Maria Lomeli', 'Christoforos Nalmpantis', 'Ram Pasunuru', 'Roberta Raileanu', 'Baptiste Rozière', 'Timo Schick', 'Jane Dwivedi-Yu', 'Asli Celikyilmaz', 'Edouard Grave', 'Yann LeCun', 'Thomas Scialom']	ct:This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2302.07842', 'html': None, 'tex': '/src/2302.07842', 'doi': 'https://doi.org/10.48550/arXiv.2302.07842'}	Submission history From: Grégoire Mialon [ view email ] [v1] Wed, 15 Feb 2023 18:25:52 UTC (324 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.07842'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.07842'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.07842'}]
2023-02-19	Geometric Clifford Algebra Networks	Machine Learning	https://arxiv.org/abs/2302.06594	Geometric Clifford Algebra Networks	https://twitter.com/dair_ai/status/1627671326176473088?s=20		2302.06594	['David Ruhe', 'Jayesh K. Gupta', 'Steven de Keninck', 'Max Welling', 'Johannes Brandstetter']	ct:We propose Geometric Clifford Algebra Networks (GCANs) for modeling dynamical systems. GCANs are based on symmetry group transformations using geometric (Clifford) algebras. We first review the quintessence of modern (plane-based) geometric algebra, which builds on isometries encoded as elements of the $\mathrm{Pin}(p,q,r)$ group. We then propose the concept of group action layers, which linearly combine object transformations using pre-specified group actions. Together with a new activation and normalization scheme, these layers serve as adjustable $\textit{geometric templates}$ that can be refined via gradient descent. Theoretical advantages are strongly reflected in the modeling of three-dimensional rigid body transformations as well as large-scale fluid dynamics simulations, showing significantly improved performance over traditional methods.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2302.06594', 'html': None, 'tex': '/src/2302.06594', 'doi': 'https://doi.org/10.48550/arXiv.2302.06594'}	Submission history From: David Ruhe [ view email ] [v1] Mon, 13 Feb 2023 18:48:33 UTC (5,164 KB) [v2] Mon, 29 May 2023 16:51:59 UTC (5,154 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.06594'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.06594'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.06594'}]
2023-02-19	Auditing large language models: a three-layered approach	Computation and Language	https://arxiv.org/abs/2302.08500	Auditing large language models: a three-layered approach	https://twitter.com/dair_ai/status/1627671327950643200?s=20		2302.08500	['Jakob Mökander', 'Jonas Schuett', 'Hannah Rose Kirk', 'Luciano Floridi']	ct:Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.	es, 2 figures. AI Ethics (2023)	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2302.08500', 'html': None, 'tex': '/src/2302.08500', 'doi': 'https://doi.org/10.48550/arXiv.2302.08500'}	Submission history From: Hannah Rose Kirk Miss [ view email ] [v1] Thu, 16 Feb 2023 18:55:21 UTC (2,889 KB) [v2] Tue, 27 Jun 2023 07:40:15 UTC (2,937 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.08500'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.08500'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.08500'}]
2023-02-19	Energy Transformer	Machine Learning	https://arxiv.org/abs/2302.07253	Energy Transformer	https://twitter.com/dair_ai/status/1627671329561346050?s=20		2302.07253	['Benjamin Hoover', 'Yuchen Liang', 'Bao Pham', 'Rameswar Panda', 'Hendrik Strobelt', 'Duen Horng Chau', 'Mohammed J. Zaki', 'Dmitry Krotov']	ct:Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion task, and obtain strong quantitative results on the graph anomaly detection and graph classification tasks.		['Machine Learning (cs.LG)', 'Disordered Systems and Neural Networks (cond-mat.dis-nn)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Neurons and Cognition (q-bio.NC)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2302.07253', 'html': None, 'tex': '/src/2302.07253', 'doi': 'https://doi.org/10.48550/arXiv.2302.07253'}	Submission history From: Dmitry Krotov [ view email ] [v1] Tue, 14 Feb 2023 18:51:22 UTC (10,884 KB) [v2] Wed, 1 Nov 2023 00:14:30 UTC (10,881 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.07253'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.07253'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.07253'}]
2023-02-12	Toolformer: Language Models Can Teach Themselves to Use Tools	Computation and Language	https://arxiv.org/abs/2302.04761	Toolformer: Language Models Can Teach Themselves to Use Tools	https://twitter.com/dair_ai/status/1624832248691191808?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2302.04761	['Timo Schick', 'Jane Dwivedi-Yu', 'Roberto Dessì', 'Roberta Raileanu', 'Maria Lomeli', 'Luke Zettlemoyer', 'Nicola Cancedda', 'Thomas Scialom']	ct:Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2302.04761', 'html': None, 'tex': '/src/2302.04761', 'doi': 'https://doi.org/10.48550/arXiv.2302.04761'}	Submission history From: Timo Schick [ view email ] [v1] Thu, 9 Feb 2023 16:49:57 UTC (202 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.04761'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.04761'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.04761'}]
2023-02-12	Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents	Artificial Intelligence	https://arxiv.org/abs/2302.01560	Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents	https://twitter.com/dair_ai/status/1624832250717036548?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2302.01560	['Zihao Wang', 'Shaofei Cai', 'Guanzhou Chen', 'Anji Liu', 'Xiaojian Ma', 'Yitao Liang']	"ct:We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose ""$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and $\underline{S}$elect"" ($\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of the plan execution process and providing self-$\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released atthis https URL."	S 2023	['Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2302.01560', 'html': 'https://arxiv.org/html/2302.01560v3', 'tex': '/src/2302.01560', 'doi': 'https://doi.org/10.48550/arXiv.2302.01560'}	Submission history From: Zihao Wang [ view email ] [v1] Fri, 3 Feb 2023 06:06:27 UTC (4,379 KB) [v2] Sun, 29 Oct 2023 17:03:08 UTC (20,805 KB) [v3] Mon, 8 Jul 2024 05:56:47 UTC (20,655 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.01560'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.01560'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.01560'}]
2023-02-12	A Categorical Archive of ChatGPT Failures	Computation and Language	https://arxiv.org/abs/2302.03494	A Categorical Archive of ChatGPT Failures	https://twitter.com/dair_ai/status/1624832252587700230?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2302.03494	['Ali Borji']	ct:Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2302.03494', 'html': None, 'tex': '/src/2302.03494', 'doi': 'https://doi.org/10.48550/arXiv.2302.03494'}	Submission history From: Ali Borji [ view email ] [v1] Mon, 6 Feb 2023 04:21:59 UTC (8,437 KB) [v2] Fri, 10 Feb 2023 01:01:51 UTC (8,826 KB) [v3] Tue, 14 Feb 2023 09:26:35 UTC (8,831 KB) [v4] Sun, 19 Feb 2023 03:12:05 UTC (10,913 KB) [v5] Tue, 21 Feb 2023 05:27:25 UTC (11,144 KB) [v6] Thu, 23 Feb 2023 00:05:29 UTC (11,641 KB) [v7] Mon, 6 Mar 2023 09:34:38 UTC (10,441 KB) [v8] Mon, 3 Apr 2023 20:02:26 UTC (10,619 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.03494'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.03494'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.03494'}]
2023-02-12	Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery	Machine Learning	https://arxiv.org/abs/2302.03668	Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery	https://twitter.com/dair_ai/status/1624832254588465156?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2302.03668	['Yuxin Wen', 'Neel Jain', 'John Kirchenbauer', 'Micah Goldblum', 'Jonas Geiping', 'Tom Goldstein']	"ct:The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical ""hard"" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also ""soft"" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface.We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification."	es, 12 figures, Code is available atthis https URL	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2302.03668', 'html': None, 'tex': '/src/2302.03668', 'doi': 'https://doi.org/10.48550/arXiv.2302.03668'}	Submission history From: Yuxin Wen [ view email ] [v1] Tue, 7 Feb 2023 18:40:18 UTC (39,519 KB) [v2] Thu, 1 Jun 2023 12:26:45 UTC (37,291 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.03668'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.03668'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.03668'}]
2023-02-12	Data Selection for Language Models via Importance Resampling	Computation and Language	https://arxiv.org/abs/2302.03169	Data Selection for Language Models via Importance Resampling	https://twitter.com/dair_ai/status/1624832256400302080?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2302.03169	['Sang Michael Xie', 'Shibani Santurkar', 'Tengyu Ma', 'Percy Liang']	ct:Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given unlabeled target samples. Due to the scale and dimensionality of the raw text data, existing methods use simple heuristics or require human experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. We instantiate the DSIR framework with hashed n-gram features for efficiency, enabling the selection of 100M documents from the full Pile dataset in 4.5 hours. To measure whether hashed n-gram features preserve the aspects of the data that are relevant to the target, we define KL reduction, a data metric that measures the proximity between the selected pretraining data and the target on some feature space. Across 8 data selection methods (including expert selection), KL reduction on hashed n-gram features highly correlates with average downstream accuracy (r=0.82). When selecting data for continued pretraining on a specific domain, DSIR performs comparably to expert curation across 8 target distributions. When pretraining general-domain models (target is Wikipedia and books), DSIR improves over random selection and heuristic filtering baselines by 2-2.5% on the GLUE benchmark. Code is available atthis https URL.	S 2023	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2302.03169', 'html': None, 'tex': '/src/2302.03169', 'doi': 'https://doi.org/10.48550/arXiv.2302.03169'}	Submission history From: Sang Michael Xie [ view email ] [v1] Mon, 6 Feb 2023 23:57:56 UTC (811 KB) [v2] Tue, 24 Oct 2023 17:39:05 UTC (1,028 KB) [v3] Sat, 18 Nov 2023 21:33:01 UTC (1,021 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.03169'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.03169'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.03169'}]
2023-02-12	Structure and Content-Guided Video Synthesis with Diffusion Models	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2302.03011	Structure and Content-Guided Video Synthesis with Diffusion Models	https://twitter.com/dair_ai/status/1624832258296229889?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""Project"": ""https://research.runwayml.com/gen1""}"	2302.03011	['Patrick Esser', 'Johnathan Chiu', 'Parmida Atighehchian', 'Jonathan Granskog', 'Anastasis Germanidis']	ct:Text-guided generative diffusion models unlock powerful image creation and editing tools. While these have been extended to video generation, current approaches that edit the content of existing footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames. In this work, we present a structure and content-guided video diffusion model that edits videos based on visual or textual descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. Our model is trained jointly on images and videos which also exposes explicit control of temporal consistency through a novel guidance method. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.	t page atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2302.03011', 'html': None, 'tex': '/src/2302.03011', 'doi': 'https://doi.org/10.48550/arXiv.2302.03011'}	Submission history From: Patrick Esser [ view email ] [v1] Mon, 6 Feb 2023 18:50:23 UTC (46,317 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.03011'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.03011'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.03011'}]
2023-02-12	A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity	Computation and Language	https://arxiv.org/abs/2302.04023	A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity	https://twitter.com/dair_ai/status/1624832260213026819?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2302.04023	['Yejin Bang', 'Samuel Cahyawijaya', 'Nayeon Lee', 'Wenliang Dai', 'Dan Su', 'Bryan Wilie', 'Holy Lovenia', 'Ziwei Ji', 'Tiezheng Yu', 'Willy Chung', 'Quyet V. Do', 'Yan Xu', 'Pascale Fung']	"ct:This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn ""prompt engineering"" fashion. We also release codebase for evaluation set extraction."	es, AACL 2023	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2302.04023', 'html': None, 'tex': '/src/2302.04023', 'doi': 'https://doi.org/10.48550/arXiv.2302.04023'}	Submission history From: Yejin Bang [ view email ] [v1] Wed, 8 Feb 2023 12:35:34 UTC (7,402 KB) [v2] Tue, 28 Feb 2023 15:20:21 UTC (7,402 KB) [v3] Thu, 9 Nov 2023 07:42:55 UTC (9,189 KB) [v4] Tue, 28 Nov 2023 09:01:12 UTC (2,273 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.04023'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.04023'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.04023'}]
2023-02-12	Noise2Music: Text-conditioned Music Generation with Diffusion Models	Sound	https://arxiv.org/abs/2302.03917	Noise2Music: Text-conditioned Music Generation with Diffusion Models	https://twitter.com/dair_ai/status/1624832262163337220?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""Project"": ""https://google-research.github.io/noise2music/""}"	2302.03917	['Qingqing Huang', 'Daniel S. Park', 'Tao Wang', 'Timo I. Denk', 'Andy Ly', 'Nanxin Chen', 'Zhengdong Zhang', 'Zhishuai Zhang', 'Jiahui Yu', 'Christian Frank', 'Jesse Engel', 'Quoc V. Le', 'William Chan', 'Zhifeng Chen', 'Wei Han']	ct:We introduce Noise2Music, where a series of diffusion models is trained to generate high-quality 30-second music clips from text prompts. Two types of diffusion models, a generator model, which generates an intermediate representation conditioned on text, and a cascader model, which generates high-fidelity audio conditioned on the intermediate representation and possibly the text, are trained and utilized in succession to generate high-fidelity music. We explore two options for the intermediate representation, one using a spectrogram and the other using audio with lower fidelity. We find that the generated audio is not only able to faithfully reflect key elements of the text prompt such as genre, tempo, instruments, mood, and era, but goes beyond to ground fine-grained semantics of the prompt. Pretrained large language models play a key role in this story -- they are used to generate paired text for the audio of the training set and to extract embeddings of the text prompts ingested by the diffusion models.Generated examples:this https URL	es	['Sound (cs.SD)', 'Machine Learning (cs.LG)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2302.03917', 'html': None, 'tex': '/src/2302.03917', 'doi': 'https://doi.org/10.48550/arXiv.2302.03917'}	Submission history From: Qingqing Huang [ view email ] [v1] Wed, 8 Feb 2023 07:27:27 UTC (333 KB) [v2] Mon, 6 Mar 2023 18:09:56 UTC (334 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.03917'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.03917'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.03917'}]
2023-02-12	Offsite-Tuning: Transfer Learning without Full Model	Computation and Language	https://arxiv.org/abs/2302.04870	Offsite-Tuning: Transfer Learning without Full Model	https://twitter.com/dair_ai/status/1624832264029831169?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""Project"": ""https://github.com/mit-han-lab/offsite-tuning""}"	2302.04870	['Guangxuan Xiao', 'Ji Lin', 'Song Han']	ct:Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion-parameter foundation models to downstream data without access to the full model. In offsite-tuning, the model owner sends a light-weight adapter and a lossy compressed emulator to the data owner, who then fine-tunes the adapter on the downstream data with the emulator's assistance. The fine-tuned adapter is then returned to the model owner, who plugs it into the full model to create an adapted foundation model. Offsite-tuning preserves both parties' privacy and is computationally more efficient than the existing fine-tuning methods that require access to the full model weights. We demonstrate the effectiveness of offsite-tuning on various large language and vision foundation models. Offsite-tuning can achieve comparable accuracy as full model fine-tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is available atthis https URL.		['Computation and Language (cs.CL)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2302.04870', 'html': None, 'tex': '/src/2302.04870', 'doi': 'https://doi.org/10.48550/arXiv.2302.04870'}	Submission history From: Guangxuan Xiao [ view email ] [v1] Thu, 9 Feb 2023 18:59:55 UTC (767 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.04870'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.04870'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.04870'}]
2023-02-12	Zero-shot Image-to-Image Translation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2302.03027	Zero-shot Image-to-Image Translation	https://twitter.com/dair_ai/status/1624832265967607813?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""Project"": ""https://pix2pixzero.github.io/""}"	2302.03027	['Gaurav Parmar', 'Krishna Kumar Singh', 'Richard Zhang', 'Yijun Li', 'Jingwan Lu', 'Jun-Yan Zhu']	ct:Large-scale text-to-image generative models have shown their remarkable ability to synthesize diverse and high-quality images. However, it is still challenging to directly apply these models for editing real images for two reasons. First, it is hard for users to come up with a perfect text prompt that accurately describes every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. In this work, we propose pix2pix-zero, an image-to-image translation method that can preserve the content of the original image without manual prompting. We first automatically discover editing directions that reflect desired edits in the text embedding space. To preserve the general content structure after editing, we further propose cross-attention guidance, which aims to retain the cross-attention maps of the input image throughout the diffusion process. In addition, our method does not need additional training for these edits and can directly use the existing pre-trained text-to-image diffusion model. We conduct extensive experiments and show that our method outperforms existing and concurrent works for both real and synthetic image editing.	e:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2302.03027', 'html': None, 'tex': '/src/2302.03027', 'doi': 'https://doi.org/10.48550/arXiv.2302.03027'}	Submission history From: Gaurav Parmar [ view email ] [v1] Mon, 6 Feb 2023 18:59:51 UTC (24,259 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.03027'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.03027'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.03027'}]
2023-02-05	REPLUG: Retrieval-Augmented Black-Box Language Models	Computation and Language	https://arxiv.org/abs/2301.12652	REPLUG: Retrieval-Augmented Black-Box Language Models	https://twitter.com/dair_ai/status/1622261780725616641?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.12652	['Weijia Shi', 'Sewon Min', 'Michihiro Yasunaga', 'Minjoon Seo', 'Rich James', 'Mike Lewis', 'Luke Zettlemoyer', 'Wen-tau Yih']	ct:We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.		['Computation and Language (cs.CL)']	{'pdf': '/pdf/2301.12652', 'html': None, 'tex': '/src/2301.12652', 'doi': 'https://doi.org/10.48550/arXiv.2301.12652'}	Submission history From: Weijia Shi [ view email ] [v1] Mon, 30 Jan 2023 04:18:09 UTC (9,032 KB) [v2] Wed, 1 Feb 2023 00:15:18 UTC (9,033 KB) [v3] Mon, 22 May 2023 23:26:11 UTC (9,033 KB) [v4] Wed, 24 May 2023 05:08:07 UTC (9,033 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.12652'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.12652'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.12652'}]
2023-02-05	Extracting Training Data from Diffusion Models	Cryptography and Security	https://arxiv.org/abs/2301.13188	Extracting Training Data from Diffusion Models	https://twitter.com/dair_ai/status/1622261782738788353?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.13188	['Nicholas Carlini', 'Jamie Hayes', 'Milad Nasr', 'Matthew Jagielski', 'Vikash Sehwag', 'Florian Tramèr', 'Borja Balle', 'Daphne Ippolito', 'Eric Wallace']	ct:Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.		['Cryptography and Security (cs.CR)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.13188', 'html': None, 'tex': '/src/2301.13188', 'doi': 'https://doi.org/10.48550/arXiv.2301.13188'}	Submission history From: Nicholas Carlini [ view email ] [v1] Mon, 30 Jan 2023 18:53:09 UTC (9,437 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.13188'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.13188'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.13188'}]
2023-02-05	The Flan Collection: Designing Data and Methods for Effective Instruction Tuning	Artificial Intelligence	https://arxiv.org/abs/2301.13688	The Flan Collection: Designing Data and Methods for Effective Instruction Tuning	https://twitter.com/dair_ai/status/1622261784668241922?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.13688	['Shayne Longpre', 'Le Hou', 'Tu Vu', 'Albert Webson', 'Hyung Won Chung', 'Yi Tay', 'Denny Zhou', 'Quoc V. Le', 'Barret Zoph', 'Jason Wei', 'Adam Roberts']	ct:We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available atthis https URL.		['Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.13688', 'html': None, 'tex': '/src/2301.13688', 'doi': 'https://doi.org/10.48550/arXiv.2301.13688'}	Submission history From: Albert Webson [ view email ] [v1] Tue, 31 Jan 2023 15:03:44 UTC (171 KB) [v2] Tue, 14 Feb 2023 16:33:33 UTC (450 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.13688'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.13688'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.13688'}]
2023-02-05	Multimodal Chain-of-Thought Reasoning in Language Models	Computation and Language	https://arxiv.org/abs/2302.00923	Multimodal Chain-of-Thought Reasoning in Language Models	https://twitter.com/dair_ai/status/1622261786559791105?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""Code"": ""https://github.com/amazon-science/mm-cot""}"	2302.00923	['Zhuosheng Zhang', 'Aston Zhang', 'Mu Li', 'Hai Zhao', 'George Karypis', 'Alex Smola']	ct:Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available atthis https URL.	hed in Transactions on Machine Learning Research	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2302.00923', 'html': 'https://arxiv.org/html/2302.00923v5', 'tex': '/src/2302.00923', 'doi': 'https://doi.org/10.48550/arXiv.2302.00923'}	Submission history From: Aston Zhang [ view email ] [v1] Thu, 2 Feb 2023 07:51:19 UTC (421 KB) [v2] Thu, 9 Feb 2023 02:10:36 UTC (421 KB) [v3] Wed, 15 Feb 2023 19:20:15 UTC (416 KB) [v4] Fri, 17 Feb 2023 04:35:55 UTC (477 KB) [v5] Mon, 20 May 2024 06:43:48 UTC (762 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.00923'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.00923'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.00923'}]
2023-02-05	Dreamix: Video Diffusion Models are General Video Editors	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2302.01329	Dreamix: Video Diffusion Models are General Video Editors	https://twitter.com/dair_ai/status/1622261788497657856?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""Project"": ""https://dreamix-video-editing.github.io/""}"	2302.01329	['Eyal Molad', 'Eliahu Horwitz', 'Dani Valevski', 'Alex Rav Acha', 'Yossi Matias', 'Yael Pritch', 'Yaniv Leviathan', 'Yedid Hoshen']	ct:Text-driven image and video diffusion models have recently achieved unprecedented generation realism. While diffusion models have been successfully applied for image editing, very few works have done so for video editing. We present the first diffusion-based method that is able to perform text-based motion and appearance editing of general videos. Our approach uses a video diffusion model to combine, at inference time, the low-resolution spatio-temporal information from the original video with new, high resolution information that it synthesized to align with the guiding text prompt. As obtaining high-fidelity to the original video requires retaining some of its high-resolution information, we add a preliminary stage of finetuning the model on the original video, significantly boosting fidelity. We propose to improve motion editability by a new, mixed objective that jointly finetunes with full temporal attention and with temporal attention masking. We further introduce a new framework for image animation. We first transform the image into a coarse video by simple image processing operations such as replication and perspective geometric projections, and then use our general video editor to animate it. As a further application, we can use our method for subject-driven video generation. Extensive qualitative and numerical experiments showcase the remarkable editing ability of our method and establish its superior performance compared to baseline methods.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2302.01329', 'html': None, 'tex': '/src/2302.01329', 'doi': 'https://doi.org/10.48550/arXiv.2302.01329'}	Submission history From: Eliahu Horwitz [ view email ] [v1] Thu, 2 Feb 2023 18:58:58 UTC (23,487 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.01329'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.01329'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.01329'}]
2023-02-05	Benchmarking Large Language Models for News Summarization	Computation and Language	https://arxiv.org/abs/2301.13848	Benchmarking Large Language Models for News Summarization	https://twitter.com/dair_ai/status/1622261790326259714?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.13848	['Tianyi Zhang', 'Faisal Ladhak', 'Esin Durmus', 'Percy Liang', 'Kathleen McKeown', 'Tatsunori B. Hashimoto']	ct:Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, and not model size, is the key to the LLM's zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LMM summaries are judged to be on par with human written summaries.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.13848', 'html': None, 'tex': '/src/2301.13848', 'doi': 'https://doi.org/10.48550/arXiv.2301.13848'}	Submission history From: Tianyi Zhang [ view email ] [v1] Tue, 31 Jan 2023 18:46:19 UTC (715 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.13848'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.13848'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.13848'}]
2023-02-05	Mathematical Capabilities of ChatGPT	Machine Learning	https://arxiv.org/abs/2301.13867	Mathematical Capabilities of ChatGPT	https://twitter.com/dair_ai/status/1622261792238886913?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.13867	['Simon Frieder', 'Luca Pinchetti', 'Alexis Chevalier', 'Ryan-Rhys Griffiths', 'Tommaso Salvatori', 'Thomas Lukasiewicz', 'Philipp Christian Petersen', 'Julius Berner']	ct:We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulating use cases that arise in the daily professional activities of mathematicians. We benchmark the models on a range of fine-grained performance metrics. For advanced mathematics, this is the most detailed evaluation effort to date. We find that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a mathematical search engine and knowledge base interface. GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty. Contrary to many positive reports in the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student. Hence, if your goal is to use ChatGPT to pass a graduate-level math exam, you would be better off copying from your average peer!	further evaluations on another ChatGPT version and on GPT-4. The GHOSTS and miniGHOSTS datasets are available atthis https URL	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2301.13867', 'html': None, 'tex': '/src/2301.13867', 'doi': 'https://doi.org/10.48550/arXiv.2301.13867'}	Submission history From: Simon Frieder [ view email ] [v1] Tue, 31 Jan 2023 18:59:03 UTC (164 KB) [v2] Thu, 20 Jul 2023 17:59:14 UTC (173 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.13867'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.13867'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.13867'}]
2023-02-05	Emergence of Maps in the Memories of Blind Navigation Agents	Artificial Intelligence	https://arxiv.org/abs/2301.13261	Emergence of Maps in the Memories of Blind Navigation Agents	https://twitter.com/dair_ai/status/1622261793987989507?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""Project"": ""https://wijmans.xyz/publication/eom/""}"	2301.13261	['Erik Wijmans', 'Manolis Savva', 'Irfan Essa', 'Stefan Lee', 'Ari S. Morcos', 'Dhruv Batra']	ct:Animal navigation research posits that organisms build and maintain internal spatial representations, or maps, of their environment. We ask if machines -- specifically, artificial intelligence (AI) navigation agents -- also build implicit (or 'mental') maps. A positive answer to this question would (a) explain the surprising phenomenon in recent literature of ostensibly map-free neural-networks achieving strong performance, and (b) strengthen the evidence of mapping as a fundamental mechanism for navigation by intelligent embodied agents, whether they be biological or artificial. Unlike animal navigation, we can judiciously design the agent's perceptual system and control the learning paradigm to nullify alternative navigation mechanisms. Specifically, we train 'blind' agents -- with sensing limited to only egomotion and no other sensing of any kind -- to perform PointGoal navigation ('go to $\Delta$ x, $\Delta$ y') via reinforcement learning. Our agents are composed of navigation-agnostic components (fully-connected and recurrent neural networks), and our experimental setup provides no inductive bias towards mapping. Despite these harsh conditions, we find that blind agents are (1) surprisingly effective navigators in new environments (~95% success); (2) they utilize memory over long horizons (remembering ~1,000 steps of past experience in an episode); (3) this memory enables them to exhibit intelligent behavior (following walls, detecting collisions, taking shortcuts); (4) there is emergence of maps and collision detection neurons in the representations of the environment built by a blind agent as it navigates; and (5) the emergent maps are selective and task dependent (e.g. the agent 'forgets' exploratory detours). Overall, this paper presents no new techniques for the AI audience, but a surprising finding, an insight, and an explanation.	ed to ICLR 2023	['Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Machine Learning (cs.LG)', 'Robotics (cs.RO)']	{'pdf': '/pdf/2301.13261', 'html': None, 'tex': '/src/2301.13261', 'doi': 'https://doi.org/10.48550/arXiv.2301.13261'}	Submission history From: Erik Wijmans [ view email ] [v1] Mon, 30 Jan 2023 20:09:39 UTC (1,243 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.13261'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.13261'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.13261'}]
2023-02-05	SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2302.01330	SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections	https://twitter.com/dair_ai/status/1622261795925671936?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2302.01330	['Zhaoxi Chen', 'Guangcong Wang', 'Ziwei Liu']	ct:In this work, we present SceneDreamer, an unconditional generative model for unbounded 3D scenes, which synthesizes large-scale 3D landscapes from random noise. Our framework is learned from in-the-wild 2D image collections only, without any 3D annotations. At the core of SceneDreamer is a principled learning paradigm comprising 1) an efficient yet expressive 3D scene representation, 2) a generative scene parameterization, and 3) an effective renderer that can leverage the knowledge from 2D images. Our approach begins with an efficient bird's-eye-view (BEV) representation generated from simplex noise, which includes a height field for surface elevation and a semantic field for detailed scene semantics. This BEV scene representation enables 1) representing a 3D scene with quadratic complexity, 2) disentangled geometry and semantics, and 3) efficient training. Moreover, we propose a novel generative neural hash grid to parameterize the latent space based on 3D positions and scene semantics, aiming to encode generalizable features across various scenes. Lastly, a neural volumetric renderer, learned from 2D image collections through adversarial training, is employed to produce photorealistic images. Extensive experiments demonstrate the effectiveness of SceneDreamer and superiority over state-of-the-art methods in generating vivid yet diverse unbounded 3D worlds.	ransactions on Pattern Analysis & Machine Intelligence (TPAMI) 2023; Project Pagethis https URLCodethis https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)']	{'pdf': '/pdf/2302.01330', 'html': None, 'tex': '/src/2302.01330', 'doi': 'https://doi.org/10.48550/arXiv.2302.01330'}	Submission history From: Zhaoxi Chen [ view email ] [v1] Thu, 2 Feb 2023 18:59:16 UTC (12,456 KB) [v2] Wed, 19 Apr 2023 08:45:36 UTC (37,145 KB) [v3] Thu, 7 Dec 2023 18:58:30 UTC (45,539 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.01330'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.01330'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.01330'}]
2023-02-05	Large Language Models Can Be Easily Distracted by Irrelevant Context	Computation and Language	https://arxiv.org/abs/2302.00093	Large Language Models Can Be Easily Distracted by Irrelevant Context	https://twitter.com/dair_ai/status/1622261798379429888?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2302.00093	['Freda Shi', 'Xinyun Chen', 'Kanishka Misra', 'Nathan Scales', 'David Dohan', 'Ed Chi', 'Nathanael Schärli', 'Denny Zhou']	ct:Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.	hed in ICML 2023	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2302.00093', 'html': None, 'tex': '/src/2302.00093', 'doi': 'https://doi.org/10.48550/arXiv.2302.00093'}	Submission history From: Xinyun Chen [ view email ] [v1] Tue, 31 Jan 2023 20:48:57 UTC (1,140 KB) [v2] Mon, 13 Feb 2023 20:08:59 UTC (1,138 KB) [v3] Tue, 6 Jun 2023 08:36:20 UTC (1,142 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2302.00093'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2302.00093'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2302.00093'}]
2023-01-29	MusicLM: Generating Music From Text	Sound	https://arxiv.org/abs/2301.11325	MusicLM: Generating Music From Text	https://twitter.com/dair_ai/status/1619716425761042436?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.11325	['Andrea Agostinelli', 'Timo I. Denk', 'Zalán Borsos', 'Jesse Engel', 'Mauro Verzetti', 'Antoine Caillon', 'Qingqing Huang', 'Aren Jansen', 'Adam Roberts', 'Marco Tagliasacchi', 'Matt Sharifi', 'Neil Zeghidour', 'Christian Frank']	"ct:We introduce MusicLM, a model generating high-fidelity music from text descriptions such as ""a calming violin melody backed by a distorted guitar riff"". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts."	mentary material atthis https URLandthis https URL	['Sound (cs.SD)', 'Machine Learning (cs.LG)', 'Audio and Speech Processing (eess.AS)']	{'pdf': '/pdf/2301.11325', 'html': None, 'tex': '/src/2301.11325', 'doi': 'https://doi.org/10.48550/arXiv.2301.11325'}	Submission history From: Timo Denk [ view email ] [v1] Thu, 26 Jan 2023 18:58:53 UTC (503 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.11325'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.11325'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.11325'}]
2023-01-29	Hungry Hungry Hippos: Towards Language Modeling with State Space Models	Machine Learning	https://arxiv.org/abs/2212.14052	Hungry Hungry Hippos: Towards Language Modeling with State Space Models	https://twitter.com/dair_ai/status/1619716427879174144?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2212.14052	['Daniel Y. Fu', 'Tri Dao', 'Khaled K. Saab', 'Armin W. Thomas', 'Atri Rudra', 'Christopher Ré']	ct:State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.	023 Camera-Ready (Notable-top-25% / Spotlight)	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)']	{'pdf': '/pdf/2212.14052', 'html': None, 'tex': '/src/2212.14052', 'doi': 'https://doi.org/10.48550/arXiv.2212.14052'}	Submission history From: Daniel Y. Fu [ view email ] [v1] Wed, 28 Dec 2022 17:56:03 UTC (1,530 KB) [v2] Mon, 13 Feb 2023 05:48:00 UTC (2,733 KB) [v3] Sat, 29 Apr 2023 03:18:40 UTC (1,316 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2212.14052'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2212.14052'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2212.14052'}]
2023-01-29	A Watermark for Large Language Models	Machine Learning	https://arxiv.org/abs/2301.10226	A Watermark for Large Language Models	https://twitter.com/dair_ai/status/1619716430127308800?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.10226	['John Kirchenbauer', 'Jonas Geiping', 'Yuxin Wen', 'Jonathan Katz', 'Ian Miers', 'Tom Goldstein']	"ct:Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of ""green"" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."	es in the main body. Published at ICML 2023. Code is available atthis http URL	['Machine Learning (cs.LG)', 'Computation and Language (cs.CL)', 'Cryptography and Security (cs.CR)']	{'pdf': '/pdf/2301.10226', 'html': 'https://arxiv.org/html/2301.10226v4', 'tex': '/src/2301.10226', 'doi': 'https://doi.org/10.48550/arXiv.2301.10226'}	Submission history From: John Kirchenbauer [ view email ] [v1] Tue, 24 Jan 2023 18:52:59 UTC (3,550 KB) [v2] Fri, 27 Jan 2023 18:54:34 UTC (3,620 KB) [v3] Tue, 6 Jun 2023 17:50:01 UTC (3,618 KB) [v4] Wed, 1 May 2024 22:04:31 UTC (3,825 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.10226'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.10226'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.10226'}]
2023-01-29	Text-To-4D Dynamic Scene Generation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2301.11280	Text-To-4D Dynamic Scene Generation	https://twitter.com/dair_ai/status/1619718845018828801?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""GitHub"": ""https://make-a-video3d.github.io/""}"	2301.11280	['Uriel Singer', 'Shelly Sheynin', 'Adam Polyak', 'Oron Ashual', 'Iurii Makarov', 'Filippos Kokkinos', 'Naman Goyal', 'Andrea Vedaldi', 'Devi Parikh', 'Justin Johnson', 'Yaniv Taigman']	ct:We present MAV3D (Make-A-Video3D), a method for generating three-dimensional dynamic scenes from text descriptions. Our approach uses a 4D dynamic Neural Radiance Field (NeRF), which is optimized for scene appearance, density, and motion consistency by querying a Text-to-Video (T2V) diffusion-based model. The dynamic video output generated from the provided text can be viewed from any camera location and angle, and can be composited into any 3D environment. MAV3D does not require any 3D or 4D data and the T2V model is trained only on Text-Image pairs and unlabeled videos. We demonstrate the effectiveness of our approach using comprehensive quantitative and qualitative experiments and show an improvement over previously established internal baselines. To the best of our knowledge, our method is the first to generate 3D dynamic scenes given a text description.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.11280', 'html': None, 'tex': '/src/2301.11280', 'doi': 'https://doi.org/10.48550/arXiv.2301.11280'}	Submission history From: Yaniv Taigman [ view email ] [v1] Thu, 26 Jan 2023 18:14:32 UTC (17,011 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.11280'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.11280'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.11280'}]
2023-01-29	ClimaX: A foundation model for weather and climate	Machine Learning	https://arxiv.org/abs/2301.10343	ClimaX: A foundation model for weather and climate	https://twitter.com/tungnd_13/status/1618642574427959296?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""Blog"": ""https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/introducing-climax-the-first-foundation-model-for-weather-and-climate/""}"	2301.10343	['Tung Nguyen', 'Johannes Brandstetter', 'Ashish Kapoor', 'Jayesh K. Gupta', 'Aditya Grover']	ct:Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings. ClimaX extends the Transformer architecture with novel encoding and aggregation blocks that allow effective use of available compute while maintaining general utility. ClimaX is pre-trained with a self-supervised learning objective on climate datasets derived from CMIP6. The pre-trained ClimaX can then be fine-tuned to address a breadth of climate and weather tasks, including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining. Compared to existing data-driven baselines, we show that this generality in ClimaX results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets. The source code is available atthis https URL.	ational Conference on Machine Learning 2023	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2301.10343', 'html': None, 'tex': '/src/2301.10343', 'doi': 'https://doi.org/10.48550/arXiv.2301.10343'}	Submission history From: Tung Nguyen [ view email ] [v1] Tue, 24 Jan 2023 23:19:01 UTC (6,790 KB) [v2] Mon, 6 Feb 2023 23:44:49 UTC (6,790 KB) [v3] Mon, 10 Jul 2023 18:48:33 UTC (6,790 KB) [v4] Sat, 9 Dec 2023 06:17:58 UTC (6,791 KB) [v5] Mon, 18 Dec 2023 18:16:12 UTC (6,792 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.10343'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.10343'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.10343'}]
2023-01-29	Open Problems in Applied Deep Learning	Machine Learning	https://arxiv.org/abs/2301.11316	Open Problems in Applied Deep Learning	https://twitter.com/dair_ai/status/1619719063915339777?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.11316	['Maziar Raissi']	"ct:This work formulates the machine learning mechanism as a bi-level optimization problem. The inner level optimization loop entails minimizing a properly chosen loss function evaluated on the training data. This is nothing but the well-studied training process in pursuit of optimal model parameters. The outer level optimization loop is less well-studied and involves maximizing a properly chosen performance metric evaluated on the validation data. This is what we call the ""iteration process"", pursuing optimal model hyper-parameters. Among many other degrees of freedom, this process entails model engineering (e.g., neural network architecture design) and management, experiment tracking, dataset versioning and augmentation. The iteration process could be automated via Automatic Machine Learning (AutoML) or left to the intuitions of machine learning students, engineers, and researchers. Regardless of the route we take, there is a need to reduce the computational cost of the iteration step and as a direct consequence reduce the carbon footprint of developing artificial intelligence algorithms. Despite the clean and unified mathematical formulation of the iteration step as a bi-level optimization problem, its solutions are case specific and complex. This work will consider such cases while increasing the level of complexity from supervised learning to semi-supervised, self-supervised, unsupervised, few-shot, federated, reinforcement, and physics-informed learning. As a consequence of this exercise, this proposal surfaces a plethora of open problems in the field, many of which can be addressed in parallel."		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)', 'Human-Computer Interaction (cs.HC)', 'Information Retrieval (cs.IR)']	{'pdf': '/pdf/2301.11316', 'html': None, 'tex': '/src/2301.11316', 'doi': 'https://doi.org/10.48550/arXiv.2301.11316'}	Submission history From: Maziar Raissi [ view email ] [v1] Thu, 26 Jan 2023 18:55:43 UTC (292 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.11316'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.11316'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.11316'}]
2023-01-29	DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature	Computation and Language	https://arxiv.org/abs/2301.11305	DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature	https://twitter.com/dair_ai/status/1619719169758613504?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.11305	['Eric Mitchell', 'Yoonho Lee', 'Alexander Khazatsky', 'Christopher D. Manning', 'Chelsea Finn']	ct:The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. Seethis https URLfor code, data, and other project information.	023	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2301.11305', 'html': None, 'tex': '/src/2301.11305', 'doi': 'https://doi.org/10.48550/arXiv.2301.11305'}	Submission history From: Eric A Mitchell [ view email ] [v1] Thu, 26 Jan 2023 18:44:06 UTC (3,041 KB) [v2] Sun, 23 Jul 2023 04:18:36 UTC (1,229 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.11305'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.11305'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.11305'}]
2023-01-29	StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis	Machine Learning	https://arxiv.org/abs/2301.09515	StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis	https://twitter.com/dair_ai/status/1619719293779976193?s=20&t=ygX07dsAPDF8_jwrxZIo1Q	"{""Project"": ""https://sites.google.com/view/stylegan-t/"", ""Code"": ""https://github.com/autonomousvision/stylegan-t""}"	2301.09515	['Axel Sauer', 'Tero Karras', 'Samuli Laine', 'Andreas Geiger', 'Timo Aila']	ct:Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed.	t page:this https URL	['Machine Learning (cs.LG)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2301.09515', 'html': None, 'tex': '/src/2301.09515', 'doi': 'https://doi.org/10.48550/arXiv.2301.09515'}	Submission history From: Axel Sauer [ view email ] [v1] Mon, 23 Jan 2023 16:05:45 UTC (22,802 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.09515'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.09515'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.09515'}]
2023-01-29	The Impossibility of Parallelizing Boosting	Machine Learning	https://arxiv.org/abs/2301.09627	The Impossibility of Parallelizing Boosting	https://twitter.com/dair_ai/status/1619719511867015168?s=20&t=ygX07dsAPDF8_jwrxZIo1Q		2301.09627	['Amin Karbasi', 'Kasper Green Larsen']	ct:The aim of boosting is to convert a sequence of weak learners into a strong learner. At their heart, these methods are fully sequential. In this paper, we investigate the possibility of parallelizing boosting. Our main contribution is a strong negative result, implying that significant parallelization of boosting requires an exponential blow-up in the total computing resources needed for training.		['Machine Learning (cs.LG)', 'Computational Complexity (cs.CC)', 'Data Structures and Algorithms (cs.DS)']	{'pdf': '/pdf/2301.09627', 'html': None, 'tex': '/src/2301.09627', 'doi': 'https://doi.org/10.48550/arXiv.2301.09627'}	Submission history From: Kasper Green Larsen [ view email ] [v1] Mon, 23 Jan 2023 18:57:16 UTC (18 KB) [v2] Sat, 25 Mar 2023 08:13:08 UTC (18 KB) [v3] Mon, 21 Aug 2023 09:11:40 UTC (19 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.09627'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.09627'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.09627'}]
2023-01-22	Dissociating language and thought in large language models	Computation and Language	https://arxiv.org/abs/2301.06627	Dissociating language and thought in large language models: a cognitive perspective	https://twitter.com/neuranna/status/1615737072207400962?s=20&t=5iWUK4z_rp1NWst7JRbnwg		2301.06627	['Kyle Mahowald', 'Anna A. Ivanova', 'Idan A. Blank', 'Nancy Kanwisher', 'Joshua B. Tenenbaum', 'Evelina Fedorenko']	ct:Large Language Models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence -- knowledge of linguistic rules and patterns -- and functional linguistic competence -- understanding and using language in the world. We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of mechanisms specialized for formal linguistic competence, distinct from functional competence.	"o lead authors contributed equally to this work; published in ""Trends in Cognnitive Sciences"", March 2024"	['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2301.06627', 'html': 'https://arxiv.org/html/2301.06627v3', 'tex': '/src/2301.06627', 'doi': 'https://doi.org/10.48550/arXiv.2301.06627'}	Submission history From: Anna Ivanova [ view email ] [v1] Mon, 16 Jan 2023 22:41:19 UTC (8,762 KB) [v2] Sat, 4 Nov 2023 21:03:17 UTC (9,107 KB) [v3] Sat, 23 Mar 2024 19:52:33 UTC (9,037 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.06627'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.06627'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.06627'}]
2023-01-22	Human-Timescale Adaptation in an Open-Ended Task Space	Machine Learning	https://arxiv.org/abs/2301.07608	Human-Timescale Adaptation in an Open-Ended Task Space	https://twitter.com/FeryalMP/status/1616035293064462338?s=20&t=RN0YZFAXWr-uH2dT2ZTSqQ		2301.07608	['Adaptive Agent Team', 'Jakob Bauer', 'Kate Baumli', 'Satinder Baveja', 'Feryal Behbahani', 'Avishkar Bhoopchand', 'Nathalie Bradley-Schmieg', 'Michael Chang', 'Natalie Clay', 'Adrian Collister', 'Vibhavari Dasagi', 'Lucy Gonzalez', 'Karol Gregor', 'Edward Hughes', 'Sheleem Kashem', 'Maria Loks-Thompson', 'Hannah Openshaw', 'Jack Parker-Holder', 'Shreya Pathak', 'Nicolas Perez-Nieves', 'Nemanja Rakicevic', 'Tim Rocktäschel', 'Yannick Schroecker', 'Jakub Sygnowski', 'Karl Tuyls', 'Sarah York', 'Alexander Zacherl', 'Lei Zhang']	ct:Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Neural and Evolutionary Computing (cs.NE)']	{'pdf': '/pdf/2301.07608', 'html': None, 'tex': '/src/2301.07608', 'doi': 'https://doi.org/10.48550/arXiv.2301.07608'}	Submission history From: Feryal Behbahani [ view email ] [v1] Wed, 18 Jan 2023 15:39:21 UTC (30,016 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.07608'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.07608'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.07608'}]
2023-01-22	AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation	Machine Learning	https://arxiv.org/abs/2301.08110	AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation	https://twitter.com/JonasAndrulis/status/1616722810608427008?s=20&t=vUEC8AZmrOJnVxuYIEJs5A		2301.08110	['Björn Deiseroth', 'Mayukh Deb', 'Samuel Weinbach', 'Manuel Brack', 'Patrick Schramowski', 'Kristian Kersting']	ct:Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2301.08110', 'html': 'https://arxiv.org/html/2301.08110v6', 'tex': '/src/2301.08110', 'doi': 'https://doi.org/10.48550/arXiv.2301.08110'}	Submission history From: Björn Deiseroth [ view email ] [v1] Thu, 19 Jan 2023 15:01:00 UTC (32,105 KB) [v2] Mon, 23 Jan 2023 23:30:02 UTC (47,675 KB) [v3] Fri, 20 Oct 2023 20:50:46 UTC (41,082 KB) [v4] Wed, 25 Oct 2023 12:05:01 UTC (41,138 KB) [v5] Sun, 5 Nov 2023 14:16:21 UTC (41,138 KB) [v6] Tue, 7 Jan 2025 17:26:26 UTC (41,301 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.08110'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.08110'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.08110'}]
2023-01-22	Everything is Connected: Graph Neural Networks	Machine Learning	https://arxiv.org/abs/2301.08210	Everything is Connected: Graph Neural Networks	https://twitter.com/PetarV_93/status/1616379369953394688?s=20&t=AqTVY30Y7IZCultzwnqBPA		2301.08210	['Petar Veličković']	ct:In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with already-impacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years -- images, text and speech processing -- can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fields.	ear in Current Opinion in Structural Biology. 14 pages, 1 figure	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Social and Information Networks (cs.SI)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2301.08210', 'html': None, 'tex': '/src/2301.08210', 'doi': 'https://doi.org/10.48550/arXiv.2301.08210'}	Submission history From: Petar Veličković [ view email ] [v1] Thu, 19 Jan 2023 18:09:43 UTC (122 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.08210'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.08210'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.08210'}]
2023-01-22	GLIGEN: Open-Set Grounded Text-to-Image Generation	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2301.07093	GLIGEN: Open-Set Grounded Text-to-Image Generation	https://twitter.com/hardmaru/status/1615766551113744384?s=20&t=wx0Y18oSmW0YenXjKRAdnA	"{""Project"": ""https://gligen.github.io/""}"	2301.07093	['Yuheng Li', 'Haotian Liu', 'Qingyang Wu', 'Fangzhou Mu', 'Jianwei Yang', 'Jianfeng Gao', 'Chunyuan Li', 'Yong Jae Lee']	ct:Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Graphics (cs.GR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.07093', 'html': None, 'tex': '/src/2301.07093', 'doi': 'https://doi.org/10.48550/arXiv.2301.07093'}	Submission history From: Yuheng Li [ view email ] [v1] Tue, 17 Jan 2023 18:58:58 UTC (14,774 KB) [v2] Mon, 17 Apr 2023 01:54:37 UTC (21,899 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.07093'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.07093'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.07093'}]
2023-01-22	InstructPix2Pix: Learning to Follow Image Editing Instructions	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2211.09800	InstructPix2Pix: Learning to Follow Image Editing Instructions	https://twitter.com/_akhaliq/status/1615947919286276096?s=20&t=pbRTn8DaPeQFApQ9okkdRg		2211.09800	['Tim Brooks', 'Aleksander Holynski', 'Alexei A. Efros']	ct:We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.	t page with code:this https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Computation and Language (cs.CL)', 'Graphics (cs.GR)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2211.09800', 'html': None, 'tex': '/src/2211.09800', 'doi': 'https://doi.org/10.48550/arXiv.2211.09800'}	Submission history From: Aleksander Holynski [ view email ] [v1] Thu, 17 Nov 2022 18:58:43 UTC (11,114 KB) [v2] Wed, 18 Jan 2023 17:31:52 UTC (11,114 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2211.09800'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2211.09800'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2211.09800'}]
2023-01-22	Dataset Distillation: A Comprehensive Review	Machine Learning	https://arxiv.org/abs/2301.07014	Dataset Distillation: A Comprehensive Review	https://twitter.com/omarsar0/status/1615745724473540609?s=20&t=r-pwuB6EhbZLXa5R6mL3NQ		2301.07014	['Ruonan Yu', 'Songhua Liu', 'Xinchao Wang']	ct:Recent success of deep learning is largely attributed to the sheer amount of data used for training deep neuralthis http URLthe unprecedented success, the massive data, unfortunately, significantly increases the burden on storage and transmission and further gives rise to a cumbersome model training process. Besides, relying on the raw data for training \emph{per se} yields concerns about privacy and copyright. To alleviate these shortcomings, dataset distillation~(DD), also known as dataset condensation (DC), was introduced and has recently attracted much research attention in the community. Given an original dataset, DD aims to derive a much smaller dataset containing synthetic samples, based on which the trained models yield performance comparable with those trained on the original dataset. In this paper, we give a comprehensive review and summary of recent advances in DD and its application. We first introduce the task formally and propose an overall algorithmic framework followed by all existing DD methods. Next, we provide a systematic taxonomy of current methodologies in this area, and discuss their theoretical interconnections. We also present current challenges in DD through extensive experiments and envision possible directions for future works.	ed by TPAMI	['Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.07014', 'html': None, 'tex': '/src/2301.07014', 'doi': 'https://doi.org/10.48550/arXiv.2301.07014'}	Submission history From: Songhua Liu [ view email ] [v1] Tue, 17 Jan 2023 17:03:28 UTC (1,190 KB) [v2] Sat, 21 Jan 2023 07:34:46 UTC (1,413 KB) [v3] Sat, 7 Oct 2023 12:16:25 UTC (1,413 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.07014'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.07014'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.07014'}]
2023-01-22	Learning-Rate-Free Learning by D-Adaptation	Machine Learning	https://arxiv.org/abs/2301.07733	Learning-Rate-Free Learning by D-Adaptation	https://twitter.com/aaron_defazio/status/1616453609956478977?s=20&t=hGWDXu4sT5f1KcH-X1IL9g		2301.07733	['Aaron Defazio', 'Konstantin Mishchenko']	ct:D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.An open-source implementation is available.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Optimization and Control (math.OC)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2301.07733', 'html': None, 'tex': '/src/2301.07733', 'doi': 'https://doi.org/10.48550/arXiv.2301.07733'}	Submission history From: Aaron Defazio [ view email ] [v1] Wed, 18 Jan 2023 19:00:50 UTC (780 KB) [v2] Fri, 20 Jan 2023 18:53:01 UTC (743 KB) [v3] Mon, 3 Apr 2023 18:39:30 UTC (837 KB) [v4] Mon, 15 May 2023 20:55:15 UTC (907 KB) [v5] Fri, 7 Jul 2023 19:08:18 UTC (902 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.07733'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.07733'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.07733'}]
2023-01-22	RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2301.07958	RecolorNeRF: Layer Decomposed Radiance Field for Efficient Color Editing of 3D Scenes	https://twitter.com/_akhaliq/status/1616265465843548160?s=20&t=duiLmtDvxCwkFmw23rYDmQ		2301.07958	['Bingchen Gong', 'Yuehao Wang', 'Xiaoguang Han', 'Qi Dou']	ct:Radiance fields have gradually become a main representation of media. Although its appearance editing has been studied, how to achieve view-consistent recoloring in an efficient manner is still under explored. We present RecolorNeRF, a novel user-friendly color editing approach for the neural radiance fields. Our key idea is to decompose the scene into a set of pure-colored layers, forming a palette. By this means, color manipulation can be conducted by altering the color components of the palette directly. To support efficient palette-based editing, the color of each layer needs to be as representative as possible. In the end, the problem is formulated as an optimization problem, where the layers and their blending weights are jointly optimized with the NeRF itself. Extensive experiments show that our jointly-optimized layer decomposition can be used against multiple backbones and produce photo-realistic recolored novel-view renderings. We demonstrate that RecolorNeRF outperforms baseline methods both quantitatively and qualitatively for color editing even in complex real-world scenes.	ear in ACM Multimedia 2023. Project website is accessible atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)', 'Graphics (cs.GR)']	{'pdf': '/pdf/2301.07958', 'html': None, 'tex': '/src/2301.07958', 'doi': 'https://doi.org/10.48550/arXiv.2301.07958'}	Submission history From: Bingchen Gong [ view email ] [v1] Thu, 19 Jan 2023 09:18:06 UTC (9,145 KB) [v2] Sun, 5 Feb 2023 15:53:56 UTC (22,312 KB) [v3] Mon, 18 Sep 2023 17:28:42 UTC (45,119 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.07958'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.07958'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.07958'}]
2023-01-15	Mastering Diverse Domains through World Models	Artificial Intelligence	https://arxiv.org/abs/2301.04104v1	Mastering Diverse Domains through World Models	https://twitter.com/dair_ai/status/1614676677757661185?s=20&t=3GITA7PeX7pGwrqvt97bYQ		2301.04104v1	['Danijar Hafner', 'Jurgis Pasukonis', 'Jimmy Ba', 'Timothy Lillicrap']	ct:General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision-making problems.	e:this https URL	['Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2301.04104v1', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2301.04104'}	Submission history From: Danijar Hafner [ view email ] [v1] Tue, 10 Jan 2023 18:12:16 UTC (2,210 KB) [v2] Wed, 17 Apr 2024 17:41:20 UTC (2,520 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.04104'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.04104'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.04104'}]
2023-01-15	Tracr: Compiled Transformers as a Laboratory for Interpretability	Machine Learning	https://arxiv.org/abs/2301.05062	Tracr: Compiled Transformers as a Laboratory for Interpretability	https://twitter.com/dair_ai/status/1614676680165187584?s=20&t=3GITA7PeX7pGwrqvt97bYQ	"{""Code"": ""https://github.com/deepmind/tracr""}"	2301.05062	['David Lindner', 'János Kramár', 'Sebastian Farquhar', 'Matthew Rahtz', 'Thomas McGrath', 'Vladimir Mikulik']	"ct:We show how to ""compile"" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study ""superposition"" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the ""programs"" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr atthis https URL."	ted at NeurIPS 2023 (Spotlight)	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (stat.ML)']	{'pdf': '/pdf/2301.05062', 'html': None, 'tex': '/src/2301.05062', 'doi': 'https://doi.org/10.48550/arXiv.2301.05062'}	Submission history From: David Lindner [ view email ] [v1] Thu, 12 Jan 2023 14:59:19 UTC (1,164 KB) [v2] Tue, 21 Feb 2023 10:51:03 UTC (1,163 KB) [v3] Wed, 22 Feb 2023 14:53:12 UTC (1,163 KB) [v4] Wed, 7 Jun 2023 13:21:51 UTC (1,153 KB) [v5] Fri, 3 Nov 2023 15:11:02 UTC (921 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.05062'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.05062'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.05062'}]
2023-01-15	Why do Nearest Neighbor Language Models Work?	Computation and Language	https://arxiv.org/abs/2301.02828	Why do Nearest Neighbor Language Models Work?	https://twitter.com/dair_ai/status/1614676687597469696?s=20&t=3GITA7PeX7pGwrqvt97bYQ	"{""Code"": ""https://github.com/frankxu2004/knnlm-why""}"	2301.02828	['Frank F. Xu', 'Uri Alon', 'Graham Neubig']	ct:Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available atthis https URL.	nt, 21 pages	['Computation and Language (cs.CL)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.02828', 'html': None, 'tex': '/src/2301.02828', 'doi': 'https://doi.org/10.48550/arXiv.2301.02828'}	Submission history From: Frank F. Xu [ view email ] [v1] Sat, 7 Jan 2023 11:12:36 UTC (1,037 KB) [v2] Tue, 17 Jan 2023 08:02:58 UTC (1,037 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.02828'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.02828'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.02828'}]
2023-01-15	Memory Augmented Large Language Models are Computationally Universal	Computation and Language	https://arxiv.org/abs/2301.04589	Memory Augmented Large Language Models are Computationally Universal	https://twitter.com/dair_ai/status/1614676689908277252?s=20&t=3GITA7PeX7pGwrqvt97bYQ		2301.04589	['Dale Schuurmans']	ct:We show that transformer-based large language models are computationally universal when augmented with an external memory. Any deterministic language model that conditions on strings of bounded length is equivalent to a finite automaton, hence computationally limited. However, augmenting such models with a read-write memory creates the possibility of processing arbitrarily large inputs and, potentially, simulating any algorithm. We establish that an existing large language model, Flan-U-PaLM 540B, can be combined with an associative read-write memory to exactly simulate the execution of a universal Turing machine, $U_{15,2}$. A key aspect of the finding is that it does not require any modification of the language model weights. Instead, the construction relies solely on designing a form of stored instruction computer that can subsequently be programmed with a specific set of prompts.	es, 0 figures	['Computation and Language (cs.CL)', 'Formal Languages and Automata Theory (cs.FL)']	{'pdf': '/pdf/2301.04589', 'html': None, 'tex': '/src/2301.04589', 'doi': 'https://doi.org/10.48550/arXiv.2301.04589'}	Submission history From: Dale Schuurmans [ view email ] [v1] Tue, 10 Jan 2023 02:37:44 UTC (14 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.04589'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.04589'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.04589'}]
2023-01-15	A Survey on Transformers in Reinforcement Learning	Machine Learning	https://arxiv.org/abs/2301.03044	A Survey on Transformers in Reinforcement Learning	https://twitter.com/dair_ai/status/1614676692538105860?s=20&t=3GITA7PeX7pGwrqvt97bYQ		2301.03044	['Wenzhe Li', 'Hao Luo', 'Zichuan Lin', 'Chongjie Zhang', 'Zongqing Lu', 'Deheng Ye']	ct:Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.	ed by TMLR	['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2301.03044', 'html': None, 'tex': '/src/2301.03044', 'doi': 'https://doi.org/10.48550/arXiv.2301.03044'}	Submission history From: Wenzhe Li [ view email ] [v1] Sun, 8 Jan 2023 14:04:26 UTC (610 KB) [v2] Thu, 13 Jul 2023 07:31:09 UTC (982 KB) [v3] Wed, 20 Sep 2023 21:12:31 UTC (987 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.03044'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.03044'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.03044'}]
2023-01-15	Scaling Laws for Generative Mixed-Modal Language Models	Computation and Language	https://arxiv.org/abs/2301.03728	Scaling Laws for Generative Mixed-Modal Language Models	https://twitter.com/dair_ai/status/1614676694920531969?s=20&t=3GITA7PeX7pGwrqvt97bYQ		2301.03728	['Armen Aghajanyan', 'Lili Yu', 'Alexis Conneau', 'Wei-Ning Hsu', 'Karen Hambardzumyan', 'Susan Zhang', 'Stephen Roller', 'Naman Goyal', 'Omer Levy', 'Luke Zettlemoyer']	ct:Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities (e.g., any permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens for language or code, and so on). To better understand the scaling properties of such mixed-modal models, we conducted over 250 experiments using seven different modalities and model sizes ranging from 8 million to 30 billion, trained on 5-100 billion tokens. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them. Specifically, we explicitly model the optimal synergy and competition due to data and model size as an additive term to previous uni-modal scaling laws. We also find four empirical phenomena observed during the training, such as emergent coordinate-ascent style training that naturally alternates between modalities, guidelines for selecting critical hyper-parameters, and connections between mixed-modal competition and training stability. Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models. Overall, our research provides valuable insights into the design and training of mixed-modal generative models, an important new class of unified models that have unique distributional properties.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.03728', 'html': None, 'tex': '/src/2301.03728', 'doi': 'https://doi.org/10.48550/arXiv.2301.03728'}	Submission history From: Armen Aghajanyan [ view email ] [v1] Tue, 10 Jan 2023 00:20:06 UTC (1,100 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.03728'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.03728'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.03728'}]
2023-01-15	DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2301.02993	DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching	https://twitter.com/dair_ai/status/1614676697516752898?s=20&t=3GITA7PeX7pGwrqvt97bYQ		2301.02993	['Tao Xie', 'Kun Dai', 'Ke Wang', 'Ruifeng Li', 'Lijun Zhao']	ct:Local feature matching between images remains a challenging task, especially in the presence of significant appearance variations, e.g., extreme viewpoint changes. In this work, we propose DeepMatcher, a deep Transformer-based network built upon our investigation of local feature matching in detector-free methods. The key insight is that local feature matcher with deep layers can capture more human-intuitive and simpler-to-match features. Based on this, we propose a Slimming Transformer (SlimFormer) dedicated for DeepMatcher, which leverages vector-based attention to model relevance among all keypoints and achieves long-range context aggregation in an efficient and effective manner. A relative position encoding is applied to each SlimFormer so as to explicitly disclose relative distance information, further improving the representation of keypoints. A layer-scale strategy is also employed in each SlimFormer to enable the network to assimilate message exchange from the residual block adaptively, thus allowing it to simulate the human behaviour that humans can acquire different matching cues each time they scan an image pair. To facilitate a better adaption of the SlimFormer, we introduce a Feature Transition Module (FTM) to ensure a smooth transition in feature scopes with different receptive fields. By interleaving the self- and cross-SlimFormer multiple times, DeepMatcher can easily establish pixel-wise dense matches at coarse level. Finally, we perceive the match refinement as a combination of classification and regression problems and design Fine Matches Module to predict confidence and offset concurrently, thereby generating robust and accurate matches. Experimentally, we show that DeepMatcher significantly outperforms the state-of-the-art methods on several benchmarks, demonstrating the superior matching capability of DeepMatcher.		['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2301.02993', 'html': None, 'tex': '/src/2301.02993', 'doi': 'https://doi.org/10.48550/arXiv.2301.02993'}	Submission history From: Dai Kun [ view email ] [v1] Sun, 8 Jan 2023 07:15:09 UTC (25,433 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.02993'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.02993'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.02993'}]
2023-01-15	Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement	Machine Learning	https://arxiv.org/abs/2301.03028	Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement	https://twitter.com/dair_ai/status/1614676699915980804?s=20&t=3GITA7PeX7pGwrqvt97bYQ		2301.03028	['Yan Li', 'Xinjiang Lu', 'Yaqing Wang', 'Dejing Dou']	ct:Time series forecasting has been a widely explored task of great importance in many applications. However, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. In this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. To ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. In addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. Extensive experiments on synthetic and real-world data show that D3VAE outperforms competitive algorithms with remarkable margins. Our implementation is available atthis https URL.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.03028', 'html': None, 'tex': '/src/2301.03028', 'doi': 'https://doi.org/10.48550/arXiv.2301.03028'}	Submission history From: Xinjiang Lu [ view email ] [v1] Sun, 8 Jan 2023 12:20:46 UTC (13,321 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.03028'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.03028'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.03028'}]
2023-01-08	Muse: Text-To-Image Generation via Masked Generative Transformers	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2301.00704	Muse: Text-To-Image Generation via Masked Generative Transformers	https://twitter.com/dair_ai/status/1612153095772938241?s=20&t=ChwZWzSmoRlZKnD54fsV6w	"{""Project"": ""https://muse-model.github.io/"", ""Code"": ""https://github.com/lucidrains/muse-maskgit-pytorch""}"	2301.00704	['Huiwen Chang', 'Han Zhang', 'Jarred Barber', 'AJ Maschinot', 'Jose Lezama', 'Lu Jiang', 'Ming-Hsuan Yang', 'Kevin Murphy', 'William T. Freeman', 'Michael Rubinstein', 'Yuanzhen Li', 'Dilip Krishnan']	ct:We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available atthis https URL		['Computer Vision and Pattern Recognition (cs.CV)', 'Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.00704', 'html': None, 'tex': '/src/2301.00704', 'doi': 'https://doi.org/10.48550/arXiv.2301.00704'}	Submission history From: Jarred Barber [ view email ] [v1] Mon, 2 Jan 2023 14:43:38 UTC (10,443 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.00704'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.00704'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.00704'}]
2023-01-08	Rethinking with Retrieval: Faithful Large Language Model Inference	Computation and Language	https://arxiv.org/abs/2301.00303	Rethinking with Retrieval: Faithful Large Language Model Inference	https://twitter.com/dair_ai/status/1612153100114055171?s=20&t=ChwZWzSmoRlZKnD54fsV6w		2301.00303	['Hangfeng He', 'Hongming Zhang', 'Dan Roth']	ct:Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)']	{'pdf': '/pdf/2301.00303', 'html': None, 'tex': '/src/2301.00303', 'doi': 'https://doi.org/10.48550/arXiv.2301.00303'}	Submission history From: Hangfeng He [ view email ] [v1] Sat, 31 Dec 2022 22:35:34 UTC (282 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.00303'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.00303'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.00303'}]
2023-01-08	SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot	Machine Learning	https://arxiv.org/abs/2301.00774	SparseGPT: Massive Language Models Can Be Accurately Pruned In One-Shot	https://twitter.com/dair_ai/status/1612153102513360901?s=20&t=ChwZWzSmoRlZKnD54fsV6w		2301.00774	['Elias Frantar', 'Dan Alistarh']	ct:We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at:this https URL.		['Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.00774', 'html': None, 'tex': '/src/2301.00774', 'doi': 'https://doi.org/10.48550/arXiv.2301.00774'}	Submission history From: Elias Frantar [ view email ] [v1] Mon, 2 Jan 2023 17:48:56 UTC (183 KB) [v2] Wed, 18 Jan 2023 17:13:49 UTC (183 KB) [v3] Wed, 22 Mar 2023 12:33:46 UTC (242 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.00774'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.00774'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.00774'}]
2023-01-08	ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders	Computer Vision and Pattern Recognition	https://arxiv.org/abs/2301.00808	ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders	https://twitter.com/dair_ai/status/1612153104329281538?s=20&t=ChwZWzSmoRlZKnD54fsV6w	"{""Code"": ""https://github.com/facebookresearch/convnext-v2""}"	2301.00808	['Sanghyun Woo', 'Shoubhik Debnath', 'Ronghang Hu', 'Xinlei Chen', 'Zhuang Liu', 'In So Kweon', 'Saining Xie']	ct:Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9% accuracy using only public training data.	nd models available atthis https URL	['Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2301.00808', 'html': None, 'tex': '/src/2301.00808', 'doi': 'https://doi.org/10.48550/arXiv.2301.00808'}	Submission history From: Saining Xie [ view email ] [v1] Mon, 2 Jan 2023 18:59:31 UTC (4,089 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.00808'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.00808'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.00808'}]
2023-01-08	Large Language Models as Corporate Lobbyists	Computation and Language	https://arxiv.org/abs/2301.01181	Large Language Models as Corporate Lobbyists	https://twitter.com/dair_ai/status/1612153106355130372?s=20&t=ChwZWzSmoRlZKnD54fsV6w	"{""Code"": ""https://github.com/JohnNay/llm-lobbyist""}"	2301.01181	['John J. Nay']	ct:We demonstrate a proof-of-concept of a large language model conducting corporate lobbying related activities. An autoregressive large language model (OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are relevant to specific public companies and provides explanations and confidence levels. For the bills the model deems as relevant, the model drafts a letter to the sponsor of the bill in an attempt to persuade the congressperson to make changes to the proposed legislation. We use hundreds of novel ground-truth labels of the relevance of a bill to a company to benchmark the performance of the model. It outperforms the baseline of predicting the most common outcome of irrelevance. We also benchmark the performance of the previous OpenAI GPT-3 model (text-davinci-002), which was the state-of-the-art model on many academic natural language tasks until text-davinci-003 was recently released. The performance of text-davinci-002 is worse than the simple baseline. Longer-term, if AI begins to influence law in a manner that is not a direct extension of human intentions, this threatens the critical role that law as information could play in aligning AI with humans. Initially, AI is being used to simply augment human lobbyists for a small portion of their daily tasks. However, firms have an incentive to use less and less human oversight over automated assessments of policy ideas and the written communication to regulatory agencies and Congressional staffers. The core question raised is where to draw the line between human-driven and AI-driven policy influence.	en-source code available here:this https URL	['Computation and Language (cs.CL)', 'Computers and Society (cs.CY)']	{'pdf': '/pdf/2301.01181', 'html': None, 'tex': None, 'doi': 'https://doi.org/10.48550/arXiv.2301.01181'}	Submission history From: John Nay [ view email ] [v1] Tue, 3 Jan 2023 16:25:52 UTC (220 KB) [v2] Wed, 4 Jan 2023 16:55:35 UTC (240 KB) [v3] Thu, 5 Jan 2023 15:06:58 UTC (244 KB) [v4] Sun, 8 Jan 2023 13:54:05 UTC (246 KB) [v5] Thu, 12 Jan 2023 20:49:46 UTC (215 KB) [v6] Tue, 17 Jan 2023 14:32:05 UTC (221 KB) [v7] Sat, 28 Jan 2023 20:49:33 UTC (256 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.01181'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.01181'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.01181'}]
2023-01-08	StitchNet: Composing Neural Networks from Pre-Trained Fragments	Machine Learning	https://arxiv.org/abs/2301.01947	StitchNet: Composing Neural Networks from Pre-Trained Fragments	https://twitter.com/dair_ai/status/1612153110452903936?s=20&t=ChwZWzSmoRlZKnD54fsV6w		2301.01947	['Surat Teerapittayanon', 'Marcus Comiter', 'Brad McDanel', 'H.T. Kung']	ct:We propose StitchNet, a novel neural network creation paradigm that stitches together fragments (one or more consecutive network layers) from multiple pre-trained neural networks. StitchNet allows the creation of high-performing neural networks without the large compute and data requirements needed under traditional model creation processes via backpropagation training. We leverage Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide the selection of these fragments in composing a network for a given task tailored to specific accuracy needs and computing resource constraints. We then show that these fragments can be stitched together to create neural networks with accuracy comparable to that of traditionally trained networks at a fraction of computing resource and data requirements. Finally, we explore a novel on-the-fly personalized model creation and inference application enabled by this new paradigm. The code is available atthis https URL.		['Machine Learning (cs.LG)', 'Artificial Intelligence (cs.AI)', 'Computer Vision and Pattern Recognition (cs.CV)']	{'pdf': '/pdf/2301.01947', 'html': None, 'tex': '/src/2301.01947', 'doi': 'https://doi.org/10.48550/arXiv.2301.01947'}	Submission history From: Surat Teerapittayanon [ view email ] [v1] Thu, 5 Jan 2023 08:02:30 UTC (395 KB) [v2] Mon, 17 Jul 2023 09:06:53 UTC (533 KB) [v3] Sat, 23 Sep 2023 05:25:34 UTC (535 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.01947'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.01947'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.01947'}]
2023-01-08	Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes	Computation and Language	https://arxiv.org/abs/2301.01751	Iterated Decomposition: Improving Science Q\&A by Supervising Reasoning Processes	https://twitter.com/dair_ai/status/1612153112638402562?s=20&t=ChwZWzSmoRlZKnD54fsV6w	"{""Code"": ""https://github.com/oughtinc/ice""}"	2301.01751	['Justin Reppert', 'Ben Rachbach', 'Charlie George', 'Luke Stebbing', 'Jungwon Byun', 'Maggie Appleton', 'Andreas Stuhlmüller']	ct:Language models (LMs) can perform complex reasoning either end-to-end, with hidden latent state, or compositionally, with transparent intermediate state. Composition offers benefits for interpretability and safety, but may need workflow support and infrastructure to remain competitive. We describe iterated decomposition, a human-in-the-loop workflow for developing and refining compositional LM programs. We improve the performance of compositions by zooming in on failing components and refining them through decomposition, additional context, chain of thought, etc. To support this workflow, we develop ICE, an open-source tool for visualizing the execution traces of LM programs. We apply iterated decomposition to three real-world tasks and improve the accuracy of LM programs over less compositional baselines: describing the placebo used in a randomized controlled trial (25% to 65%), evaluating participant adherence to a medical intervention (53% to 70%), and answering NLP questions on the Qasper dataset (38% to 69%). These applications serve as case studies for a workflow that, if automated, could keep ML systems interpretable and safe even as they scale to increasingly complex tasks.		['Computation and Language (cs.CL)', 'Artificial Intelligence (cs.AI)', 'Human-Computer Interaction (cs.HC)']	{'pdf': '/pdf/2301.01751', 'html': None, 'tex': '/src/2301.01751', 'doi': 'https://doi.org/10.48550/arXiv.2301.01751'}	Submission history From: Andreas Stuhlmüller [ view email ] [v1] Wed, 4 Jan 2023 18:34:25 UTC (1,879 KB) [v2] Thu, 5 Jan 2023 01:35:43 UTC (1,879 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.01751'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.01751'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.01751'}]
2023-01-08	A Succinct Summary of Reinforcement Learning	Artificial Intelligence	https://arxiv.org/abs/2301.01379	A Succinct Summary of Reinforcement Learning	https://twitter.com/dair_ai/status/1612153114773053446?s=20&t=ChwZWzSmoRlZKnD54fsV6w		2301.01379	['Sanjeevan Ahilan']	ct:This document is a concise summary of many key results in single-agent reinforcement learning (RL). The intended audience are those who already have some familiarity with RL and are looking to review, reference and/or remind themselves of important ideas in the field.		['Artificial Intelligence (cs.AI)', 'Machine Learning (cs.LG)']	{'pdf': '/pdf/2301.01379', 'html': None, 'tex': '/src/2301.01379', 'doi': 'https://doi.org/10.48550/arXiv.2301.01379'}	Submission history From: Sanjeevan Ahilan [ view email ] [v1] Tue, 3 Jan 2023 22:17:55 UTC (43 KB)	[{'label': 'NASA ADS', 'url': 'https://ui.adsabs.harvard.edu/abs/arXiv:2301.01379'}, {'label': 'Google Scholar', 'url': 'https://scholar.google.com/scholar_lookup?arxiv_id=2301.01379'}, {'label': 'Semantic Scholar', 'url': 'https://api.semanticscholar.org/arXiv:2301.01379'}]
