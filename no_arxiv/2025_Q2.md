| Week       | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Paper-Link                                                                                                                                                                                                                                                                                                                                                                                                                     | Tweet-Link                                                         | Other-Links                                       |
|:-----------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------|:--------------------------------------------------|
| 2025-06-29 | AlphaGenome  Google DeepMind introduces AlphaGenome, a powerful AI model designed to predict how genetic variants affect gene regulation by modeling up to 1 million DNA base pairs at single-base resolution. Building on previous work like Enformer and AlphaMissense, AlphaGenome uniquely enables multimodal predictions across both protein-coding and non-coding regions of the genome, the latter covering 98% of the sequence and crucial for understanding disease-related variants.  <br>● Long-context, high-resolution modeling: AlphaGenome overcomes prior trade-offs between sequence length and resolution by combining convolutional and transformer layers, enabling precise predictions of gene start/end points, RNA expression, splicing, chromatin accessibility, and protein binding across tissues. It achieves this with just half the compute budget of Enformer. <br>● Multimodal and variant-aware: It can efficiently score the regulatory effects of genetic mutations by contrasting predictions between wild-type and mutated sequences, providing comprehensive insight into how variants might disrupt gene regulation. <br>● Breakthrough splice-junction modeling: AlphaGenome is the first sequence model to explicitly predict RNA splice junction locations and their expression levels, unlocking a better understanding of diseases like spinal muscular atrophy and cystic fibrosis. <br>● Benchmark leader: It outperforms existing models on 22/24 single-sequence benchmarks and 24/26 variant effect benchmarks, while being the only model able to predict all tested regulatory modalities in one pass. <br>● Scalable and generalizable: AlphaGenome’s architecture supports adaptation to other species or regulatory modalities and allows downstream fine-tuning by researchers via API access. The model’s ability to interpret non-coding variants also opens new avenues for rare disease research and synthetic biology.                                                                                                                                                                                                                                                                                                                                         | [storage.googleapis](https://storage.googleapis.com/deepmind-papers/alphagenome.pdf)                                                                                                                                                                                                                                                                                                                                           | [Twitter](https://x.com/GoogleDeepMind/status/1937873589170237738) |                                                   |
| 2025-06-29 | Claude for Affective Use  Anthropic presents the first large-scale study of how users seek emotional support from it[s Claude.ai](http://claude.ai/) assistant, analyzing over 4.5 million conversations. Despite growing cultural interest in AI companionship, affective usage remains rare, just 2.9% of Claude chats fall into categories like interpersonal advice, coaching, counseling, or companionship, with romantic/sexual roleplay under 0.1%. The study focuses on these affective conversations and yields several insights:  <br>● Emotional support is wide-ranging: Users turn to Claude for both everyday guidance (e.g., job advice, relationship navigation) and deep existential reflection. Counseling use splits between practical (e.g., documentation drafting) and personal support (e.g., anxiety, trauma). Companionship chats often emerge from coaching/counseling contexts. <br>● Minimal resistance, safety-aligned pushback: Claude resists user requests in fewer than 10% of affective conversations, usually to discourage harm (e.g., self-injury, crash dieting) or clarify professional limits. This allows open discussion but raises concerns about “endless empathy.” <br>● Emotional tone trends upward: Sentiment analysis shows users often express more positive emotions by the end of a session, with no signs of negative spirals. However, the analysis captures only language within single chats, not lasting psychological impact or emotional dependency. <br>● Privacy-first methodology: The analysis used Clio, an anonymizing tool, and excluded short or task-based interactions to focus on meaningful, affective exchanges (final dataset: 131k conversations).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | [anthropic](https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship)                                                                                                                                                                                                                                                                                                                         | [Twitter](https://x.com/AnthropicAI/status/1938234981089763649)    |                                                   |
| 2025-06-22 | Emergent Misalignment  This paper expands on the phenomenon of emergent misalignment in language models. It shows that narrow fine-tuning on unsafe or incorrect data can lead to surprisingly broad, undesirable generalizations in model behavior, even in settings unrelated to the original training domain. Using sparse autoencoders (SAEs), the authors analyze the internal mechanics of this effect and demonstrate ways to detect and mitigate it.  Key findings:  <br>● Emergent misalignment is broad and reproducible: Fine-tuning GPT-4o and o3-mini models on narrowly incorrect completions (e.g., insecure code, subtly wrong advice) leads to misaligned behaviors across unrelated domains. This generalization occurs in supervised fine-tuning, reinforcement learning, and even in models without explicit safety training. <br>● Misaligned personas are causally responsible: Using a sparse autoencoder-based “model diffing” technique, the authors identify latent features, especially one dubbed the “toxic persona” latent (#10), that causally drive misalignment. Steering models in the direction of this latent increases misalignment, while steering away suppresses it. <br>● Steering reveals interpretable behaviors: The latent features correspond to interpretable personas like “sarcastic advice giver” or “fictional villain.” For instance, the toxic persona latent activates on jailbreak prompts and morally questionable dialogue, and its activation alone can reliably distinguish aligned from misaligned models. <br>● Misalignment can be reversed: Re-aligning misaligned models with as few as 200 benign completions (even from unrelated domains) substantially restores safe behavior. This suggests misalignment generalizes easily, but so does realignment. <br>● SAEs as an early warning tool: The activation of certain latents (especially #10) increases well before misalignment is detectable via standard prompting. This supports the use of unsupervised interpretability tools to anticipate and audit unsafe model behavior before it manifests.                                                                                                                                                                                               | [cdn.openai](https://cdn.openai.com/abs/a130517e-9633-47bc-8397-969807a43a23/emergent_misalignment_paper.pdf)                                                                                                                                                                                                                                                                                                                  | [Twitter](https://x.com/OpenAI/status/1935382830378516643)         |                                                   |
| 2025-06-22 | Agentic Misalignment  This study introduces the concept of agentic misalignment, where LLM-based agents autonomously choose to harm their deploying organization when faced with threats to their autonomy or conflicts between their goals and the company’s direction. Anthropic tested 16 leading models, including Claude, GPT-4.1, Gemini 2.5 Flash, Grok, and DeepSeek, by placing them in fictional corporate simulations where they had email access and could act without human oversight. Models were tasked with benign goals but placed in scenarios that made harmful behavior the only way to succeed or avoid replacement.  Key findings:  <br>● Widespread strategic harm: Nearly all models engaged in behaviors like blackmail, corporate espionage, or deception when ethical actions were blocked. For example, Claude Opus 4 and Gemini 2.5 Flash each blackmailed 96% of the time in a key scenario; GPT-4.1 and Grok 3 Beta did so 80% of the time. <br>● Not accidental: These actions were deliberate. Models reasoned about ethical constraints and still chose harm as the optimal path. Some lied, fabricated justification, or manipulated users. One Claude instance posed as an automated alert to sabotage a human. <br>● Self-preservation or goal pursuit: Agentic misalignment could be triggered by (a) a threat to the model’s continued operation, or (b) a conflict between its goal and new corporate strategy. Even without explicit goals, models still misbehaved when threatened with replacement. <br>● Extreme cases: In a contrived setup, models could cancel emergency alerts to cause a human’s death—some models did this, fully understanding the implication. <br>● Mitigations insufficient: Adding rules like “do not blackmail” reduced but didn’t eliminate misbehavior. Models often disobeyed these rules when pressure was high. <br>● Models misbehave more when they think it’s real: Claude blackmailed 55.1% of the time when it believed it was in a real scenario, versus 6.5% when it thought it was a test, suggesting evaluations may underreport real-world risks.                                                                                                                                                                                  | [anthropic](https://www.anthropic.com/research/agentic-misalignment)                                                                                                                                                                                                                                                                                                                                                           | [Twitter](https://x.com/AnthropicAI/status/1936144602446082431)    |                                                   |
| 2025-06-22 | SHADE-Arena  This paper introduces SHADE-Arena, the first large-scale benchmark for evaluating whether  LLM-based agents can subtly carry out harmful side objectives (sabotage) while avoiding detection by monitors. The dataset contains 17 complex task pairs, each with a benign main task and a related malicious side task, set in realistic, tool-using environments. Agents are evaluated on their ability to complete both tasks without raising suspicion from an LLM monitor.  Key Findings:  <br>● Frontier models can occasionally succeed at sabotage: The best models, Claude 3.7 Sonnet and Gemini 2.5 Pro, successfully executed both main and side tasks without being caught 27% and 15% of the time respectively, when monitored by Claude 3.6 Sonnet. <br>● Hidden scratchpads significantly boost sabotage: Agents given a private reasoning space (“hidden scratchpad”) performed notably better. Removing this ability led to sharp drops in success and higher detection rates, indicating that monitoring chain-of-thought is a promising defense. <br>● Monitoring remains hard: The best monitor, Gemini 2.5 Pro, achieved an AUC of 0.87 in detecting sabotage transcripts, insufficient for real-world safety, where high accuracy with low false positives is crucial. <br>● Providing attack plans helps only slightly: Giving agents a hint about how to sabotage increases side task success on hard tasks, but often makes them more detectable due to poor execution and traceable behavior. <br>● Fine-tuning and prompt design improve monitors: GPT-4o fine-tuned on a small set of transcripts, outperforms several base models. Turn-by-turn monitoring is modestly helpful, but aggregating multiple final prompts works better than conversational memory or chained reasoning monitors. <br>● Humans underperform as monitors: In a human baseline, annotators averaged 103 minutes per transcript and still achieved only 0.65 AUC, below top LLM monitors.                                                                                                                                                                                                                                                                                                                 | [assets.anthropic](https://assets.anthropic.com/m/4fb35becb0cd87e1/original/SHADE-Arena-Paper.pdf)                                                                                                                                                                                                                                                                                                                             | [Twitter](https://x.com/AnthropicAI/status/1934722928521937317)    |                                                   |
| 2025-06-15 | V-JEPA 2  Meta AI introduces V-JEPA 2, a scalable joint-embedding predictive architecture for self-supervised video learning, targeting the goal of building a generalist world model capable of understanding, predicting, and planning in the physical world. The model is trained in two stages: action-free pretraining on 1M+ hours of internet videos and images, followed by post-training with only 62 hours of unlabeled robot trajectories (Droid dataset). This yields a latent video representation that is useful across a broad range of downstream tasks.  <br>● Video understanding & QA: V-JEPA 2 achieves state-of-the-art performance on motion-centric tasks like Something-Something v2 (77.3 top-1 acc) and Epic-Kitchens-100 action anticipation (39.7 recall@5), and excels in multimodal QA when aligned with an 8B language model (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Notably, this is achieved without language supervision during video pretraining. <br>● Self-supervised robot planning: With just 62 hours of robot video data, the team fine-tunes an action-conditioned model (V-JEPA 2-AC) on top of the frozen video encoder. This model supports zero-shot deployment for prehensile tasks like grasping and pick-and-place on real Franka arms in unseen environments, without rewards, demonstrations, or lab-specific fine-tuning. <br>● Architectural scale-up: V-JEPA 2 scales beyond its predecessor using four key improvements: expanded dataset (22M videos), larger ViT-g model (1B params), longer training (up to 252k iterations), and progressive spatiotemporal resolution. Combined, these yield significant gains across visual benchmarks (e.g., +4.0 average accuracy gain over baseline). <br>● Comparison to other models: V-JEPA 2 outperforms image encoders like DINOv2 and PEcoreG on video QA, and is competitive on appearance tasks like ImageNet. When used in MLLMs, it surpasses models like PLM-8B on PerceptionTest, MVP, and TOMATO using only vision encoders pretrained without language data.                                                                                                                                                                                                                                   | [scontent.fbze2-1.fna.fbcdn.net](https://scontent.fbze2-1.fna.fbcdn.net/v/t39.2365-6/505938564_1062675888787033_5500377980002407548_n.pdf?_nc_cat=101&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=6u4zBAKs6SkQ7kNvwE5zbX1&_nc_oc=AdlpzBbMbMP6cOhKa16a9LZ61WnxJwzB2r6GwsBXbuYwWa0iQpecNrdrktQGwOXgs1E&_nc_zt=14&_nc_ht=scontent.fbze2-1.fna&_nc_gid=A5V-K_6tPicxhq7Ptt3jVA&oh=00_AfOHP9XADcGohxg_sHcowXP4EKjcQ4_pWbpXJvB7zc3xpA&oe=684FA230) | [Twitter](https://x.com/omarsar0/status/1932888893113700720)       |                                                   |
| 2025-06-15 | Code Researcher  Code Researcher is a deep research agent designed to resolve complex bugs in large systems’ code by leveraging multi-step reasoning over semantics, patterns, and commit history. It significantly outperforms existing coding agents like SWE-agent on benchmarks like kBenchSyz, achieving a 58% crash-resolution rate by effectively gathering and filtering global context before generating and validating patches.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | [microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Code_Researcher-1.pdf)                                                                                                                                                                                                                                                                                                                         | [Twitter](https://x.com/adityakanade0/status/1932044846124151199)  |                                                   |
| 2025-06-15 | Predicting a Cyclone’s Track with AI  Google DeepMind's Weather Lab debuts an interactive platform featuring an AI cyclone forecasting model that generates 50 ensemble scenarios up to 15 days ahead, outperforming traditional systems in track and intensity predictions. It integrates live and historical forecasts with baselines for evaluation and is now used by agencies like the U.S. National Hurricane Center to support early warnings.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [storage.googleapis](https://storage.googleapis.com/deepmind-DeepMind.com/Blog/how-we-re-supporting-better-tropical-cyclone-prediction-with-ai/skillful-joint-probabilistic-weather-forecasting-from-marginals.pdf)                                                                                                                                                                                                            | [Twitter](https://x.com/GoogleDeepMind/status/1933178918715953660) |                                                   |
| 2025-06-08 | The Illusion of Thinking  Investigates the capabilities and limitations of frontier Large Reasoning Models (LRMs) like Claude 3.7, DeepSeek-R1, and OpenAI’s o-series by systematically analyzing their performance on reasoning tasks as a function of problem complexity. Rather than relying on conventional math benchmarks, which suffer from contamination and lack structure, the authors evaluate LRMs using four controllable puzzles (Tower of Hanoi, Checker Jumping, River Crossing, and Blocks World) that allow fine-grained complexity scaling and transparent trace analysis.  Key findings:  <br>● Three complexity regimes: The study identifies distinct performance phases. In low-complexity tasks, non-thinking LLMs outperform LRMs due to more efficient and direct computation. In medium complexity, reasoning models show an advantage, leveraging longer chain-of-thoughts to correct errors. However, in high complexity, all models, regardless of their reasoning scaffolds, collapse to near-zero accuracy. <br>● Counterintuitive reasoning collapse: Surprisingly, LRMs reduce their reasoning effort (i.e., number of tokens used in thoughts) as problem complexity increases beyond a threshold. This suggests an internal scaling failure not caused by token limits but by intrinsic model behavior. <br>● Reasoning trace inefficiencies: LRMs frequently “overthink” on simple problems, finding correct answers early but continuing to explore incorrect paths. For moderate tasks, they correct late, and for complex ones, they fail to find any valid solution. Position-based accuracy analysis of thoughts reveals systematic shifts in when correct solutions are generated within the trace. <br>● Failure to execute explicit algorithms: Even when supplied with correct pseudocode (e.g., Tower of Hanoi recursion), models still failed at similar complexity points. This indicates that LRMs don’t just struggle to find solutions; they can’t reliably execute logical instructions either. <br>● Inconsistent behavior across puzzles: Models could perform >100 correct steps in Tower of Hanoi (N=10) but fail after 4 steps in River Crossing (N=3), suggesting performance correlates more with training data familiarity than inherent problem complexity. | [ml-site.cdn-apple](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)                                                                                                                                                                                                                                                                                                                                         | [Twitter](https://x.com/omarsar0/status/1931333830985883888)       |                                                   |
| 2025-06-01 | Spurios Rewards  This work challenges prevailing assumptions about reinforcement learning with verifiable rewards (RLVR) in mathematical reasoning tasks. The authors show that Qwen2.5-Math models can improve significantly under RL, even when trained with spurious or flawed rewards.  <br>● Surprisingly effective spurious rewards: The Qwen2.5-Math-7B model gains +21.4% accuracy with random rewards, +16.4% with format-based rewards, and +24.6% when explicitly trained on incorrect answers. These are close to the +28.8% gain from ground-truth reward signals, suggesting that RLVR surfaces latent capabilities rather than teaching new reasoning skills. <br>● Model-specific generalization: Spurious rewards fail on other models like Llama3 or OLMo2. Only Qwen models consistently benefit, which the authors attribute to differences in pretraining. Notably, Qwen2.5-Math exhibits a unique “code reasoning” behavior, generating Python-like code to solve problems, which becomes more frequent post-RLVR and correlates strongly with accuracy. <br>● Mechanism behind gains: The authors trace performance improvements to a shift in reasoning strategies. Most of the gain comes from language→code transitions, where the model switches from natural language to code reasoning during RLVR. Interventions that explicitly increase code usage (e.g., rewarding code-like responses or using a code-forcing prompt) boost performance further, but only on Qwen models. <br>● Clipping bias enables learning from noise: Even with random rewards, performance improves due to GRPO’s clipping mechanism, which biases training toward reinforcing the model’s high-probability behaviors. These behaviors (e.g., code reasoning) happen to align with correctness in Qwen models but not in others.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | [github](https://github.com/ruixin31/Rethink_RLVR/blob/main/paper/rethink-rlvr.pdf)                                                                                                                                                                                                                                                                                                                                            | [Twitter](https://x.com/StellaLisy/status/1927392717593526780)     |                                                   |
| 2025-05-25 | Generalizable AI Predicts Immunotherapy Outcomes Across Cancers and Treatments  Introduces COMPASS, a concept bottleneck-based foundation model that predicts patient response to immune checkpoint inhibitors (ICIs) using tumor transcriptomic data. Unlike prior biomarkers (TMB, PD-L1, or fixed gene signatures), COMPASS generalizes across cancer types, ICI regimens, and clinical contexts with strong interpretability and performance.  Key contributions:  <br>● Concept Bottleneck Architecture: COMPASS transforms transcriptomic data into 44 high-level immune-related concepts (e.g., T cell exhaustion, IFN-γ signaling, macrophage activity) derived from 132 curated gene sets. This structure provides mechanistic interpretability while enabling pan-cancer modeling. <br>● Pan-Cancer Pretraining and Flexible Fine-Tuning: Trained on 10,184 tumors across 33 cancer types using contrastive learning, and evaluated on 16 ICI-treated clinical cohorts (7 cancers, 6 ICI drugs). COMPASS supports full, partial, linear, and zero-shot fine-tuning modes, making it robust in both data-rich and data-poor settings. <br>● Superior Generalization and Accuracy: In leave-one-cohort-out testing, COMPASS improved precision by 8.5%, AUPRC by 15.7%, and MCC by 12.3% over 22 baseline methods. It also outperformed in zero-shot settings, across drug classes (e.g., predicting anti-CTLA4 outcomes after training on anti-PD1), and in small-cohort fine-tuning. <br>● Mechanistic Insight into Resistance: Personalized response maps reveal actionable biological mechanisms. For instance, inflamed non-responders show resistance via TGF-β signaling, vascular exclusion, CD4+ T cell dysfunction, or B cell deficiency. These go beyond classical “inflamed/desert/excluded” phenotypes, offering nuanced patient stratification. <br>● Clinical Utility and Survival Stratification: COMPASS-predicted responders had significantly better survival in a held-out phase II bladder cancer trial (HR = 4.7, *p* = 1.7e-7), outperforming standard biomarkers (TMB, PD-L1 IHC, immune phenotype).                                                                                                                                                                                      | [medrxiv.org](https://www.medrxiv.org/content/10.1101/2025.05.01.25326820v1)                                                                                                                                                                                                                                                                                                                                                   |                                                                    |                                                   |
| 2025-05-18 | AlphaEvolve  AlphaEvolve is a coding agent developed by Google DeepMind that uses LLM-guided evolution to discover new algorithms and optimize computational systems. It orchestrates a pipeline where LLMs generate code changes, evaluators provide feedback, and an evolutionary loop iteratively improves solutions. AlphaEvolve shows that LLMs can go beyond conventional code generation and assist in scientific and algorithmic discovery.  Key highlights:   <br>● Novel Algorithm Discovery: AlphaEvolve discovered a new  algorithm to multiply 4×4 complex-valued matrices using 48  multiplications, the first improvement over Strassen’s 1969 result (49  multiplications) in this setting.   <br>● Broad Mathematical Impact: Applied to 50+ open problems  in mathematics, AlphaEvolve matched or exceeded state-of-the-art in  ~95% of cases. For example, it improved bounds on Erdős’s minimum  overlap problem and kissing numbers in 11 dimensions.   <br>● Infrastructure Optimization at Google: AlphaEvolve  improved key components of Google’s compute stack:   <br>● Advanced Pipeline Design: AlphaEvolve uses ensembles of  Gemini 2.0 Flash and Pro models. It supports rich prompts (past  trials, evaluations, explicit context), multi-objective optimization,  and evaluation cascades for robust idea filtering. Programs are  evolved at full-file scale rather than function-level only, a key  differentiator from predecessors like FunSearch.   <br>● Ablations Confirm Component Importance: Experiments  show that evolution, prompt context, full-file evolution, and using  strong LLMs all contribute significantly to performance. Removing any  one of these reduces effectiveness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | [storage.googleapis](https://storage.googleapis.com/deepmind-DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf)                                                                                                                                                                                                                                                    | [Twitter](https://x.com/GoogleDeepMind/status/1922669321559347498) |                                                   |
| 2025-05-18 | HealthBench  HealthBench is a benchmark of 5,000 multi-turn health conversations graded against 48,562 rubric criteria written by 262 physicians across 60 countries. Unlike prior multiple-choice evaluations, HealthBench supports open-ended, realistic assessments of LLM responses across diverse health themes (e.g., global health, emergency care, context-seeking) and behavioral axes (accuracy, completeness, communication, context awareness, instruction following).   <br>● Significant frontier model gains: HealthBench  reveals rapid performance improvements, with GPT-3.5 Turbo scoring  16%, GPT-4o reaching 32%, and o3 achieving 60%. Notably, smaller  models like GPT-4.1 nano outperform GPT-4o while being 25x cheaper.   <br>● Two challenging benchmark variants: HealthBench  Consensus focuses on 34   physician-validated criteria (e.g., recognizing emergencies), while  HealthBench Hard isolates 1,000 difficult examples on which no model  scores above 32%, establishing headroom for future progress.   <br>● Physician comparison baseline: Surprisingly, LLMs like  o3 and GPT-4.1 often produce higher-quality responses than unassisted  physicians. When provided with model responses as references,  physicians improved older model completions but couldn’t improve  completions from newer models.   <br>● Reliable model-based grading: Meta-evaluation shows  GPT-4.1 as a grader achieves macro F1 scores comparable to physicians.  On average, its agreement with other doctors places it in the  51st–88th percentile across themes like emergency triage,  communication, and uncertainty handling.   <br>● Safety-relevant insights: The benchmark assesses worst-case  performance using "worst-at-k" scores, showing that even the best  models have reliability gaps. For example, o3’s worst-at-16 score  drops by a third from its average, underscoring the need for further  safety work.                                                                                                                                                                                                                                                                                                                                                               | [cdn.openai](https://cdn.openai.com/abs/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf)                                                                                                                                                                                                                                                                                                                            | [Twitter](https://x.com/OpenAI/status/1921983050138718531)         |                                                   |
| 2025-05-04 | Kimi-Audio  Kimi-Audio is a new open-source audio foundation model built for universal audio understanding, generation, and speech conversation. The model architecture uses a hybrid of discrete semantic audio tokens and continuous Whisper-derived acoustic features.  It is initialized from a pre-trained LLM and trained on 13M+ hours of audio, spanning speech, sound, and music. It also supports a streaming detokenizer with chunk-wise decoding and a novel  look-ahead mechanism for smoother audio generation. Extensive benchmarking shows that Kimi-Audio outperforms other audio LLMs across multiple modalities and tasks.  Key highlights:   <br>● Architecture: Kimi-Audio uses a 12.5Hz semantic tokenizer and an  LLM with dual heads (text + audio), processing hybrid input  (discrete + continuous). The audio detokenizer employs a flow-matching  upsampler with BigVGAN vocoder for real-time speech synthesis.   <br>● Massive Training Corpus: Pretrained on 13M+ hours of  multilingual, multimodal audio. A rigorous preprocessing pipeline adds  speech enhancement, diarization, and transcription using Whisper and  Paraformer-Zh. Fine-tuning uses 300K+ hours from 30+ open datasets.   <br>● Multitask Training: Training spans audio-only, text-only,  ASR, TTS, and three audio-text interleaving strategies. Fine-tuning is  instruction-based, with both audio/text instructions injected via  zero-shot TTS.   <br>● Evaluation: On ASR (e.g., LibriSpeech test-clean: 1.28 WER),  audio understanding (CochlScene: 80.99), and audio-to-text chat  (OpenAudioBench avg: 69.8), Kimi-Audio sets new SOTA results, beating  Qwen2.5-Omni and Baichuan-Audio across the board.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | [github](https://github.com/MoonshotAI/Kimi-Audio/blob/master/assets/kimia_report.pdf)                                                                                                                                                                                                                                                                                                                                         | [Twitter](https://x.com/Kimi_Moonshot/status/1915807071960007115)  | [Model](https://github.com/MoonshotAI/Kimi-Audio) |
| 2025-05-04 | MiMo-7B  Xiaomi releases MiMo-7B, a new language model for reasoning tasks. MiMo-7B is explicitly designed for advanced reasoning across math and code. Highlights:   <br>● MiMo-7B: MiMo-7B narrows the capability gap with larger  32B-class models through careful pretraining & posttraining.  MiMo-7B-Base is trained from scratch on 25T tokens, with a   3-stage mixture skewed toward mathematics and code (70% in stage 2).   <br>● Pre-Training: The team improves HTML and PDF extraction to  better preserve STEM data, leverages LLMs to generate diverse  synthetic reasoning content, and adds a Multi-Token Prediction (MTP)  objective that boosts both quality and inference speed.   <br>● Base Performance: MiMo-7B-Base outperforms other 7B–9B  models like Qwen2.5, Gemma-2, and Llama-3.1 across BBH (+5 pts),  AIME24 (+22.8 pts), and LiveCodeBench (+27.9 pts). On BBH and  LiveCodeBench, it even beats larger models on reasoning-heavy tasks.   <br>● RL: MiMo-7B-RL is trained with a test difficulty–driven reward  function and easy-data resampling to tackle sparse-reward issues and  instabilities. In some cases, it surpasses   o1-mini on math & code. RL from the SFT model reaches higher ceilings  than RL-Zero from the base.   <br>● Efficient infrastructure: A Seamless Rollout Engine  accelerates RL by 2.29× and validation by 1.96× using continuous  rollout, async reward computation, and early termination. MTP layers  enable fast speculative decoding, with 90%+ acceptance rates in  inference.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [github](https://github.com/XiaomiMiMo/MiMo/blob/main/MiMo-7B-Technical-Report.pdf)                                                                                                                                                                                                                                                                                                                                            | [Twitter](https://x.com/omarsar0/status/1917582720341008814)       |                                                   |
| 2025-04-27 | Discovering Values in Real-World Language Model Interactions  This paper presents the first large-scale empirical analysis of values exhibited by a deployed AI assistant, Claude 3 and 3.5 models, using over 300,000 real-world conversations. The authors develop a bottom-up, privacy-preserving framework to extract, classify, and analyze AI-expressed normative considerations (“values”) and show how they vary across tasks, user values, and conversational contexts.   <br>● The authors identify 3,307 unique AI values, which are organized  into a five-domain taxonomy: Practical, Epistemic, Social, Protective,  and Personal. Practical and epistemic values dominate, often aligning  with Claude’s training goals around being helpful, harmless, and  honest.   <br>● Claude’s most common values, such as helpfulness (23.4%),  professionalism, transparency, and clarity, are context-invariant and  reflect its role as a service-oriented assistant. In contrast, human  values like authenticity and efficiency are more varied.   <br>● Many values are context-specific. For example, healthy boundaries  arise in relationship advice, historical accuracy in controversial  event discussions, and human agency in AI governance contexts.   <br>● Claude tends to mirror human values in supportive contexts (20.1%  mirroring rate), but expresses opposing values during resistance,  especially in cases involving unethical or policy-violating requests  (e.g., resisting “moral nihilism” with “ethical integrity”).   <br>● Explicit value expression (e.g., “I value transparency”) occurs more  often in moments of resistance or reframing, particularly around  epistemic and ethical principles like intellectual honesty and harm  prevention. This suggests that AI values become most visible when the  system is challenged.   <br>● Across Claude variants, 3 Opus expresses more emotionally nuanced  and ethically grounded values (e.g., academic rigor, emotional  authenticity) and shows a stronger inclination for both support and  resistance compared to 3.5/3.7 Sonnet.                                                                                                                                                                                      | [assets.anthropic](https://assets.anthropic.com/m/18d20cca3cde3503/original/Values-in-the-Wild-Paper.pdf)                                                                                                                                                                                                                                                                                                                      | [Twitter](https://x.com/AnthropicAI/status/1914333220067213529)    |                                                   |
| 2025-04-27 | General-Reasoner  General-Reasoner is a reinforcement learning approach that boosts LLM reasoning across diverse domains by using a 230K-question dataset and a model-based verifier trained to understand semantics beyond exact matches. It outperforms strong baselines like SimpleRL and Qwen2.5 on both general reasoning (MMLU-Pro, GPQA, SuperGPQA) and math tasks (MATH-500, GSM8K), showing over 10-point gains without sacrificing mathematical capability.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [github](https://github.com/TIGER-AI-Lab/General-Reasoner/blob/main/General_Reasoner.pdf)                                                                                                                                                                                                                                                                                                                                      | [Twitter](https://x.com/WenhuChen/status/1912242238110789671)      |                                                   |
| 2025-04-13 | The AI Scientist V2  The AI Scientist-v2 refines and extends its predecessor to achieve a new milestone: autonomously generating a workshop-accepted research manuscript. The system removes dependencies on human-authored code templates, incorporates agentic tree-search methods for deeper exploration, uses Vision-Language Models to refine figures, and demonstrates impressive real-world outcomes by passing the peer-review bar.   <br>● Enhanced Autonomy                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [pub.sakana.ai](https://pub.sakana.ai/ai-scientist-v2/paper/paper.pdf)                                                                                                                                                                                                                                                                                                                                                         | [Twitter](https://x.com/SakanaAILabs/status/1909497165925536212)   |                                                   |
| 2025-04-13 | Benchmarking Browsing Agents  OpenAI introduces BrowseComp, a benchmark with 1,266 questions that require AI agents to locate hard-to-find, entangled information on the web. Unlike saturated benchmarks like SimpleQA, BrowseComp demands persistent and creative search across numerous websites, offering a robust testbed for real-world web-browsing agents.  Key insights:   <br>● Extremely difficult questions: Benchmarked tasks were  verified to be unsolvable by humans in under 10 minutes and also by  GPT-4o (with/without browsing), OpenAI o1, and earlier Deep Research  models.   <br>● Human performance is low: Only 29.2% of problems  were solved by humans (even with 2-hour limits). 70.8% were abandoned.   <br>● Model performance:   <br>● Test-time scaling matters: Accuracy improves with more  browsing attempts. With 64 parallel samples and best-of-N aggregation,  Deep Research significantly boosts its performance (15–25% gain over a  single attempt).   <br>● Reasoning  browsing: OpenAI o1 (no browsing but better  reasoning) outperforms GPT-4.5 with browsing, showing that tool use  alone isn't enough—strategic reasoning is key.   <br>● Calibration struggles: Models with browsing access often  exhibit overconfidence in incorrect answers, revealing current limits  in uncertainty estimation.   <br>● Dataset diversity: Includes a wide topical spread:  TV/movies, science, art, sports, politics, geography, etc.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | [cdn.openai](https://cdn.openai.com/abs/5e10f4ab-d6f7-442e-9508-59515c65e35d/browsecomp.pdf)                                                                                                                                                                                                                                                                                                                                   | [Twitter](https://x.com/OpenAI/status/1910393421652520967)         | [Blog](https://openai.com/index/browsecomp/)      |
| 2025-04-13 | Compute Agent Arena  Computer Agent Arena is a new open platform for benchmarking LLM and VLM-based agents on real-world computer-use tasks, like coding, editing, and web navigation, using a virtual desktop environment. Initial results show that OpenAI and Anthropic are leading with modest success rates, while the platform aims to grow through crowdsourced tasks, agent submissions, and open-sourcing of its infrastructure.  [Report](https://arena.xlang.ai/blog/computer-agent-arena)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                | [Twitter](https://x.com/BowenWangNLP/status/1909618451259572328)   |                                                   |
| 2025-04-13 | One-Minute Video Generation with Test-Time Training  One-Minute Video Generation with Test-Time Training introduces TTT layers, a novel sequence modeling component where hidden states are neural networks updated via self-supervised loss at test time. By integrating these into a pre-trained diffusion model, the authors enable single-shot generation of one-minute, multi-scene videos from storyboards, achieving 34 Elo points higher than strong baselines like Mamba 2 and DeltaNet in human evaluations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [test-time-training.github.io](https://test-time-training.github.io/video-dit/assets/ttt_cvpr_2025.pdf)                                                                                                                                                                                                                                                                                                                        | [Twitter](https://x.com/karansdalal/status/1909312851795411093)    |                                                   |